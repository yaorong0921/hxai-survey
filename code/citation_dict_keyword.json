{"debiased-cam to mitigate image perturbations with faithful visual explanations of machine learning": [], "how can explainability methods be used to support bug identification in computer vision models?": [{"title": "The Role of Human Knowledge in Explainable AI", "author": "A Tocchetti", "abstract": "As the performance and complexity of machine learning models have grown significantly over the last years, there has been an increasing need to develop methodologies to describe their behaviour. Such a need has mainly arisen due to the widespread use of black-box models, ie, high-performing models whose internal logic is challenging to describe and understand. Therefore, the machine learning and AI field is facing a new challenge: making models more explainable through appropriate techniques. The final goal of an explainability", "keywords": {"keywords": ["Explainable AI", "Machine learning models", "Human knowledge", "Black-box models", "Model explainability"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer science", "content": "Methodological", "application area": "AI explainability"}}], "towards relatable explainable ai with the perceptual process": [{"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang", "abstract": "Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias). Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary", "keywords": {"keywords": ["machine learning", "visual explanations", "systematic error", "bias", "Debiased-CAM"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning explainability"]}}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang", "abstract": "Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi", "keywords": {"keywords": ["Debiased-CAM", "image perturbations", "visual explanations", "machine learning", "bias"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image recognition"]}}, {"title": "Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideation", "author": "Y Wang", "abstract": "Feedback in creativity support tools can help crowdworkers to improve their ideations. However, current feedback methods require human assessment from facilitators or peers. This is not scalable to large crowds. We propose Interpretable Directed Diversity to automatically predict ideation quality and diversity scores, and provide AI explanations\u2014Attribution, Contrastive Attribution, and Counterfactual Suggestions\u2014to feedback on why ideations were scored (low), and how to get higher scores. These explanations provide multi", "keywords": {"keywords": ["Interpretable Directed Diversity", "model explanations", "iterative crowd ideation", "ideation quality", "diversity scores"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Creativity Support Tools"]}}, {"title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "author": "H Kaur", "abstract": "Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact--an explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick's sensemaking theory, which focuses on who the", "keywords": {"keywords": ["AI interpretability", "sensemaking theory", "ML models", "explanation", "human understanding"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": ["computer science"], "content": ["methodological"], "application area": ["ML interpretability"]}}], "understanding the impact of explanations on advice-taking: a user study for ai-based clinical decision support systems": [], "user trust in recommendation systems: a comparison of content-based, collaborative and demographic filtering": [], "data-centric explanations: explaining training data of machine learning systems to promote transparency": [{"title": "Towards a science of human-ai decision making: a survey of empirical studie", "author": "V Lai", "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of", "keywords": {"keywords": ["human-ai decision making", "survey", "empirical studies", "adoption of AI", "augmenting human decision making"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["human-ai decision making"]}}, {"title": "Improved alzheimer's disease detection by MRI using multimodal machine learning algorithm", "author": "G Battineni", "abstract": "Adult-onset dementia disorders represent a challenge for modern medicine. Alzheimer's disease (AD) represents the most diffused form of adult-onset dementias. For half a century, the diagnosis of AD was based on clinical and exclusion criteria, with an accuracy of 85%, which did not allow for a definitive diagnosis, which could only be confirmed by post-mortem evaluation. Machine learning research applied to Magnetic Resonance Imaging (MRI) techniques can contribute to a faster diagnosis of AD and may contribute to predicting the", "keywords": {"keywords": ["Alzheimer's disease", "MRI", "multimodal machine learning algorithm", "dementia disorders", "diagnosis"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Biology", "content": "Application", "application area": "Medical imaging"}}, {"title": "BreastScreening-AI: Evaluating medical intelligent agents for human-AI interaction", "author": "FM Calisto", "abstract": "In this paper, we developed BreastScreening-AI within two scenarios for the classification of multimodal beast images:(1) Clinician-Only; and (2) Clinician-AI. The novelty relies on the introduction of a deep learning method into a real clinical workflow for medical imaging diagnosis. We attempt to address three high-level goals in the two above scenarios. Concretely, how clinicians: i) accept and interact with these systems, revealing whether are explanations and functionalities required; ii) are receptive to the introduction of AI-assisted", "keywords": {"keywords": ["BreastScreening-AI", "medical intelligent agents", "multimodal breast images", "deep learning", "AI-assisted"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Medical Imaging Diagnosis"]}}, {"title": "Mitigating Bias in Algorithmic Systems: A Fish-Eye View of Problems and Solutions Across Domain", "author": "K Orphanou", "abstract": "Mitigating bias in algorithmic systems is a critical issue drawing attention across communities within the information and computer sciences. Given the complexity of the problem and the involvement of multiple stakeholders\u2013including developers, end users and third-parties\u2013there is a need to understand the landscape of the sources of bias, and the solutions being proposed to address them, from a broad, cross-domain perspective. This survey provides a \u201cfish-eye view,\u201d examining approaches across four areas of research. The", "keywords": {"keywords": ["mitigating bias", "algorithmic systems", "fish-eye view", "sources of bias", "solutions"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["fairness in algorithms"]}}, {"title": "An Agile Software Development Life Cycle Model for Machine Learning Application Developmen", "author": "R Ranawana", "abstract": "and extended the existing agile methodology and added the role of the  application  development life cycle, in which the application developer only focus on the application development", "keywords": {"keywords": ["agile methodology", "software development", "machine learning application", "application developer", "development life cycle"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purpose": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["agile software development"]}}, {"title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models", "author": "A Balayn", "abstract": "Deep learning models for image classification suffer from dangerous issues often discovered after deployment. The process of identifying bugs that cause these issues remains limited and understudied. Especially, explainability methods are often presented as obvious tools for bug identification. Yet, the current practice lacks an understanding of what kind of explanations can best support the different steps of the bug identification process, and how practitioners could interact with those explanations. Through a formative study and", "keywords": {"keywords": ["bug identification", "explainability methods", "computer vision models", "image classification", "deep learning"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Computer Vision"]}}, {"title": "The Past, Present, and Prospective Future of XAI: A Comprehensive Revie", "author": "MU Islam", "abstract": "With the increasing growth and availability of data, Artificial Intelligence (AI) based black-box models have shown significant effectiveness to solve real-world and mission-critical problems in a wide range of domains such as finance, healthcare, security, natural language processing, transportation, military, and retail. However, the importance of interpretable and explainable decision-making processes within AI-based systems is becoming crucial to provide transparency and confidence among end-users from diverse backgrounds", "keywords": {"keywords": ["XAI", "interpretable decision-making", "explainable AI", "artificial intelligence", "transparency"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI interpretability"]}}, {"title": "Telling Stories from Computational Notebooks: AI-Assisted Presentation Slides Creation for Presenting Data Science Wor", "author": "C Zheng", "abstract": "Creating presentation slides is a critical but time-consuming task for data scientists. While researchers have proposed many AI techniques to lift data scientists' burden on data preparation and model selection, few have targeted the presentation creation task. Based on the needs identified from a formative study, this paper presents NB2Slides, an AI system that facilitates users to compose presentations of their data science work. NB2Slides uses deep learning methods as well as example-based prompts to generate slides from computational", "keywords": {"keywords": ["computational notebooks", "AI-assisted", "presentation slides", "data science work", "deep learning methods"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Prescriptive", "discipline": "Computer science", "content": "Application", "application area": "Data science"}}, {"title": "Facets of algorithmic literacy: Information, experience, and individual factors predict attitudes toward algorithmic system", "author": "DE Silva", "abstract": "Algorithmic decision-making systems are ubiquitous in digital media, but the public has been largely unable to negotiate the role of algorithms in society. Building from the concept of attitude-behavior consistency for political behavior, we develop a framework for fostering algorithmic literacy to develop well-informed attitudes toward algorithms. As algorithms are increasingly relevant to broad societal effects, an integrative approach is needed for a full account of how the public makes sense of algorithms and their role in society. We designed", "keywords": {"keywords": ["algorithmic literacy", "attitudes toward algorithms", "information", "experience", "individual factors"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["sociology"], "content": ["application"], "application area": ["digital media"]}}, {"title": "The Role of Explanations of AI Systems: Beyond Trust and Helping to Form Mental Model", "author": "M Norkute", "abstract": "This paper discusses research that explored different roles for explanations of AI systems. A lot of the research focuses on investigating the role of explanations in mediating the level of users' trust in the AI system and helping them form correct mental models about it. This paper argues that more research should be dedicated to investigate the alternative roles that explanations could play in supporting the user's interactions with AI systems such as helping them enrich the AI suggestions they are presented with or correct them, help users do tasks", "keywords": {"keywords": ["explanations", "AI systems", "trust", "mental models", "interactions"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["AI systems"]}}, {"title": "Muhammad Usama Islam, Md. Mozaharul Mottalib, Mehedi Hassa", "author": "ZI Alam"}, {"title": "Investigating Explanations that Target Training Data", "author": "AI Anik", "abstract": "To promote transparency in black-box machine learning systems, different explanation approaches have been developed and discussed in the literature. However, training dataset information is rarely communicated in these explanations despite the utmost importance of training data to a system trained with machine learning techniques. We investigated explanations that focus on communicating training dataset information to end-users in our work. In this position paper, we discuss our prototype explanations and highlight findings", "keywords": {"keywords": ["explanations", "training data", "transparency", "black-box machine learning", "prototype explanations"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["technical"], "application area": ["machine learning"]}}, {"title": "Adversarial Attacks Against DNNs Towards Real-World Threa", "author": "R Duan", "abstract": "Deep neural networks (DNNs) have reshaped the computer vision research in recent years. As fueled by powerful computational resources and massive amount of data, DNNs now dominate a wide range of visual benchmarks including image classification, face recognition, etc. Nonetheless, these successes come with bitterness: an increasing amount of studies has shown the limitations of DNNs on certain inputs, eg, adversarial examples. Studies have shown that by malicious crafting human imperceptible adversarial", "keywords": {"keywords": ["adversarial attacks", "DNNs", "computer vision", "adversarial examples", "image classification"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Deep Learning"]}}, {"title": "HAEM: Obtaining Higher-Quality Classification Task Results with AI Worker", "author": "Y Yamashita", "abstract": "Obtaining high-quality results for a fixed set of classification tasks with a limited budget is a critical issue in crowdsourcing. The introduction of AI models to complement the process should be explored. However, there are few existing approaches to directly address the problem, which have been proposed in the context of how to train AI models using noisy crowdsourced data. This paper presents a more direct approach for solving the problem of introducing AI to improve the task results of human workers for a fixed number of tasks with a", "keywords": {"keywords": ["crowdsourcing", "AI worker", "classification tasks", "high-quality results", "limited budget"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Crowdsourcing"]}}, {"title": "Mitigating Bias in Algorithmic Systems-A Fish-Eye Vie", "author": "K Orphanou", "abstract": "Mitigating bias in algorithmic systems is a critical issue drawing attention across communities within the information and computer sciences. Given the complexity of the problem and the involvement of multiple stakeholders\u2013including developers, end users and third-parties\u2013there is a need to understand the landscape of the sources of bias, and the solutions being proposed to address them, from a broad, cross-domain perspective. This survey provides a \u201cfish-eye view,\u201d examining approaches across four areas of research. The", "keywords": {"keywords": ["bias", "algorithmic systems", "mitigation", "cross-domain perspective", "fish-eye view"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Comparing Perception of Disclosure of Different Types of Information Related to Automated Tools", "author": "V Bracamonte", "abstract": "", "keywords": {"keywords": ["perception", "disclosure", "automated tools", "types of information", "comparing"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Technology"]}}, {"title": "Towards a multi-stakeholder value-based assessment framework for algorithmic system", "author": "M Yurrita", "abstract": "In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical", "keywords": {"keywords": ["algorithmic system", "multi-stakeholder", "value-based assessment", "machine learning", "ethics"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Prescriptive", "discipline": "Computer science", "content": "Application", "application area": "Algorithmic system assessment"}}, {"title": "Artificial intelligence pitfalls and how to avoid the", "author": "M Armstrong-Barnes", "abstract": "Artificial intelligence (AI) is a mainstream technology and has become a cornerstone in digital transformation initiatives. Founded on sound academic principles, why do so many projects fail? Getting AI into business pipelines is a relatively new discipline that faces several of the same challenges as traditional software development and many that are specific to the discipline. This paper aims to highlight the common pitfalls that hamper AI projects, from initiatives that are interesting science projects to abstract dreams. Putting the", "keywords": {"keywords": ["artificial intelligence", "pitfalls", "projects", "digital transformation", "business pipelines"], "research type": ["review"], "methodology": [], "purposes": [], "discipline": ["computer science"], "content": [], "application area": []}}], "does the whole exceed its parts? the effect of ai explanations on complementary team performance": [{"title": "To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making", "author": "Z Bu\u00e7inca", "abstract": "People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI", "keywords": {"keywords": ["AI-assisted decision-making", "overreliance", "cognitive forcing functions", "explanations", "dual-process theory"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["AI decision support"]}}, {"title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models", "author": "T Wu", "abstract": "While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which", "keywords": {"keywords": ["Polyjuice", "counterfactuals", "NLP models", "perturbations", "GPT-2"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Technical", "application area": "Natural Language Processing"}}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang", "abstract": "This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy\u2014improve people's understanding of the AI model, help people recognize the model uncertainty, and support people's calibrated trust in the model. Through randomized controlled experiments", "keywords": {"keywords": ["explainable AI", "AI-assisted decision-making", "comparative study", "exploratory", "empirical"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI explanations"]}}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "Is the most accurate ai the best teammate? optimizing ai for teamwork", "author": "G Bansal"}, {"title": "An update on drug-induced pigmentation", "author": "AF Nahhas"}, {"title": "Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks", "author": "Z Lu"}, {"title": "Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs", "author": "A Marasovi\u0107"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai", "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of", "keywords": {"keywords": ["human-ai decision making", "empirical studies", "predictive performance", "full automation", "AI assistance"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["decision making"]}}, {"title": "Facilitator training program: The universit\u00e9 laval interprofessional initiative", "author": "\u00c9 Milot"}, {"title": "Deciding fast and slow: The role of cognitive biases in ai-assisted decision-making", "author": "C Rastogi"}, {"title": "The flaws of policies requiring human oversight of government algorithms", "author": "B Green"}, {"title": "Human evaluation of spoken vs. visual explanations for open-domain qa", "author": "AV Gonzalez"}, {"title": "The impact of algorithmic risk assessments on human predictions and its analysis via crowdsourcing studies", "author": "R Fogliato"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen"}, {"title": "The utility of explainable ai in ad hoc human-machine teaming", "author": "R Paleja"}, {"title": "Machine learning applications for therapeutic tasks with genomics data", "author": "K Huang"}, {"title": "Interpretable machine learning: Moving from mythos to diagnostics", "author": "V Chen"}, {"title": "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts", "author": "T Wu", "abstract": "Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system", "keywords": {"keywords": ["AI chains", "human-AI interaction", "transparent", "controllable", "large language models"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["AI-assisted tasks"]}}, {"title": "Interpreting deep learning models in natural language processing: A review", "author": "X Sun"}, {"title": "Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning", "author": "KZ Gajos"}, {"title": "Human-AI symbiosis: A survey of current approaches", "author": "Z Zahedi"}, {"title": "How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions", "author": "HF Cheng"}, {"title": "Explainable recommendation: when design meets trust calibration", "author": "M Naiseh"}, {"title": "Do explanations help users detect errors in open-domain QA? an evaluation of spoken vs. visual explanations", "author": "AV Gonz\u00e1lez"}, {"title": "The need to move away from agential-AI: Empirical investigations, useful concepts and open issues", "author": "F Cabitza"}, {"title": "Investigations of Performance and Bias in Human-AI Teamwork in Hiring", "author": "A Peng"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainable", "author": "A Simkute"}, {"title": "How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI.", "author": "ZT Zhang", "abstract": "Given the opaqueness and complexity of modern AI algorithms, there is currently a strong focus on developing transparent and explainable AI, especially in high-stakes domains. We claim that opaqueness and complexity are not the core issues for end users when interacting with AI. Instead, we propose that the output uncertainty inherent to AI systems is the actual problem, with opaqueness and complexity as contributing factors. Transparency and explainability should therefore not be the end goals, as such a focus tends to place the", "keywords": {"keywords": ["output uncertainty", "interactions", "AI systems", "transparency", "explainability"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["AI systems"]}}, {"title": "Double Trouble: How to not explain a text classifier's decisions using counterfactuals synthesized by masked language models?", "author": "TM Pham"}, {"title": "Improving Human-AI Partnerships in Child Welfare: Understanding Worker Practices, Challenges, and Desires for Algorithmic Decision Support", "author": "A Kawakami"}, {"title": "On the Influence of Explainable AI on Automation Bias", "author": "M Schemmer"}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "Bayesian modeling of human\u2013AI complementarity", "author": "M Steyvers"}, {"title": "Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Beyond Fairness Metrics: Roadblocks and Challenges for Ethical AI in Practice", "author": "J Chen"}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization", "author": "M Fan"}, {"title": "Augmenting scientific creativity with an analogical search engine", "author": "HB Kang"}, {"title": "AI in design idea development: A workshop on creativity and human-AI collaboration", "author": "FA Figoli", "abstract": "In a fast-paced society, AI systems can prove to be reliable teammates alongside human agents during the early stages of the design process, capable of helping to manage the increasing complexity of projects. Therefore, the introduction of AI systems into the design process is analysed according to the implications on the designer's creativity and the kind of human-AI collaboration that is established, highlighting trust balance and the new role played by the designer. The main aspects covered by the study were tested in a workshop", "keywords": {"keywords": ["AI", "design", "creativity", "human-AI collaboration", "workshop"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Design"]}}, {"title": "Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment", "author": "Z Li"}, {"title": "ExSum: From Local Explanations to Model Understanding", "author": "Y Zhou"}, {"title": "How transparency modulates trust in artificial intelligence", "author": "J Zerilli"}, {"title": "Machine learning for fraud detection in e-Commerce: A research agenda", "author": "N Tax"}, {"title": "Natural Language Deduction through Search over Statement Compositions", "author": "K Bostrom"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky"}, {"title": "On the Relationship Between Explanations, Fairness Perceptions, and Decisions", "author": "J Schoeffer"}, {"title": "Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness", "author": "K Donahue"}, {"title": "Human-AI Collaboration Enables More Empathic Conversations in Text-based Peer-to-Peer Mental Health Support", "author": "A Sharma"}, {"title": "Pilot attitudes toward AI in the cockpit: implications for design", "author": "ZT Zhang", "abstract": "As the aviation industry is actively working on adopting AI for air traffic, stakeholders agree on the need for a human-centered approach. However, automation design is often driven by user-centered intentions, while the development is actually technology-centered. This can be attributed to a discrepancy between the system designers' perspective and complexities in real-world use. The same can be currently observed with AI applications where most design efforts focus on the interface between humans and AI, while the overall system", "keywords": {"keywords": ["pilot attitudes", "AI in the cockpit", "human-centered approach", "automation design", "technology-centered development"], "research type": ["Theoretical"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "BreastScreening-AI: Evaluating medical intelligent agents for human-AI interactions", "author": "FM Calisto"}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation", "author": "V Lai"}, {"title": "Mediators: Conversational Agents Explaining NLP Model Behavior", "author": "N Feldhus"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging", "author": "R Fogliato"}, {"title": "Improving mathematical questioning in teacher training", "author": "D Datta"}, {"title": "Under-reliance or misalignment? How proxy outcomes limit measurement of appropriate reliance in AI-assisted decision-making", "author": "L GUERDAN"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations", "author": "A Balagopalan"}, {"title": "Communicating uncertain information from deep learning models to users", "author": "HV Subramanian", "abstract": "The use of Artificial Intelligence (AI) decision support systems is increasing in high-stakes contexts, such as healthcare, defense, and finance. Uncertainty information may help users better leverage AI predictions, especially when combined with domain knowledge. I conducted two human-subject experiments to examine the effects of uncertainty information with AI recommendations. The experimental stimuli are from an existing image recognition deep learning model, one popular approach to AI. In Paper I, I evaluated the effect of the", "keywords": {"keywords": ["Artificial Intelligence", "uncertainty information", "deep learning models", "AI recommendations", "human-subject experiments"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image recognition"]}}, {"title": "Homophily and Incentive Effects in Use of Algorithms", "author": "R Fogliato"}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "HINT: Integration Testing for AI-based features with Humans in the Loop", "author": "QZ Chen"}, {"title": "Do explanations increase the effectiveness of AI-crowd generated fake news warnings?", "author": "Z Epstein"}, {"title": "On the Effect of Information Asymmetry in Human-AI Teams", "author": "P Hemmer"}, {"title": "A Mental-Model Centric Landscape of Human-AI Symbiosis", "author": "Z Zahedi"}, {"title": "Data Work and Data Tracking Technologies in Fertility Care: A Holistic Approach", "author": "MC Figueiredo"}, {"title": "Improving the Utility and Usability of Visualization in AI-driven Scientific Discovery", "author": "Q Wang"}, {"title": "The Conflict Between Explainable and Accountable Decision-Making Algorithms", "author": "G Lima"}, {"title": "Robustness and Usefulness in AI Explanation Methods", "author": "E Galinkin", "abstract": "Explainability in machine learning has become incredibly important as machine learning-powered systems become ubiquitous and both regulation and public sentiment begin to demand an understanding of how these systems make decisions. As a result, a number of explanation methods have begun to receive widespread adoption. This work summarizes, compares, and contrasts three popular explanation methods: LIME, SmoothGrad, and SHAP. We evaluate these methods with respect to: robustness, in the sense of sample", "keywords": {"keywords": ["explainability", "machine learning", "explanation methods", "LIME", "SmoothGrad"], "research type": ["review"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning explanation"]}}, {"title": "Principles and Interactive Tools for Evaluating and Improving the Behavior of Natural Language Processing models", "author": "T Wu"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "AI in Your Mind: Counterbalancing Perceived Agency and Experience in Human-AI Interaction", "author": "AHC Hwang"}, {"title": "Building Trust by Supporting Situation Awareness: Exploring Pilots' Design Requirements for Decision Support Tools", "author": "C STORATH"}, {"title": "The Role of Explanations of AI Systems: Beyond Trust and Helping to Form Mental Models", "author": "M Norkute"}, {"title": "Factors that influence the adoption of human-AI collaboration in clinical decision-making", "author": "P Hemmer"}, {"title": "Step 3: What Should Be Happening?", "author": "B Bakhshandeh"}, {"title": "Doubting AI Predictions: Influence-Driven Second Opinion Recommendation", "author": "M De-Arteaga"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "A Study of Explainability Features to Scrutinize Faceted Filtering Results", "author": "J Qu", "abstract": "Faceted search systems enable users to filter results by selecting values along different dimensions or facets. Traditionally, facets have corresponded to properties of information items that are part of the document metadata. Recently, faceted search systems have begun to use machine learning to automatically associate documents with facet-values that are more subjective and abstract. Examples include search systems that support topic-based filtering of research articles, concept-based filtering of medical documents, and tag-based", "keywords": {"keywords": ["faceted search", "explanation", "filtering results", "machine learning", "subjective facets"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Information Retrieval"]}}, {"title": "Towards a Responsible AI Development Lifecycle: Lessons From Information Security", "author": "E Galinkin"}, {"title": "Artificial Intelligence in the design process: the impact on creativity and team collaboration", "author": "F Mattioli"}, {"title": "Improving Learning-to-Defer Algorithms Through Fine-Tuning", "author": "N Raman"}, {"title": "Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions", "author": "A Parrish"}, {"title": "Journal of Responsible Technology", "author": "A Simkute"}, {"title": "Training Novices: The Role of Human-AI Collaboration and Knowledge Transfer", "author": "P Spitzer"}, {"title": "Walking on Eggshells: Using Analogies to Promote Appropriate Reliance in Human-AI Decision Making", "author": "G HE"}, {"title": "An Empirical Investigation of Reliance on AI-Assistance in a Noisy-Image Classification Task", "author": "HT Lemus"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "\" If it didn't happen, why would I change my decision?\": How Judges Respond to Counterfactual Explanations for the Public Safety Assessment", "author": "Y Yacoby", "abstract": "Researchers and policymakers are interested in algorithmic explanations as a mechanism for enabling more fair and responsible decision-making. In this study, we shed light on how judges interpret and respond to algorithmic explanations in the context of pretrial risk assessment instruments (PRAI). We found that, at first, all judges misinterpreted the counterfactuals in the explanations as real--rather than hypothetical--changes to defendants' criminal history profiles. Once judges understood the counterfactuals, they ignored them", "keywords": {"keywords": ["algorithmic explanations", "judges", "interpretation", "response", "pretrial risk assessment instruments"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Law"], "content": ["Methodological"], "application area": ["Judicial system"]}}, {"title": "Perceive, Understand & Predict-Empirical Indication for Facets in Subjective Information Processing Awareness", "author": "TPP Schrills"}, {"title": "Assessing Human-AI Interaction Early through Factorial Surveys: A Study on the Guidelines for Human-AI Interaction", "author": "T Li"}, {"title": "Glancee: An Adaptable System for Instructors to Grasp Student Learning Status in Synchronous Online Classes", "author": "S Ma"}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang"}, {"title": "ProtoTEx: Explaining Model Decisions with Prototype Tensors", "author": "A Das"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideation", "author": "Y Wang"}, {"title": "A Unifying Framework for Combining Complementary Strengths of Humans and ML toward Better Predictive Decision-Making", "author": "C Rastogi"}, {"title": "The Response Shift Paradigm to Quantify Human Trust in AI Recommendations", "author": "A Shafti"}, {"title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "author": "H Kaur", "abstract": "Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact--an explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick's sensemaking theory, which focuses on who the", "keywords": {"keywords": ["sensible AI", "interpretability", "explainability", "sensemaking theory", "ML-based systems"], "research type": "theoretical", "methodology": "qualitative", "purposes": "explanatory", "discipline": "computer science", "content": "methodological", "application area": "AI interpretability"}}, {"title": "XFlag: Explainable Fake News Detection Model on Social Media", "author": "SY Chien"}, {"title": "Machine Learning for Humans: Building Models that Adapt to Behavior", "author": "AS Hilgard"}, {"title": "Designing for Responsible Trust in AI Systems: A Communication Perspective", "author": "QV Liao"}, {"title": "Workshop on Trust and Reliance in AI-Human Teams (TRAIT)", "author": "G Bansal"}, {"title": "Designing Resilient AI-based Robo-Advisors: A Prototype for Real Estate Appraisal", "author": "M Schemmer"}, {"title": "Common Pitfalls When Explaining AI and Why Mechanistic Explanation Is a Hard Problem", "author": "DC Elton"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand"}, {"title": "Exploring How Anomalous Model Input and Output Alerts Affect Decision-Making in Healthcare", "author": "M Radensky"}, {"title": "\u201cWhy Do I Care What's Similar?\u201d Probing Challenges in AI-Assisted Child Welfare Decision-Making through Worker-AI Interface Design Concepts", "author": "A Kawakami"}, {"title": "Trust between Humans and AI: A Short Review", "author": "A Kumar", "abstract": "The ubiquity of AI systems in our daily lives is undeniable: we talk to AI assistants, we let algorithms drive our cars, we seek their recommendations on what to buy, and so on. While we have made significant progress across varied domains in building fairly accurate and efficient AI systems, in most cases there still exists a need for human supervision and/or intervention. This need for collaboration between humans and AI is due to many reasons. On one hand is the complementary nature of their abilities. While AI can look through vast", "keywords": {"keywords": ["Trust", "Humans", "AI", "Collaboration", "Supervision"], "research type": ["Review"], "methodology": [], "purposes": [], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-AI Interaction"]}}, {"title": "Artificial intelligence in the design process: The Impact on Creativity and Team Collaboration", "author": "FA Figoli"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Machine Learning for Fraud Detection in E-Commerce: A Research Agenda", "author": "B van den Akker"}, {"title": "DeXAR: Deep Explainable Sensor-Based Activity Recognition in Smart-Home Environments", "author": "L Arrotta"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}], "effects of semantic segmentation visualization on trust, situation awareness, and cognitive load in highly automated vehicles": [{"title": "Orias: On-the-fly object identification and action selection for highly automated vehicles", "author": "M Colley", "abstract": "Automated vehicles are about to enter the mass market. However, such systems regularly meet limitations of varying criticality. Even basic tasks such as Object Identification can be challenging, for example, under bad weather or lighting conditions or for (partially) occluded objects. One common approach is to shift control to manual driving in such circumstances, however, post-automation effects can occur in these control transitions. Therefore, we present ORIAS, a system capable of asking the driver to (1) identify/label unrecognized", "keywords": [["automation", 0.5297], ["driving", 0.4347], ["automated", 0.4133], ["vehicles", 0.4097], ["driver", 0.4076], ["manual", 0.3027], ["label", 0.2756], ["unrecognized", 0.2744], ["control", 0.2521], ["identification", 0.2509]]}, {"title": "How should automated vehicles communicate critical situations? a comparative analysis of visualization concepts", "author": "M Colley", "abstract": "Passengers of automated vehicles will likely engage in non-driving related activities like reading and, therefore, be disengaged from the driving task. However, especially in critical situations such as unexpected pedestrian crossings, it can be assumed that passengers request information about the vehicle's intention and an explanation. Some concepts were proposed for such communication from the automated vehicle to the passenger. However, results are not comparable due to varying information content and scenarios. We present a", "keywords": {"keywords": ["automated vehicles", "communication", "visualization concepts", "passenger engagement", "critical situations"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["transportation"]}}, {"title": "Effects of pedestrian behavior, time pressure, and repeated exposure on crossing decisions in front of automated vehicles equipped with external communication", "author": "M Colley", "abstract": "Automated vehicles are expected to substitute driver-pedestrian communication via LED strips or displays. This communication is expected to improve trust and the crossing process in general. However, numerous factors such as other pedestrians' behavior, perceived time pressure, or previous experience influence crossing decisions. Therefore, we report the results of a triply subdivided Virtual Reality study (N= 18) evaluating these. Results show that external communication was perceived as hedonically pleasing, increased perceived", "keywords": {"keywords": ["pedestrian behavior", "time pressure", "repeated exposure", "crossing decisions", "automated vehicles"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Transportation", "content": "Application", "application area": "Vehicle-pedestrian interaction"}}, {"title": "Intelligent mobility in the city: the influence of system and context factors on drivers' takeover willingness and trust in automated vehicles", "author": "M Lanzer", "abstract": "Automated driving in urban environments not only has the potential to improve traffic flow and heighten driver comfort but also to increase traffic safety, particularly for vulnerable road users such as pedestrians. For these benefits to take effect, drivers need to trust and use automated vehicles. This decision is influenced by both system and context factors. However, it is not yet clear how these factors interact with each other, especially for automated driving in city scenarios with crossing pedestrians. Therefore, we conducted an", "keywords": {"keywords": ["intelligent mobility", "automated vehicles", "traffic safety", "drivers", "trust"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Urban transportation"]}}, {"title": "Germane Elements for the Evaluation of Transitional Interfaces", "author": "J Friedl", "abstract": "As Mixed Reality (MR) and Transitional Interfaces (TIs) become more common, evaluation needs to step up to the task. In this position paper we elaborate on elements from current research in MR and cognitive psychology that appear to be relevant for evaluating TIs. The four factors considered in this position paper are presence, physical discomfort, spatial orientation and cognitive load. For each of these aspects we discuss the meaning, their relevance for TIs and how they can be evaluated.", "keywords": {"keywords": ["Mixed Reality", "Transitional Interfaces", "Evaluation", "Presence", "Cognitive psychology"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Human-computer interaction"]}}, {"title": "Descriptive and Prescriptive Visual Guidance to Improve Shared Situational Awareness in Human-Robot Teaming", "author": "A Tabrez", "abstract": "In collaborative tasks involving human and robotic teammates, live communication between agents has potential to substantially improve task efficiency and fluency. Effective communication provides essential situational awareness to adapt successfully during uncertain situations and encourage informed decision-making. In contrast, poor communication can lead to incongruous mental models resulting in mistrust and failures. In this work, we first introduce characterizations of and generative algorithms for two", "keywords": {"keywords": ["shared situational awareness", "human-robot teaming", "visual guidance", "communication", "decision-making"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["collaborative tasks"]}}, {"title": "Effects of Augmented-Reality-Based Assisting Interfaces on Drivers' Object-wise Situational Awareness in Highly Autonomous Vehicles", "author": "X Gao", "abstract": "Although partially autonomous driving (AD) systems are already available in production vehicles, drivers are still required to maintain a sufficient level of situational awareness (SA) during driving. Previous studies have shown that providing information about the AD's capability using user interfaces can improve the driver's SA. However, displaying too much information increases the driver's workload and can distract or overwhelm the driver. Therefore, to design an efficient user interface (UI), it is necessary to understand its effect", "keywords": {"keywords": ["augmented reality", "assisting interfaces", "object-wise situational awareness", "highly autonomous vehicles", "user interface"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Application", "application area": "Advanced Driver-Assistance Systems (ADAS)"}}, {"title": "Effects of Scene Detection, Scene Prediction, and Maneuver Planning Visualizations on Trust, Situation Awareness, and Cognitive Load in Highly Automated Vehicles", "author": "M Colley", "abstract": "The successful introduction of automated vehicles (AVs) depends on the user's acceptance. To gain acceptance, the intended user must trust the technology, which itself relies on an appropriate understanding. Visualizing internal processes could aid in this. For example, the functional hierarchy of autonomous vehicles distinguishes between perception, prediction, and maneuver planning. In each of these stages, visualizations including possible uncertainties (or errors) are possible. Therefore, we report the results of an online study (N", "keywords": {"keywords": ["Automated vehicles", "Trust", "Situation awareness", "Cognitive load", "Visualizations"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Descriptive", "discipline": "Computer science", "content": "Methodological", "application area": "Human factors in automated driving"}}, {"title": "A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review", "author": "P JANSEN", "abstract": "With the increasing integration of automation technology into vehicle systems, the scope of in-vehicle interaction is getting broader. According to the Society of Automotive Engineers (SAE) taxonomy J3016 [324], there are six levels of driving automation, ranging from level 0 (no driving automation) to level 5 (full driving automation) in the context of motor vehicles and their operation on roadways. With automated vehicles (AVs)(SAE levels 3-5) changing the role of the driver, automotive user interfaces (UIs) undergo a paradigm shift [80] and the", "keywords": {"keywords": ["human sensor", "actuator", "in-vehicle interaction", "automation technology", "automotive user interfaces"], "research type": ["review"], "methodology": [], "purposes": ["explanatory"], "discipline": ["technology"], "content": ["application"], "application area": ["automotive"]}}, {"title": "Toward an Adaptive Situational Awareness Support System for Urban Driving", "author": "T Wu", "abstract": "A lack of sufficient situational awareness is a primary cause of traffic crashes due to human error. Redirecting a driver's attention to critical objects is essential, but alerting driver about all critical objects can lead to distraction. This paper develops and evaluates an adaptive support system that incorporates drivers' fixations as a proxy for their situational awareness. We implement an experimental system that detects a driver's gaze on important objects in the traffic scene and adapts a cueing strategy in an augmented reality-based driver", "keywords": {"keywords": ["situation awareness", "urban driving", "adaptive support system", "fixations", "gaze detection"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer science", "content": "Application", "application area": "Traffic safety"}}, {"title": "Workshop on Prosocial Behavior in Future Mixed Traffic", "author": "H Sahin", "abstract": "\u201cProsocial Behavior ''means cooperating and acting in a way to benefit others. Since more and more diverse road users (such as electronic bicycles, scooters, etc.) but also vehicles at different levels of automation are sharing the safety-critical road environment, acting prosocial will become increasingly important in the future for both human and automated traffic participants. A few papers so far have already begun to address this issue, but currently, there exist no systematic methodological approaches to research this area. In", "keywords": {"keywords": ["prosocial behavior", "mixed traffic", "future", "road users", "methodological approaches"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Transportation"], "content": ["Application"], "application area": ["Road safety"]}}, {"title": "Challenges of Explainability, Cooperation, and External Communication of Automated Vehicles", "author": "M Colley", "abstract": "In this position paper, we describe current research questions in the area of interaction with automated vehicles from the viewpoint of users within the vehicle (ie, passengers) and from the viewpoint of parties outside the vehicle (eg, other manual drivers or pedestrians). First, we briefly introduce the topics of", "keywords": {"keywords": ["interaction", "automated vehicles", "explainability", "cooperation", "external communication"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["transportation"]}}], "expanding explainability: towards social transparency in ai systems": [{"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan", "abstract": "Explainability of AI systems is critical for users to take informed actions and hold systems accountable. While\" opening the opaque box\" is important, understanding who opens the box can govern if the Human-AI interaction is effective. In this paper, we conduct a mixed-methods study of how two different groups of whos--people with and without a background in AI--perceive different types of AI explanations. These groups were chosen to look at how disparities in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively", "keywords": {"keywords": ["explainable AI", "perceptions", "AI explanations", "AI backgrounds", "creator-consumer gap"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-AI interaction"]}}, {"title": "Human-Centered AI", "author": "B Shneiderman"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "The messiness of the menstruator: assessing personas and functionalities of menstrual tracking apps", "author": "A Pichon"}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang"}, {"title": "Design decision framework for ai explanations", "author": "O Anuyah"}, {"title": "Data-driven analytics leveraging artificial intelligence in the era of COVID-19: an insightful review of recent developments", "author": "A Majeed"}, {"title": "Machine learning in governments: Benefits, challenges and future directions", "author": "Y Pi"}, {"title": "Investigating Explainability of Generative AI for Code through Scenario-based Design", "author": "J Sun"}, {"title": "Explainable artificial intelligence in education", "author": "H Khosravi"}, {"title": "From treatment to healing: Envisioning a decolonial digital mental health", "author": "SR Pendse", "abstract": "The field of digital mental health is making strides in the application of technology to broaden access to care. We critically examine how these technology-mediated forms of care might amplify historical injustices, and erase minoritized experiences and expressions of mental distress and illness. We draw on decolonial thought and critiques of identity-based algorithmic bias to analyze the underlying power relations impacting digital mental health technologies today, and envision new pathways towards a decolonial digital mental health", "keywords": {"keywords": ["decolonial", "digital mental health", "historical injustices", "algorithmic bias", "minoritized experiences"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["sociology"], "content": ["methodological"], "application area": ["mental health"]}}, {"title": "Tinkering: A Way Towards Designing Transparent Algorithmic User Interfaces.", "author": "D Showkat"}, {"title": "Where do stories come from? examining the exploration process in investigative data journalism", "author": "D Showkat"}, {"title": "Supporting Responsible Data and Algorithmic Practices in The News Media", "author": "D Showkat"}, {"title": "Amortized Generation of Sequential Counterfactual Explanations for Black-box Models", "author": "S Verma"}, {"title": "Sensemaking and the Chemtrail Conspiracy on the Internet: Insights from Believers and Ex-believers", "author": "S Xiao"}, {"title": "LEx: A Framework for Operationalising Layers of Machine Learning Explanations", "author": "R Singh"}, {"title": "A user-centred framework for explainable artificial intelligence in human-robot interaction", "author": "M Matarese"}, {"title": "Seven challenges for harmonizing explainability requirements", "author": "J Chen"}, {"title": "Building Trustworthy AI. Main questions, possible solutions and a case study as example", "author": "S Korhonen"}, {"title": "Better together? an evaluation of ai-supported code translation", "author": "JD Weisz", "abstract": "Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced", "keywords": {"keywords": ["generative machine learning", "code translation", "imperfect outputs", "controlled study", "software engineers"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["AI-assisted code translation"]}}, {"title": "Bayesian Inverse Reinforcement Learning", "author": "DRE Amir"}, {"title": "Understanding the Design Space of AI-Mediated Social Interaction in Online Learning: Challenges and Opportunities", "author": "Q Wang"}, {"title": "Microalgal drugs: A promising therapeutic reserve for the future", "author": "A Ahirwar"}, {"title": "Human-Centric Research for NLP: Towards a Definition and Guiding Questions", "author": "B Kotnis"}, {"title": "Learning Explainable Templated Graphical Models", "author": "VR Embar"}, {"title": "Prediction for Retrospection: Integrating Algorithmic Stress Prediction into Personal Informatics Systems for College Students' Mental Health", "author": "T Kim"}, {"title": "Toward a sociology of machine learning explainability: Human\u2013machine interaction in deep neural network-based automated trading", "author": "C Borch"}, {"title": "Practical Algorithms for Resource Allocation and Decision Making", "author": "DC McElfresh"}, {"title": "Towards Transparency in Dermatology Image Datasets with Skin Tone Annotations by Experts, Crowds, and an Algorithm", "author": "M Groh"}, {"title": "Analysis and Generation of Verbal Humor in Portuguese Paper type: Doctoral Consortium", "author": "ML In\u00e1cio", "abstract": "Computational Creativity is the field of research focused on formalizing, discussing, and developing computational systems capable of producing artifacts which would be considered as creative by humans. For Natural Language Processing, working with creativity is specially challenging and intriguing, since it raises questions about figurative language, pragmatics, ambiguity, and other topics which are still open for fruitful research. From the many tasks involving language within the Computational Creativity area, this", "keywords": {"keywords": ["verbal humor", "Portuguese", "computational creativity", "natural language processing", "figurative language"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["computational linguistics"]}}, {"title": "What AI Practitioners Say about Human-AI Trust: Its Role, Importance, and Factors That Affect It", "author": "O Vereschak"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Governing Artificial Intelligence in Post-Pandemic Society", "author": "A Arunagiri"}, {"title": "Time to get conversational: assessment of the potential of conversational user interfaces for mobile banking", "author": "V Fahn"}, {"title": "User Redirection and Alignment for Virtual Reality Experiences in Arbitrary Physical Spaces", "author": "JL Thomas"}, {"title": "Augmented Reality-Based Explainable AI Strategies for Establishing Appropriate Reliance and Trust in Human-Robot Teaming", "author": "MB Luebbers"}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk"}, {"title": "Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions", "author": "H Park"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Self-Explaining Social Robots: An Explainable Behavior Generation Architecture for Human-Robot Interaction", "author": "S Stange", "abstract": "In recent years, the ability of intelligent systems to be understood by developers and users has received growing attention. This holds in particular for social robots, which are supposed to act autonomously in the vicinity of human users and are known to raise peculiar, often unrealistic attributions and expectations. However, explainable models that, on the one hand, allow a robot to generate lively and autonomous behavior and, on the other, enable it to provide human-compatible explanations for this behavior are missing. In", "keywords": {"keywords": ["self-explaining", "social robots", "explainable behavior generation", "human-robot interaction"], "research type": [" theoretical"], "methodology": [], "purposes": [], "discipline": ["computer science"], "content": [], "application area": ["human-robot interaction"]}}, {"title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems", "author": "C Panigutti"}, {"title": "CAeSaR: An Online Payment Anti-Fraud Integration System With Decision Explainability", "author": "C Wang"}, {"title": "Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI", "author": "M Pushkarna"}, {"title": "Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty", "author": "J Jiang"}, {"title": "Audit and Assurance of AI Algorithms: A framework to ensure ethical algorithmic practices in Artificial Intelligence", "author": "R Akula"}, {"title": "From explanations to shared understandings of AI", "author": "I Nicenboim"}, {"title": "Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task", "author": "B Leichtmann"}, {"title": "Algorithmic nudge to make better choices: Evaluating effectiveness of XAI frameworks to reveal biases in algorithmic decision making to users", "author": "P Juneja"}, {"title": "Sensemaking and the Chemtrail Conspiracy on the Internet", "author": "S XIAO"}, {"title": "Co-Designing AI Agents to Support Social Connectedness Among Online Learners: Functionalities, Social Characteristics, and Ethical Challenges", "author": "Q Wang", "abstract": "Due to the lack of face-to-face interactions, online learners frequently experience social isolation that negatively impacts students' well-being and learning experiences. Many text-based AI agents have been equipped with different social characteristics and functionalities to support people who are socially isolated. However, the design of agent's functionalities, social characteristics, and ethical challenges in promoting social connectedness among online learners are underexplored. Taking a co-design approach, we included 23 online", "keywords": {"keywords": ["AI agents", "social connectedness", "online learners", "functionalities", "ethical challenges"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Education"], "content": ["Application"], "application area": ["Online education"]}}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang"}, {"title": "Model reports, a supervision tool for Machine Learning engineers and users", "author": "DUC Tribe"}, {"title": "The Algorithmic Imprint", "author": "U Ehsan"}, {"title": "When a Robot Tells You That It Can Lie", "author": "K Rogers"}, {"title": "Model reports, a supervision tool for Machine Learning engineers and users", "author": "A Saboni"}, {"title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India", "author": "D Ramesh"}, {"title": "Transparent but incomprehensible: Investigating the relation between transparency, explanations, and usability in automated decision-making", "author": "J Dexe"}, {"title": "Machine Intelligence, Bureaucracy, and Human Control", "author": "JB Bullock"}, {"title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "author": "H Kaur"}, {"title": "Understanding and Being Understood: User Strategies for Identifying and Recovering From Mistranslations in Machine Translation-Mediated Chat", "author": "S Robertson", "abstract": "Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in", "keywords": {"keywords": ["machine translation", "cross-lingual communication", "mistranslations", "language skills", "user strategies"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Language and Communication"]}}, {"title": "Situated Case Studies for a Human-Centered Design of Explanation User Interfaces", "author": "C M\u00fcller-Birn"}, {"title": "A study of UX practitioners roles in designing real-world, enterprise ML systems", "author": "S Zdanowska"}, {"title": "Improving Trustworthiness of AI Solutions: A Qualitative Approach to Support Ethically-Grounded AI Design", "author": "A Vianello"}, {"title": "Promoting Human Competences by Appropriate Modes of Interaction for Human-Centered-AI", "author": "T Herrmann"}, {"title": "'We can't find fault with a friend': The Mediation of Accountability on Instant Loan Platforms in India", "author": "D Ramesh"}, {"title": "Designing for Responsible Trust in AI Systems: A Communication Perspective", "author": "QV Liao"}, {"title": "The Roles and Modes of Human Interactions with Automated Machine Learning Systems", "author": "TT Khuat"}, {"title": "Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretability", "author": "JJ Benjamin"}, {"title": "Workshop on Trust and Reliance in AI-Human Teams (TRAIT)", "author": "G Bansal"}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications", "author": "M Nourani", "abstract": "While EXplainable Artificial Intelligence (XAI) approaches aim to improve human-AI collaborative decision-making by improving model transparency and mental model formations, experiential factors associated with human users can cause challenges in ways system designers do not anticipate. In this paper, we first showcase a user study on how anchoring bias can potentially affect mental model formations when users initially interact with an intelligent system and the role of explanations in addressing this bias. Using a video", "keywords": {"keywords": ["Explainable Artificial Intelligence", "User backgrounds", "Impressions", "Anchoring bias", "Mental model formations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Human-AI collaboration"]}}, {"title": "New smartphone app to navigate blind people to stand in lines with distances", "author": "H Takagi"}, {"title": "Creative Uses of AI Systems and their Explanations: A Case Study from Insurance", "author": "M Benk"}, {"title": "Data Cards: Purposeful and Transparent Documentation for Responsible AI", "author": "M Pushkarna"}, {"title": "Robustness Testing of Data and Knowledge Driven Anomaly Detection in Cyber-Physical Systems", "author": "X Zhou"}, {"title": "Amortized Generation of Sequential Algorithmic Recourses for Black-Box Models", "author": "S Verma"}, {"title": "Tell Me Something That Will Help Me Trust You: A Survey of Trust Calibration in Human-Agent Interaction", "author": "GJ Cancro"}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study", "author": "L Chazette"}, {"title": "AI Opacity and Explainability in Tort Litigation", "author": "H Fraser"}, {"title": "Human-Centered Explainable AI (HCXAI): beyond opening the black-box of AI", "author": "U Ehsan"}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare Systeme", "author": "M Balci", "abstract": "Zusammenfassung Nachdem die Forschung zur Softwareanforderung Erkl\u00e4rbarkeit sich haupts\u00e4chlich auf das Fachgebiet der k\u00fcnstlichen Intelligenz beschr\u00e4nkt hat, wird dieser aktuell aus der endnutzerzentrierten Perspektive betrachtet. Das Requirements Engineering (RE) sowie die Mensch-Maschine Kommunikation (MMK) befassen sich mit der Erfassung der Bed\u00fcrfnisse, Ziele und des Kontextes um Erkl\u00e4rbarkeitsanforderungen zu definieren und Evaluationsm\u00f6glichkeiten bereitzustellen. Trotz dessen existiert noch keine Darstellung der", "keywords": {"keywords": ["Requirements Engineering", "Erkl\u00e4rbarkeit", "Softwareanforderung", "Mensch-Maschine Kommunikation", "Workflow"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Other"]}}], "explain yourself! transparency for positive ux in autonomous driving": [{"title": "Towards CRISP-ML (Q): a machine learning process model with quality assurance methodolog", "author": "S Studer", "abstract": "Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications", "keywords": {"keywords": ["machine learning", "process model", "quality assurance", "success", "efficiency"], "research type": ["theoretical"], "methodology": [], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Explanations in autonomous driving: A surve", "author": "D Omeiza", "abstract": "The automotive industry has witnessed an increasing level of development in the past decades; from manufacturing manually operated vehicles to manufacturing vehicles with a high level of automation. With the recent developments in Artificial Intelligence (AI), automotive companies now employ blackbox AI models to enable vehicles to perceive their environment and make driving decisions with little or no input from a human. With the hope to deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV by society", "keywords": {"keywords": ["autonomous driving", "explanations", "AI models", "perception", "autonomous vehicles"], "research type": ["review"], "methodology": [], "purposes": [], "discipline": ["computer science"], "content": [], "application area": []}}, {"title": "Trustworthy autonomous vehicle", "author": "D Fern\u00e1ndez Llorca", "abstract": "If future harmonised efforts for vehicle type-approval for AVs and regulatory frameworks for  autonomous mobility at EU level are designed to be trustworthy, we humbly hope that they", "keywords": {"keywords": ["trustworthy autonomous vehicle", "harmonised efforts", "vehicle type-approval", "regulatory frameworks", "autonomous mobility"], "research type": "Theoretical", "methodology": "Qualitative", "purposes": "Prescriptive", "discipline": "Sociology", "content": "Application", "application area": "Transportation"}}, {"title": "From Spoken Thoughts to Automated Driving Commentary: Predicting and Explaining Intelligent Vehicles' Action", "author": "D Omeiza", "abstract": "Commentary driving is a technique in which drivers verbalise their observations, assessments and intentions. By speaking out their thoughts, both learning and expert drivers are able to create a better understanding and awareness of their surroundings. In the intelligent vehicle context, automated driving commentary can provide intelligible explanations about driving actions, and thereby assist a driver or an end-user during driving operations in challenging and safety-critical scenarios. In this paper, we conducted a field", "keywords": {"keywords": ["commentary driving", "intelligent vehicles", "automated driving", "action prediction", "explanation"], "research type": ["Empirical"], "methodology": ["Quantitive"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Automotive technology"]}}, {"title": "Effects of Scene Detection, Scene Prediction, and Maneuver Planning Visualizations on Trust, Situation Awareness, and Cognitive Load in Highly Automated Vehicle", "author": "M Colley", "abstract": "The successful introduction of automated vehicles (AVs) depends on the user's acceptance. To gain acceptance, the intended user must trust the technology, which itself relies on an appropriate understanding. Visualizing internal processes could aid in this. For example, the functional hierarchy of autonomous vehicles distinguishes between perception, prediction, and maneuver planning. In each of these stages, visualizations including possible uncertainties (or errors) are possible. Therefore, we report the results of an online study (N", "keywords": {"keywords": ["automated vehicles", "trust", "situation awareness", "cognitive load", "visualizations"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Human-Computer Interaction"], "content": ["Application"], "application area": ["Automotive"]}}, {"title": "Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solution", "author": "H Park", "abstract": "Enterprises have recently adopted AI to human resource management (HRM) to evaluate employees' work performance evaluation. However, in such an HRM context where multiple stakeholders are complexly intertwined with different incentives, it is problematic to design AI reflecting one stakeholder group's needs (eg, enterprises, HR managers). Our research aims to investigate what tensions surrounding AI in HRM exist among stakeholders and explore design solutions to balance the tensions. By conducting stakeholder-centered", "keywords": {"keywords": ["AI", "human resource management", "algorithmic evaluation", "stakeholders", "design solutions"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Human resource management"]}}, {"title": "Towards Scenario-Based and Question-Driven Explanations in Autonomous Vehicle", "author": "Y Zhang", "abstract": "Benefit from the progress in the field of explainable artificial intelligence (XAI), explanations have been increasingly prospective in the autonomous vehicle (AV) context. Providing explanations has been proved to be vital for human-AV interaction, but what and how to explain are still to be addressed. This study seeks to bridge the areas of XAI and human-AV interaction by combining perspectives of both users and researchers. In this paper, a conceptual framework of explanation models was proposed to indicate what aspects to", "keywords": {"keywords": ["explanation", "autonomous vehicle", "explainable artificial intelligence", "human-AV interaction", "conceptual framework"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["autonomous vehicles"]}}, {"title": "Digital Literacy Through Design: Self-explanatory Devices by Use of For", "author": "J Hyl\u00e9n", "abstract": "Everyday technology is easier than ever to use and harder than ever to misuse, for this, we can attribute credits to how user friendly and errorproof minimal and autonomous artefacts are today. However, this puts a significant distance between users and the inner workings of their artefacts, the artefacts have become almost completely opaque black boxes. Users do not know how their artefacts function, and if something breaks, they want to modify them, or they have a need to create something of their own, they do not know how or where to start", "keywords": {"keywords": ["Digital Literacy", "Design", "Self-explanatory Devices", "Technology", "User Friendliness"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Education"]}}, {"title": "Challenges of Explainability, Cooperation, and External Communication of Automated Vehicle", "author": "M Colley", "abstract": "In this position paper, we describe current research questions in the area of interaction with automated vehicles from the viewpoint of users within the vehicle (ie, passengers) and from the viewpoint of parties outside the vehicle (eg, other manual drivers or pedestrians). First, we briefly introduce the topics of", "keywords": {"keywords": ["automated vehicle", "explainability", "cooperation", "external communication", "interaction"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory", "descriptive", "explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": "transportation"}}, {"title": "Shed Light on the Path of Human-Machine Interaction in Autonomous Vehicles: Where Did We Come from, Where We are Going", "author": "S Mosaferchi", "abstract": "As vehicles have become more complicated, Human-Machine Interaction (HMI) is becoming crucial. Nowadays, HMI is a substantial element in autonomous vehicles (AVs). Numerous studies are lately being published regarding significant parameters of AVs which can affect HMI. Hence, it is imperative to have a comprehensive prospect, to improve people's acceptance of AVs. In AVs, the driver's role will change to a passenger, however, it can be switched in some special situations. This work has mapped from 62 papers which were", "keywords": {"keywords": ["Human-Machine Interaction", "Autonomous Vehicles", "Driver-Passenger Role", "Acceptance of AVs", "AV Parameters"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Autonomous Vehicles"]}}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Stud", "author": "L Chazette", "abstract": "Quality aspects such as ethics, fairness, and transparency have been proven to be essential for trustworthy software systems. Explainability has been identified not only as a means to achieve all these three aspects in systems, but also as a way to foster users' sentiments of trust. Despite this, research has only marginally focused on the activities and practices to develop explainable systems. To close this gap, we recommend six core activities and associated practices for the development of explainable systems based on the results of a", "keywords": {"keywords": ["explainable systems", "ethical software", "fairness", "transparency", "trust"], "research type": ["literature review", "interview study"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["software development"]}}], "exploring and promoting diagnostic transparency and explainability in online symptom checkers": [{"title": "Towards a science of human-ai decision making: a survey of empirical studie", "author": "V Lai", "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of", "keywords": [["ai", 0.5552], ["predictive", 0.4488], ["automation", 0.4187], ["systems", 0.2512], ["human", 0.2263], ["technologies", 0.2167], ["assistance", 0.2117], ["criminal", 0.2111], ["inaccurate", 0.2053], ["augment", 0.194]]}, {"title": "Towards Relatable Explainable AI with the Perceptual Proces", "author": "W Zhang", "abstract": "Machine learning models need to provide contrastive explanations, since people often seek to understand why a puzzling prediction occurred instead of some expected outcome. Current contrastive explanations are rudimentary comparisons between examples or raw features, which remain difficult to interpret, since they lack semantic meaning. We argue that explanations must be more relatable to other concepts, hypotheticals, and associations. Inspired by the perceptual process from cognitive psychology, we propose the XAI", "keywords": {"keywords": ["explainable AI", "contrastive explanations", "interpretability", "perceptual process", "relatability"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Proof-of-vax: Studying user preferences and perception of Covid vaccination certificate", "author": "M Kowalewski", "abstract": "Digital tools play an important role in fighting the current global COVID-19 pandemic. We conducted a representative online study in Germany on a sample of 599 participants to evaluate the user perception of vaccination certificates. We investigated five different variants of vaccination certificates, based on deployed and planned designs in a between-group design, including paper-based and app-based variants. Our main results show that the willingness to use and adopt vaccination certificates is generally high. Overall, paper", "keywords": {"keywords": ["vaccination certificates", "user perception", "digital tools", "COVID-19", "Germany"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Sociology", "content": "Application", "application area": "Public Health"}}, {"title": "Three directions for the design of human-centered machine translatio", "author": "S Robertson", "abstract": "\u25cb How can we design systems that help people make use of imperfect translation?  \u25cb  How can machine translation models provide greater control to users over different aspects  of a translation, such as tone? What aspects of a translation would users want control over?", "keywords": {"keywords": ["human-centered", "machine translation", "imperfect translation", "greater control", "translation aspects"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["application"], "application area": ["language translation"]}}, {"title": "Latent classes associated with the intention to use a symptom checker for self-triag", "author": "S Aboueid", "abstract": "It is currently unknown which attitude-based profiles are associated with symptom checker use for self-triage. We sought to identify, among university students, attitude-based latent classes (population profiles) and the association between latent classes with the future use of symptom checkers for self-triage. Informed by the Technology Acceptance Model and a larger mixed methods study, a cross-sectional survey was developed and administered to students (aged between 18 and 34 years of age) at a University in Ontario. Latent class", "keywords": {"keywords": ["latent classes", "attitude-based profiles", "symptom checker", "self-triage", "technology acceptance model"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Health sciences", "content": "Application", "application area": "Healthcare"}}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderatio", "author": "V Lai", "abstract": "Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, eg, moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for", "keywords": {"keywords": ["Human-AI Collaboration", "Conditional Delegation", "Content Moderation", "Benchmark Datasets", "Out-of-Distribution Examples"], "research type": "Case Study", "methodology": "Mixed-Methods", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Application", "application area": "Social Media Moderation"}}, {"title": "Designing Chatbots with Black Americans with Chronic Conditions: Overcoming Challenges against COVID-1", "author": "J Kim", "abstract": "Recently, chatbots have been deployed in health care in various ways such as providing educational information, and monitoring and triaging symptoms. However, they can be ineffective when they are designed without a careful consideration of the cultural context of the users, especially for marginalized groups. Chatbots designed without cultural understanding may result in loss of trust and disengagement of the user. In this paper, through an interview study, we attempt to understand how chatbots can be better designed", "keywords": {"keywords": ["Chatbot design", "Black Americans", "Chronic conditions", "Cultural context", "Trust and engagement"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Healthcare"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Enabling Automated, Conversational Health Coaching with Human-Centered Artificial Intelligenc", "author": "EG Mitchell", "abstract": "Health coaching is a promising approach to support self-management of chronic conditions like type 2 diabetes; however, there aren't enough coaching practitioners to support those in need. Advances in Artificial Intelligence (AI) and Machine Learning (ML) have the potential to enable innovative, automated health coaching interventions, but important gaps remain in applying AI and ML to coaching interventions. This thesis aims to identify computational approaches and interactive technologies that enable automated health coaching systems", "keywords": {"keywords": ["automated health coaching", "conversational", "human-centered", "artificial intelligence", "machine learning"], "research type": ["theoretical"], "methodology": ["mixed methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["healthcare"]}}, {"title": "Exploring the Effects of Interactive Dialogue in Improving User Control for Explainable Online Symptom Checker", "author": "Y Sun", "abstract": "There has been a major push to improve the transparency of online symptom checkers (OSCs) by providing more explanations to users about their functioning and conclusions. However, not all users will want explanations about all aspects of these systems. A more user-centered approach is necessary for personalizing user experience of explanations. With this in mind, we designed and tested an interactive dialogue interface to afford user control to receive only those explanations that they would like to read. A user study (N= 152)", "keywords": {"keywords": ["interactive dialogue", "user control", "online symptom checker", "explainable", "transparency"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Healthcare"]}}, {"title": "Online Health Consultation Interfaces: Exploring Shared Decision Making with a Cognitive and Affective Trust Approac", "author": "L Zhang", "abstract": "The pandemic has caused significant changes in all aspects of healthcare, creating new opportunities to explore what patient-centred care is and how it can be realised in the changing environments. This study intends to explore the practice of shared decision-making (SDM), a key concept in in-person medical encounters, on online health consultation (OHC) platforms. More specifically, we aimed at investigating how cognitive and affective trust can play a role in the design process of OHC interfaces. Firstly, we proposed a", "keywords": {"keywords": ["shared decision-making", "online health consultation", "cognitive trust", "affective trust", "OHC interfaces"], "research type": ["empirical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["healthcare"]}}, {"title": "Logic and Pragmatics in AI Explanatio", "author": "CH Tsai", "abstract": "This paper reviews logical approaches and challenges raised for explaining AI. We discuss the issues of presenting explanations as accurate computational models that users cannot understand or use. Then, we introduce pragmatic approaches that consider explanation a sort of speech act that commits to felicity conditions, including intelligibility, trustworthiness, and usefulness to the users. We argue Explainable AI (XAI) is more than a matter of accurate and complete computational explanation, that it requires pragmatics to address the issues it", "keywords": {"keywords": ["Logic", "Pragmatics", "AI Explanation", "Explainable AI (XAI)", "Felicity Conditions"], "research type": ["Review"], "methodology": ["N/A"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Explanation"]}}, {"title": "Health Care Professionals' Experiences of Web-Based Symptom Checkers for Triage: Cross-sectional Survey Stud", "author": "S Kujala", "abstract": "Background Web-based symptom checkers are promising tools that provide help to patients seeking guidance on health problems. Many health organizations have started using them to enhance triage. Patients use the symptom checker to report their symptoms online and submit the report to the health care center through the system. Health care professionals (registered nurse, practical nurse, general physician, physiotherapist, etc) receive patient inquiries with urgency rating, decide on actions to be taken, and communicate these to the", "keywords": {"keywords": ["web-based symptom checkers", "triage", "health care professionals", "patient inquiries", "urgent care"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Healthcare"], "content": ["Application"], "application area": ["Healthcare triage"]}}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertis", "author": "Q Zhang", "abstract": "People consider recommendations from AI systems in diverse domains ranging from recognizing tumors in medical images to deciding which shoes look cute with an outfit. Implicit in the decision process is the perceived expertise of the AI system. In this paper, we investigate how people trust and rely on an AI assistant that performs with different levels of expertise relative to the person, ranging from completely overlapping expertise to perfectly complementary expertise. Through a series of controlled online lab studies where", "keywords": {"keywords": ["Human-AI Teams", "Complementary Expertise", "Trust", "Expertise", "AI Assistant"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-AI Collaboration"]}}, {"title": "Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideatio", "author": "Y Wang", "abstract": "Feedback in creativity support tools can help crowdworkers to improve their ideations. However, current feedback methods require human assessment from facilitators or peers. This is not scalable to large crowds. We propose Interpretable Directed Diversity to automatically predict ideation quality and diversity scores, and provide AI explanations\u2014Attribution, Contrastive Attribution, and Counterfactual Suggestions\u2014to feedback on why ideations were scored (low), and how to get higher scores. These explanations provide multi", "keywords": {"keywords": ["Interpretable Directed Diversity", "iterative crowd ideation", "model explanation", "ideation quality", "ideation diversity"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Creativity Support Tools"]}}, {"title": "\u201cSometimes I feel that I'm being left behind\u201d: Exploring Computing Device Use by People with Upper Extremity Impairment During the COVID-19 Pandemi", "author": "BF Lewis", "abstract": "In this paper, we explore how computing device use by people with upper extremity impairment (UEI) was affected by COVID-19. Someone with UEI has reduced use of their shoulders, upper arms, forearms, hands, and/or fingers. We conducted six (6) semi-structured interviews with participants with UEI in the US. We found that people with UEI increased computing device use during COVID-19 not only for remote interactions but also in person. Additionally, social distancing for COVID-19 safety created the need for new", "keywords": {"keywords": ["upper extremity impairment", "computing device use", "COVID-19", "remote interactions", "social distancing"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["People with disabilities"]}}, {"title": "Examining AI Methods for Micro-Coaching Dialog", "author": "E Mitchell", "abstract": "Conversational interaction, for example through chatbots, is well-suited to enable automated health coaching tools to support self-management and prevention of chronic diseases. However, chatbots in health are predominantly scripted or rule-based, which can result in a stagnant and repetitive user experience in contrast with more dynamic, data-driven chatbots in other domains. Consequently, little is known about the tradeoffs of pursuing data-driven approaches for health chatbots. We examined multiple artificial intelligence (AI) approaches", "keywords": {"keywords": ["micro-coaching dialog", "AI methods", "chatbots", "automated health coaching", "chronic diseases"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Healthcare"]}}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Stud", "author": "L Chazette", "abstract": "Quality aspects such as ethics, fairness, and transparency have been proven to be essential for trustworthy software systems. Explainability has been identified not only as a means to achieve all these three aspects in systems, but also as a way to foster users' sentiments of trust. Despite this, research has only marginally focused on the activities and practices to develop explainable systems. To close this gap, we recommend six core activities and associated practices for the development of explainable systems based on the results of a", "keywords": {"keywords": ["explainable systems", "ethics", "fairness", "transparency", "trust"], "research type": "Literature review", "methodology": "Qualitative", "purposes": "Prescriptive", "discipline": "Computer science", "content": "Methodological", "application area": "Trustworthy software systems"}}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare System", "author": "M Balci", "abstract": "Zusammenfassung Nachdem die Forschung zur Softwareanforderung Erkl\u00e4rbarkeit sich haupts\u00e4chlich auf das Fachgebiet der k\u00fcnstlichen Intelligenz beschr\u00e4nkt hat, wird dieser aktuell aus der endnutzerzentrierten Perspektive betrachtet. Das Requirements Engineering (RE) sowie die Mensch-Maschine Kommunikation (MMK) befassen sich mit der Erfassung der Bed\u00fcrfnisse, Ziele und des Kontextes um Erkl\u00e4rbarkeitsanforderungen zu definieren und Evaluationsm\u00f6glichkeiten bereitzustellen. Trotz dessen existiert noch keine Darstellung der", "keywords": {"keywords": ["Requirements Engineering", "Workflows", "Erkl\u00e4rbarkeit", "Explizite Anforderungsdefinition", "Evaluation"], "research type": "Theoretical", "methodology": "Qualitative", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Software Engineering"}}], "how should ai systems talk to users when collecting their personal information? effects of role framing and self-referencing on human-ai interaction": [{"title": "Social cues and implications for designing expert and competent artificial agents: A systematic review", "author": "TW Liew", "abstract": "Artificial agents such as embodied virtual agents, chatbots, voice user interface agents, and robots simulate human roles for dispensing information to people. According to the computers-are-social-actors paradigm, people respond to these technological artifacts with the same social rules originated from human-to-human social routines despite recognizing the artificiality of the entities' intents, motivations, or emotions. Among the various applications of social rules in human-agent interactions, this study focuses on the social", "keywords": {"keywords": ["social cues", "designing expert artificial agents", "competent artificial agents", "systematic review", "human-agent interactions"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Antecedents of human-robot collaboration: theoretical extension of the technology acceptance model", "author": "MO Parvez", "abstract": "Purpose This study aims to extend the technology acceptance model (TAM) to examine whether the introduction of robots influences employees' behavioral intentions to use robots and awareness of robots to promote human\u2013robot collaboration (HRC). Besides, the role of strategic human resource management (HRM) involvement as a moderator in the perception of robots as a team member was investigated. Design/methodology/approach Data were collected from 500 respondents via the Amazon Mechanical Turk platform. After data", "keywords": {"keywords": ["human-robot collaboration", "technology acceptance model", "strategic human resource management", "behavioral intentions", "awareness"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Workplace collaboration"]}}, {"title": "AI in Your Mind: Counterbalancing Perceived Agency and Experience in Human-AI Interaction", "author": "AHC Hwang", "abstract": "In this mixed-methods study, we attempt to capture users' conception of AI through the two-dimensional mind perception framework (perceived agency vs. experience) in cognitive psychology [13] and a series of drawing tasks. Our data illustrate how participants perceive AI entities with physical embodiment, depicting AI through devices, imaginary human figures, or full techno-ecosystems. Furthermore, we apply users' mind maps of AI entities to highlight risks in human-AI interaction (HAII) and propose design solutions accordingly. We", "keywords": {"keywords": ["AI", "mind perception", "human-AI interaction", "perceived agency", "experience"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Prescriptive"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-AI Interaction"]}}, {"title": "What should AI know? Information disclosure in human-AI collaboration", "author": "I Cvetkovic", "abstract": "AI-assisted Design Thinking shows great potential for supporting collaborative creative work. To foster creative thinking processes within teams with individualized suggestions, AI has to rely on data provided by the teams. As a prerequisite, team members need to weigh their disclosure preferences against the potential benefits of AI when disclosing information. To shed light on these decisions, we identify relevant information such as emotional states or discussion arguments that design thinking teams could provide to AI to enjoy the benefits of", "keywords": {"keywords": ["AI-assisted Design Thinking", "information disclosure", "human-AI collaboration", "creative work", "data"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Collaborative creative work"]}}, {"title": "Trust in Human-AI Interaction: Scoping Out Models, Measures, and Methods", "author": "T Ueno", "abstract": "Trust has emerged as a key factor in people's interactions with AI-infused systems. Yet, little is known about what models of trust have been used and for what systems: robots, virtual characters, smart vehicles, decision aids, or others. Moreover, there is yet no known standard approach to measuring trust in AI. This scoping review maps out the state of affairs on trust in human-AI interaction (HAII) from the perspectives of models, measures, and methods. Findings suggest that trust is an important and multi-faceted topic of study within", "keywords": {"keywords": ["trust", "human-AI interaction", "models", "measures", "methods"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-AI Interaction"]}}], "human-ai interaction in human resource management: understanding why employees resist algorithmic evaluation at workplaces and how to mitigate burdens": [{"title": "For What It's Worth: Humans Overwrite Their Economic Self-interest to Avoid Bargaining With AI Systems", "author": "A Erlei", "abstract": "As algorithms are increasingly augmenting and substituting human decision-making, understanding how the introduction of computational agents changes the fundamentals of human behavior becomes vital. This pertains to not only users, but also those parties who face the consequences of an algorithmic decision. In a controlled experiment with 480 participants, we exploit an extended version of two-player ultimatum bargaining where responders choose to bargain with either another human, another human with an AI", "keywords": {"keywords": ["computational agents", "human decision-making", "ultimatum bargaining", "AI systems", "economic self-interest"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Economics"], "content": ["Application"], "application area": ["Decision-making"]}}, {"title": "Understanding UI in Crowdsourcing by Mitigating Unconscious Bias in Job Advertisement", "author": "H Morzogh", "abstract": "SUMMARY We investigated unconscious bias embedded in job advertisements for the  pre-thesis. With a focus on linguistic bias, we investigated how gender bias words impact potential  job applicants. Although organizations aim to build diversity in the workplace, it is important to  emphasize that a job advertisement where certain words consist of unconscious bias, has an  impact on underrepresented groups [1, 2, 3]. The topic was inspired by the increasing demand  for diversity during the recent rising social justice movement fueled by the Black Lives Matter", "keywords": {"keywords": ["unconscious bias", "job advertisement", "linguistic bias", "gender bias", "diversity"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Methodological"], "application area": ["Crowdsourcing"]}}, {"title": "Voices of Sexual Assault Survivors: Understanding Survivors' Experiences of Interactional Breakdowns and Design Ideas for Solutions", "author": "H Park", "abstract": "From initial case-reporting at the crime scene to finishing legal procedures, survivors of sexual assault navigate numerous human and technological resources provided by various organizations. During the help-seeking process, survivors unavoidably interact with multiple professional stakeholders (eg, legal authorities) and technologies (eg, checking their case status on a court website). In the long and complex process, survivors experience interactional breakdowns with technology, and/or humans, but few studies have explored", "keywords": {"keywords": ["sexual assault survivors", "interactional breakdowns", "design ideas", "help-seeking process", "professional stakeholders"], "research type": "Empirical", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Sociology", "content": "Methodological", "application area": "Sexual assault assistance"}}, {"title": "Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions", "author": "H Park", "abstract": "Enterprises have recently adopted AI to human resource management (HRM) to evaluate employees' work performance evaluation. However, in such an HRM context where multiple stakeholders are complexly intertwined with different incentives, it is problematic to design AI reflecting one stakeholder group's needs (eg, enterprises, HR managers). Our research aims to investigate what tensions surrounding AI in HRM exist among stakeholders and explore design solutions to balance the tensions. By conducting stakeholder-centered", "keywords": {"keywords": ["AI", "human resource management", "algorithmic evaluation", "stakeholder-centered solutions", "tensions"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Human Resource Management"]}}, {"title": "The Algorithmic Management of Work and its Implications in Different Contexts", "author": "S Baiocco", "abstract": "This paper provides a conceptual framework for the emerging phenomenon of algorithmic man-agement and outlines some of the implications for work, from work organisation to working condi-tions (job quality). The paper defines algorithmic management as the use of computer-programmed procedures for the coordination of labour input in an organisation and puts it into context to discuss its usage in both digital labour platforms and'regular'workplaces and companies, exploring its implications and providing a few policy", "keywords": {"keywords": ["algorithmic management", "work organization", "working conditions", "digital labour platforms", "job quality"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["sociology"], "content": ["application"], "application area": ["workforce management"]}}, {"title": "Artificial Intelligence in Staffing", "author": "LA Nguyen", "abstract": "The key reasons for the transformation of the domestic labor market and the grounds for changing approaches to the formation of staffing are identified. The development of", "keywords": {"keywords": ["Artificial Intelligence", "Staffing", "domestic labor market", "changing approaches", "formation"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["economics"], "content": ["methodological"], "application area": ["Human Resources"]}}, {"title": "Capable but Amoral? Comparing AI and Human Expert Collaboration in Ethical Decision Making", "author": "S Tolmeijer", "abstract": "While artificial intelligence (AI) is increasingly applied for decision-making processes, ethical decisions pose challenges for AI applications. Given that humans cannot always agree on the right thing to do, how would ethical decision-making by AI systems be perceived and how would responsibility be ascribed in human-AI collaboration? In this study, we investigate how the expert type (human vs. AI) and level of expert autonomy (adviser vs. decider) influence trust, perceived responsibility, and reliance. We find that participants", "keywords": {"keywords": ["artificial intelligence", "ethical decision-making", "human-AI collaboration", "trust", "perceived responsibility"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["ethical decision making"]}}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand", "abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI", "keywords": {"keywords": ["Cognitive biases", "XAI-assisted decision-making", "Systematic review", "Explainability techniques", "Human cognitive constraints"], "research type": "Review", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Computer science", "content": "Methodological", "application area": "Artificial intelligence"}}, {"title": "People Analytics and the Promise of the Good Life\u2014A Critical Transformative Perspective", "author": "M Kl\u00f6pper", "abstract": "Technologies such as people analytics software pose a potential threat towards the (information) privacy of employees and can reinforce existing social injustice. Therefore, it is crucial to design, develop, and deploy the systems with the well-being of employees in mind and to educate users about the harmful impact these tools can have on their personal and professional lives. Yet, the underlying assumptions of these tools are oftentimes hardly questioned. Especially in light of the ongoing public debate on remote work, with politicians", "keywords": {"keywords": ["people analytics", "privacy", "social injustice", "well-being", "remote work"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Methodological"], "application area": ["Workplace analytics"]}}], "trust in collaborative automation in high stakes software engineering work: a case study at nasa": [{"title": "Better together? an evaluation of ai-supported code translation", "author": "JD Weisz", "abstract": "Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced", "keywords": {"keywords": ["code translation", "ai-supported", "generative machine learning", "software engineers", "Java-to-Python"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Source Code Translation"]}}, {"title": "Workshop on Trust and Reliance in AI-Human Teams (TRAIT)", "author": "G Bansal", "abstract": "As humans increasingly interact (and even collaborate) with AI systems during decision-making, creative exercises, and other tasks, appropriate trust and reliance are necessary to ensure proper usage and adoption of these systems. Specifically, people should understand when to trust or rely on an algorithm's outputs and when to override them. While significant research focus has aimed to measure and promote trust in human-AI interaction, the field lacks synthesized definitions and understanding of results across contexts. Indeed", "keywords": {"keywords": ["trust", "reliance", "AI-human teams", "decision-making", "human-AI interaction"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI systems"]}}, {"title": "Trust in Clinical AI: Expanding the Unit of Analysis", "author": "JT Brownea", "abstract": "From diagnosis to patient scheduling, AI is increasingly being considered across different clinical applications. Despite increasingly powerful clinical AI, uptake into actual clinical workflows remains limited. One of the major challenges is developing appropriate trust with clinicians. In this paper, we investigate trust in clinical AI in a wider perspective beyond user interactions with the AI. We offer several points in the clinical AI development, usage, and monitoring process that can have a significant impact on trust. We argue that the calibration", "keywords": {"keywords": ["Trust", "Clinical AI", "Clinicians", "Development", "Workflow"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Healthcare"]}}], "building trust in interactive machine learning via user contributed interpretable rules": [{"title": "The Role of Human Knowledge in Explainable AI", "author": "A Tocchetti", "abstract": "As the performance and complexity of machine learning models have grown significantly over the last years, there has been an increasing need to develop methodologies to describe their behaviour. Such a need has mainly arisen due to the widespread use of black-box models, ie, high-performing models whose internal logic is challenging to describe and understand. Therefore, the machine learning and AI field is facing a new challenge: making models more explainable through appropriate techniques. The final goal of an explainability", "keywords": {"keywords": ["Explainable AI", "Machine Learning", "Model Explainability", "Human Knowledge", "Black-box Models"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Explainability"]}}], "explaining recommendations in e-learning: effects on adolescents' trust": [{"title": "Explaining Artificial Intelligence with Tailored Interactive Visualisations", "author": "J Ooge", "abstract": "Artificial intelligence (AI) is becoming ubiquitous in the lives of both researchers and non-researchers, but AI models often lack transparency. To make well-informed and trustworthy decisions based on these models, people require explanations that indicate how to interpret the model outcomes. This paper presents our ongoing research in explainable AI, which investigates how visual analytics interfaces and visual explanations, tailored to the target audience and application domain, can make AI models more transparent and allow", "keywords": {"keywords": ["artificial intelligence", "explainable AI", "visual analytics interfaces", "visual explanations", "transparency"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["application"], "application area": ["AI transparency"]}}], "intuitively assessing ml model reliability through example-based explanations and editing model inputs": [{"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar", "abstract": "Expert decision makers are starting to rely on data-driven automated agents to assist them with various tasks. For this collaboration to perform properly, the human decision maker must have a mental model of when and when not to rely on the agent. In this work, we aim to ensure that human decision makers learn a valid mental model of the agent's strengths and weaknesses. To accomplish this goal, we propose an exemplar-based teaching strategy where humans solve a set of selected examples and with our help generalize from them to", "keywords": {"keywords": ["teaching strategy", "classifier", "exemplars", "automated agents", "mental model"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Prescriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-AI Collaboration"]}}, {"title": "Visualization Guidelines for Model Performance Communication Between Data Scientists and Subject Matter Experts", "author": "A Suh", "abstract": "Presenting the complexities of a model's performance is a communication bottleneck that threatens collaborations between data scientists and subject matter experts. Accuracy and error metrics alone fail to tell the whole story of a model-its risks, strengths, and limitations-making it difficult for subject matter experts to feel confident in deciding to use a model. As a result, models may fail in unexpected ways if their weaknesses are not clearly understood. Alternatively, models may go unused, as subject matter experts disregard poorly presented", "keywords": {"keywords": ["visualization", "model performance", "communication", "data scientists", "subject matter experts"], "research type": ["review"], "methodology": [], "purposes": ["explanatory"], "discipline": [], "content": ["methodological"], "application area": ["data science collaboration"]}}], "are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making": [{"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu", "abstract": "Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in", "keywords": {"keywords": ["out-of-distribution", "human-AI decision making", "complementary performance", "experimental setup", "AI performance"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["decision making"]}}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "You'd better stop! Understanding human reliance on machine learning models under covariate shift", "author": "CW Chiang"}, {"title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspective", "author": "H Lakkaraju"}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang"}, {"title": "Appropriate fairness perceptions? On the effectiveness of explanations in enabling people to assess the fairness of automated decision systems", "author": "J Schoeffer"}, {"title": "Knowledge graph-based rich and confidentiality preserving Explainable Artificial Intelligence (XAI)", "author": "JM Ro\u017eanec"}, {"title": "When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models", "author": "A Rechkemmer"}, {"title": "The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending", "author": "M Dikmen"}, {"title": "Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making", "author": "M Schemmer", "abstract": "Many important decisions in daily life are made with the help of advisors, eg, decisions about medical treatments or financial investments. Whereas in the past, advice has often been received from human experts, friends, or family, advisors based on artificial intelligence (AI) have become more and more present nowadays. Typically, the advice generated by AI is judged by a human and either deemed reliable or rejected. However, recent work has shown that AI advice is not always beneficial, as humans have shown to be", "keywords": {"keywords": ["AI-based advice", "human-AI decision-making", "reliance", "artificial intelligence", "beneficial"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Psychology", "Human-Computer Interaction"], "content": ["Application"], "application area": ["Decision-making"]}}, {"title": "Toward Benchmarking Group Explanations: Evaluating the Effect of Aggregation Strategies versus Explanation.", "author": "F Barile"}, {"title": "Better together? an evaluation of ai-supported code translation", "author": "JD Weisz"}, {"title": "The influences of task design on crowdsourced judgement: A case study of recidivism risk evaluation", "author": "X Duan"}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation", "author": "V Lai"}, {"title": "Will You Accept the AI Recommendation? Predicting Human Behavior in AI-Assisted Decision Making", "author": "X Wang"}, {"title": "Under-reliance or misalignment? How proxy outcomes limit measurement of appropriate reliance in AI-assisted decision-making", "author": "L GUERDAN"}, {"title": "Predicting surgical outcomes for chronic exertional compartment syndrome using a machine learning framework with embedded trust by interrogation strategies.", "author": "H Andrew"}, {"title": "From Correlation to Causation: Formalizing Interpretable Machine Learning as a Statistical Process", "author": "L Klein"}, {"title": "Explainable Predictive Process Monitoring: A User Evaluation", "author": "W Rizzi"}, {"title": "Homophily and Incentive Effects in Use of Algorithms", "author": "R Fogliato", "abstract": "As algorithmic tools increasingly aid experts in making consequential decisions, the need to understand the precise factors that mediate their influence has grown commensurately. In this paper, we present a crowdsourcing vignette study designed to assess the impacts of two plausible factors on AI-informed decision-making. First, we examine homophily--do people defer more to models that tend to agree with them?--by manipulating the agreement during training between participants and the algorithmic tool. Second, we considered incentives", "keywords": {"keywords": ["algorithmic tools", "homophily", "incentive effects", "AI-informed decision-making", "crowdsourcing vignette study"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "Predicting surgical outcomes for chronic exertional compartment syndrome using a machine learning framework with embedded trust by interrogation \u2026", "author": "A Houston"}, {"title": "A Cognitive Work Analysis Approach to Explainable Artificial Intelligence in Non-Expert Financial Decision-Making", "author": "M Dikmen"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Fairness and Explanation in AI-Informed Decision Making", "author": "A Angerschmid"}, {"title": "Building Trust by Supporting Situation Awareness: Exploring Pilots' Design Requirements for Decision Support Tools", "author": "C STORATH"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Understanding the Role of Explanation Modality in AI-assisted Decision-making", "author": "V Robbemond"}, {"title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems", "author": "C Panigutti"}, {"title": "Investigating Trust in Human-Machine Learning Collaboration: A Pilot Study on Estimating Public Anxiety from Speech", "author": "AA Tutul"}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert Users", "author": "C Bove", "abstract": "The increasing usage of complex Machine Learning models for decision-making has raised interest in explainable artificial intelligence (XAI). In this work, we focus on the effects of providing accessible and useful explanations to non-expert users. More specifically, we propose generic XAI design principles for contextualizing and allowing the exploration of explanations based on local feature importance. To evaluate the effectiveness of these principles for improving users' objective understanding and satisfaction, we conduct a", "keywords": {"keywords": ["explainable artificial intelligence", "XAI", "local feature importance", "contextualization", "exploration"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Technical Explanation"], "application area": ["Machine Learning"]}}, {"title": "Toward a decision process of the best machine learning model for multi-stakeholders: a crowdsourcing survey method", "author": "T Yokota"}, {"title": "Creating a User Model to Support User-specific Explanations of AI Systems", "author": "O Chambers"}, {"title": "Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task", "author": "B Leichtmann"}, {"title": "Generating Recommendations with Post-Hoc Explanations for Citizen Science", "author": "D Ben Zaken"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideation", "author": "Y Wang"}, {"title": "Designing for Responsible Trust in AI Systems: A Communication Perspective", "author": "QV Liao"}, {"title": "Human-Centered Evaluation of Explanations", "author": "J Boyd-Graber"}, {"title": "Towards Explainability for AI Fairness", "author": "J Zhou"}, {"title": "Exploring the Effects of Machine Learning Literacy Interventions on Laypeople's Reliance on Machine Learning Models", "author": "CW Chiang", "abstract": "Today, machine learning (ML) technologies have penetrated almost every aspect of people's lives, yet public understandings of these technologies are often limited. This highlights the urgent need of designing effective methods to increase people's machine learning literacy, as the lack of relevant knowledge may result in people's inappropriate usage of machine learning technologies. In this paper, we focus on an ML-assisted decision-making setting and conduct a human-subject randomized experiment to explore how", "keywords": {"keywords": ["machine learning literacy", "interventions", "laypeople", "reliance", "ML-assisted decision-making"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer science", "content": "Application", "application area": "Machine learning"}}, {"title": "Tell Me Something That Will Help Me Trust You: A Survey of Trust Calibration in Human-Agent Interaction", "author": "GJ Cancro"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}], "rapid assisted visual search": [{"title": "Hybrid intelligence-combining the human in the loop with the computer in the loop: a systematic literature review", "author": "C Wiethof", "abstract": "The paper aims at establishing a common ground and understanding of collaborative learning approaches between humans and computers to encourage Hybrid Intelligence. Thereby, we put a special focus on identifying how humans and computers learn from each other through an iterative and interdependent process involving the human-in-theloop as well as the computer-in-the-loop. To approach this aim, a systematic literature review is conducted. Therefore, we reviewed 2098 publications in three relevant databases and found", "keywords": {"keywords": ["Hybrid Intelligence", "collaborative learning", "human in the loop", "computer in the loop", "systematic literature review"], "research type": "Review", "methodology": "Mixed-methods", "purposes": "Descriptive", "discipline": "Computer Science", "content": "Methodological", "application area": "Collaborative learning"}}, {"title": "Forward reasoning decision support: toward a more complete view of the human-AI interaction design space", "author": "ZT Zhang", "abstract": "Decision support systems based on AI are usually designed to generate complete outputs entirely automatically and to explain those to users. However, explanations, no matter how well designed, might not adequately address the output uncertainty of such systems in many applications. This is especially the case when the human-out-of-the-loop problem persists, which is a fundamental human limitation. There is no reason to limit decision support systems to such backward reasoning designs, though. We argue how more interactive", "keywords": {"keywords": ["decision support systems", "AI", "human-AI interaction", "output uncertainty", "forward reasoning"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["automated decision making"]}}, {"title": "Pilot attitudes toward AI in the cockpit: implications for design", "author": "ZT Zhang", "abstract": "As the aviation industry is actively working on adopting AI for air traffic, stakeholders agree on the need for a human-centered approach. However, automation design is often driven by user-centered intentions, while the development is actually technology-centered. This can be attributed to a discrepancy between the system designers' perspective and complexities in real-world use. The same can be currently observed with AI applications where most design efforts focus on the interface between humans and AI, while the overall system", "keywords": {"keywords": ["AI", "cockpit", "pilot attitudes", "design", "human-centered approach"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["technology", "human factors"], "content": ["methodological"], "application area": ["aviation industry"]}}], "digging into user control: perceptions of adherence and instability in transparent models": [{"title": "Facilitating knowledge sharing from domain experts to data scientists for building nlp model", "author": "S Park", "abstract": "Data scientists face a steep learning curve in understanding a new domain for which they want to build machine learning (ML) models. While input from domain experts could offer valuable help, such input is often limited, expensive, and generally not in a form readily consumable by a model development pipeline. In this paper, we propose Ziva, a framework to guide domain experts in sharing essential domain knowledge to data scientists for building NLP models. With Ziva, experts are able to distill and share their domain knowledge", "keywords": {"keywords": ["knowledge sharing", "domain experts", "data scientists", "NLP models", "Ziva"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Prescriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Natural Language Processing"]}}, {"title": "After-action review for AI (AAR/AI", "author": "J Dodge", "abstract": "AI? In this article, we investigate this approach by adapting a technique called After-Action  Review (AAR) for use with AI.  adaptation AAR/AI (pronounced \u201carf-eye,\u201d short for \u201cAAR for AI\u201d).", "keywords": {"keywords": ["After-Action Review", "AI", "AAR/AI", "adaptation", "technique"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "From\u201d explainable ai\u201d to\u201d graspable ai", "author": "M Ghajargar", "abstract": "Since the advent of Artificial Intelligence (AI) and Machine Learning (ML), researchers have asked how intelligent computing systems could interact with and relate to their users and their surroundings, leading to debates around issues of biased AI systems, ML black-box, user trust, user's perception of control over the system, and system's transparency, to name a few. All of these issues are related to how humans interact with AI or ML systems, through an interface which uses different interaction modalities. Prior studies address these issues from", "keywords": {"keywords": ["Artificial Intelligence", "Machine Learning", "Intelligent Computing Systems", "User Interaction", "System Transparency"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Application", "application area": "Human-computer Interaction"}}, {"title": "Hybrid intelligence-combining the human in the loop with the computer in the loop: a systematic literature review", "author": "C Wiethof", "abstract": "The paper aims at establishing a common ground and understanding of collaborative learning approaches between humans and computers to encourage Hybrid Intelligence. Thereby, we put a special focus on identifying how humans and computers learn from each other through an iterative and interdependent process involving the human-in-theloop as well as the computer-in-the-loop. To approach this aim, a systematic literature review is conducted. Therefore, we reviewed 2098 publications in three relevant databases and found", "keywords": {"keywords": ["hybrid intelligence", "human-computer collaboration", "collaborative learning", "iterative interdependent process", "systematic literature review"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["learning"]}}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Propertie", "author": "JE Dodge", "abstract": "Assessing AI systems is difficult. Humans rely on AI systems in increasing ways, both visible and invisible, meaning a variety of stakeholders need a variety of assessment tools (eg, a professional auditor, a developer, and an end user all have different needs). We posit that it is possible to provide explanations and assessment processes that enable AI non-experts observing multiple intelligent agents in sequential domains to differentiate the agents with respect to a property (eg, quality or fairness), as well as articulate justification for their", "keywords": {"keywords": ["AI assessment", "manipulable properties", "explanations", "assessment processes", "intelligent agents"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "The Design of an Interactive Topic Modeling Application for Media Conten", "author": "L Ham", "abstract": "provide a qualitative window into the contents of a large document collection. The  20-topic  models: the dynamic topic model estimated from all of the previous years, a static topic model", "keywords": {"keywords": ["qualitative", "interactive", "topic modeling", "media content", "document collection"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Media Analysis"]}}], "evaluating saliency map explanations for convolutional neural networks: a user study": [{"title": "Secure and robust machine learning for healthcare: A survey", "author": "A Qayyum", "abstract": "Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to", "keywords": {"keywords": ["machine learning", "deep learning", "healthcare", "secure", "robust"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["healthcare"]}}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "Debugging tests for model explanations", "author": "J Adebayo"}, {"title": "When explanations lie: Why many modified bp attributions fail", "author": "L Sixt"}, {"title": "A human-grounded evaluation benchmark for local explanations of machine learning", "author": "S Mohseni"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "Code-free deep learning for multi-modality medical image classification", "author": "E Korot"}, {"title": "\u201cLet me explain!\u201d: exploring the potential of virtual agents in explainable AI interaction design", "author": "K Weitz"}, {"title": "Local and global explanations of agent behavior: Integrating strategy summaries with saliency maps", "author": "T Huber"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi", "abstract": "After Deep Learning (DL) regained popularity recently, the Artificial Intelligence (AI) or Machine Learning (ML) field is undergoing rapid growth concerning research and real-world application development. Deep Learning has generated complexities in algorithms, and researchers and users have raised concerns regarding the usability and adoptability of Deep Learning systems. These concerns, coupled with the increasing human-AI interactions, have created the emerging field that is Human-Centered Machine Learning", "keywords": {"keywords": ["Deep Learning", "Human-Centered Machine Learning", "Artificial Intelligence", "Machine Learning", "Complexities"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-AI Interaction"]}}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable AI", "author": "M Chromik"}, {"title": "How to support users in understanding intelligent systems? Structuring the discussion", "author": "M Eiband"}, {"title": "Measure utility, gain trust: practical advice for XAI researchers", "author": "B Davis"}, {"title": "Visual, textual or hybrid: the effect of user expertise on different explanations", "author": "M Szymanski"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen"}, {"title": "Wait, but why?: assessing behavior explanation strategies for real-time strategy games", "author": "J Robertson"}, {"title": "Quantitative evaluation of machine learning explanations: A human-grounded benchmark", "author": "S Mohseni", "abstract": "Research in interpretable machine learning proposes different computational and human subject approaches to evaluate model saliency explanations. These approaches measure different qualities of explanations to achieve diverse goals in designing interpretable machine learning systems. In this paper, we propose a benchmark for image and text domains using multi-layer human attention masks aggregated from multiple human annotators. We then present an evaluation study to compare model saliency explanations", "keywords": {"keywords": ["machine learning", "interpretability", "explanations", "benchmark", "evaluation"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Descriptive", "discipline": "Computer Science", "content": "Methodological", "application area": "Artificial Intelligence"}}, {"title": "GANterfactual\u2014Counterfactual Explanations for Medical Non-experts Using Generative Adversarial Learning", "author": "S Mertes"}, {"title": "Designing Interactive Transfer Learning Tools for ML Non-Experts", "author": "S Mishra"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "Metrics for saliency map evaluation of deep learning explanation methods", "author": "T Gomez"}, {"title": "Explainable Goal-Driven Agents and Robots--A Comprehensive Review", "author": "F Sado"}, {"title": "Artificial intelligence for diagnosis of fractures on plain radiographs: A scoping review of current literature", "author": "C Rainey"}, {"title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization", "author": "J Borowski"}, {"title": "Natural Images are More Informative for Interpreting CNN Activations than State-of-the-Art Synthetic Feature Visualizations", "author": "J Borowski"}, {"title": "Explainable artificial intelligence in education", "author": "H Khosravi"}, {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "author": "L Sixt", "abstract": "Current state of the art computer vision applications rely on highly complex models. Their interpretability is mostly limited to post-hoc methods which are not guaranteed to be faithful to the model. To elucidate a model's decision, we present a novel interpretable model based on an invertible deep convolutional network. Our model generates meaningful, faithful, and ideal counterfactuals. Using PCA on the classifier's input, we can also create \u201cisofactuals\u201d\u2013image interpolations with the same outcome but visually meaningful different features", "keywords": {"keywords": ["interpretability", "invertibility", "deep convolutional network", "counterfactuals", "isosurfaces"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["computer vision"]}}, {"title": "Challenges for machine learning in clinical translation of big data imaging studies", "author": "NK Dinsdale"}, {"title": "Crowdsourcing and evaluating concept-driven explanations of machine learning models", "author": "S Mishra"}, {"title": "Deep learning for medical image analysis", "author": "K Lu"}, {"title": "LEx: A Framework for Operationalising Layers of Machine Learning Explanations", "author": "R Singh"}, {"title": "Towards Explainable Artificial Intelligence and Explanation User Interfaces to Open the 'Black Box'of Automated ECG Interpretation", "author": "K Rjoob"}, {"title": "Meaningfully debugging model mistakes using conceptual counterfactual explanations", "author": "A Abid"}, {"title": "Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional networks", "author": "W Zhang"}, {"title": "Post hoc explanations may be ineffective for detecting unknown spurious correlation", "author": "J Adebayo"}, {"title": "Evaluation of saliency-based explainability method", "author": "SZS Samuel"}, {"title": "Occlusion method to obtain saliency maps for CNN", "author": "T Eriksson", "abstract": "This Bachelor project will study convolutional neural networks created for image classification. Furthermore, it will specifically use an explanatory model for how the network decided a certain classification output. This is to increase the interpretability of the network. However, the completeness of the explanatory model needs to be high for it to be useful. A saliency map of how valuable each image pixel is for the classification will be created, by occluding parts of the image. The MNIST dataset was used, which contains handwritten", "keywords": {"keywords": ["Convolutional Neural Networks", "Image Classification", "Explanatory Model", "Saliency Map", "Occlusion Method"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Image Analysis"]}}, {"title": "Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability", "author": "R Levin"}, {"title": "Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems", "author": "M Nourani"}, {"title": "A reinforcement learning based dirt-exploration for cleaning-auditing robot", "author": "T Pathmakumar"}, {"title": "Two4two: Evaluating interpretable machine learning-a synthetic dataset for controlled experiments", "author": "M Schuessler"}, {"title": "Meaningfully explaining model mistakes using conceptual counterfactuals", "author": "A Abid"}, {"title": "Explainable Deep Reinforcement Learning: State of the Art and Challenges", "author": "GA Vouros"}, {"title": "Demystifying Artificial Intelligence", "author": "M Mondal"}, {"title": "Explainability and Continuous Learning with Capsule Networks.", "author": "J Mohr"}, {"title": "Explainable Predictive Process Monitoring: A User Evaluation", "author": "W Rizzi"}, {"title": "How to Support Users in Understanding Intelligent Systems? Structuring the", "author": "M EIBAND", "abstract": "Linked by this shared goal, the diverse terms are often employed interchangeably\u2013and yet, prior work implies divergent assumptions about how users may best be supported. For instance, work on interpretability has recently been criticised for unclear use of the term [29, 71], a survey on explainability in recommenders found incompatible existing taxonomies [76], and discussions about system transparency and accountability revealed diverging assumptions (ie disclosing source code vs system auditing through experts)[32]. A recent", "keywords": {"keywords": ["users", "intelligent systems", "interpretability", "explainability", "system transparency"], "research type": "Theoretical", "methodology": "Qualitative", "purposes": "Explanatory", "discipline": "Computer science", "content": "Application", "application area": "User support"}}, {"title": "Diagnostic performance for detecting bone marrow edema of the hip on dual-energy CT: Deep learning model vs. musculoskeletal physicians and radiologists", "author": "CS Park"}, {"title": "Contributions to Deep Transfer Learning: from Supervised to Reinforcement Learning", "author": "M Sabatelli"}, {"title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?", "author": "A Balayn"}, {"title": "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Challenges and Solutions", "author": "E Petersen"}, {"title": "Comparing the genetic and environmental architecture of blood count, blood biochemistry and urine biochemistry biological ages with machine learning", "author": "A Le Goallec"}, {"title": "Scientific Inference With Interpretable Machine Learning: Analyzing Models to Learn About Real-World Phenomena", "author": "T Freiesleben"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Perception Visualization: Seeing Through the Eyes of a DNN", "author": "L Giulivi"}, {"title": "Monitoring Shortcut Learning using Mutual Information", "author": "M Adnan"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang", "abstract": "Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi", "keywords": {"keywords": ["Debiased-CAM", "image perturbations", "visual explanations", "machine learning", "bias"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Computer science", "content": "Methodological", "application area": "Artificial intelligence"}}, {"title": "On Stylistic Differences Between 3D Meshes", "author": "C McMahon"}, {"title": "Comparison of attention models and post-hoc explanation methods for embryo stage identification: a case study", "author": "T Gomez"}, {"title": "Artificial Intelligence for diagnosis of fractures on plain radiographs: a scoping review of current literature.", "author": "C Rainey\u2013corresponding"}, {"title": "Understanding Diversity in Human-AI Data: What Cognitive Style Disaggregation Reveals", "author": "A Anderson"}, {"title": "One step further: evaluating interpreters using metamorphic testing", "author": "M Fan"}, {"title": "Developing Future Human-Centered Smart Cities: Critical Analysis of Smart City Security, Interpretability, and Ethical Challenges", "author": "K Ahmada"}, {"title": "Algorithmic nudge to make better choices: Evaluating effectiveness of XAI frameworks to reveal biases in algorithmic decision making to users", "author": "P Juneja"}, {"title": "Inductive Biases for Higher-Order Visual Cognition", "author": "S Shekhar"}, {"title": "The effect of machine learning explanations on user trust for automated diagnosis of COVID-19", "author": "K Goel"}, {"title": "How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement and \u2026", "author": "D Buschek", "abstract": "The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI", "keywords": {"keywords": ["intelligent systems", "usability principles", "transparency", "scrutability", "human-computer interaction"], "research type": "review", "methodology": "qualitative", "purposes": "explanatory", "discipline": "computer science", "content": "methodological", "application area": "user understanding of intelligent systems"}}, {"title": "Peer-reviewed Journal Articles", "author": "Y Zhao"}, {"title": "Towards Explainable Artificial Intelligence and Explanation User Interfaces to Open the 'Black Box'of Automated ECG Interpretation", "author": "C Knoery"}, {"title": "Defining Explanation and Explanatory Depth in XAI", "author": "S Buijsman"}, {"title": "From YouTube to the brain: Transfer learning can improve brain-imaging predictions with deep learning", "author": "N Malik"}, {"title": "Enhancing Model Assessment in Vision-based Interactive Machine Teaching through Real-time Saliency Map Visualization", "author": "Z Zhou"}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications", "author": "M Nourani"}, {"title": "Explainable Digital Forensics AI: Towards Mitigating Distrust in AI-Based Digital Forensics Analysis with Interpretable Models", "author": "AA Solanke"}, {"title": "Explainable digital forensics AI: Towards mitigating distrust in AI-based digital forensics analysis using interpretable models", "author": "AA Solanke"}, {"title": "Semi-supervised learning with natural language processing for right ventricle classification in echocardiography\u2014a scalable approach", "author": "E Hagberg"}, {"title": "Towards a multi-stakeholder value-based assessment framework for algorithmic systems", "author": "M Yurrita", "abstract": "In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical", "keywords": {"keywords": ["algorithmic systems", "multi-stakeholder", "value-based assessment", "ML-driven systems", "ethical values"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["algorithmic governance"]}}, {"title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset", "author": "L Sixt"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XAI", "author": "Q Wang"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Studying CNN representations through activation dimensionality reduction and visualization", "author": "NS Dey"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Delegating Agency? The Effects of XAI, Personality Traits, and the Moral Significance of the Application on the Reliance on Autonomous Systems: A User Study", "author": "E Kallina"}, {"title": "Hierarchical goals contextualize local reward decomposition explanations", "author": "F Rietz"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}, {"title": "Representaciones del aprendizaje reutilizando los gradientes de la retropropagacion", "author": "R Reyes-Ochoa"}, {"title": "Explainable AI methods for Cyber-Physical systems", "author": "IO Myklebust", "abstract": "Fremskrittene i kunstig intelligens (AI) det siste ti\u00e5ret har banet vei for et innovativt og mer digitalisert samfunn. Til dags dato er AI-l\u00f8sninger til stede i mange situasjoner vi m\u00f8ter hver dag. Basert p\u00e5 denne evolusjonen blir det n\u00e5 gjort mer forskning p\u00e5 forsterket l\u00e6ring (RL). For robotikk kan dette v\u00e6re en gamechanger, siden RL gj\u00f8r systemer i stand til \u00e5 l\u00e6re basert p\u00e5 erfaring, noe som kan \u00f8ke graden av autonomi i fremtidens roboter. Det st\u00f8rste gjennombruddet er til stede i feltet dyp forsterkende l\u00e6ring (DRL), der RL kombineres med", "keywords": {"keywords": ["AI", "Cyber-Physical systems", "Explainable AI", "Reinforcement Learning", "Deep Reinforcement Learning"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Robotics"]}}, {"title": "Intelligence Artificielle", "author": "M Mondal"}, {"title": "Intelligence Artificielle: Un aper\u00e7u", "author": "M Mondal"}], "proxy tasks and subjective measures can be misleading in evaluating explainable ai systems": [{"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal", "abstract": "Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with", "keywords": {"keywords": ["explainable AI", "complementary team performance", "decision-making tasks", "mixed-method user studies", "team accuracy"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "Evaluating the quality of machine learning explanations: A survey on methods and metrics", "author": "J Zhou"}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making", "author": "Z Bu\u00e7inca"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies", "author": "EM Kenny"}, {"title": "How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection", "author": "M Jacobs"}, {"title": "Machine learning explainability for external stakeholders", "author": "U Bhatt"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "Does explainable artificial intelligence improve human decision-making?", "author": "Y Alufaisan", "abstract": "Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model. Using real datasets, we", "keywords": {"keywords": ["explainable AI", "human decision-making", "interpretability", "trust", "user insights"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Application", "application area": "Artificial Intelligence"}}, {"title": "Predictive text encourages predictable writing", "author": "KC Arnold"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Designing AI for trust and collaboration in time-constrained medical decisions: a sociotechnical lens", "author": "M Jacobs"}, {"title": "What do you see? Evaluation of explainable artificial intelligence (XAI) interpretability through neural backdoors", "author": "YS Lin"}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable AI", "author": "M Chromik"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "Assessing the impact of automated suggestions on decision making: Domain experts mediate model errors but take less initiative", "author": "A Levy"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "Visual, textual or hybrid: the effect of user expertise on different explanations", "author": "M Szymanski"}, {"title": "How useful are the machine-generated interpretations to general users? a human evaluation on guessing the incorrectly predicted labels", "author": "H Shen", "abstract": "Explaining to users why automated systems make certain mistakes is important and challenging. Researchers have proposed ways to automatically produce interpretations for deep neural network models. However, it is unclear how useful these interpretations are in helping users figure out why they are getting an error. If an interpretation effectively explains to users how the underlying deep neural network model works, people who were presented with the interpretation should be better at predicting the model's outputs than those who", "keywords": {"keywords": ["machine-generated interpretations", "general users", "human evaluation", "guessing", "incorrectly predicted labels"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Error prediction and explanation"]}}, {"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer"}, {"title": "Evaluating the interpretability of generative models by interactive reconstruction", "author": "A Ross"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies", "author": "O Vereschak"}, {"title": "Designing for human-AI complementarity in K-12 education", "author": "K Holstein"}, {"title": "Leveraging latent features for local explanations", "author": "R Luss"}, {"title": "Users & machine learning-based curation systems", "author": "H Heuer"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "Human evaluation of spoken vs. visual explanations for open-domain qa", "author": "AV Gonzalez", "abstract": "While research on explaining predictions of open-domain QA systems (ODQA) to users is gaining momentum, most works have failed to evaluate the extent to which explanations improve user trust. While few works evaluate explanations using user studies, they employ settings that may deviate from the end-user's usage in-the-wild: ODQA is most ubiquitous in voice-assistants, yet current research only evaluates explanations using a visual display, and may erroneously extrapolate conclusions about the most performant explanations to", "keywords": {"keywords": ["open-domain QA systems", "explanations", "user trust", "voice-assistants", "visual display"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-Computer Interaction"]}}, {"title": "Explainable machine learning with prior knowledge: an overview", "author": "K Beckh"}, {"title": "Outcome-explorer: A causality guided interactive visual interface for interpretable algorithmic decision making", "author": "MN Hoque"}, {"title": "Quantitative evaluation of machine learning explanations: A human-grounded benchmark", "author": "S Mohseni"}, {"title": "An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability", "author": "F Sovrano"}, {"title": "Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning", "author": "KZ Gajos"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "Play MNIST for me! User studies on the effects of post-hoc, example-based explanations & error rates on debugging a deep learning, black-box classifier", "author": "C Ford"}, {"title": "Studying and exploiting the relationship between model accuracy and explanation quality", "author": "Y Jia"}, {"title": "Diagnosing ai explanation methods with folk concepts of behavior", "author": "A Jacovi"}, {"title": "How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions", "author": "HF Cheng", "abstract": "Machine learning tools have been deployed in various contexts to support human decision-making, in the hope that human-algorithm collaboration can improve decision quality. However, the question of whether such collaborations reduce or exacerbate biases in decision-making remains underexplored. In this work, we conducted a mixed-methods study, analyzing child welfare call screen workers' decision-making over a span of four years, and interviewing them on how they incorporate algorithmic predictions into their", "keywords": {"keywords": ["child welfare workers", "racial disparities", "algorithmic decisions", "machine learning tools", "biases"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Sociology", "content": "Application", "application area": "Decision-making"}}, {"title": "Do explanations help users detect errors in open-domain QA? an evaluation of spoken vs. visual explanations", "author": "AV Gonz\u00e1lez"}, {"title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis.", "author": "CH Tsai"}, {"title": "Intuitively assessing ml model reliability through example-based explanations and editing model inputs", "author": "H Suresh"}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization", "author": "M Fan"}, {"title": "Machine Learning Explainability for External Stakeholders", "author": "A Weller"}, {"title": "Visual Intelligence through Human Interaction", "author": "R Krishna"}, {"title": "Towards explainable mcts", "author": "H Baier"}, {"title": "Simulated User Studies for Explanation Evaluation", "author": "V Chen"}, {"title": "Explaining by Conversing: The Argument for Conversational Xai Systems", "author": "W Marrakchi", "abstract": "The interest in chatbots and conversational agents is as old as artificial intelligence (AI) itself. Recently, multiple members of the HCI community including Weld and Bansal (2018) have suggested that conversational explanation systems is the best path forward for explainable human-agent interaction. This recommendation is often presented without its supporting arguments so we embarked on this thesis to shed some light on the call behind conversational explainable AI (XAI) systems. First, we survey the research on the need for", "keywords": {"keywords": ["Conversational Xai Systems", "Explainable AI", "Chatbots", "Human-agent interaction", "HCI community"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Conversational agents"]}}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation", "author": "V Lai"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Features of Explainability: How users understand counterfactual and causal explanations for categorical and continuous features in XAI", "author": "G Warren"}, {"title": "Co-designing Effective Human-Algorithm Collaborations for Suicide Risk Assessment with Clinicians, Teens, and Families 15-300, Fall 2020", "author": "E Swecker"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations", "author": "A Balagopalan"}, {"title": "On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods", "author": "K Amarasinghe"}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "HINT: Integration Testing for AI-based features with Humans in the Loop", "author": "QZ Chen"}, {"title": "Opening the AI Black Box for a better HCI", "author": "FM Cau", "abstract": "In this statement, we will focus on the opportunities and problems set by two specific applications of Artificial Intelligence (AI) techniques in Human-Computer Interaction (HCI). We focused on these during our latest research that we consider them as two important themes for developing the syllabus. The first problem is the introduction of AI support for helping the user in making decisions in domains such as finance, military, transportation, healthcare, law etc. Nowadays, Deep Neural Networks are consistently overperforming", "keywords": {"keywords": ["Artificial Intelligence", "Human-Computer Interaction", "Deep Neural Networks", "Decision-making support", "AI Black Box"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Various domains"]}}, {"title": "Audit, Don't Explain--Recommendations Based on a Socio-Technical Understanding of ML-Based Systems", "author": "H Heuer"}, {"title": "Use-Case-Grounded Simulations for Explanation Evaluation", "author": "V Chen"}, {"title": "It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI", "author": "A Papenmeier"}, {"title": "On the Objective Evaluation of Post Hoc Explainers", "author": "Z Carmichael"}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk"}, {"title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems", "author": "C Panigutti"}, {"title": "Trust and Reliance in XAI--Distinguishing Between Attitudinal and Behavioral Measures", "author": "N Scharowski"}, {"title": "It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy", "author": "A Bell"}, {"title": "Review of white box methods for explanations of convolutional neural networks in image classification tasks", "author": "MP Ayyar"}, {"title": "Towards Involving End-users in Interactive Human-in-the-loop AI Fairness", "author": "Y Nakao", "abstract": "Ensuring fairness in artificial intelligence (AI) is important to counteract bias and discrimination in far-reaching applications. Recent work has started to investigate how humans judge fairness and how to support machine learning (ML) experts in making their AI models fairer. Drawing inspiration from an Explainable AI (XAI) approach called explanatory debugging used in interactive machine learning, our work explores designing interpretable and interactive human-in-the-loop interfaces that allow ordinary end-users without any", "keywords": {"keywords": ["fairness", "artificial intelligence", "bias", "discrimination", "interactive human-in-the-loop AI"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["ethics"]}}, {"title": "GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information", "author": "H Dang"}, {"title": "Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task", "author": "B Leichtmann"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics. Electronics 2021, 10, 593", "author": "J Zhou"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Evaluating human understanding in XAI systems", "author": "D Gentile"}, {"title": "Considerations for applying logical reasoning to explain neural network outputs", "author": "FM Cau"}, {"title": "Human-Centered Evaluation of Explanations", "author": "J Boyd-Graber"}, {"title": "Advancing Nearest Neighbor Explanation-by-Example with Critical Classification Regions", "author": "EM Kenny"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand", "abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI", "keywords": {"keywords": ["Cognitive biases", "XAI", "Explainability", "Decision-making", "Systematic review"], "research type": "Review", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer science", "content": "Methodological", "application area": "Artificial intelligence"}}, {"title": "\u201cWhy Do I Care What's Similar?\u201d Probing Challenges in AI-Assisted Child Welfare Decision-Making through Worker-AI Interface Design Concepts", "author": "A Kawakami"}, {"title": "Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias", "author": "D Solans"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XAI", "author": "Q Wang"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Designing for human\u2013AI complementarity in K\u201012 education", "author": "K Holstein"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}], "explaining models: an empirical study of how explanations impact fairness judgment": [{"title": "Manipulating and measuring model interpretability", "author": "F Poursabzi-Sangdeh", "abstract": "As such, we argue that to understand interpretability, it is necessary to directly manipulate  diferent factors and measure their efects. What is or is not interpretable must be defned by", "keywords": {"keywords": ["model interpretability", "manipulation", "measurement", "effects", "defining"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao"}, {"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making", "author": "Y Zhang"}, {"title": "Counterfactual explanations for machine learning: A review", "author": "S Verma"}, {"title": "WeBuildAI: Participatory framework for algorithmic governance", "author": "MK Lee"}, {"title": "Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation", "author": "MK Lee"}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "Good counterfactuals and where to find them: A case-based technique for generating counterfactuals for explainable ai (xai)", "author": "MT Keane"}, {"title": "No explainability without accountability: An empirical study of explanations and feedback in interactive ml", "author": "A Smith-Renner"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "A human-centered agenda for intelligible machine learning", "author": "JW Vaughan", "abstract": "To build machine learning systems that are reliable, trustworthy, and fair, we must be able to provide relevant stakeholders with an understanding of how these systems work. Yet what makes a system \u201cintelligible\u201d is difficult to pin down. Intelligibility is a fundamentally human-centered concept that lacks a one-size-fits-all solution. Although many intelligibility techniques have been proposed in the machine learning literature, there are many more open questions about how best to provide stakeholders with the information they need to", "keywords": {"keywords": ["intelligible machine learning", "reliable", "trustworthy", "fair", "stakeholders"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies", "author": "EM Kenny"}, {"title": "If only we had better counterfactual explanations: Five key deficits to rectify in the evaluation of counterfactual xai techniques", "author": "MT Keane"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "On generating plausible counterfactual and semi-factual explanations for deep learning", "author": "EM Kenny"}, {"title": "Operationalizing human-centered perspectives in explainable AI", "author": "U Ehsan"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "What are people doing about XAI user experience? A survey on AI explainability research and practice", "author": "JJ Ferreira"}, {"title": "In AI we trust? Factors that influence trustworthiness of AI-infused decision-making processes", "author": "M Ashoori"}, {"title": "Fairness and decision-making in collaborative shift scheduling systems", "author": "A Uhde"}, {"title": "Bridging machine learning and mechanism design towards algorithmic fairness", "author": "J Finocchiaro", "abstract": "Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (eg, school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or", "keywords": {"keywords": ["algorithmic fairness", "machine learning", "mechanism design", "context-dependent nature", "strategic behavior"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["decision-making systems"]}}, {"title": "What to expect from opening up 'black boxes'? Comparing perceptions of justice between human and automated agents", "author": "N Schlicker"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Personalising explainable recommendations: literature and conceptualisation", "author": "M Naiseh"}, {"title": "Human-centered approaches to fair and responsible AI", "author": "MK Lee"}, {"title": "Designing AI for trust and collaboration in time-constrained medical decisions: a sociotechnical lens", "author": "M Jacobs"}, {"title": "Explainable automation: Personalized and adaptive uis to foster trust and understanding of driving automation systems", "author": "P Wintersberger"}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "Algorithmic bias: review, synthesis, and future research directions", "author": "N Kordzadeh"}, {"title": "Exploring explainability: a definition, a model, and a knowledge catalogue", "author": "L Chazette", "abstract": "The growing complexity of software systems and the influence of software-supported decisions in our society awoke the need for software that is transparent, accountable, and trust-worthy. Explainability has been identified as a means to achieve these qualities. It is recognized as an emerging non-functional requirement (NFR) that has a significant impact on system quality. However, in order to incorporate this NFR into systems, we need to understand what explainability means from a software engineering perspective and how it", "keywords": {"keywords": ["explainability", "software systems", "transparency", "accountability", "trustworthiness"], "research type": ["theoretical"], "methodology": ["qualitative", "quantitative"], "purposes": ["exploratory", "descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["software engineering"]}}, {"title": "Explaining data-driven decisions made by AI systems: The counterfactual approach", "author": "C Fern\u00e1ndez-Lor\u00eda"}, {"title": "Instance-based counterfactual explanations for time series classification", "author": "E Delaney"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Introduction to explainable AI", "author": "QV Liao"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh"}, {"title": "Soliciting stakeholders' fairness notions in child maltreatment predictive systems", "author": "HF Cheng"}, {"title": "Fairness perceptions of algorithmic decision-making: A systematic review of the empirical literature", "author": "C Starke"}, {"title": "\" Ignorance and Prejudice\" in Software Fairness", "author": "JM Zhang"}, {"title": "Do i look like a criminal? examining how race presentation impacts human judgement of recidivism", "author": "K Mallari"}, {"title": "Ethical machines: The human-centric use of artificial intelligence", "author": "B Lepri", "abstract": "Today's increased availability of large amounts of human behavioral data and advances in artificial intelligence (AI) are contributing to a growing reliance on algorithms to make consequential decisions for humans, including those related to access to credit or medical treatments, hiring, etc. Algorithmic decision-making processes might lead to more objective decisions than those made by humans who may be influenced by prejudice, conflicts of interest, or fatigue. However, algorithmic decision-making has been criticized for its potential", "keywords": {"keywords": ["ethical machines", "human-centric", "artificial intelligence", "algorithmic decision-making", "consequential decisions"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["ethical AI"]}}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Conceptualising contestability: Perspectives on contesting algorithmic decisions", "author": "H Lyons"}, {"title": "Learning to rationalize for nonmonotonic reasoning with distant supervision", "author": "F Brahman"}, {"title": "An algorithmic framework for fairness elicitation", "author": "C Jung"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "A framework and benchmarking study for counterfactual generating methods on tabular data", "author": "RMB de Oliveira"}, {"title": "A study on fairness and trust perceptions in automated decision making", "author": "J Schoeffer"}, {"title": "On the relation of trust and explainability: Why to engineer for trustworthiness", "author": "L K\u00e4stner", "abstract": "Recently, requirements for the explainability of software systems have gained prominence. One of the primary motivators for such requirements is that explainability is expected to facilitate stakeholders' trust in a system. Although this seems intuitively appealing, recent psychological studies indicate that explanations do not necessarily facilitate trust. Thus, explainability requirements might not be suitable for promoting trust. One way to accommodate this finding is, we suggest, to focus on trustworthiness instead of trust. While", "keywords": {"keywords": ["trust", "explainability", "engineering", "trustworthiness", "stakeholders"], "research type": ["theoretical"], "methodology": [], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["software systems"]}}, {"title": "Show or suppress? Managing input uncertainty in machine learning model explanations", "author": "D Wang"}, {"title": "Can explainable AI explain unfairness? A framework for evaluating explainable AI", "author": "K Alikhademi"}, {"title": "Patients' perceptions of using artificial intelligence (AI)-based technology to comprehend radiology imaging data", "author": "Z Zhang"}, {"title": "Position: We Can Measure XAI Explanations Better with Templates.", "author": "J Dodge"}, {"title": "Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits", "author": "B Richardson"}, {"title": "Adversarial xai methods in cybersecurity", "author": "A Kuppa"}, {"title": "An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability", "author": "F Sovrano"}, {"title": "Personalization finder: A search interface for identifying and self-controlling web search personalization", "author": "Y Yamamoto"}, {"title": "Evidence-based explanation to promote fairness in AI systems", "author": "JJ Ferreira"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik", "abstract": "The interdisciplinary field of explainable artificial intelligence (XAI) aims to foster human understanding of black-box machine learning models through explanation-generating methods. Although the social sciences suggest that explanation is a social and iterative process between an explainer and an explainee, explanation user interfaces and their user interactions have not been systematically explored in XAI research yet. Therefore, we review prior XAI research containing explanation user interfaces for ML-based intelligent systems", "keywords": {"keywords": ["explainable artificial intelligence", "explanation user interfaces", "human-XAI interaction", "review", "design principles"], "research type": "review", "methodology": "qualitative", "purpose": "exploratory", "discipline": "computer science", "content": "methodological", "application area": "XAI"}}, {"title": "Perceptions of fairness and trustworthiness based on explanations in human vs. automated decision-making", "author": "J Schoeffer"}, {"title": "Toward XAI for Intelligent Tutoring Systems: a case study", "author": "V Putnam"}, {"title": "Play MNIST for me! User studies on the effects of post-hoc, example-based explanations & error rates on debugging a deep learning, black-box classifier", "author": "C Ford"}, {"title": "Appropriate fairness perceptions? On the effectiveness of explanations in enabling people to assess the fairness of automated decision systems", "author": "J Schoeffer"}, {"title": "The Role of Accuracy in Algorithmic Process Fairness Across Multiple Domains", "author": "M Albach"}, {"title": "Bridging the gap between explainable AI and uncertainty quantification to enhance trustability", "author": "D Seu\u00df"}, {"title": "Global explanations with local scoring", "author": "M Setzu"}, {"title": "Hybrid intelligence-combining the human in the loop with the computer in the loop: a systematic literature review", "author": "C Wiethof"}, {"title": "Towards the development of artificial intelligence-based systems: Human-centered functional requirements and open problems", "author": "TM Fagbola"}, {"title": "From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence", "author": "DA Melis", "abstract": "We take inspiration from the study of human explanation to inform the design and evaluation of interpretability methods in machine learning. First, we survey the literature on human explanation in philosophy, cognitive science, and the social sciences, and propose a list of design principles for machinegenerated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for generating explanations that adhere to these principles. We show that this method can be", "keywords": [["interpretability", 0.6874], ["explanations", 0.4467], ["information", 0.3462], ["explanation", 0.3356], ["inform", 0.3002], ["machinegenerated", 0.2629], ["learning", 0.2595], ["philosophy", 0.2461], ["cognitive", 0.2454], ["evaluation", 0.2392]]}, {"title": "The impact of explanations on AI competency prediction in VQA", "author": "K Alipour"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "Cognitive and Emotional Response to Fairness in AI\u2013A Systematic Review", "author": "J Baleis"}, {"title": "Capturing the Trends, Applications, Issues, and Potential Strategies of Designing Transparent AI Agents", "author": "L Sun"}, {"title": "Using Explainability to Help Children UnderstandGender Bias in AI", "author": "GI Melsi\u00f3n"}, {"title": "Improving users' mental model with attention\u2010directed counterfactual edits", "author": "K Alipour"}, {"title": "Model-agnostic explanations using minimal forcing subsets", "author": "X Han"}, {"title": "No explainability without accountability: an empirical study of explanations and feedback in interactive ML", "author": "A Smith"}, {"title": "\u201d Because AI is 100% right and safe\u201d: User Attitudes and Sources of AI Authority in India", "author": "S Kapania"}, {"title": "Position: Who Gets to Harness (X) AI? For Billion-Dollar Organizations Only.", "author": "J Dodge", "abstract": "Recently, researchers have made a number of tremendous advancements in AI capabilities. However, I will show that the fixed financial cost of making such advancements is high, and further, so are the recurring energy costs. As a result we see \u201cAI haves and have nots\u201d with wildly differing amounts of power between these two groups. This means our research community needs to carefully consider the role XAI has in mediating communication between stakeholders in the face of such an important power dynamic. This paper aims to", "keywords": {"keywords": ["AI", "financial cost", "energy costs", "AI haves and have nots", "XAI"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["AI power dynamics"]}}, {"title": "From human explanation to model interpretability: A framework based on weight of evidence", "author": "D Alvarez-Melis"}, {"title": "Position: The Case Against Case-Based Explanation.", "author": "J Dodge"}, {"title": "Fairness, explainability and in-between: understanding the impact of different explanation methods on non-expert users' perceptions of fairness toward an algorithmic \u2026", "author": "A Shulner-Tal"}, {"title": "Designing for the human in the loop: Transparency and control in interactive machine learning", "author": "A Renner"}, {"title": "Handling Climate Change Using Counterfactuals: Using Counterfactuals in Data Augmentation to Predict Crop Growth in an Uncertain Climate Future", "author": "M Temraz"}, {"title": "On fairness and interpretability", "author": "JM Jose"}, {"title": "How Do People Rank Multiple Mutant Agents?", "author": "J Dodge"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky"}, {"title": "On the Relationship Between Explanations, Fairness Perceptions, and Decisions", "author": "J Schoeffer"}, {"title": "A Framework for Fairness: A Systematic Review of Existing Fair AI Solutions", "author": "B Richardson", "abstract": "In a world of daily emerging scientific inquisition and discovery, the prolific launch of machine learning across industries comes to little surprise for those familiar with the potential of ML. Neither so should the congruent expansion of ethics-focused research that emerged as a response to issues of bias and unfairness that stemmed from those very same applications. Fairness research, which focuses on techniques to combat algorithmic bias, is now more supported than ever before. A large portion of fairness research has gone to", "keywords": {"keywords": ["fairness", "systematic review", "existing fair AI solutions", "algorithmic bias", "machine learning"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Let's Go to the Alien Zoo: Introducing an Experimental Framework to Study Usability of Counterfactual Explanations for Machine Learning", "author": "U Kuhl"}, {"title": "TExSS: Transparency and Explanations in Smart Systems", "author": "AM Smith-Renner"}, {"title": "How to explain AI systems to end users: a systematic literature review and research agenda", "author": "S Laato"}, {"title": "Towards trustworthy deception detection: Benchmarking model robustness across domains, modalities, and languages", "author": "M Glenski"}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation", "author": "V Lai"}, {"title": "Bias: Friend or foe? user acceptance of gender stereotypes in automated career recommendations", "author": "C Wang"}, {"title": "AI-TAM: a model to investigate user acceptance and collaborative intention inhuman-in-the-loop AI applications", "author": "I Baroni"}, {"title": "Features of Explainability: How users understand counterfactual and causal explanations for categorical and continuous features in XAI", "author": "G Warren"}, {"title": "Fairness perceptions of artificial intelligence decision-making", "author": "L JIANG"}, {"title": "Spotting When Algorithms Are Wrong", "author": "S Buijsman", "abstract": "Users of sociotechnical systems often have no way to independently verify whether the system output which they use to make decisions is correct; they are epistemically dependent on the system. We argue that this leads to problems when the system is wrong, namely to bad decisions and violations of the norm of practical reasoning. To prevent this from occurring we suggest the implementation of defeaters: information that a system is unreliable in a specific case (undercutting defeat) or independent information that the output", "keywords": {"keywords": ["sociotechnical systems", "algorithmic accuracy", "verification", "epistemic dependence", "defeaters"], "research type": ["theoretical"], "methodology": [], "purposes": ["explanatory", "prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["algorithmic decision-making"]}}, {"title": "Comparing Human Reasoning and Explainable AI", "author": "CJ Helgstrand"}, {"title": "Ranking for Decision Making: Fairness and Usability", "author": "C Kuhlman"}, {"title": "I humanize, therefore I understand? Effects of explanations and humanization of intelligent systems on perceived and objective user understanding", "author": "T Ngo"}, {"title": "Homophily and Incentive Effects in Use of Algorithms", "author": "R Fogliato"}, {"title": "What is the Bureaucratic Counterfactual? Categorical versus Algorithmic Prioritization in US Social Policy", "author": "RA Johnson"}, {"title": "Smart Contracts, Real-Virtual World Convergence and Economic Implications", "author": "W Lehr"}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "Explaining Artificial Intelligence with Tailored Interactive Visualisations", "author": "J Ooge"}, {"title": "Exploiting Domain-specific Knowledge for Classifier Learning-AU-based Facial Expression Analysis and Emotion Recognition", "author": "D Seuss"}, {"title": "Enhancing Fairness Perception\u2013Towards Human-Centred AI and Personalized Explanations Understanding the Factors Influencing Laypeople's Fairness Perceptions \u2026", "author": "A Shulner-Tal", "abstract": "Whether we like it or not, algorithmic decision-making systems (ADMSs) are all around us. These systems assist both public institutions and private organizations in making decisions that exert a significant impact on our lives. The widespread use of artificial intelligence (AI) and machine learning (ML) systems and the potential risks of using them are the subjects of intensive, ongoing research. It is imperative to ensure their fairness and transparency. The understanding that ADMSs should be subject to human supervision and examined for", "keywords": {"keywords": ["fairness perception", "algorithmic decision-making systems", "artificial intelligence", "machine learning", "transparency"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI ethics"]}}, {"title": "Fair Governance with Humans and Machines", "author": "Y Hermstr\u00fcwer"}, {"title": "Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating", "author": "B Smyth"}, {"title": "Conceptualising fairness: three pillars for medical algorithms and health equity", "author": "L Sikstrom"}, {"title": "Assessing Pain Research: A Narrative Review of Emerging Pain Methods, Their Technosocial Implications, and Opportunities for Multidisciplinary \u2026", "author": "SE Berger"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Fairness and Explanation in AI-Informed Decision Making", "author": "A Angerschmid"}, {"title": "Computational Perspectives on Democracy", "author": "A Kahng"}, {"title": "Explainable AI; Are we there yet?", "author": "N Tanwar"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "Computer says\u2026", "author": "N Wheeler", "abstract": "\u2019, a sketch entitled \"The computer says no!\u201d is about  computer says no\". The mother becomes  extremely annoyed, and storms out. This sketch shows what can happen when a computer", "keywords": {"keywords": ["computer", "sketch", "annoyance", "mother", "technology"], "research type": ["Case study"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Entertainment"]}}, {"title": "A Review of Taxonomies of Explainable Artificial Intelligence (XAI) Methods", "author": "T Speith"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Investigating Explanations that Target Training Data.", "author": "AI Anik"}, {"title": "Shahin: Faster Algorithms for Generating Explanations for Multiple Predictions", "author": "S Hasani"}, {"title": "Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty", "author": "J Jiang"}, {"title": "Social impacts of algorithmic decision-making: A research agenda for the social sciences", "author": "F Gerdon"}, {"title": "Data, Machine Learning, and Human Domain Experts: None Is Better than Their Collaboration", "author": "P Kumar"}, {"title": "Towards Involving End-users in Interactive Human-in-the-loop AI Fairness", "author": "Y Nakao"}, {"title": "Toward a decision process of the best machine learning model for multi-stakeholders: a crowdsourcing survey method", "author": "T Yokota"}, {"title": "Calibrating Trust in AI-Assisted Decision Making", "author": "A Turner", "abstract": "With the proliferation of AI products, humans and AI are increasingly working in partnership with each other to make decisions. For this type of collaboration to be successful, humans need to understand AI capability in order to effectively calibrate their trust. In these partnerships, it's critical to explain decisions and predictions in a manner that can be understood by humans in order to encourage trust calibration. The field of explainable AI is focused on integrating explainability into AI, but is geared towards making AI models more", "keywords": {"keywords": ["trust", "AI-assisted decision making", "explainable AI", "calibration", "partnerships"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI ethics"]}}, {"title": "Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations", "author": "J Dai"}, {"title": "Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness", "author": "Y Nakao"}, {"title": "HAEM: Obtaining Higher-Quality Classification Task Results with AI Workers", "author": "Y Yamashita"}, {"title": "\" If it didn't happen, why would I change my decision?\": How Judges Respond to Counterfactual Explanations for the Public Safety Assessment", "author": "Y Yacoby"}, {"title": "Algorithmic nudge to make better choices: Evaluating effectiveness of XAI frameworks to reveal biases in algorithmic decision making to users", "author": "P Juneja"}, {"title": "The challenges of providing explanations of AI systems when they do not behave like users expect", "author": "M Riveiro"}, {"title": "Efficient Construction and Explanation of Machine Learning Models through Database Techniques", "author": "S Hasani"}, {"title": "Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)", "author": "B Smyth"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning", "author": "C Kim", "abstract": "While the computer vision problem of searching for activities in videos is usually addressed by using discriminative models, their decisions tend to be opaque and difficult for people to understand. We propose a case study of a novel machine learning approach for generative searching and ranking of motion capture activities with visual explanation. Instead of directly ranking videos in the database given a text query, our approach uses a variant of Generative Adversarial Networks (GANs) to generate exemplars based on the query and uses them to", "keywords": {"keywords": ["generative machine learning", "visual explanation", "discriminative models", "motion capture activities", "Generative Adversarial Networks (GANs)"], "research type": "Case study", "methodology": "Quantitative", "purposes": "Descriptive", "discipline": "Computer science", "content": "Methodological", "application area": "Computer vision"}}, {"title": "The effect of explaining algorithmic decisions on the perception of fairness", "author": "BW van der Meulen"}, {"title": "Artificial Intelligence in Urban Planning", "author": "T Sanchez"}, {"title": "EXPLAINING DEVELOPMENTAL NEUROTOXICITY BY XAI", "author": "N TANWAR"}, {"title": "A Human-Centric Perspective on Fairness and Transparency in Algorithmic Decision-Making", "author": "J Schoeffer"}, {"title": "Human-Centered Evaluation of Explanations", "author": "J Boyd-Graber"}, {"title": "Fair Machine Learning in Healthcare: A Review", "author": "Q Feng"}, {"title": "Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretability", "author": "JJ Benjamin"}, {"title": "Exploring Bias Against Women in Artificial Intelligence: Practitioners' Views on Systems of Discrimination", "author": "CKT Bui"}, {"title": "Involve Humans in Algorithmic Fairness Issue: A Systematic Review", "author": "D Wu"}, {"title": "Do Humans Prefer Debiased AI Algorithms? A Case Study in Career Recommendation", "author": "C Wang", "abstract": "Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, eg along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when human-fair-AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real", "keywords": {"keywords": ["fairness", "AI algorithms", "human preferences", "interaction", "bias"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["career recommendation"]}}, {"title": "AI Fairness in the Financial Industry: A Machine Learning Pipeline Approach", "author": "J Xu"}, {"title": "Interpretable Machine Learning in Social Sciences: Use Cases and Limitations", "author": "A Suvorova"}, {"title": "User Acceptance of Gender Stereotypes in Automated Career Recommendations", "author": "C Wang"}, {"title": "Towards Explainability for AI Fairness", "author": "J Zhou"}, {"title": "Sensitivity to Risk Profiles of Users When Developing AI Systems", "author": "R Cohen"}, {"title": "Attention-based and Causal Explanations in Computer Vision", "author": "K Alipour"}, {"title": "The multisided complexity of fairness in recommender systems", "author": "N Sonboli"}, {"title": "Towards a Quality Model for Ai-Based Software", "author": "B Gezici"}, {"title": "Tell Me Something That Will Help Me Trust You: A Survey of Trust Calibration in Human-Agent Interaction", "author": "GJ Cancro"}, {"title": "Analytical modelling and UK Government policy", "author": "M Oldfield", "abstract": "In the last decade, the UK Government has attempted to implement improved processes and procedures in modelling and analysis in response to the Laidlaw report of 2012 and the Macpherson review of 2013. The Laidlaw report was commissioned after failings during the Intercity West Coast Rail (ICWC) Franchise procurement exercise by the Department for Transport (DfT) that led to a legal challenge of the analytical models used within the exercise. The Macpherson review looked into the quality assurance of Government", "keywords": {"keywords": ["analytical modelling", "UK Government policy", "Laidlaw report", "Macpherson review", "government quality assurance"], "research type": ["case study"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["sociology"], "content": ["Application"], "application area": ["public policy"]}}, {"title": "FairRover: explorative model building for fair and responsible machine learning", "author": "H Zhang"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "\" There Is Not Enough Information\": On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making", "author": "J Schoeffer"}, {"title": "Human-Centered Explainable AI (HCXAI): beyond opening the black-box of AI", "author": "U Ehsan"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}, {"title": "\u500b\u4eba\u306e\u91cd\u8981\u306a\u610f\u601d\u6c7a\u5b9a\u3092\u52a9\u3051\u308b\u6279\u5224\u53ef\u80fd\u306a\u30b7\u30b9\u30c6\u30e0\u306e\u5b9f\u73fe\u306b\u5411\u3051\u3066", "author": "\u4e2d\u5c3e\u60a0\u91cc"}, {"title": "La gestion algorithmique de la main d'\u0153uvre: mise en lumi\u00e8re des impacts sur les travailleurs et des bonnes pratiques", "author": "A Bujold"}, {"title": "\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u7684\u516c\u5e73\u611f\u77e5", "author": "\u848b\u8def\u8fdc\uff0c \u66f9\u674e\u6885\uff0c \u79e6\u6615\uff0c \u8c2d\u73b2\uff0c \u9648\u6668\uff0c \u5f6d\u5c0f\u6590"}, {"title": "Structuration de donn\u00e9es multidimensionnelles: une approche bas\u00e9e instance pour l'exploration de donn\u00e9es m\u00e9dicales", "author": "J Falip"}, {"title": "\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u7684\u7814\u7a76\u7efc\u8ff0", "author": "\u5b54\u7965\u7ef4\uff0c \u5510\u946b\u6cfd\uff0c \u738b\u5b50\u660e", "abstract": "The performance of decision making by artificial intelligence has exceeded the capability of the human being in many specific domains. Countries like China and the USA have promulgated artificial intelligence development strategies and action plans to encourage the applications of artificial intelligence. In the artificial intelligence decision-making process, the inherent black-box algorithms and opaque system information lead to highly correct but incomprehensible results, which hinder the further development of artificial intelligence. For", "keywords": {"keywords": ["artificial intelligence", "decision making", "explanation", "interpretablility", "research review"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["technical"], "application area": ["artificial intelligence"]}}], "i can do better than your ai: expertise and explanations": [{"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making", "author": "Y Zhang", "abstract": "Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to", "keywords": {"keywords": ["AI-assisted decision making", "confidence", "explanation", "accuracy", "trust calibration"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Decision Making"]}}, {"title": "Evolution and revolution: Personality research for the coming world of robots, artificial intelligence, and autonomous systems", "author": "G Matthews"}, {"title": "Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks", "author": "Z Lu"}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable AI", "author": "M Chromik"}, {"title": "Trust in smart personal assistants: A systematic literature review and development of a research agenda", "author": "N Zierau"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh"}, {"title": "The effects of controllability and explainability in a social recommender system", "author": "CH Tsai"}, {"title": "Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition", "author": "M Nourani"}, {"title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies", "author": "O Vereschak"}, {"title": "Toward personalized XAI: A case study in intelligent tutoring systems", "author": "C Conati"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro", "abstract": "Research in the social sciences has shown that expectations are an important factor in explanations as used between humans: rather than explaining the cause of an event per se, the explainer will often address another event that did not occur but that the explainee might have expected. For AI-powered systems, this finding suggests that explanation-generating systems may need to identify such end user expectations. In general, this is a challenging task, not the least because users often keep them implicit; there is thus a need to investigate", "keywords": {"keywords": ["expectations", "explanations", "AI systems", "end users", "implicit"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI systems"]}}, {"title": "Finding AI's faults with AAR/AI: An empirical study", "author": "R Khanna"}, {"title": "Brokerbot: A cryptocurrency chatbot in the social-technical gap of trust", "author": "M Lee"}, {"title": "Mind the (persuasion) gap: Contrasting predictions of intelligent DSS with user beliefs to improve interpretability", "author": "M Chromik"}, {"title": "Toward XAI for Intelligent Tutoring Systems: a case study", "author": "V Putnam"}, {"title": "A social evaluation of the perceived goodness of explainability in machine learning", "author": "J Wanner"}, {"title": "Micro-entries: Encouraging deeper evaluation of mental models over time for interactive data systems", "author": "JE Block"}, {"title": "Alert override patterns with a medication clinical decision support system in an academic emergency department: retrospective descriptive study", "author": "J Yoo"}, {"title": "Controllability and explainability in a hybrid social recommender system", "author": "CH Tsai"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models", "author": "A Rechkemmer", "abstract": "Previous research shows that laypeople's trust in a machine learning model can be affected by both performance measurements of the model on the aggregate level and performance estimates on individual predictions. However, it is unclear how people would trust the model when multiple performance indicators are presented at the same time. We conduct an exploratory human-subject experiment to answer this question. We find that while the level of model confidence significantly affects people's belief in model accuracy, both the model's", "keywords": {"keywords": ["confidence", "accuracy", "trust", "machine learning models", "performance indicators"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Trust in machine learning models"]}}, {"title": "Hive: evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainable", "author": "A Simkute"}, {"title": "The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending", "author": "M Dikmen"}, {"title": "Amplifying domain expertise in clinical data pipelines", "author": "P Rahman"}, {"title": "Toward Mindful Machines: A Wizard-of-Oz Dialogue Study", "author": "K Funakoshi"}, {"title": "Explainable NLP for Human-AI Collaboration", "author": "P Lertvittayakumjorn"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky"}, {"title": "Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems", "author": "M Nourani"}, {"title": "Amplifying Domain Expertise in Medical Data Pipelines", "author": "P Rahman"}, {"title": "Identifying critical design elements for increasing trust in computers through co-designing XAI for a mobile banking app", "author": "A Blandin", "abstract": "Explainable Artificial Intelligence (XAI) has established itself as a central tool for developing advanced AI systems in even high-stakes domains, as techniques of explanation have underpinned an increased trust in their deployment by experts. While there is excellent research and ongoing developments in this area, it was felt that there was insufficient understanding of how to apply XAI properly when the explanations were not designed for experts. To develop this understanding, particularly in the area of trust where XAI markets", "keywords": {"keywords": ["Explainable Artificial Intelligence", "trust", "computers", "co-designing", "mobile banking app"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Mobile banking"]}}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "A Cognitive Work Analysis Approach to Explainable Artificial Intelligence in Non-Expert Financial Decision-Making", "author": "M Dikmen"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.0", "author": "JP Wanner"}, {"title": "It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI", "author": "A Papenmeier"}, {"title": "Transparent-AI Blueprint: Developing a Conceptual Tool to Support the Design of Transparent AI Agents", "author": "Z Zhou"}, {"title": "A Diary Study of Social Explanations for Recommendations in Daily Life", "author": "Z Zhang"}, {"title": "Assessing the communication gap between AI models and healthcare professionals: explainability, utility and trust in AI-driven clinical decision-making", "author": "O Wysocki"}, {"title": "How do People Perceive the Role of AI in Human-AI Collaboration to Solve Everyday Tasks?", "author": "E Papachristos"}, {"title": "Journal of Responsible Technology", "author": "A Simkute"}, {"title": "Understanding User Perception of Explainable Algorithmic Decision-Making Systems: A Systematic Literature Review", "author": "S Bae", "abstract": "Improving the accuracy of algorithmic prediction has gained attention in Information Systems research in recent decades. Information systems which include algorithmic prediction have been seen to provide organisational value. However, as decisions based on these opaque algorithms become more ubiquitous, public demand for explanations for its output have naturally increased. This review evaluates research that examines the impact of providing explanations for the predictions made by algorithms, on how users respond to the", "keywords": {"keywords": ["Explainable algorithmic decision-making systems", "User perception", "Systematic literature review", "Algorithmic prediction", "Explanations"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Information Systems"], "content": ["Methodological"], "application area": ["AI Ethics"]}}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang"}, {"title": "Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning", "author": "C Kim"}, {"title": "Assessing and Finding Faults in AI: Two Empirical Studies", "author": "R Khanna"}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications", "author": "M Nourani"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand"}, {"title": "Trust and AI in Clinical Decision Support", "author": "B Wilson"}, {"title": "Tell Me Something That Will Help Me Trust You: A Survey of Trust Calibration in Human-Agent Interaction", "author": "GJ Cancro"}, {"title": "Structuration de donn\u00e9es multidimensionnelles: une approche bas\u00e9e instance pour l'exploration de donn\u00e9es m\u00e9dicales", "author": "J Falip"}], "the effect of explanations and algorithmic accuracy on visual recommender systems of artistic images": [{"title": "What are people doing about XAI user experience? A survey on AI explainability research and practic", "author": "JJ Ferreira", "abstract": "Explainability is a hot topic nowadays for artificial intelligent (AI) systems. The role of machine learning (ML) models on influencing human decisions shed light on the back-box of computing systems. AI based system are more than just ML models. ML models are one element for the AI explainability'design and needs to be combined with other elements so it can have significant meaning for people using AI systems. There are different goals and motivations for AI explainability. Regardless the goal for AI explainability, there are more to", "keywords": [["explainability", 0.7121], ["ai", 0.4927], ["ml", 0.341], ["intelligent", 0.2966], ["machine", 0.2767], ["learning", 0.2416], ["models", 0.2386], ["artificial", 0.2372], ["computing", 0.237], ["systems", 0.2333]]}, {"title": "A review of recent deep learning approaches in human-centered machine learnin", "author": "T Kaluarachchi"}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checker", "author": "CH Tsai"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risk", "author": "M Naiseh"}, {"title": "Detailing explanations in the recommender system based on matching temporal knowledg", "author": "S Chalyi"}, {"title": "Designing with AI for digital marketin", "author": "M Sinha"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interface", "author": "M Chromik"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluation", "author": "F Sperrle"}, {"title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis", "author": "CH Tsai"}, {"title": "Towards a Human-Centric Design Framework for AI Assisted Music Productio", "author": "A Tsiros"}, {"title": "Why or why not? The effect of justification styles on chatbot recommendation", "author": "D Wilkinson", "abstract": "Chatbots or conversational recommenders have gained increasing popularity as a new paradigm for Recommender Systems (RS). Prior work on RS showed that providing explanations can improve transparency and trust, which are critical for the adoption of RS. Their interactive and engaging nature makes conversational recommenders a natural platform to not only provide recommendations but also justify the recommendations through explanations. The recent surge of interest inexplainable AI enables diverse styles of", "keywords": {"keywords": ["chatbot recommendation", "justification styles", "transparency", "trust", "explainable AI"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Recommender Systems"]}}, {"title": "NASA-TLX\u2013based workload assessment for academic resource recommender syste", "author": "AH Afridi"}, {"title": "Towards the Human-Centered Design of Intelligent Agents in Medical Imaging Diagnosi", "author": "FM Calisto"}, {"title": "Explaining Autonomous Decisions in Swarms of Human-on-the-Loop Small Unmanned Aerial System", "author": "A Agrawal"}, {"title": "Algorithmic and HCI Aspects for Explaining Recommendations of Artistic Image", "author": "V Dominguez"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender System", "author": "M Radensky"}, {"title": "Preference prediction based on a photo gallery analysis with scene recognition and object detectio", "author": "AV Savchenko"}, {"title": "BreastScreening-AI: Evaluating medical intelligent agents for human-AI interaction", "author": "FM Calisto"}, {"title": "A survey on effects of adding explanations to recommender system", "author": "A Vultureanu\u2010Albi\u015fi"}, {"title": "Interpretable video tag recommendation with multimedia deep learning framewor", "author": "Z Yang"}, {"title": "I humanize, therefore I understand? Effects of explanations and humanization of intelligent systems on perceived and objective user understandin", "author": "T Ngo", "abstract": "The functioning of intelligent systems can be opaque to users. Yet, users need to make informed choices about them. This work compares two knowledge mechanisms, ie, ways for users to achieve an understanding of intelligent systems: explanation and humanization. In online experiment (N= 416), we compared the effects of a control condition without any explanation against a) a neutral and b) a humanized how-explanations as well as c) active humanization on (perceived and objective) user understanding and systems perceptions", "keywords": {"keywords": ["intelligent systems", "explanations", "humanization", "user understanding", "systems perceptions"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Human-computer interaction"]}}, {"title": "First Workshop on Adaptive and Personalized Explainable User Interfaces (APEx-UI 2022", "author": "E Purificato"}, {"title": "Intelligent User Interfaces: Trends and application area", "author": "T Helldin"}, {"title": "The risks of opening the car bonnet: A semi-systematic review of explainability in AI system", "author": "CE Serrano"}, {"title": "Chile's new interdisciplinary institute for foundational research on dat", "author": "M Arenas"}, {"title": "Explain and Conquer: Personalised Text-based Reviews to Achieve Transparenc", "author": "ILR Botana"}, {"title": "Understanding User Perception of Explainable Algorithmic Decision-Making Systems: A Systematic Literature Revie", "author": "S Bae"}, {"title": "Similarity-Based Explanations meet Matrix Factorization via Structure-Preserving Embedding", "author": "LB Marinho"}, {"title": "Hot Topics in Intelligent User Interface", "author": "C Ozcan"}, {"title": "\u0406\u041d\u0424\u041e\u0420\u041c\u0410\u0426\u0406\u0419\u041d\u0410 \u0422\u0415\u0425\u041d\u041e\u041b\u041e\u0413\u0406\u042f \u041f\u041e\u0411\u0423\u0414\u041e\u0412\u0418 \u041f\u041e\u042f\u0421\u041d\u0415\u041d\u042c \u0417 \u0423\u0420\u0410\u0425\u0423\u0412\u0410\u041d\u041d\u042f\u041c \u0422\u0415\u041c\u041f\u041e\u0420\u0410\u041b\u042c\u041d\u0418\u0425 \u0417\u041c\u0406\u041d \u0423 \u0412\u0418\u041c\u041e\u0413\u0410\u0425 \u041a\u041e\u0420\u0418\u0421\u0422\u0423\u0412\u0410\u0427\u0406\u0412 \u0420\u0415\u041a\u041e\u041c\u0415\u041d\u0414\u0410\u0426\u0406\u0419\u041d\u041e\u0407 ", "author": "S Chalyi"}, {"title": "\u0422\u0435\u043c\u043f\u043e\u0440\u0430\u043b\u044c\u043d\u0456 \u043f\u0430\u0442\u0435\u0440\u043d\u0438 \u0432\u043f\u043e\u0434\u043e\u0431\u0430\u043d\u044c \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0456\u0432 \u0432 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u0444\u043e\u0440\u043c\u0443\u0432\u0430\u043d\u043d\u044f \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u044c \u0432 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0456\u0439\u043d\u0456\u0439 \u0441\u0438\u0441\u0442\u0435\u043c", "author": "\u0421\u0424 \u0427\u0430\u043b\u0438\u0439", "abstract": "\u0420\u043e\u0437\u0433\u043b\u044f\u043d\u0443\u0442\u043e \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0432\u0440\u0430\u0445\u0443\u0432\u0430\u043d\u043d\u044f \u0437\u043c\u0456\u043d \u0432\u0438\u043c\u043e\u0433 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0456\u0439\u043d\u043e\u0457 \u0441\u0438\u0441\u0442\u0435\u043c\u0438 \u043f\u0440\u0438 \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0456 \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u044c \u0449\u043e\u0434\u043e \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0456\u0439. \u0426\u044f \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u043c\u043e\u0436\u0435 \u0432\u0438\u043d\u0438\u043a\u043d\u0443\u0442\u0438 \u0432 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0456 \u0446\u0438\u043a\u043b\u0456\u0447\u043d\u0438\u0445 \u0437\u043c\u0456\u043d \u0432\u0438\u043c\u043e\u0433 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430. \u0407\u0457 \u0440\u0456\u0448\u0435\u043d\u043d\u044f \u043f\u043e\u0432'\u044f\u0437\u0430\u043d\u0435 \u0437 \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u043e\u044e \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u043d\u044f, \u0449\u043e \u043f\u043e\u0440\u0456\u0432\u043d\u044e\u0454 \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u043d\u0456 \u0432\u0430\u0440\u0456\u0430\u043d\u0442\u0438 \u0432\u0438\u0431\u043e\u0440\u0443 \u0442\u0430\u043a\u043e\u0433\u043e \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430. \u0420\u043e\u0437\u0440\u043e\u0431\u043b\u0435\u043d\u0456 \u043c\u043e\u0434\u0435\u043b\u0456 \u0442\u0435\u043c\u043f\u043e\u0440\u0430\u043b\u044c\u043d\u0438\u0445 \u043f\u0430\u0442\u0435\u0440\u043d\u0456\u0432 \u0441\u043a\u043b\u0430\u0434\u0430\u044e\u0442\u044c\u0441\u044f \u0437 \u043c\u043d\u043e\u0436\u0438\u043d\u0438 \u0442\u0435\u043c\u043f\u043e\u0440\u0430\u043b\u044c\u043d\u0438\u0445 \u0432\u0456\u0434\u043d\u043e\u0448\u0435\u043d\u044c \u043c\u0456\u0436 \u043f\u043e\u0434\u0456\u044f\u043c\u0438 \u0432\u0438\u0431\u043e\u0440\u0443 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430\u043c\u0438 \u0442\u043e\u0432\u0430\u0440\u0456\u0432 \u0442\u0430 \u043f\u043e\u0441\u043b\u0443\u0433. \u041f\u0435\u0440\u0448\u0438\u0439 \u043f\u0430\u0442\u0435\u0440\u043d \u043c\u0456\u0441\u0442\u0438\u0442\u044c \u0430\u043b\u044c\u0442\u0435\u0440\u043d\u0430\u0442\u0438\u0432\u0438 \u0443", "keywords": {"keywords": ["\u0432\u0438\u043c\u043e\u0433 \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0430", "\u043f\u043e\u044f\u0441\u043d\u0435\u043d\u043d\u044f", "\u043f\u0430\u0442\u0435\u0440\u043d\u0438", "\u0442\u043e\u0432\u0430\u0440\u0438", "\u043f\u043e\u0441\u043b\u0443\u0433\u0438"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Recommendation Systems"]}}, {"title": "\u0423\u0417\u0413\u041e\u0414\u0416\u0415\u041d\u041d\u042f \u041f\u041e\u042f\u0421\u041d\u0415\u041d\u041d\u042f \u0412 \u0406\u041d\u0422\u0415\u041b\u0415\u041a\u0422\u0423\u0410\u041b\u042c\u041d\u0406\u0419 \u0406\u041d\u0424\u041e\u0420\u041c\u0410\u0426\u0406\u0419\u041d\u0406\u0419 \u0421\u0418\u0421\u0422\u0415\u041c\u0406 \u0417 \u0411\u0410\u0417\u041e\u0412\u0418\u041c\u0418 \u0417\u041d\u0410\u041d\u041d\u042f\u041c\u0418 \u0429\u041e\u0414\u041e \u041f\u0420\u0415\u0414\u041c\u0415\u0422\u041d\u041e\u0407 \u041e\u0411\u041b\u0410\u0421\u0422", "author": "S Chalyi"}], "the effects of example-based explanations in a machine learning interface": [{"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao", "abstract": "A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm", "keywords": {"keywords": ["explainable AI", "user experiences", "design practices", "XAI", "algorithm"], "research type": ["empirical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["AI products"]}}, {"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making", "author": "Y Zhang"}, {"title": "Human-centered tools for coping with imperfect algorithms during medical decision-making", "author": "CJ Cai"}, {"title": "\" Hello AI\": uncovering the onboarding needs of medical practitioners for human-AI collaborative decision-making", "author": "CJ Cai"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "Evaluating saliency map explanations for convolutional neural networks: a user study", "author": "A Alqaraawi"}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "How do visual explanations foster end users' appropriate trust in machine learning?", "author": "F Yang"}, {"title": "The state of the art in enhancing trust in machine learning models with the use of visualizations", "author": "A Chatzimparmpas"}, {"title": "No explainability without accountability: An empirical study of explanations and feedback in interactive ml", "author": "A Smith-Renner"}, {"title": "COGAM: measuring and moderating cognitive load in machine learning model explanations", "author": "A Abdul", "abstract": "Interpretable machine learning models trade-off accuracy for simplicity to make explanations more readable and easier to comprehend. Drawing from cognitive psychology theories in graph comprehension, we formalize readability as visual cognitive chunks to measure and moderate the cognitive load in explanation visualizations. We present Cognitive-GAM (COGAM) to generate explanations with desired cognitive load and accuracy by combining the expressive nonlinear generalized additive models (GAM) with simpler sparse linear", "keywords": {"keywords": ["COGAM", "cognitive load", "machine learning model explanations", "interpretable machine learning models", "visual cognitive chunks"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning model explanations"]}}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies", "author": "EM Kenny"}, {"title": "Relatif: Identifying explanatory training samples via relative influence", "author": "E Barshan"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "What are people doing about XAI user experience? A survey on AI explainability research and practice", "author": "JJ Ferreira"}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "What is\" intelligent\" in intelligent user interfaces? a meta-analysis of 25 years of IUI", "author": "ST V\u00f6lkel"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks", "author": "Z Lu"}, {"title": "Personalising explainable recommendations: literature and conceptualisation", "author": "M Naiseh", "abstract": "Explanations in intelligent systems aim to enhance a users' understandability of their reasoning process and the resulted decisions and recommendations. Explanations typically increase trust, user acceptance and retention. The need for explanations is on the rise due to the increasing public concerns about AI and the emergence of new laws, such as the General Data Protection Regulation (GDPR) in Europe. However, users are different in their needs for explanations, and such needs can depend on their dynamic context. Explanations", "keywords": {"keywords": ["explainable recommendations", "personalisation", "literature", "conceptualisation", "explanations"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["intelligent systems"]}}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "Exploring explainability: a definition, a model, and a knowledge catalogue", "author": "L Chazette"}, {"title": "Human-centered XAI: Developing design patterns for explanations of clinical decision support systems", "author": "TAJ Schoonderwoerd"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Explaining multimodal deceptive news prediction models", "author": "S Volkova"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh"}, {"title": "Evaluating machine learning algorithms for predicting maize yield under conservation agriculture in Eastern and Southern Africa", "author": "W Mupangwa"}, {"title": "Human-ai collaboration in a cooperative game setting: Measuring social perception and outcomes", "author": "Z Ashktorab"}, {"title": "A framework of artificial intelligence augmented design support", "author": "J Liao"}, {"title": "Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition", "author": "M Nourani", "abstract": "Explainable machine learning and artificial intelligence models have been used to justify a model's decision-making process. This added transparency aims to help improve user performance and understanding of the underlying model. However, in practice, explainable systems face many open questions and challenges. Specifically, designers might reduce the complexity of deep learning models in order to provide interpretability. The explanations generated by these simplified models, however, might not accurately justify and be truthful to", "keywords": {"keywords": ["Explainable AI", "Verifying Veracity", "Video Activity Recognition", "Transparency", "Deep learning models"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Transparency"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Introduction of human-centric AI assistant to aid radiologists for multimodal breast image classification", "author": "FM Calisto"}, {"title": "A study of automatic metrics for the evaluation of natural language explanations", "author": "M Clinciu"}, {"title": "Combined Cognitive-Motivational Modules Delivered via an LMS Increase Undergraduate Biology Grades.", "author": "JG Cromley"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "On the relation of trust and explainability: Why to engineer for trustworthiness", "author": "L K\u00e4stner"}, {"title": "AI should embody our values: Investigating journalistic values to inform AI technology design", "author": "T Komatsu"}, {"title": "From Philosophy to Interfaces: An Explanatory Method and a Tool Inspired by Achinstein's Theory of Explanation", "author": "F Sovrano"}, {"title": "Revealing neural network bias to non-experts through interactive counterfactual examples", "author": "CM Myers"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence", "author": "W Jin", "abstract": "The ability to explain decisions to its end-users is a necessity to deploy AI as critical decision support. Yet making AI explainable to end-users is a relatively ignored and challenging problem. To bridge the gap, we first identified twelve end-user-friendly explanatory forms that do not require technical knowledge to comprehend, including feature-, example-, and rule-based explanations. We then instantiated the explanatory forms as prototyping cards in four AI-assisted critical decision-making tasks, and conducted a user study to co-design low", "keywords": {"keywords": ["End-User-Centered", "Explainable Artificial Intelligence", "Prototyping Framework", "Critical Decision-Making", "User Study"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Prescriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Explainability"]}}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "Explainable recommendation: when design meets trust calibration", "author": "M Naiseh"}, {"title": "Achievements and challenges in explaining deep learning based computer-aided diagnosis systems", "author": "A Lucieri"}, {"title": "Alert override patterns with a medication clinical decision support system in an academic emergency department: retrospective descriptive study", "author": "J Yoo"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "Capturing the Trends, Applications, Issues, and Potential Strategies of Designing Transparent AI Agents", "author": "L Sun"}, {"title": "ChatrEx: Designing explainable chatbot interfaces for enhancing usefulness, transparency, and trust", "author": "A Khurana"}, {"title": "A social evaluation of the perceived goodness of explainability in machine learning", "author": "J Wanner"}, {"title": "How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI.", "author": "ZT Zhang", "abstract": "Given the opaqueness and complexity of modern AI algorithms, there is currently a strong focus on developing transparent and explainable AI, especially in high-stakes domains. We claim that opaqueness and complexity are not the core issues for end users when interacting with AI. Instead, we propose that the output uncertainty inherent to AI systems is the actual problem, with opaqueness and complexity as contributing factors. Transparency and explainability should therefore not be the end goals, as such a focus tends to place the", "keywords": {"keywords": ["Output Uncertainty", "AI algorithms", "Transparent AI", "Explainable AI", "End User Interactions"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["AI Interactions"]}}, {"title": "Intelligent Decision Assistance Versus Automated Decision-Making: Enhancing Knowledge Work Through Explainable Artificial Intelligence", "author": "M Schemmer"}, {"title": "EUCA: the End-User-Centered Explainable AI Framework", "author": "W Jin"}, {"title": "No explainability without accountability: an empirical study of explanations and feedback in interactive ML", "author": "A Smith"}, {"title": "Intuitively assessing ml model reliability through example-based explanations and editing model inputs", "author": "H Suresh"}, {"title": "Effect of confidence indicators on trust in AI-generated profiles", "author": "T Bruzzese"}, {"title": "Amplifying domain expertise in clinical data pipelines", "author": "P Rahman"}, {"title": "Non-monotonic Explanation Functions", "author": "L Amgoud"}, {"title": "Position: The Case Against Case-Based Explanation.", "author": "J Dodge"}, {"title": "Designing to Support Sensemaking in Cross-Lingual Computer-Mediated Communication Using NLP Techniques", "author": "H Lim"}, {"title": "2D Breast Cancer Diagnosis Explainable Visualizations", "author": "NL Mourao", "abstract": "In recent years, several AI systems are emerging in medical imaging. These systems have the objective of helping physicians in the decision-making process. Despite being present on the clinical workflow, their acceptance is not optimal. This occurs because there is ambiguity in the decision process, and physicians may consider those AI systems to be 'black-boxes'. With the introduction of a XAI technique into an AI system, physicians can explore the output (eg, image segmentation) and have a better understanding of the AI", "keywords": {"keywords": ["AI", "medical imaging", "decision-making", "explainable AI", "XAI"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Medical Imaging"], "content": ["Application"], "application area": ["Breast Cancer Diagnosis"]}}, {"title": "Designing for the human in the loop: Transparency and control in interactive machine learning", "author": "A Renner"}, {"title": "Algorithmic and HCI Aspects for Explaining Recommendations of Artistic Images", "author": "V Dominguez"}, {"title": "How Do People Rank Multiple Mutant Agents?", "author": "J Dodge"}, {"title": "Attitudes and Folk Theories of Data Subjects on Transparency and Accuracy in Emotion Recognition", "author": "G Grill"}, {"title": "Pilot attitudes toward AI in the cockpit: implications for design", "author": "ZT Zhang"}, {"title": "BreastScreening-AI: Evaluating medical intelligent agents for human-AI interactions", "author": "FM Calisto"}, {"title": "Learning Skills from Distance: A Solar Photovoltaic Site Survey from Students' View", "author": "A Masek"}, {"title": "Transparency and Trust in AI", "author": "N Scharowski"}, {"title": "CACTUS: Detecting and Resolving Conflicts in Objective Functions", "author": "S Das"}, {"title": "Amplifying Domain Expertise in Medical Data Pipelines", "author": "P Rahman", "abstract": "Digitization of medical documents has led to increased availability of data for analysis. This has induced domains to incorporate data-driven decision-making. However, going from data to decision-making involves a pipeline that can be broken into three stages: collection, cleaning, and analysis. The specialized nature of certain datasets, especially in the medical field, requires domain expertise at every pipeline step. Domain experts refer to individuals who are not necessarily trained in computational fields but are experts in the data domain", "keywords": {"keywords": ["medical data pipelines", "domain expertise", "data-driven decision-making", "specialized datasets", "domain experts"], "research type": ["theory"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["healthcare"]}}, {"title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?", "author": "RS Zimmermann"}, {"title": "Crowdsourcing Creative Work", "author": "J Oppenlaender"}, {"title": "Communicating about the Limitations of AI: The Effect of Message Framing and Ownership on the Trust of Artificial Intelligence", "author": "T Kima"}, {"title": "How to Guide Task-oriented Chatbot Users, and When: A Mixed-methods Study of Combinations of Chatbot Guidance Types and Timings", "author": "SF Yeh"}, {"title": "Task Decomposition and Role Sharing for Real-time Human-AI Swarm Collaboration", "author": "S Karakama"}, {"title": "Fairness and Explanation in AI-Informed Decision Making", "author": "A Angerschmid"}, {"title": "Contextualising local explanations for non-expert users: an XAI pricing interface for insurance.", "author": "C Bove"}, {"title": "The risk of algorithm transparency: How algorithm complexity drives the effects on use of advice", "author": "CA Lehmann"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "Transparent-AI Blueprint: Developing a Conceptual Tool to Support the Design of Transparent AI Agents", "author": "Z Zhou", "abstract": "With the increasing prevalence of artificial intelligence (AI) agents, the transparency of agents has become vital in addressing interaction issues (eg, trust, usefulness, and understandability). However, determining the transparency of AI agents requires a systematic consideration of complex related factors, including stakeholders, algorithms, context, etc. Thus, in our study, we presented an overview of studies on the transparency of AI agents through multiple-stage bibliometric analysis, and identified an ontological", "keywords": {"keywords": ["transparent AI agents", "artificial intelligence", "transparency", "interaction issues", "systematic consideration"], "research type": ["review"], "methodology": ["Quantitative", "qualitative"], "purposes": ["Exploratory", "descriptive"], "discipline": ["computer science"], "content": ["Methodological"], "application area": ["AI agent design"]}}, {"title": "Understanding the Role of Explanation Modality in AI-assisted Decision-making", "author": "V Robbemond"}, {"title": "Investigating Explanations that Target Training Data.", "author": "AI Anik"}, {"title": "Towards Automating Model Explanations with Certified Robustness Guarantees", "author": "M Huai"}, {"title": "AQX: Explaining Air Quality Forecast for Verifying Domain Knowledge using Feature Importance Visualization", "author": "R Palaniyappan Velumani"}, {"title": "Development of a Concept for Privacy Explanations and its Prototypical Evaluation", "author": "JRC Droste"}, {"title": "Explanation using examples", "author": "H Park"}, {"title": "It's Just a Recipe?\u2014Comparing Expert and Lay User Understanding of Algorithmic Systems", "author": "T Ngo"}, {"title": "Towards Self-Explainable Adaptive Systems (SEAS): A Requirements Driven Approach.", "author": "N Khalid"}, {"title": "Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task", "author": "B Leichtmann"}, {"title": "Understanding User Perception of Explainable Algorithmic Decision-Making Systems: A Systematic Literature Review", "author": "S Bae", "abstract": "Improving the accuracy of algorithmic prediction has gained attention in Information Systems research in recent decades. Information systems which include algorithmic prediction have been seen to provide organisational value. However, as decisions based on these opaque algorithms become more ubiquitous, public demand for explanations for its output have naturally increased. This review evaluates research that examines the impact of providing explanations for the predictions made by algorithms, on how users respond to the", "keywords": {"keywords": ["algorithmic decision-making", "explanation", "user perception", "systematic literature review", "organisational value"], "research type": ["literature review"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["Information Systems"], "content": ["methodological"], "application area": ["algorithmic prediction"]}}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Algorithmic nudge to make better choices: Evaluating effectiveness of XAI frameworks to reveal biases in algorithmic decision making to users", "author": "P Juneja"}, {"title": "Exploring preferences on chatbot's guidance type and timing", "author": "SF Yeh"}, {"title": "Communicating the Limitations of AI: The Effect of Message Framing and Ownership on Trust in Artificial Intelligence", "author": "T Kim"}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang"}, {"title": "Exploring Users' Preferences for Chatbot's Guidance Type and Timing", "author": "MH Wu"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning", "author": "C Kim"}, {"title": "A Game-Based Approach for Helping Designers Learn Machine Learning Concepts", "author": "CM Myers"}, {"title": "Publications-MIMBCD-UI/meta Wiki", "author": "FM Calisto", "abstract": "We are concerned about communicating our progress across the scientific community.  Therefore, when we achieve a potential contribution, we start to communicating our findings", "keywords": {"keywords": ["progress", "communication", "scientific community", "contribution", "findings"], "research type": ["review"], "methodology": ["NA"], "purposes": ["Descriptive"], "discipline": ["NA"], "content": ["Application"], "application area": ["science communication"]}}, {"title": "The Design and Evaluation of Neural Attention Mechanisms for Explaining Text Classifiers", "author": "S Carton"}, {"title": "Considerations for applying logical reasoning to explain neural network outputs", "author": "FM Cau"}, {"title": "Explainable Artificial Intelligence in Cybersecurity: A Brief Review", "author": "S Hariharan"}, {"title": "Designing Resilient AI-based Robo-Advisors: A Prototype for Real Estate Appraisal", "author": "M Schemmer"}, {"title": "A generic and adaptive approach to explainable AI in autonomic systems: the case of the smart home", "author": "E Houz\u00e9"}, {"title": "Towards a multi-stakeholder value-based assessment framework for algorithmic systems", "author": "M Yurrita"}, {"title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset", "author": "L Sixt"}, {"title": "Tell Me Something That Will Help Me Trust You: A Survey of Trust Calibration in Human-Agent Interaction", "author": "GJ Cancro"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Modeling Adoption of Intelligent Agents in Medical Imaging", "author": "FM Calisto", "abstract": "Artificial intelligence has the potential to transform many application domains fundamentally. One notable example is clinical radiology. A growing number of decision-making support systems are available for lesion detection and segmentation, two fundamental steps to accomplish diagnosis and treatment planning. This paper proposes a model based on the unified theory of acceptance and use of technology to study the determinants for the adoption of intelligent agents across the medical imaging workflow. We tested the model via", "keywords": {"keywords": ["intelligent agents", "medical imaging", "adoption", "decision-making support systems", "unified theory of acceptance and use of technology"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Clinical Radiology"]}}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Delegating Agency? The Effects of XAI, Personality Traits, and the Moral Significance of the Application on the Reliance on Autonomous Systems: A User Study", "author": "E Kallina"}, {"title": "Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis", "author": "E Schoop"}, {"title": "Erkl\u00e4rbare KI in der medizinischen Diagnose\u2013Erfolge und Herausforderungen", "author": "A Lucieri"}, {"title": "Structuration de donn\u00e9es multidimensionnelles: une approche bas\u00e9e instance pour l'exploration de donn\u00e9es m\u00e9dicales", "author": "J Falip"}], "to explain or not to explain: the effects of personal characteristics when explaining music recommendations": [{"title": "To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making", "author": "Z Bu\u00e7inca", "abstract": "People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI", "keywords": {"keywords": ["AI-assisted decision-making", "overreliance", "cognitive forcing functions", "explanations", "dual-process theory"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Decision-making"]}}, {"title": "MusicBot: Evaluating critiquing-based music recommenders with conversational interaction", "author": "Y Jin"}, {"title": "Recommender systems: an overview, research trends, and future directions", "author": "PK Singh"}, {"title": "What are people doing about XAI user experience? A survey on AI explainability research and practice", "author": "JJ Ferreira"}, {"title": "Effects of personal characteristics in control-oriented user interfaces for music recommender systems", "author": "Y Jin"}, {"title": "Personalising explainable recommendations: literature and conceptualisation", "author": "M Naiseh"}, {"title": "What's in a user? Towards personalising transparency for music recommender interfaces", "author": "M Millecamp"}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checkers", "author": "CH Tsai"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh"}, {"title": "Survey on individual differences in visualization", "author": "Z Liu", "abstract": "Developments in data visualization research have enabled visualization systems to achieve great general usability and application across a variety of domains. These advancements have improved not only people's understanding of data, but also the general understanding of people themselves, and how they interact with visualization systems. In particular, researchers have gradually come to recognize the deficiency of having one\u2010size\u2010fits\u2010all visualization interfaces, as well as the significance of individual differences in the use of data", "keywords": {"keywords": ["individual differences", "visualization", "usability", "application", "data"], "research type": ["Review"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Data Visualization"]}}, {"title": "Contextplay: Evaluating user control for context-aware music recommendation", "author": "Y Jin"}, {"title": "To explain or not to explain: the effects of personal characteristics when explaining feature-based recommendations in different domains", "author": "M Millecamp"}, {"title": "Generating and understanding personalized explanations in hybrid recommender systems", "author": "P Kouki"}, {"title": "Toward personalized XAI: A case study in intelligent tutoring systems", "author": "C Conati"}, {"title": "Users & machine learning-based curation systems", "author": "H Heuer"}, {"title": "Cogito ergo quid? The effect of cognitive style in a transparent mobile music recommender system", "author": "M Millecamp"}, {"title": "\u201cKnowing me, knowing you\u201d: personalized explanations for a music recommender system", "author": "M Martijn"}, {"title": "Exploring Tangible Algorithmic Imaginaries in Movie Recommendations", "author": "O Alvarado"}, {"title": "Service-aware interactive presentation of items for decision-making", "author": "N Mauro"}, {"title": "Exploring the need for transparency in educational recommender systems", "author": "J Barria-Pineda", "abstract": "Educational Recommender Systems (EdRecSys) are different in nature from conventional Recommender Systems (RecSys)--mostly related to e-commerce--as the main goal of EdRecSys is supporting students learning'instead of maximizing users' satisfaction from consuming the recommended items. Thus, research on transparency for traditional RecSys is hard to transfer from e-commerce contexts to educational scenarios, as the level of knowledge of the end-user (ie the student) is crucial for generating and evaluating the", "keywords": {"keywords": ["educational recommender systems", "transparency", "conventional recommender systems", "students learning", "end-user knowledge"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["education"], "content": ["technical"], "application area": ["academic research"]}}, {"title": "The GIFT of scrutable learner models: why and how", "author": "J Kay"}, {"title": "Open, Scrutable and Explainable Interest Models for Transparent Recommendation.", "author": "M Guesmi"}, {"title": "Understanding the Effects of Explanation Types and User Motivations on Recommender System Use", "author": "Q Li"}, {"title": "Explainability in Music Recommender Systems", "author": "D Afchar"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "LEMONS: Listenable Explanations for Music recOmmeNder Systems", "author": "AB Melchiorre"}, {"title": "Input or Output: Effects of Explanation Focus on the Perception of Explainable Recommendation with Varying Level of Details.", "author": "M Guesmi"}, {"title": "Controllability and explainability in a hybrid social recommender system", "author": "CH Tsai"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "Interactive Music Genre Exploration with Visualization and Mood Control", "author": "Y Liang", "abstract": "Recommender systems can be used to help users discover novel items and explore new tastes, for example in music genre exploration. However, little work has studied how to improve users' understandability and acceptance of the novel items as well as support users to explore a new domain. In this paper, we investigate how two different visualizations and mood control influence the perceived control, informativeness and understandability of a music genre exploration tool, and further to improve the helpfulness for new music genre", "keywords": {"keywords": ["music genre exploration", "visualization", "mood control", "perceived control", "informativeness"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Music Recommendation Tool"]}}, {"title": "On-demand personalized explanation for transparent recommendation", "author": "M Guesmi"}, {"title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis.", "author": "CH Tsai"}, {"title": "Beyond explaining single item recommendations", "author": "N Tintarev"}, {"title": "Explainable artificial intelligence in education", "author": "H Khosravi"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainable", "author": "A Simkute"}, {"title": "Why or why not? The effect of justification styles on chatbot recommendations", "author": "D Wilkinson"}, {"title": "Music Recommendation Systems: Techniques, Use Cases, and Challenges", "author": "M Schedl"}, {"title": "The Moderating Effect of Active Engagement on Appreciation of Popularity in Song Recommendations", "author": "MP Graus"}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trust", "author": "J Ooge"}, {"title": "A Multi-Dimensional Conceptualization Framework for Personalized Explanations in Recommender Systems", "author": "QU Ain", "abstract": "Recommender systems (RS) have become an integral component of our daily lives by helping decision making easier for us. The use of recommendations has, however, increased the demand for explanations that are convincing enough to help users trust the provided recommendations. The recommendations are desired by the users to be understandable as well as personalized to their individual needs and preferences. Research on personalized explainable recommendation has emerged only recently. To help", "keywords": {"keywords": ["personalized explanations", "recommender systems", "trust", "understandable", "user preferences"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommendation systems"]}}, {"title": "Personal user interfaces for recommender systems", "author": "M Millecamp"}, {"title": "Depicting recommendations in academia: how ResearchGate communicates with its users (via design or upon request) about recommender algorithms", "author": "L Monteiro-Krebs"}, {"title": "Trespassing the gates of research: identifying algorithmic mechanisms that can cause distortions and biases in academic social media", "author": "L Monteiro-Krebs"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky"}, {"title": "Classifeye: classification of personal characteristics based on eye tracking data in a recommender system interface", "author": "M Millecamp"}, {"title": "Music recommendation systems: A survey", "author": "M Kle\u0107"}, {"title": "Reinforcement Learning over Sentiment-Augmented Knowledge Graphs towards Accurate and Explainable Recommendation", "author": "SJ Park"}, {"title": "The Use of Responsible Artificial Intelligence Techniques in the Context of Loan Approval Processes", "author": "E Purificato"}, {"title": "Generating User-Centred Explanations via Illocutionary Question Answering: From Philosophy to Interfaces", "author": "F Sovrano"}, {"title": "First Workshop on Adaptive and Personalized Explainable User Interfaces (APEx-UI 2022)", "author": "E Purificato", "abstract": "Adaptation and personalization are crucial aspects of the design and development of successful Artificial Intelligence systems, from search engines and recommender systems to wearable devices. The increased desire for customization inevitably leads to the need for the end-user to understand the rationale behind displaying that specific tailored content. User interfaces play a central role to provide the right explanations to the end-users. While adaptive and personalized user interfaces are well-known and advanced research fields, a", "keywords": {"keywords": ["adaptive", "personalized", "explainable user interfaces", "AI systems", "explanations"], "research type": "Theoretical", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Application", "application area": "Artificial Intelligence"}}, {"title": "Explaining Health Recommendations to Lay Users: The Dos and Dont's.", "author": "M Szymanski"}, {"title": "Enhancing Fairness Perception\u2013Towards Human-Centred AI and Personalized Explanations Understanding the Factors Influencing Laypeople's Fairness Perceptions \u2026", "author": "A Shulner-Tal"}, {"title": "Exploring Tangible Algorithmic Imaginaries in Movie Recommendations", "author": "OL Alvarado Rodriguez"}, {"title": "Improving Fairness and Transparency for Artists in Music Recommender Systems", "author": "K Dinnissen"}, {"title": "TastePaths: Enabling Deeper Exploration and Understanding of Personal Preferences in Recommender Systems", "author": "S Petridis"}, {"title": "Exploring the Effects of Interactive Dialogue in Improving User Control for Explainable Online Symptom Checkers", "author": "Y Sun"}, {"title": "Erkl\u00e4rung der Selbstst\u00e4ndigkeit", "author": "Z Wang"}, {"title": "Incorporating Personality Traits in User Modeling for EUD", "author": "F Cena"}, {"title": "Visually Explaining Uncertain Price Predictions in Agrifood: A User-Centred Case-Study", "author": "J Ooge"}, {"title": "Explaining music recommendations: the effects of user feedback on explanations.", "author": "JM Geelhoed", "abstract": "This thesis used a fully operating content-based music recommender system (MRS) to examine the effects of adding feedback options to explanations. Explanations are an often used in recommender systems to for example, justify suggestions, increase satisfaction, or increase trust in the system (Tintarev & Masthoff, 2015). However, if an explanation isn't correct according to the user this could have counterproductive effects. Being able to provide the recommender feedback on why the explanation is not correct could help overcome this", "keywords": {"keywords": ["music recommendations", "user feedback", "explanations", "content-based music recommender system", "counterproductive effects"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Music recommendation"]}}, {"title": "A Diary Study of Social Explanations for Recommendations in Daily Life", "author": "Z Zhang"}, {"title": "Journal of Responsible Technology", "author": "A Simkute"}, {"title": "\u201cYou might also like\u201d-The technological consumers understanding of transparent AI", "author": "EK Haugstulen"}, {"title": "Beyond-Accuracy Perspectives on Graph Neural Network-Based Models for Behavioural User Profiling", "author": "E Purificato"}, {"title": "When Does Information Transparency Reduce Downside of Personalization? Role of Need for Cognition and Perceived Control", "author": "L Lambillotte"}, {"title": "Evaluating Explainable Interfaces for a Knowledge Graph-Based Recommender System.", "author": "E Purificato"}, {"title": "Is More Always Better? The Effects of Personal Characteristics and Level of Detail on the Perception of Explanations in a Recommender System", "author": "MA Chatti"}, {"title": "Need for Cognition Among Users of Self-Monitoring Systems for Physical Activity: Survey Study", "author": "K Halttu"}, {"title": "A systematic review of interaction design strategies for group recommendation systems", "author": "OL Alvarado Rodriguez"}, {"title": "Explaining User Models with Different Levels of Detail for Transparent Recommendation: A User Study", "author": "M Guesmi", "abstract": "In this paper, we shed light on explaining user models for transparent recommendation while considering user personal characteristics. To this end, we developed a transparent Recommendation and Interest Modeling Application (RIMA) that provides interactive, layered explanations of the user model with three levels of detail (basic, intermediate, advanced) to meet the demands of different types of end-users. We conducted a within-subject study (N= 31) to investigate the relationship between personal characteristics and", "keywords": {"keywords": ["Transparent recommendation", "User models", "Personal characteristics", "Explanation", "User study"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Recommendation systems"]}}, {"title": "The Explanatory Gap in Algorithmic News Curation", "author": "H Heuer"}, {"title": "Investigating the efficacy of persuasive strategies on promoting fair recommendations", "author": "SM Mousavifar"}, {"title": "THE GIFTOFSCRUTABLE LEARNER MODELS: WHYAND HOW", "author": "J Kay"}, {"title": "Towards a Construction Kit for Visual Recommender Systems", "author": "LM Freller"}], "what can ai do for me? evaluating machine learning interpretations in cooperative play": [{"title": "Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?", "author": "A Jacovi", "abstract": "With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around", "keywords": {"keywords": ["interpetability", "NLP systems", "faithfulness", "evaluation research", "literature survey"], "research type": ["Review"], "methodology": [], "purpose": ["Descriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Natural Language Processing"]}}, {"title": "On human predictions with explanations and predictions of machine learning models: A case study on deception detection", "author": "V Lai"}, {"title": "Beyond accuracy: The role of mental models in human-AI team performance", "author": "G Bansal"}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal"}, {"title": "Can you unpack that? learning to rewrite questions-in-context", "author": "A Elgohary"}, {"title": "Evaluating saliency map explanations for convolutional neural networks: a user study", "author": "A Alqaraawi"}, {"title": "Debugging tests for model explanations", "author": "J Adebayo"}, {"title": "The state of the art in enhancing trust in machine learning models with the use of visualizations", "author": "A Chatzimparmpas"}, {"title": "No explainability without accountability: An empirical study of explanations and feedback in interactive ml", "author": "A Smith-Renner"}, {"title": "Human decision making with machine assistance: An experiment on bailing and jailing", "author": "N Grgi\u0107-Hla\u010da"}, {"title": "Neurips 2020 efficientqa competition: Systems, analyses and lessons learned", "author": "S Min", "abstract": "We review the EfficientQA competition from NeurIPS 2020. The competition focused on open-domain question answering (QA), where systems take natural language questions as input and return natural language answers. The aim of the competition was to build systems that can predict correct answers while also satisfying strict on-disk memory budgets. These memory budgets were designed to encourage contestants to explore the trade-off between storing retrieval corpora or the parameters of learned models. In this report, we describe the", "keywords": {"keywords": ["Neurips 2020", "EfficientQA competition", "open-domain question answering", "on-disk memory budgets", "systems"], "research type": "Empirical", "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": "Computer Science", "content": "Application", "application area": "Natural Language Processing"}}, {"title": "Open data visualizations and analytics as tools for policy-making", "author": "L Hagen"}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "NormLime: A new feature importance metric for explaining deep neural networks", "author": "I Ahern"}, {"title": "Personalising explainable recommendations: literature and conceptualisation", "author": "M Naiseh"}, {"title": "Gradient-based analysis of NLP models is manipulable", "author": "J Wang"}, {"title": "Quizbowl: The case for incremental question answering", "author": "P Rodriguez"}, {"title": "What question answering can learn from trivia nerds", "author": "J Boyd-Graber"}, {"title": "Leveraging rationales to improve human task performance", "author": "D Das"}, {"title": "How can I choose an explainer? An application-grounded evaluation of post-hoc explanations", "author": "S Jesus"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu", "abstract": "Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in", "keywords": {"keywords": ["human-AI teams", "complementary performance", "out-of-distribution", "interactive explanations", "human decision making"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Interdisciplinary"], "content": ["Application"], "application area": ["Decision Making"]}}, {"title": "How useful are the machine-generated interpretations to general users? a human evaluation on guessing the incorrectly predicted labels", "author": "H Shen"}, {"title": "Evaluating the interpretability of generative models by interactive reconstruction", "author": "A Ross"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies", "author": "O Vereschak"}, {"title": "What's in a Name? Answer Equivalence For Open-Domain Question Answering", "author": "C Si"}, {"title": "An alliance of humans and machines for machine learning: Hybrid intelligent systems and their design principles", "author": "J Ostheimer"}, {"title": "Fool me twice: Entailment from wikipedia gamification", "author": "JM Eisenschlos"}, {"title": "Human evaluation of spoken vs. visual explanations for open-domain qa", "author": "AV Gonzalez"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen"}, {"title": "Inducing and embedding senses with scaled gumbel softmax", "author": "F Guo", "abstract": "Methods for learning word sense embeddings represent a single word with multiple sense-specific vectors. These methods should not only produce interpretable sense embeddings, but should also learn how to select which sense to use in a given context. We propose an unsupervised model that learns sense embeddings using a modified Gumbel softmax function, which allows for differentiable discrete sense selection. Our model produces sense embeddings that are competitive (and sometimes state of the art) on multiple similarity", "keywords": {"keywords": ["word sense embeddings", "sense-specific vectors", "interpreted sense embeddings", "context selection", "Gumbel softmax"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Natural Language Processing"]}}, {"title": "Improving the interpretability of neural sentiment classifiers via data augmentation", "author": "H Chen"}, {"title": "Interpreting deep learning models in natural language processing: A review", "author": "X Sun"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions", "author": "HF Cheng"}, {"title": "Sage advice? The impacts of explanations for machine learning models on human decision-making in spam detection", "author": "MC Stites"}, {"title": "Do explanations help users detect errors in open-domain QA? an evaluation of spoken vs. visual explanations", "author": "AV Gonz\u00e1lez"}, {"title": "Which Evaluations Uncover Sense Representations that Actually Make Sense?", "author": "J Boyd-Graber"}, {"title": "Investigations of Performance and Bias in Human-AI Teamwork in Hiring", "author": "A Peng"}, {"title": "No explainability without accountability: an empirical study of explanations and feedback in interactive ML", "author": "A Smith"}, {"title": "\u201d Because AI is 100% right and safe\u201d: User Attitudes and Sources of AI Authority in India", "author": "S Kapania", "abstract": "Most prior work on human-AI interaction is set in communities that indicate skepticism towards AI, but we know less about contexts where AI is viewed as aspirational. We investigated the perceptions around AI systems by drawing upon 32 interviews and 459 survey respondents in India. Not only do Indian users accept AI decisions (79.2% respondents indicate acceptance), we find a case of AI authority\u2014AI has a legitimized power to influence human actions, without requiring adequate evidence about the capabilities of", "keywords": {"keywords": ["AI", "user attitudes", "sources of authority", "India", "perceptions"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Human-AI Interaction"]}}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "Expl (AI) ned: The impact of explainable artificial intelligence on cognitive processes", "author": "K Bauer"}, {"title": "Towards a User Integration Framework for Personal Health Decision Support and Recommender Systems", "author": "K Herrmanny"}, {"title": "Explainable deep learning in healthcare: A methodological survey from an attribution view", "author": "D Jin"}, {"title": "Designing for the human in the loop: Transparency and control in interactive machine learning", "author": "A Renner"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "HOW NATURAL LANGUAGE PROCESSING CAN OPEN MACHINE LEARNING'S BLACK BOX", "author": "J BOYD-GRABER"}, {"title": "Improving mathematical questioning in teacher training", "author": "D Datta"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "The Role of Human Knowledge in Explainable AI", "author": "A Tocchetti", "abstract": "As the performance and complexity of machine learning models have grown significantly over the last years, there has been an increasing need to develop methodologies to describe their behaviour. Such a need has mainly arisen due to the widespread use of black-box models, ie, high-performing models whose internal logic is challenging to describe and understand. Therefore, the machine learning and AI field is facing a new challenge: making models more explainable through appropriate techniques. The final goal of an explainability", "keywords": {"keywords": ["Explainable AI", "Machine learning models", "Behaviour description", "Black-box models", "Model explainability"], "research type": "Review", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer science", "content": "Methodological", "application area": "Artificial intelligence"}}, {"title": "Towards Human-Centered Natural Language Processing", "author": "AV Gonzalez"}, {"title": "Right for the Right Reasons: Training Neural Networks to Be Interpretable, Robust, and Consistent with Expert Knowledge", "author": "AS Ross"}, {"title": "Towards Better Interpretability of Machine Learning-Based Decision Support Systems", "author": "X Wang"}, {"title": "Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold", "author": "S Ruder"}, {"title": "Human Interpretation of Saliency-based Explanation Over Text", "author": "H Schuff"}, {"title": "On the Objective Evaluation of Post Hoc Explainers", "author": "Z Carmichael"}, {"title": "Transparent-AI Blueprint: Developing a Conceptual Tool to Support the Design of Transparent AI Agents", "author": "Z Zhou"}, {"title": "Journal: CHI Conference on Human Factors in Computing Systems, 2022", "author": "S Palani"}, {"title": "SAGE Intrusion Detection System: Sensitivity Analysis Guided Explainability for Machine Learning.", "author": "M Smith"}, {"title": "Play for Real (ism)-Using Games to Predict Human-AI interactions in the Real World", "author": "RD Guttman", "abstract": "AI-enabled decision support systems have repeatedly failed in real world applications despite the underlying model operating as designed. Often this was because the system was used in an unexpected manner. Our goal is to enable better prediction of how systems will be used prior to their implementation as well as to improve existing designs, by taking human behavior into account. There are several challenges to collecting such data. Not having access to an existing prediction engine requires the simulation of such a system's", "keywords": {"keywords": ["AI-enabled decision support systems", "human-AI interactions", "simulation", "prediction", "real world applications"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["AI-enabled decision support systems"]}}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang"}, {"title": "Evaluation Paradigms in Question Answering", "author": "P Rodriguez"}, {"title": "Evaluating Machine Intelligence With Question Answering", "author": "P Rodriguez"}, {"title": "Opponent cart-pole dynamics for reinforcement learning of competing agents", "author": "X Huang"}, {"title": "Explaining the Road Not Taken", "author": "H Shen"}, {"title": "\" Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification", "author": "J Bastings"}, {"title": "Interpretable Machine Learning in Social Sciences: Use Cases and Limitations", "author": "A Suvorova"}, {"title": "Evaluating Machine Intelligence with Question Answering", "author": "J Boyd-Graber"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XAI", "author": "Q Wang"}, {"title": "Revisiting Methods for Finding Influential Examples", "author": "A S\u00f8gaard", "abstract": "Several instance-based explainability methods for finding influential training examples for test-time decisions have been proposed recently, including Influence Functions, TraceIn, Representer Point Selection, Grad-Dot, and Grad-Cos. Typically these methods are evaluated using LOO influence (Cook's distance) as a gold standard, or using various heuristics. In this paper, we show that all of the above methods are unstable, ie, extremely sensitive to initialization, ordering of the training data, and batch size. We suggest that this is", "keywords": {"keywords": ["influential examples", "explainability methods", "LOO influence", "stability", "training data"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Evaluating Deep Taylor Decomposition for Reliability Assessment in the Wild", "author": "S Brandl"}, {"title": "Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance", "author": "GBB Nushi"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}], "what data should i protect? recommender and planning support for data security analysts": [{"title": "MENTOR: the design and evaluation of a protection services recommender system", "author": "MF Franco", "abstract": "Cyberattacks are the cause of several damages on governments and companies in the last years. Such damage includes not only leaks of sensitive information, but also economic loss due to downtime of services. The security market size worth billions of dollars, which represents investments to acquire protection services and training response teams to operate such services, determines a considerable part of the investment in technologies around the world. Although a vast number of protection services are available, it is neither", "keywords": {"keywords": ["cyberattacks", "protection services", "recommender system", "economic loss", "security market"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Cybersecurity"]}}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh", "abstract": "With the increase in data volume, velocity and types, intelligent human-agent systems have become popular and adopted in different application domains, including critical and sensitive areas such as health and security. Humans' trust, their consent and receptiveness to recommendations are the main requirement for the success of such services. Recently, the demand on explaining the recommendations to humans has increased both from humans interacting with these systems so that they make an informed decision and, also", "keywords": {"keywords": ["explainable recommendations", "intelligent systems", "delivery methods", "modalities", "risks"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["diverse domains"]}}, {"title": "ProtectDDoS: A Platform for Trustworthy Offering and Recommendation of Protections", "author": "M Franco", "abstract": "As the dependency of businesses on digital services increases, their vulnerability to cyberattacks increases, too. Besides providing innovative services, business owners must focus on investing in robust cybersecurity mechanisms to countermeasure cyberattacks. Distributed Denial-of-Service (DDoS) attacks remain one of the most dangerous cyberattacks, eg, leading to service disruption, financial loss, and reputation harm. Although protection measures exist, a catalog of solutions is missing, which could help network", "keywords": {"keywords": ["DDoS attacks", "cybersecurity mechanisms", "business vulnerability", "protection solutions", "service disruption"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["technical"], "application area": ["cybersecurity"]}}, {"title": "On the Recommendation of Protection Services", "author": "MF Franco", "abstract": "Cyberattacks are the cause of several damages on governments and companies in the last years. Such damage includes not only leaks of sensitive information, but also economic loss due to downtime of services. The security market size worth billions of dollars, which represents investments to acquire protection services and training response teams to operate such services, determines a considerable part of the investment in technologies around the world. Although a vast number of protection services are available, it is neither", "keywords": {"keywords": ["cyberattacks", "protection services", "economic loss", "downtime of services", "investment in technologies"], "research type": "Review", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Application", "application area": "Cybersecurity"}}, {"title": "KRAKEN: A Knowledge-Based Recommender system for Analysts, to Kick Exploration up a Notch", "author": "R Brisse", "abstract": "During a computer security investigation, a security analyst has to explore the logs available to understand what happened in the compromised system. For such tasks, visual analysis tools have been developed to help with log exploration. They provide visualisations of aggregated logs, and help navigate data efficiently. However, even using visualisation tools, the task can still be difficult and tiresome. The amount and the numerous dimensions of the logs to analyse, the potential stealthiness and complexity of the attack may end with the", "keywords": [["logs", 0.4438], ["visualisations", 0.4218], ["analyse", 0.4163], ["log", 0.405], ["visualisation", 0.3882], ["analysis", 0.323], ["investigation", 0.3068], ["attack", 0.2723], ["security", 0.2679], ["tools", 0.2493]]}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky", "abstract": "Explanations are well-known to improve recommender systems' transparency. These explanations may be local, explaining individual recommendations, or global, explaining the recommender model overall. Despite their widespread use, there has been little investigation into the relative benefits of the two explanation approaches. We conducted a 30-participant exploratory study and a 30-participant controlled user study with a research-paper recommender to analyze how providing local, global, or both explanations influences", "keywords": {"keywords": ["explainer systems", "recommender systems", "transparency", "local explanations", "global explanations"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory", "explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommendation systems"]}}, {"title": "Developing Persona Analytics Towards Persona Science", "author": "J Salminen", "abstract": "Much of the reported work on personas suffers from the lack of empirical evidence. To address this issue, we introduce Persona Analytics (PA), a system that tracks how users interact with data-driven personas. PA captures users' mouse and gaze behavior to measure users' interaction with algorithmically generated personas and use of system features for an interactive persona system. Measuring these activities grants an understanding of the behaviors of a persona user, required for quantitative measurement of persona use to obtain", "keywords": {"keywords": ["Persona Analytics", "Data-driven personas", "User interaction", "Mouse behavior", "Gaze behavior"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["User experience"]}}, {"title": "Estudio Comparativo De Un Marco Legal Y Normativas Vigentes Aplicadas En La Inteligencia Artificial Dentro Del Panorama Nacional E Internacional", "author": "MCJ Ivonne", "abstract": "En los \u00faltimos a\u00f1os la Inteligencia Artificial (IA) cada vez est\u00e1 m\u00e1s pr\u00f3xima a las personas tanto a nivel laboral como personal. El acceso a dispositivos m\u00f3viles, ordenadores, tabletas, y dispositivos de Internet de las Cosas han acercado a las personas con la IA. Su integraci\u00f3n en sistemas inteligentes (ie, aceptaci\u00f3n de recomendaciones de pel\u00edculas y amigos en redes sociales, acceso a dispositivo m\u00f3vil usando la huella o el rostro, comunicaci\u00f3n con bots de charla, asistentes de centros de llamadas) est\u00e1 causando que", "keywords": {"keywords": ["Inteligencia Artificial", "marco legal", "normativas vigentes", "panorama nacional", "panorama internacional"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Law"], "content": ["Technical"], "application area": ["Legal Framework"]}}], "beyond the ranked list: user-driven exploration and diversification of social recommendation": [{"title": "To explain or not to explain: the effects of personal characteristics when explaining music recommendation", "author": "M Millecamp", "abstract": "Recommender systems have been increasingly used in online services that we consume daily, such as Facebook, Netflix, YouTube, and Spotify. However, these systems are often presented to users as a\" black box\", ie the rationale for providing individual recommendations remains unexplained to users. In recent years, various attempts have been made to address this black box issue by providing textual explanations or interactive visualisations that enable users to explore the provenance of recommendations. Among", "keywords": {"keywords": ["recommender systems", "explanations", "personal characteristics", "black box", "music recommendation"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Online services"]}}, {"title": "Explaining recommendations in an interactive hybrid social recommende", "author": "CH Tsai"}, {"title": "PaperPoles: Facilitating adaptive visual exploration of scientific publications by citation link", "author": "J He"}, {"title": "Evaluating visual explanations for similarity-based recommendations: User perception and performanc", "author": "CH Tsai"}, {"title": "Directions for professional social matching system", "author": "T Olsson"}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checker", "author": "CH Tsai"}, {"title": "Explaining and exploring job recommendations: a user-driven approach for interacting with knowledge-based job recommender system", "author": "F Guti\u00e9rrez"}, {"title": "Relational social recommendation: Application to the academic domai", "author": "S Amal"}, {"title": "The effects of controllability and explainability in a social recommender syste", "author": "CH Tsai"}, {"title": "Visual, textual or hybrid: the effect of user expertise on different explanation", "author": "M Szymanski"}, {"title": "Designing Explanation Interfaces for Transparency and Beyond", "author": "CH Tsai", "abstract": "In this work-in-progress paper, we presented a participatory process of designing explanation interfaces for a social recommender system with multiple explanatory goals. We went through four stages to identify the key components of the recommendation model, expert mental model, user mental model, and target mental model. We reported the results of an online survey of current system users (N= 14) and a controlled user study with a group of target users (N= 15). Based on the findings, we proposed five set of explanation interfaces", "keywords": {"keywords": ["Explanation interfaces", "Transparency", "Social recommender system", "Participatory process", "User study"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Social recommendation"]}}, {"title": "From chance to serendipity: knowledge workers' experiences of serendipitous social encounter", "author": "E Olshannikova"}, {"title": "Generating and understanding personalized explanations in hybrid recommender system", "author": "P Kouki"}, {"title": "Newsviz: depicting and controlling preference profiles using interactive treemaps in news recommender system", "author": "J Kunkel"}, {"title": "The impact of displaying diversity information on the formation of self-assembling team", "author": "D G\u00f3mez-Zar\u00e1"}, {"title": "Subprofile-aware diversification of recommendation", "author": "M Kaya"}, {"title": "User-controlled hybrid recommendation for academic paper", "author": "B Rahdari"}, {"title": "\u201cKnowing me, knowing you\u201d: personalized explanations for a music recommender syste", "author": "M Martijn"}, {"title": "CourseQ: the impact of visual and interactive course recommendation in university environment", "author": "B Ma"}, {"title": "Triggers and connection-making for serendipity via user interface in recommender system", "author": "AH Afridi"}, {"title": "Understanding the effects of control and transparency in searching as learnin", "author": "C di Sciascio", "abstract": "In this paper, we analyze the benefits of adopting user interfaces that offer control and transparency for searching in contexts of learning activities. Concretely, we conducted a user study with pharmacy students performing a problem-solving task in the course of a university lecture. The task involved finding scientific papers containing relevant information to solve a clinical case. Students were split into two independent groups and assigned one search tool to perform the task. The baseline group worked with PubMed, a popular search", "keywords": {"keywords": ["control and transparency", "searching", "learning", "user interface", "scientific papers"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Pharmacy"], "content": ["Application"], "application area": ["Education"]}}, {"title": "Generation of Hints to Overcome Difficulty in Operating Interactive Recommender Systems", "author": "Y Nakao"}, {"title": "Controllability and explainability in a hybrid social recommender syste", "author": "CH Tsai"}, {"title": "Electrochemical tomography as a nondestructive technique to study localized corrosion of metal", "author": "MC Van Ede"}, {"title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis", "author": "CH Tsai"}, {"title": "Search Result Diversification with Guarantee of Topic Proportionalit", "author": "SM Sarwar"}, {"title": "Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia Research Agend", "author": "C von der Weth"}, {"title": "Digital Social Matching Ecosystem for Knowledge Work", "author": "J Huhtam\u00e4ki"}, {"title": "Visualizing Program Genres' Temporal-Based Similarity in Linear TV Recommendation", "author": "V Bogina"}, {"title": "Bursting scientific filter bubbles: Boosting innovation via novel author discover", "author": "J Portenoy"}, {"title": "Diversity exposure in social recommender systems: a social capital theory perspectiv", "author": "CH Tsai", "abstract": "Meeting other scholars at conferences is often a stochastic, intuition-driven process. Social recommender systems can support identifying new collaboration partners that one might not naturally choose. However, to boost the accumulation of social capital, such systems must be designed for diversifying social connections. This paper draws from the extant theory on social capital and diversity exposure in recommendation systems to discuss the importance of social diversity exposure and presents design directions for social recommender systems", "keywords": {"keywords": ["social recommender systems", "diversity exposure", "social capital theory", "collaboration partners", "design directions"], "research type": "theoretical", "methodology": "qualitative", "purposes": "explanatory", "discipline": "computer science", "content": "methodological", "application area": "academic collaboration"}}, {"title": "Facilitating organisational fluidity with computational social matchin", "author": "J Huhtam\u00e4ki"}, {"title": "Identifying Group-Specific Mental Models of Recommender Systems: A Novel Quantitative Approac", "author": "J Kunkel"}, {"title": "Exploring User-Controlled Hybrid Recommendation in a Conference Context", "author": "CH Tsai"}, {"title": "LIMEADE: A General Framework for Explanation-Based Human Tuning of Opaque Machine Learner", "author": "BCG Lee"}, {"title": "Featuristic: An interactive hybrid system for generating explainable recommendations\u2013beyond system accurac", "author": "S Naveed"}, {"title": "Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discover", "author": "J Portenoy"}, {"title": "Investigation of egocentric social structures for diversity-enhancing followee recommendation", "author": "E Skenderi"}, {"title": "Developing a conversational recommendation systemfor navigating limited option", "author": "VS Bursztyn"}, {"title": "A visual recommendation system for co-authorship social networks (ChinaVis 2018", "author": "K Yan"}, {"title": "Expanding Controllability of Hybrid Recommender Systems: From Positive to Negative Relevanc", "author": "B Rahdari", "abstract": "A hybrid recommender system fuses multiple data sources, usually with static and nonadjustable weightings, to deliver recommendations. One limitation of this approach is the problem to match user preference in all situations. In this paper, we present two user-controllable hybrid recommender interface, which offer a set of sliders to dynamically tune the impact of different sources of relevance on the final ranking. Two user studies were performed to design and evaluate the proposed interfaces.", "keywords": {"keywords": ["hybrid recommender system", "user-controllable interface", "relevance tuning", "user preference matching", "user studies"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Recommendation Systems"]}}, {"title": "Visualizing Feature-based Similarity for Research Paper Recommendation", "author": "C Breitinger"}, {"title": "\u591a\u69d8\u306a\u9078\u629e\u884c\u52d5\u3092\u4fc3\u3059\u305f\u3081\u306e\u30dd\u30b8\u30c6\u30a3\u30d6\u30fb\u30cd\u30ac\u30c6\u30a3\u30d6\u60c5\u5831\u306b\u7740\u76ee\u3057\u305f\u9078\u629e\u80a2\u63d0\u793a\u624b", "author": "\u6e05\u6c34\u53cb\u9806\uff0c \u53cc\u898b\u4eac\u4ecb\uff0c \u5bfa\u7530\u52aa"}, {"title": "Explaining Health Recommendations to Lay Users: The Dos and Dont's", "author": "M Szymanski"}, {"title": "User-controllable Recommendation Against Filter Bubble", "author": "W Wang"}, {"title": "Intelligent User Interfaces: Trends and application area", "author": "T Helldin"}, {"title": "An empirical investigation on user understanding of algorithm", "author": "PT Ngo"}, {"title": "Investigating course choice motivations in university environment", "author": "B Ma"}, {"title": "Exploration and Explanation: An Interactive Course Recommendation System for University Environment", "author": "B Maa"}, {"title": "Design and Development of Decision Support Based Hybrid Recommender Syste", "author": "LV ARYA"}, {"title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis. In: Proceedings of Joint Workshop on Interfaces and Human ", "author": "CH Tsai", "abstract": "Controllable and explainable intelligent user interfaces have been used to provide transparent recommendations. Many researchers have explored interfaces that support user control and provide explanations of the recommendation process and models. To extend the works to real-world decision-making scenarios, we need to understand further the users' mental models of the enhanced system components. In this paper, we make a step in this direction by investigating a free form feedback left by users of social recommender systems", "keywords": {"keywords": ["user feedback", "controllable", "explainable", "social recommender systems", "linguistic analysis"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Recommender Systems"]}}, {"title": "Harnessing Scholarly Literature as Data to Curate, Explore, and Evaluate Scientific Researc", "author": "J Portenoy"}, {"title": "Personalized Interfaces for Music Recommender System", "author": "M Millecamp"}, {"title": "Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia Research Agend", "author": "CWA Abdul"}, {"title": "\u30e9\u30f3\u30ad\u30f3\u30b0\u30d0\u30a4\u30a2\u30b9\u306e\u7de9\u548c\u306b\u5411\u3051\u305f\u30dd\u30b8\u30c6\u30a3\u30d6\u60c5\u5831\u306e\u5229\u7528\u3068\u95b2\u89a7\u30d1\u30bf\u30fc\u30f3\u306e\u5206", "author": "\u6e05\u6c34\u53cb\u9806\uff0c \u53cc\u898b\u4eac\u4ecb\uff0c \u725f\u7530\u5c06\u53f2\uff0c \u76ca\u5b50\u5b97"}, {"title": "Sosiaalinen sovittaminen:\u201d ty\u00f6el\u00e4m\u00e4n Tinder\u201d tuuppaa tietoty\u00f6l\u00e4isen entist\u00e4 parempiin verkostoyhteyksii", "author": "J Huhtam\u00e4ki"}], "interpreting interpretability: understanding data scientists' use of interpretability tools for machine learning": [{"title": "Manipulating and measuring model interpretability", "author": "F Poursabzi-Sangdeh", "abstract": "As such, we argue that to understand interpretability, it is necessary to directly manipulate  diferent factors and measure their efects. What is or is not interpretable must be defned by", "keywords": {"keywords": ["interpretability", "manipulating", "measuring", "factors", "effects"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?", "author": "A Jacovi"}, {"title": "Explainable deep learning models in medical image analysis", "author": "A Singh"}, {"title": "Problems with Shapley-value-based explanations as feature importance measures", "author": "IE Kumar"}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal"}, {"title": "Interpretable machine learning: Fundamental principles and 10 grand challenges", "author": "C Rudin"}, {"title": "Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai", "author": "A Jacovi"}, {"title": "The false hope of current approaches to explainable artificial intelligence in health care", "author": "M Ghassemi"}, {"title": "Human factors in model interpretability: Industry practices, challenges, and needs", "author": "SR Hong"}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "Rapid trust calibration through interpretable and uncertainty-aware AI", "author": "R Tomsett", "abstract": "Artificial intelligence (AI) systems hold great promise as decision-support tools, but we must be able to identify and understand their inevitable mistakes if they are to fulfill this potential. This is particularly true in domains where the decisions are high-stakes, such as law, medicine, and the military. In this Perspective, we describe the particular challenges for AI decision support posed in military coalition operations. These include having to deal with limited, low-quality data, which inevitably compromises AI performance. We suggest that", "keywords": {"keywords": ["trust calibration", "interpretability", "uncertainty-aware AI", "AI decision support", "military coalition operations"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["Application"], "application area": "military decision making"}}, {"title": "Explaining by Removing: A Unified Framework for Model Explanation.", "author": "I Covert"}, {"title": "Causal shapley values: Exploiting causal knowledge to explain individual predictions of complex models", "author": "T Heskes"}, {"title": "A human-centered agenda for intelligible machine learning", "author": "JW Vaughan"}, {"title": "On the privacy risks of model explanations", "author": "R Shokri"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "Notions of explainability and evaluation approaches for explainable artificial intelligence", "author": "G Vilone"}, {"title": "How much automation does a data scientist want?", "author": "D Wang"}, {"title": "The role of domain expertise in user trust and the impact of first impressions with intelligent systems", "author": "M Nourani"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Principles of explanation in human-AI systems", "author": "ST Mueller", "abstract": "Explainable Artificial Intelligence (XAI) has re-emerged in response to the development of modern AI and ML systems. These systems are complex and sometimes biased, but they nevertheless make decisions that impact our lives. XAI systems are frequently algorithm-focused; starting and ending with an algorithm that implements a basic untested idea about explainability. These systems are often not tested to determine whether the algorithm helps users accomplish any goals, and so their explainability remains unproven. We propose an", "keywords": {"keywords": ["Explainable Artificial Intelligence", "XAI", "human-AI systems", "algorithm", "user goals"], "research type": ["Theoretical"], "methodology": [], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["AI systems"]}}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable AI", "author": "M Chromik"}, {"title": "Survey of XAI in digital pathology", "author": "M Pocevi\u010di\u016bt\u0117"}, {"title": "Reliable post hoc explanations: Modeling uncertainty in explainability", "author": "D Slack"}, {"title": "IEEE P7001: A proposed standard on transparency", "author": "AFT Winfield"}, {"title": "Explaining data-driven decisions made by AI systems: The counterfactual approach", "author": "C Fern\u00e1ndez-Lor\u00eda"}, {"title": "Effect of information presentation on fairness perceptions of machine learning predictors", "author": "N Van Berkel"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "You never fake alone. Creative AI in action", "author": "K De Vries"}, {"title": "e-vil: A dataset and benchmark for natural language explanations in vision-language tasks", "author": "M Kayser", "abstract": "Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for", "keywords": {"keywords": ["natural language explanations", "vision-language tasks", "dataset", "benchmark", "evaluation frameworks"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Computer Vision"]}}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "Generalized linear mixed-model (GLMM) trees: A flexible decision-tree method for multilevel and longitudinal data", "author": "M Fokkema"}, {"title": "What is the optimal attribution method for explainable ophthalmic disease classification?", "author": "A Singh"}, {"title": "Evaluating the interpretability of generative models by interactive reconstruction", "author": "A Ross"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "An overview of verification and validation challenges for inspection robots", "author": "M Fisher"}, {"title": "Uncertainty aware and explainable diagnosis of retinal disease", "author": "A Singh"}, {"title": "Epistemic values in feature importance methods: Lessons from feminist epistemology", "author": "L Hancox-Li"}, {"title": "Shapley Residuals: Quantifying the limits of the Shapley value for explanations", "author": "I Kumar", "abstract": "Popular feature importance techniques compute additive approximations to nonlinear models by first defining a cooperative game describing the value of different subsets of the model's features, then calculating the resulting game's Shapley values to attribute credit additively between the features. However, the specific modeling settings in which the Shapley values are a poor approximation for the true game have not been well-described. In this paper we utilize an interpretation of Shapley values as the result of an orthogonal", "keywords": {"keywords": ["Shapley values", "feature importance", "nonlinear models", "cooperative game", "additive approximation"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Explainability pitfalls: Beyond dark patterns in explainable AI", "author": "U Ehsan"}, {"title": "Rationale-inspired natural language explanations with commonsense", "author": "BP Majumder"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "Challenging common interpretability assumptions in feature attribution explanations", "author": "J Dinu"}, {"title": "Show or suppress? Managing input uncertainty in machine learning model explanations", "author": "D Wang"}, {"title": "The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective", "author": "S Krishna"}, {"title": "To engage or not to engage with AI for critical judgments: How professionals deal with opacity when using AI for medical diagnosis", "author": "S Lebovitz"}, {"title": "Machine learning practices outside big tech: How resource constraints challenge responsible development", "author": "A Hopkins"}, {"title": "Interpretable machine learning: Moving from mythos to diagnostics", "author": "V Chen"}, {"title": "Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens", "author": "H Park", "abstract": "Recently, Artificial Intelligence (AI) has been used to enable efficient decision-making in managerial and organizational contexts, ranging from employment to dismissal. However, to avoid employees' antipathy toward AI, it is important to understand what aspects of AI employees like and/or dislike. In this paper, we aim to identify how employees perceive current human resource (HR) teams and future algorithmic management. Specifically, we explored what factors negatively influence employees' perceptions of AI making work", "keywords": {"keywords": ["Human-AI Interaction", "Human Resource Management", "Algorithmic Evaluation", "Employees' Perceptions", "Burdens"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Workplace"]}}, {"title": "The messiness of the menstruator: assessing personas and functionalities of menstrual tracking apps", "author": "A Pichon"}, {"title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspective", "author": "H Lakkaraju"}, {"title": "Teaching responsible data science: Charting new pedagogical territory", "author": "A Lewis"}, {"title": "Towards Fairness in Practice: A Practitioner-Oriented Rubric for Evaluating Fair ML Toolkits", "author": "B Richardson"}, {"title": "reshape: A framework for interactive explanations in xai based on shap", "author": "M Chromik"}, {"title": "Explainability in Music Recommender Systems", "author": "D Afchar"}, {"title": "Explainable Artificial Intelligence for Predictive Analytics on Customer Turnover: A User-Friendly Interface for Non-expert Users", "author": "J Souza"}, {"title": "Gam changer: Editing generalized additive models with interactive visualization", "author": "ZJ Wang"}, {"title": "A machine learning regression model for the screening and design of potential SARS-CoV-2 protease inhibitors", "author": "GIB Janairo"}, {"title": "Reliable and trustworthy machine learning for health using dataset shift detection", "author": "C Park", "abstract": "Unpredictable ML model behavior on unseen data, especially in the health domain, raises serious concerns about its safety as repercussions for mistakes can be fatal. In this paper, we explore the feasibility of using state-of-the-art out-of-distribution detectors for reliable and trustworthy diagnostic predictions. We select publicly available deep learning models relating to various health conditions (eg, skin cancer, lung sound, and Parkinson's disease) using various input data types (eg, image, audio, and motion data). We demonstrate that", "keywords": {"keywords": ["machine learning", "health", "dataset shift detection", "reliable", "trustworthy"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Healthcare"]}}, {"title": "Diagnosing ai explanation methods with folk concepts of behavior", "author": "A Jacovi"}, {"title": "Achievements and challenges in explaining deep learning based computer-aided diagnosis systems", "author": "A Lucieri"}, {"title": "From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence", "author": "DA Melis"}, {"title": "Creating powerful and interpretable models with regression networks", "author": "L O'Neill"}, {"title": "Companies Committed to Responsible AI: From Principles towards Implementation and Regulation?", "author": "PB de Laat"}, {"title": "Decoding human behavior with big data? Critical, constructive input from the decision sciences", "author": "K Katsikopoulos"}, {"title": "Use of time-to-event analysis to develop on-scene return of spontaneous circulation prediction for out-of-hospital cardiac arrest patients", "author": "JH Park"}, {"title": "Explainable AI for tailored electricity consumption feedback\u2013An experimental evaluation of visualizations", "author": "J Wastensteiner"}, {"title": "Goal-Directed Self-Tracking in the Management of Chronic Health Conditions", "author": "J Schroeder"}, {"title": "Beyond explaining single item recommendations", "author": "N Tintarev", "abstract": "This chapter gives an overview of the area of explanations in recommender systems. We approach the literature from the angle of evaluation: that is, we are interested in what makes an explanation \u201cgood\u201d. The chapter starts by describing how explanations can be affected by how recommendations are presented, and the role the interaction with the recommender system plays wrt explanations. Next, we introduce a number of explanation styles on three levels focusing on the individual user, contextualization, and self-actualization. In this", "keywords": {"keywords": ["explanations", "recommender systems", "evaluation", "presentation", "interaction"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommendation systems"]}}, {"title": "Who Am I? A Design Probe Exploring Real-Time Transparency about Online and Offline User Profiling Underlying Targeted Ads", "author": "NM Barbosa"}, {"title": "Predicting the dust events frequency around a degraded ecosystem and determining the contribution of their controlling factors using gradient boosting-based \u2026", "author": "Z Ebrahimi-Khusfi"}, {"title": "Improved data-driven root cause analysis in fog computing environment", "author": "C Bulla"}, {"title": "Explaining Interpretable Machine Learning: Theory, Methods and Applications", "author": "M Benk"}, {"title": "Considerations when learning additive explanations for black-box models", "author": "S Tan"}, {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "author": "L Sixt"}, {"title": "Tinkering: A Way Towards Designing Transparent Algorithmic User Interfaces.", "author": "D Showkat"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainable", "author": "A Simkute"}, {"title": "AIST: An interpretable attention-based deep learning model for crime prediction", "author": "Y Rayhan"}, {"title": "Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?", "author": "W Jin", "abstract": "Being able to explain the prediction to clinical end-users is a necessity to leverage the power of artificial intelligence (AI) models for clinical decision support. For medical images, a feature attribution map, or heatmap, is the most common form of explanation that highlights important features for AI models' prediction. However, it is still unknown how well heatmaps perform on explaining decisions on multi-modal medical images, where each modality/channel carries distinct clinical meanings of the same underlying biomedical", "keywords": {"keywords": ["Explainable AI", "Multi-Modal Medical Imaging", "Clinical Decision Support", "Feature Attribution Map", "Heatmap"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Biomedical Imaging"], "content": ["Application"], "application area": ["Medical Diagnosis"]}}, {"title": "Interpreting expert annotation differences in animal behavior", "author": "M Tjandrasuwita"}, {"title": "Fairness, explainability, privacy, and robustness for trustworthy algorithmic decision making", "author": "S Majumdar"}, {"title": "Breast Cancer Medical Imaging Multimodality Lesion Contours Annotating Method", "author": "FM Calisto"}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "Where do stories come from? examining the exploration process in investigative data journalism", "author": "D Showkat"}, {"title": "Fast TreeSHAP: Accelerating SHAP Value Computation for Trees", "author": "J Yang"}, {"title": "A multistakeholder approach towards evaluating ai transparency mechanisms", "author": "A Lucic"}, {"title": "A survey of domain knowledge elicitation in applied machine learning", "author": "D Kerrigan"}, {"title": "Supporting Responsible Data and Algorithmic Practices in The News Media", "author": "D Showkat"}, {"title": "Wikipedia ORES Explorer: Visualizing Trade-offs For Designing Applications With Machine Learning API", "author": "Z Ye", "abstract": "With the growing industry applications of Artificial Intelligence (AI) systems, pre-trained models and APIs have emerged and greatly lowered the barrier of building AI-powered products. However, novice AI application designers often struggle to recognize the inherent algorithmic trade-offs and evaluate model fairness before making informed design decisions. In this study, we examined the Objective Revision Evaluation System (ORES), a machine learning (ML) API in Wikipedia used by the community to build anti-vandalism", "keywords": {"keywords": ["Artificial Intelligence", "Machine Learning", "Pre-trained models", "API", "Algorithmic Trade-offs"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Wikipedia Anti-vandalism"]}}, {"title": "Beyond Fairness Metrics: Roadblocks and Challenges for Ethical AI in Practice", "author": "J Chen"}, {"title": "Explainable artificial intelligence for human-friendly explanations to predictive analytics on big data", "author": "J do Nascimento Souza"}, {"title": "From human explanation to model interpretability: A framework based on weight of evidence", "author": "D Alvarez-Melis"}, {"title": "Quantitative intersectional data (QUINTA): a# metoo case study", "author": "A Boyd"}, {"title": "Seven challenges for harmonizing explainability requirements", "author": "J Chen"}, {"title": "Learning how to approve updates to machine learning algorithms in non-stationary settings", "author": "J Feng"}, {"title": "Explainable artificial intelligence for data science on customer churn", "author": "CK Leung"}, {"title": "Machine learning and prediction in psychological assessment: Some promises and pitfalls.", "author": "M Fokkema"}, {"title": "Machine Learning Explainability for External Stakeholders", "author": "A Weller"}, {"title": "Explainable deep learning in healthcare: A methodological survey from an attribution view", "author": "D Jin", "abstract": "The increasing availability of large collections of electronic health record (EHR) data and unprecedented technical advances in deep learning (DL) have sparked a surge of research interest in developing DL based clinical decision support systems for diagnosis, prognosis, and treatment. Despite the recognition of the value of deep learning in healthcare, impediments to further adoption in real healthcare settings remain due to the black\u2010box nature of DL. Therefore, there is an emerging need for interpretable DL, which allows end", "keywords": {"keywords": ["Explainable deep learning", "Healthcare", "Electronic health record", "Interpretable deep learning", "Clinical decision support systems"], "research type": "Review", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer science", "content": "Application", "application area": "Healthcare"}}, {"title": "How transparency modulates trust in artificial intelligence", "author": "J Zerilli"}, {"title": "How Explainability Contributes to Trust in AI", "author": "A Ferrario"}, {"title": "Rationale production to support clinical decision-making", "author": "N Taylor"}, {"title": "Machine Learning Explanations as Boundary Objects: How AI Researchers Explain and Non-Experts Perceive Machine Learning", "author": "A Ayobi"}, {"title": "Novel Topological Shapes of Model Interpretability", "author": "HJ van Veen"}, {"title": "A Framework for Fairness: A Systematic Review of Existing Fair AI Solutions", "author": "B Richardson"}, {"title": "Simulated User Studies for Explanation Evaluation", "author": "V Chen"}, {"title": "Explaining by Conversing: The Argument for Conversational Xai Systems", "author": "W Marrakchi"}, {"title": "Explanation from Specification", "author": "H Naik"}, {"title": "Mental Models of Adversarial Machine Learning", "author": "L Bieringer", "abstract": "Although machine learning (ML) is widely used in practice, little is known about practitioners' actual understanding of potential security challenges. In this work, we close this substantial gap in the literature and contribute a qualitative study focusing on developers' mental models of the ML pipeline and potentially vulnerable components. Studying mental models has helped in other security fields to discover root causes or improve risk communication. Our study reveals four characteristic ranges in mental models of industrial practitioners. The", "keywords": {"keywords": ["Adversarial Machine Learning", "Mental models", "ML pipeline", "Security challenges", "Risk communication"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Cybersecurity"]}}, {"title": "Towards rigorous interpretations: a formalisation of feature attribution", "author": "D Afchar"}, {"title": "Foundations of Languages for Interpretability and Bias Detection", "author": "P Barcel\u00f3"}, {"title": "Two4two: Evaluating interpretable machine learning-a synthetic dataset for controlled experiments", "author": "M Schuessler"}, {"title": "Human-centric explanation facilities", "author": "M Chromik"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Binary Classification: An Introductory Machine Learning Tutorial for Social Scientists", "author": "VP Ta"}, {"title": "VitrAI: Applying Explainable AI in the Real World", "author": "M Hanussek"}, {"title": "Social Aspects of Machine Learning Model Evaluation: Model Interpretation and Justification from ML-practitioners' Perspective.", "author": "V Zakharova"}, {"title": "Improving mathematical questioning in teacher training", "author": "D Datta"}, {"title": "Machine Learning Explainability In Finance: An Application to Default Risk Analysis", "author": "D Surenans", "abstract": "We hence use mortgage default predictions as our applied use case. But our explainability  approach can be equally valuable in many other financial applications of machine learning.", "keywords": {"keywords": ["Machine Learning Explainability", "Finance", "Default Risk Analysis", "Mortgage", "Financial Applications"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Economics"], "content": ["Application"], "application area": ["Default Risk Analysis"]}}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "Interpretable Machine Learning Model Selection for Breast Cancer Diagnosis Based on K-means Clustering", "author": "DN Ouedraogo"}, {"title": "A psychological theory of explainability", "author": "SCH Yang"}, {"title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?", "author": "A Balayn"}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "Building Trust in Interactive Machine Learning via User Contributed Interpretable Rules", "author": "L Guo"}, {"title": "Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values", "author": "ZJ Wang"}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "Explaining Automated Decisions in Practice: Insights from the Swedish Credit Scoring Industry", "author": "F Matz"}, {"title": "How Domain Experts Work with Data: Situating Data Science in the Practices and Settings of Craftwork", "author": "JY Jung", "abstract": "Domain experts play an essential role in data science by helping data scientists situate their technical work beyond the statistical analysis of large datasets. How domain experts themselves may engage with data science tools as a type of end-user remains largely invisible. Understanding data science as domain expert-driven depends on understanding how domain experts use data. Drawing on an ethnographic study of a craft brewery in Korea, we show how craft brewers worked with data by situating otherwise abstract data", "keywords": {"keywords": ["domain experts", "data science", "craftwork", "end-users", "ethnographic study"], "research type": "Empirical", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Sociology", "content": "Application", "application area": "Craft brewery"}}, {"title": "OpenXAI: Towards a Transparent Evaluation of Model Explanations", "author": "C Agarwal"}, {"title": "Practical Algorithms for Resource Allocation and Decision Making", "author": "DC McElfresh"}, {"title": "Use-Case-Grounded Simulations for Explanation Evaluation", "author": "V Chen"}, {"title": "Artificial agents' explainability to support trust: considerations on timing and context", "author": "G Papagni"}, {"title": "Are contrastive explanations useful?", "author": "J Forrest"}, {"title": "Unfooling Perturbation-Based Post Hoc Explainers", "author": "Z Carmichael"}, {"title": "Human Interpretation of Saliency-based Explanation Over Text", "author": "H Schuff"}, {"title": "Greek-Turkish arms race: an approach using neural networks", "author": "\u03a3 \u039d\u03c4\u03b5\u03bb\u03bb\u03ae\u03c2"}, {"title": "Improving the Utility and Usability of Visualization in AI-driven Scientific Discovery", "author": "Q Wang"}, {"title": "Approaches for Improving Data Acquisition in Sensor-Based mHealth Applications", "author": "C Park", "abstract": "Mobile health (mHealth) applications enable people with little or no clinical experience to measure vital signs and screen for different health conditions with mobile devices. While such applications provide accessible health to the general population, recent mHealth applications leverage diverse sensors, require complex data acquisition procedures, and rely on complicated, black-box algorithms. Users are often uncertain about the quality of acquired data and resulting health-related predictions. They are also exposed to silent", "keywords": {"keywords": ["mHealth applications", "data acquisition", "sensor-based", "vital signs", "screening"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["exploratory", "descriptive"], "discipline": ["computer science"], "content": ["technical"], "application area": ["healthcare"]}}, {"title": "TalkToModel: Understanding Machine Learning Models With Open Ended Dialogues", "author": "D Slack"}, {"title": "The Conflict Between Explainable and Accountable Decision-Making Algorithms", "author": "G Lima"}, {"title": "IEEE P7001: a new standard on Transparency", "author": "AFT Winfield"}, {"title": "An explainable XGBoost model improved by SMOTE-ENN technique for maize lodging detection based on multi-source unmanned aerial vehicle images", "author": "L Han"}, {"title": "Robustness and Usefulness in AI Explanation Methods", "author": "E Galinkin"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Machine Learning Models Interpretations: User Demands Exploration", "author": "A Smirnova"}, {"title": "GAM (e) changer or not? An evaluation of interpretable machine learning models based on additive model constraints", "author": "P Zschech"}, {"title": "On the Objective Evaluation of Post Hoc Explainers", "author": "Z Carmichael"}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk", "abstract": "Building trust in AI-based systems is deemed critical for their adoption and appropriate use. Recent research has thus attempted to evaluate how various attributes of these systems affect user trust. However, limitations regarding the definition and measurement of trust in AI have hampered progress in the field, leading to results that are inconsistent or difficult to compare. In this work, we provide an overview of the main limitations in defining and measuring trust in AI. We focus on the attempt of giving trust in AI a numerical value and its", "keywords": {"keywords": ["trust in AI", "measurement", "limitations", "adoption", "user trust"], "research type": ["Review"], "methodology": ["Mixed-Methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI systems"]}}, {"title": "A Quantitative Comparison of Local Explanation Methods", "author": "H Akyuz"}, {"title": "Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions", "author": "H Park"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Topic for a Bachelor/Master's Thesis", "author": "M Learning"}, {"title": "A Study of Explainability Features to Scrutinize Faceted Filtering Results", "author": "J Qu"}, {"title": "Cognitive Mechanisms for Reasoning with Uncertainty Visualizations", "author": "A Kale"}, {"title": "Towards a Responsible AI Development Lifecycle: Lessons From Information Security", "author": "E Galinkin"}, {"title": "Human-centric Explanation Facilities: Explainable AI for the Pragmatic Understanding of Non-expert End Users", "author": "M Chromik"}, {"title": "A three layer spatial-spectral hyperspectral image classification model using guided median filters", "author": "S Dinc"}, {"title": "Towards Predictable Feature Attribution: Revisiting and Improving Guided BackPropagation", "author": "G Zheng", "abstract": "Recently, backpropagation (BP)-based feature attribution methods have been widely adopted to interpret the internal mechanisms of convolutional neural networks (CNNs), and expected to be human-understandable (lucidity) and faithful to decision-making processes (fidelity). In this paper, we introduce a novel property for feature attribution: predictability, which means users can forecast behaviors of the interpretation methods. With the evidence that many attribution methods have unexpected and harmful phenomena like class", "keywords": {"keywords": ["feature attribution", "guided backpropagation", "predictability", "interpretation methods", "convolutional neural networks"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Making SHAP Rap: Bridging Local and Global Insights Through Interaction and Narratives", "author": "M Chromik"}, {"title": "Ethical assessment of AI systems in healthcare: A use case", "author": "F Laufenberg"}, {"title": "Introduction to data deduplication approaches", "author": "GR Sinha"}, {"title": "Analyzing and Evaluating Post hoc Explanation Methods for Black Box Machine Learning", "author": "J Pombra"}, {"title": "AI-Moderated Decision-Making: Capturing and Balancing Anchoring Bias in Sequential Decision Tasks", "author": "JM Echterhoff"}, {"title": "Journal of Responsible Technology", "author": "A Simkute"}, {"title": "From Captions to Explanations: A Multimodal Transformer-based Architecture for Natural Language Explanation Generation", "author": "I Rio-Torto"}, {"title": "Spatio-temporal modelling of asthma-prone areas using a machine learning optimized with metaheuristic algorithms", "author": "SV Razavi-Termeh"}, {"title": "Understanding Diversity in Human-AI Data: What Cognitive Style Disaggregation Reveals", "author": "A Anderson"}, {"title": "Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations", "author": "J Dai", "abstract": "As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across various population subgroups including the minority groups. For instance, it should not be the case that explanations associated with instances belonging to a particular gender subgroup (eg, female) are less accurate than those associated with other genders. However, there is little to no research that assesses if there", "keywords": {"keywords": ["fairness", "explanation quality", "post hoc explanations", "disparities", "population subgroups"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Artificial Concepts of Artificial Intelligence: Institutional Compliance and Resistance in AI Startups", "author": "AA Winecoff"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimization", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "Investigating Data Work Across Domains: New Perspectives on the Work of Creating Data", "author": "K Pine"}, {"title": "Similarity-Based Explanations meet Matrix Factorization via Structure-Preserving Embeddings", "author": "LB Marinho"}, {"title": "Assessing Human-AI Interaction Early through Factorial Surveys: A Study on the Guidelines for Human-AI Interaction", "author": "T Li"}, {"title": "Online Logistics Booking System Utilizing Artificial Intelligence", "author": "HMM Ghazal"}, {"title": "Modeling Disclosive Transparency in NLP Application Descriptions", "author": "M Saxon"}, {"title": "A survey on attention mechanisms for medical applications: are we moving towards better algorithms?", "author": "T Gon\u00e7alves"}, {"title": "Beyond General Purpose Machine Translation: The Need for Context-specific Empirical Research to Design for Appropriate User Trust", "author": "WH Deng", "abstract": "Machine Translation (MT) has the potential to help people overcome language barriers and is widely used in high-stakes scenarios, such as in hospitals. However, in order to use MT reliably and safely, users need to understand when to trust MT outputs and how to assess the quality of often imperfect translation results. In this paper, we discuss research directions to support users to calibrate trust in MT systems. We share findings from an empirical study in which we conducted semi-structured interviews with 20 clinicians to understand how they", "keywords": {"keywords": ["machine translation", "trust", "user trust", "quality assessment", "empirical research"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": "Language translation"}}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "The Stakeholder Playbook for Explaining AI Systems", "author": "R Hoffman"}, {"title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India", "author": "D Ramesh"}, {"title": "On Interpretability and Similarity in Concept-Based Machine Learning", "author": "L Kwuida"}, {"title": "Development and Validation of an Interpretable 3-day Intensive Care Unit Readmission Prediction Model Using Explainable Boosting Machines", "author": "S Hegselmann"}, {"title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "author": "H Kaur"}, {"title": "Result-driven Interactive Visual Support of Parameter Selection for Dimensionality Reduction", "author": "L Joppien"}, {"title": "Situated Case Studies for a Human-Centered Design of Explanation User Interfaces", "author": "C M\u00fcller-Birn"}, {"title": "'We can't find fault with a friend': The Mediation of Accountability on Instant Loan Platforms in India", "author": "D Ramesh"}, {"title": "Machine Learning for Humans: Building Models that Adapt to Behavior", "author": "AS Hilgard", "abstract": "As machine learning continues to exhibit remarkable performance across a wide range of experimental tasks, there is an increasing enthusiasm to deploy these models in the real world. However, the traditional supervised learning framework optimizes performance without consideration to the use of these models by humans. In nearly all applications, human interaction affects the generation of input data, outcomes, or both. For example, a doctor may choose to either incorporate or override a machine-generated medical risk", "keywords": {"keywords": ["machine learning", "adapt", "behavior", "supervised learning", "human interaction"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["application"], "application area": ["real-world deployment"]}}, {"title": "Designing for Responsible Trust in AI Systems: A Communication Perspective", "author": "QV Liao"}, {"title": "Human-Centered Evaluation of Explanations", "author": "J Boyd-Graber"}, {"title": "ImageExplorer: Multi-Layered Touch Exploration to Encourage Skepticism Towards Imperfect AI-Generated Image Captions", "author": "J Lee"}, {"title": "Design Implications for Explanations: A Case Study on Supporting Reflective Assessment of Potentially Misleading Videos", "author": "O Inel"}, {"title": "Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretability", "author": "JJ Benjamin"}, {"title": "Estimating concentrations for particle and gases in a mechanically ventilated building in Hong Kong: multivariate method and machine learning", "author": "W Che"}, {"title": "Leveraging Analysis History for Improved In Situ Visualization Recommendation", "author": "W Epperson"}, {"title": "Investigating Data Work Across Domains", "author": "NH M\u00d8LLER"}, {"title": "Interpretable Machine Learning Models for Predicting and Explaining Vehicle Fuel Consumption Anomalies", "author": "A Barbado"}, {"title": "Machine Learning Interpretability to Detect Fake Accounts in Instagram", "author": "A Sallah", "abstract": "This study is related to the detection of fake accounts on Instagram dataset that used by previous works. For this purpose, various Machine Learning algorithms have been used such as Bagging and Boosting to detect fake accounts on Instagram. Machine Learning now allows eight to learn directly from data rather than human knowledge, with an increased level of accuracy. To balance the two classes of data, we used the SMOTE algorithm which allows to obtain the same number of individuals for each class. We also incorporated", "keywords": {"keywords": ["Machine Learning", "Fake Accounts", "Instagram", "Bagging", "Boosting"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": "Social Media"}}, {"title": "Analysing Large Repositories of Medical Images", "author": "I \u0160tajduhar"}, {"title": "Researching Human-AI Collaboration through the Design of Language-Based Query Assistance", "author": "M Pafla"}, {"title": "Steven C. Dang", "author": "KR Koedinger"}, {"title": "Interpretable Machine Learning in Social Sciences: Use Cases and Limitations", "author": "A Suvorova"}, {"title": "Interactive Model Cards: A Human-Centered Approach to Model Documentation", "author": "A Crisan"}, {"title": "Parallel CNN Network Learning-Based Video Object Recognition for UAV Ground Detection", "author": "H Liu"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand"}, {"title": "Exploring the Effects of Machine Learning Literacy Interventions on Laypeople's Reliance on Machine Learning Models", "author": "CW Chiang"}, {"title": "Explainable Artificial Intelligence in Healthcare: Opportunities, Gaps and Challenges and a Novel Way to Look at the Problem Space", "author": "P Korica"}, {"title": "Creative Uses of AI Systems and their Explanations: A Case Study from Insurance", "author": "M Benk", "abstract": "Recent works have recognized the need for human-centered perspectives when designing and evaluating human-AI interactions and explainable AI methods. Yet, current approaches fall short at intercepting and managing unexpected user behavior resulting from the interaction with AI systems and explainability methods of different stake-holder groups. In this work, we explore the use of AI and explainability methods in the insurance domain. In an qualitative case study with participants with different roles and professional backgrounds, we", "keywords": {"keywords": ["AI systems", "explainable AI methods", "insurance", "human-centered perspectives", "case study"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["insurance"]}}, {"title": "Too Late to be Creative? AI-Empowered Tools in Creative Processes", "author": "AHC Hwang"}, {"title": "Mental Models and Interpretability in AI Fairness Tools and Code Environments", "author": "J Thompson"}, {"title": "Supporting Remote Survey Data Analysis by Co-researchers with Learning Disabilities through Inclusive and Creative Practices and Data Science Approaches.", "author": "D Chapko"}, {"title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset", "author": "L Sixt"}, {"title": "Summarize with Caution: Comparing Global Feature Attributions.", "author": "A Okeson"}, {"title": "SoloFinger: Robust Microgestures while Grasping Everyday Objects", "author": "A Sharma"}, {"title": "Fair AI-(how) does it work?", "author": "J Anslinger"}, {"title": "Process and Information Needs When Searching for and Selecting Apps for Smoking Cessation: Qualitative Study Using Contextual Inquiry", "author": "Y Hendriks"}, {"title": "Cognitive Pairwise Comparisons for Human Centric Design in Artificial Intelligence and Data Science", "author": "KKF Yuen"}, {"title": "How Different Groups Prioritize Ethical Values for Responsible AI", "author": "M Jakesch", "abstract": "Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US", "keywords": {"keywords": ["Ethical values", "Responsible AI", "Prioritization", "Groups", "Public"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "Strategies for Selecting and Adapting Machine Learning Systems to Support Different Types of Experts", "author": "A Okeson"}, {"title": "The merger-driven evolution of early-type galaxies and the connection with their dark matter halos", "author": "C Cannarozzo"}, {"title": "Erkl\u00e4rbare KI in der medizinischen Diagnose\u2013Erfolge und Herausforderungen", "author": "A Lucieri"}, {"title": "Faire KI-(wie) geht das?", "author": "J Anslinger"}], "no explainability without accountability: an empirical study of explanations and feedback in interactive ml": [{"title": "Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai", "author": "A Jacovi", "abstract": "Trust is a central component of the interaction between people and AI, in that'incorrect'levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (ie, trust between people) as defined by", "keywords": {"keywords": ["trust", "artificial intelligence", "cognitive mechanism", "interpersonal trust", "interaction"], "research type": ["theoretical"], "methodology": ["not applicable"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["technology"]}}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "How useful are the machine-generated interpretations to general users? a human evaluation on guessing the incorrectly predicted labels", "author": "H Shen"}, {"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "A Survey of Human\u2010Centered Evaluations in Human\u2010Centered Machine Learning", "author": "F Sperrle"}, {"title": "Explainability pitfalls: Beyond dark patterns in explainable AI", "author": "U Ehsan"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro", "abstract": "Research in the social sciences has shown that expectations are an important factor in explanations as used between humans: rather than explaining the cause of an event per se, the explainer will often address another event that did not occur but that the explainee might have expected. For AI-powered systems, this finding suggests that explanation-generating systems may need to identify such end user expectations. In general, this is a challenging task, not the least because users often keep them implicit; there is thus a need to investigate", "keywords": {"keywords": ["expectations", "explanations", "AI systems", "end users", "implicits"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explanation systems"]}}, {"title": "Consistent transcription and translation of speech", "author": "M Sperber"}, {"title": "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts", "author": "T Wu"}, {"title": "The human-AI relationship in decision-making: AI explanation to support people on justifying their decisions", "author": "JJ Ferreira"}, {"title": "Leveraging explainability for comprehending referring expressions in the real world", "author": "FI Dogan"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence", "author": "W Jin"}, {"title": "Who Am I? A Design Probe Exploring Real-Time Transparency about Online and Offline User Profiling Underlying Targeted Ads", "author": "NM Barbosa"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainable", "author": "A Simkute"}, {"title": "From Who You Know to What You Read: Augmenting Scientific Recommendations with Implicit Social Networks", "author": "HB Kang"}, {"title": "Interpretation of Sentiment Analysis in Aeschylus's Greek Tragedy", "author": "VK Yeruva"}, {"title": "EUCA: the End-User-Centered Explainable AI Framework", "author": "W Jin", "abstract": "Challenges. Compared to developing XAI for technical users that mainly needs to deal with technical challenges in XAI [33], developing XAI for end-users faces even greater challenges: 1) No technical knowledge: unlike technical users, end-users typically do not possess technical knowledge in AI, machine learning, data science, or programming,", "keywords": {"keywords": ["End-User-Centered Explainable AI", "Technical Challenges", "XAI", "Technical Knowledge", "AI Framework"], "research type": ["Theoretical"], "methodology": [], "purposes": [], "discipline": ["Computer Science"], "content": [], "application area": []}}, {"title": "\u201d Because AI is 100% right and safe\u201d: User Attitudes and Sources of AI Authority in India", "author": "S Kapania"}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Augmenting scientific creativity with an analogical search engine", "author": "HB Kang"}, {"title": "Interaction with Explanations in the XAINES Project", "author": "M Hartmann"}, {"title": "Justifying Social-Choice Mechanism Outcome for Improving Participant Satisfaction", "author": "SA Suryanarayana"}, {"title": "Initial Responses to False Positives in AI-Supported Continuous Interactions: A Colonoscopy Case Study", "author": "N Van Berkel"}, {"title": "Explainable NLP for Human-AI Collaboration", "author": "P Lertvittayakumjorn"}, {"title": "Using Data to\" Git Gud\": A Push for a Player-Centric approach tothe Use of Data in Esports", "author": "E Kleinman"}, {"title": "The Role of Interactive Visualization in Fostering Trust in AI", "author": "E Beauxis-Aussalet", "abstract": "The increasing use of artificial intelligence (AI) technologies across application domains has prompted our society to pay closer attention to AI's trustworthiness, fairness, interpretability, and accountability. In order to foster trust in AI, it is important to consider the potential of interactive visualization, and how such visualizations help build trust in AI systems. This manifesto discusses the relevance of interactive visualizations and makes the following four claims: i) trust is not a technical problem, ii) trust is dynamic, iii) visualization cannot address", "keywords": {"keywords": ["interactive visualization", "trust", "AI systems", "interpretability", "accountability"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["Application"], "application area": ["AI trustworthiness"]}}, {"title": "Using Data to\" Git Gud\": A Push for a Player-Centric approach to the Use of Data in Esports", "author": "E Kleinman"}, {"title": "Interpretation of sentiment analysis with human-in-the-loop", "author": "VK Yeruva"}, {"title": "Mediators: Conversational Agents Explaining NLP Model Behavior", "author": "N Feldhus"}, {"title": "RL4HCI: Reinforcement Learning for Humans, Computers, and Interaction", "author": "D Glowacka"}, {"title": "Can Explainable AI Foster Trust in a Customer Dialogue System?", "author": "E Stoll"}, {"title": "User centric explanations: a breakthrough for explainable models", "author": "A Hassan"}, {"title": "Fall Semester Research A Working HITL Diplomacy Bot", "author": "S Schulhoff"}, {"title": "Communicating about the Limitations of AI: The Effect of Message Framing and Ownership on the Trust of Artificial Intelligence", "author": "T Kima"}, {"title": "Explaining Black Box with visual exploration of Latent Space", "author": "F Bodria"}, {"title": "Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty", "author": "J Jiang", "abstract": "In recent years, AI explainability (XAI) has received wide attention. Although XAI is expected to play a positive role in decision-making and advice acceptance, various opposing effects have also been found. The opposing effects of XAI highlight the critical role of context, especially human factors, in understanding XAI's impacts. This study investigates the effects of providing three types of post-hoc explanations (alternative advice, prediction confidence scores, and prediction rationale) on two context-specific user decision-making outcomes (AI", "keywords": {"keywords": ["AI explainability", "XAI", "user decision-making", "epistemic uncertainty", "human factors"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "How Accurate Does It Feel?\u2013Human Perception of Different Types of Classification Mistakes", "author": "A Papenmeier"}, {"title": "Data, Machine Learning, and Human Domain Experts: None Is Better than Their Collaboration", "author": "P Kumar"}, {"title": "Journal of Responsible Technology", "author": "A Simkute"}, {"title": "Communicating the Limitations of AI: The Effect of Message Framing and Ownership on Trust in Artificial Intelligence", "author": "T Kim"}, {"title": "Developing Self-Advocacy Skills through Machine Learning Education: The Case of Ad Recommendation on Facebook", "author": "Y Register"}, {"title": "'We can't find fault with a friend': The Mediation of Accountability on Instant Loan Platforms in India", "author": "D Ramesh"}, {"title": "TOWARDS USER-CENTRIC EXPLANATIONS FOR EXPLAINABLE MODELS: A REVIEW", "author": "A Hassan"}, {"title": "How Does Netflix \u201cUnderstand\u201d Me?: Exploring End-user Needs for Human-centered Explanations", "author": "Y Choi"}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications", "author": "M Nourani"}, {"title": "Spring Semester Research An Improved HITL Diplomacy Bot", "author": "S Schulhoff", "abstract": "A human-in-the-loop interface will supplement past Diplomacy work (Niculae et al., 2015; Peskov et al., 2020; Paquette et al., 2019; Anthony et al., 2020). These changes, inspired by user experiences in the Alpha test, discussion with my advisor, and feedback from my PI, should be the final changes to this iteration of the Diplomacy Bot, unless the Beta test reveals the need for additional changes. We changed aspects of both the functionality (Section 2) and the display (Section 3) of the Discord bot. We will begin a Beta test shortly", "keywords": {"keywords": ["HITL Diplomacy Bot", "human-in-the-loop interface", "Alpha test", "Beta test", "functionality", "display"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "Explanation Techniques Using Markov Logic Networks", "author": "KM Al Farabi"}, {"title": "Towards a Quality Model for Ai-Based Software", "author": "B Gezici"}, {"title": "Human-Centered Explainable AI (HCXAI): beyond opening the black-box of AI", "author": "U Ehsan"}], "why is 'chicago' deceptive? towards building model-driven tutorials for humans": [{"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal", "abstract": "Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with", "keywords": {"keywords": ["AI explanations", "team performance", "complementary", "decision-making tasks", "user studies"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Artificial intelligence explanations"]}}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making", "author": "Z Bu\u00e7inca"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI", "author": "A F\u00fcgener"}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "Evaluating and characterizing human rationales", "author": "S Carton"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "It takes two to lie: One to lie, and one to listen", "author": "D Peskov"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer", "abstract": "Hybrid Intelligence is an emerging concept that emphasizes the complementary nature of human intelligence and artificial intelligence (AI). One key requirement for collaboration between humans and AI is the interpretability of the decisions provided by the AI to enable humans to assess whether to comply with the presented decisions. Due to the black-box nature of state-of-the-art AI, the explainable AI (XAI) research community has developed various means to increase interpretability. However, many studies show that increased", "keywords": {"keywords": ["Human-AI Complementarity", "Hybrid Intelligence Systems", "Interpretability", "Explainable AI (XAI)", "Collaboration"], "research type": ["Structured Literature Review"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Collaboration"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Deciding fast and slow: The role of cognitive biases in ai-assisted decision-making", "author": "C Rastogi"}, {"title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations", "author": "S Arora"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen"}, {"title": "Show or suppress? Managing input uncertainty in machine learning model explanations", "author": "D Wang"}, {"title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspective", "author": "H Lakkaraju"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence", "author": "W Jin"}, {"title": "Harnessing explanations to bridge ai and humans", "author": "V Lai"}, {"title": "Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning", "author": "KZ Gajos", "abstract": "When people receive advice while making difficult decisions, they often make better decisions in the moment and also increase their knowledge in the process. However, such incidental learning can only occur when people cognitively engage with the information they receive and process this information thoughtfully. How do people process the information and advice they receive from AI, and do they engage with it deeply enough to enable learning? To answer these questions, we conducted three experiments in which individuals", "keywords": {"keywords": ["AI assistance", "incidental learning", "cognitive engagement", "information processing"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Psychology"], "content": ["Application"], "application area": ["Decision making"]}}, {"title": "On the diversity and limits of human explanations", "author": "C Tan"}, {"title": "How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions", "author": "HF Cheng"}, {"title": "RECAST: Enabling User Recourse and Interpretability of Toxicity Detection Models with Interactive Visualization", "author": "AP Wright"}, {"title": "Decision-Focused Summarization", "author": "CC Hsu"}, {"title": "Intelligent Decision Assistance Versus Automated Decision-Making: Enhancing Knowledge Work Through Explainable Artificial Intelligence", "author": "M Schemmer"}, {"title": "EUCA: the End-User-Centered Explainable AI Framework", "author": "W Jin"}, {"title": "On the Influence of Explainable AI on Automation Bias", "author": "M Schemmer"}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization", "author": "M Fan", "abstract": "Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX", "keywords": {"keywords": ["Human-AI Collaboration", "UX Evaluation", "Explanation", "Synchronization", "Usability Test Videos"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Descriptive", "discipline": "Computer Science", "content": "Application", "application area": "Human-Computer Interaction"}}, {"title": "Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines", "author": "S Gabriel"}, {"title": "Towards Explainable Evaluation Metrics for Natural Language Generation", "author": "C Leiter"}, {"title": "Explainable NLP for Human-AI Collaboration", "author": "P Lertvittayakumjorn"}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation", "author": "V Lai"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Under-reliance or misalignment? How proxy outcomes limit measurement of appropriate reliance in AI-assisted decision-making", "author": "L GUERDAN"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "On the Effect of Information Asymmetry in Human-AI Teams", "author": "P Hemmer", "abstract": "Over the last years, the rising capabilities of artificial intelligence (AI) have improved human decision-making in many application areas. Teaming between AI and humans may even lead to complementary team performance (CTP), ie, a level of performance beyond the ones that can be reached by AI or humans individually. Many researchers have proposed using explainable AI (XAI) to enable humans to rely on AI advice appropriately and thereby reach CTP. However, CTP is rarely demonstrated in previous work as often the focus is on the", "keywords": {"keywords": ["information asymmetry", "human-AI teams", "complementary team performance", "explainable AI", "human decision-making"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["decision-making"]}}, {"title": "Crebot: Exploring Interactive Question Prompts for Critical Paper Reading", "author": "Z Peng"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Stakeholder perceptions of the safety and assurance of artificial intelligence in healthcare", "author": "MA Sujan"}, {"title": "Sample Efficient Learning of Predictors that Complement Humans", "author": "MA Charusaie"}, {"title": "Human-Centered Approaches to Designing Intelligent Agents' Manner for Supporting High-Level Thinking", "author": "Z Peng"}, {"title": "Argumentative Explanations for Pattern-Based Text Classifiers", "author": "P Lertvittayakumjorn"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}], "designing theory-driven user-centric explainable ai": [{"title": "A survey on explainable artificial intelligence (xai): Toward medical xa", "author": "E Tjoa", "abstract": "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This", "keywords": {"keywords": ["explainable artificial intelligence", "medical", "transparency", "accountability", "reliability"], "research type": ["review"], "methodology": ["meta-analysis"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["medical sector"]}}, {"title": "XAI\u2014Explainable artificial intelligenc", "author": "D Gunning"}, {"title": "Manipulating and measuring model interpretabilit", "author": "F Poursabzi-Sangdeh"}, {"title": "Questioning the AI: informing design practices for explainable AI user experience", "author": "QV Liao"}, {"title": "Interpreting interpretability: understanding data scientists' use of interpretability tools for machine learnin", "author": "H Kaur"}, {"title": "\" Hello AI\": uncovering the onboarding needs of medical practitioners for human-AI collaborative decision-makin", "author": "CJ Cai"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI system", "author": "S Mohseni"}, {"title": "Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI system", "author": "B Shneiderman"}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performanc", "author": "G Bansal"}, {"title": "What do we want from Explainable Artificial Intelligence (XAI)?\u2013A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI researc", "author": "M Langer"}, {"title": "Contrastive explanation: A structural-model approac", "author": "T Miller", "abstract": "This paper presents a model of contrastive explanation using structural casual models. The topic of causal explanation in artificial intelligence has gathered interest in recent years as researchers and practitioners aim to increase trust and understanding of intelligent decision-making. While different sub-fields of artificial intelligence have looked into this problem with a sub-field-specific view, there are few models that aim to capture explanation more generally. One general model is based on structural causal models. It defines an", "keywords": {"keywords": ["contrastive explanation", "structural causal models", "artificial intelligence", "causal explanation", "intelligent decision-making"], "research type": ["theoretical"], "methodology": [], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": []}}, {"title": "Proxy tasks and subjective measures can be misleading in evaluating explainable AI system", "author": "Z Bu\u00e7inca"}, {"title": "Public health and risk communication during COVID-19\u2014enhancing psychological needs to promote sustainable behavior chang", "author": "T Porat"}, {"title": "Expanding explainability: Towards social transparency in ai system", "author": "U Ehsan"}, {"title": "Explainable AI in industr", "author": "K Gade"}, {"title": "How do visual explanations foster end users' appropriate trust in machine learning", "author": "F Yang"}, {"title": "Explainable ai in healthcar", "author": "U Pawar"}, {"title": "COGAM: measuring and moderating cognitive load in machine learning model explanation", "author": "A Abdul"}, {"title": "A review of possible effects of cognitive biases on interpretation of rule-based machine learning model", "author": "T Kliegr"}, {"title": "From data to insights: A layered storytelling approach for multimodal learning analytic", "author": "R Martinez-Maldonado"}, {"title": "Physician understanding, explainability, and trust in a hypothetical machine learning risk calculato", "author": "WK Diprose", "abstract": "Objective Implementation of machine learning (ML) may be limited by patients' right to \u201cmeaningful information about the logic involved\u201d when ML influences healthcare decisions. Given the complexity of healthcare decisions, it is likely that ML outputs will need to be understood and trusted by physicians, and then explained to patients. We therefore investigated the association between physician understanding of ML outputs, their ability to explain these to patients, and their willingness to trust the ML outputs, using various ML", "keywords": {"keywords": ["physician understanding", "explainability", "trust", "machine learning", "risk calculator"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Healthcare"], "content": ["Application"], "application area": ["Healthcare decision making"]}}, {"title": "Explanation ontology: a model of explanations for user-centered A", "author": "S Chari"}, {"title": "Mental models of AI agents in a cooperative game settin", "author": "KI Gero"}, {"title": "The TA Framework: Designing real-time teaching augmentation for K-12 classroom", "author": "P An"}, {"title": "Artificial intelligence in medicine: technical basis and clinical application", "author": "L Xing"}, {"title": "Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic revie", "author": "AM Antoniadi"}, {"title": "Why these explanations? Selecting intelligibility types for explanation goals", "author": "BY Lim"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their need", "author": "H Suresh"}, {"title": "Notions of explainability and evaluation approaches for explainable artificial intelligenc", "author": "G Vilone"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teacher", "author": "B Ghai"}, {"title": "How much automation does a data scientist want", "author": "D Wang", "abstract": "We argue there is no need of a fully automated data science and machine learning work,  but rather that a human-in-the-loop explainable AutoML is a promising future. We call for fellow", "keywords": {"keywords": ["automation", "data scientist", "AutoML", "human-in-the-loop", "explainable"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "CheXplain: enabling physicians to explore and understand data-driven, AI-enabled medical imaging analysi", "author": "Y Xie"}, {"title": "Ambiguity-aware ai assistants for medical data analysi", "author": "M Schaekermann"}, {"title": "What are people doing about XAI user experience? A survey on AI explainability research and practic", "author": "JJ Ferreira"}, {"title": "Algorithmic management in a work contex", "author": "MH Jarrahi"}, {"title": "Directions for explainable knowledge-enabled system", "author": "I Tiddi"}, {"title": "Deep neural networks and tabular data: A surve", "author": "V Borisov"}, {"title": "Explainable artificial intelligence for process mining: A general overview and application of a novel local explanation approach for predictive process monitorin", "author": "N Mehdiyev"}, {"title": "Mental models of mere mortals with explanations of reinforcement learnin", "author": "A Anderson"}, {"title": "A Taxonomy for Human Subject Evaluation of Black-Box Explanations in XAI", "author": "M Chromik"}, {"title": "A qualitative research framework for the design of user-centered displays of explanations for machine learning model predictions in healthcar", "author": "AJ Barda", "abstract": "There is an increasing interest in clinical prediction tools that can achieve high prediction accuracy and provide explanations of the factors leading to increased risk of adverse outcomes. However, approaches to explaining complex machine learning (ML) models are rarely informed by end-user needs and user evaluations of model interpretability are lacking in the healthcare domain. We used extended revisions of previously-published theoretical frameworks to propose a framework for the design of user-centered displays of explanations", "keywords": {"keywords": ["explanations", "machine learning", "clinical prediction", "interpretability", "healthcare"], "research type": "theoretical", "methodology": "qualitative", "purposes": "exploratory", "discipline": "computer science", "content": "methodological", "application area": "healthcare"}}, {"title": "Principles of explanation in human-AI system", "author": "ST Mueller"}, {"title": "Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycl", "author": "S Dhanorkar"}, {"title": "Scenario-based requirements elicitation for user-centric explainable A", "author": "D Cirqueira"}, {"title": "A survey on artificial intelligence assuranc", "author": "FA Batarseh"}, {"title": "A review of recent deep learning approaches in human-centered machine learnin", "author": "T Kaluarachchi"}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable A", "author": "M Chromik"}, {"title": "Charting the design and analytics agenda of learnersourcing system", "author": "H Khosravi"}, {"title": "Revisiting the modifiable areal unit problem in deep traffic prediction with visual analytic", "author": "W Zeng"}, {"title": "Artificial intelligence in glioma imaging: challenges and advance", "author": "W Jin"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experienc", "author": "B Ghai", "abstract": "The wide adoption of Machine Learning technologies has created a rapidly growing demand for people who can train ML models. Some advocated the term\" machine teacher\" to refer to the role of people who inject domain knowledge into ML models. One promising learning paradigm is Active Learning (AL), by which the model intelligently selects instances to query the machine teacher for labels. However, in current AL settings, the human-AI interface remains minimal and opaque. We begin considering AI explanations as a core", "keywords": {"keywords": ["Explainable active learning", "local explanations", "annotator experience", "machine teacher", "human-AI interface"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checker", "author": "CH Tsai"}, {"title": "Fairness and transparency in recommendation: The users' perspectiv", "author": "N Sonboli"}, {"title": "Understanding machine learning software defect prediction", "author": "G Esteves"}, {"title": "Explanations in autonomous driving: A surve", "author": "D Omeiza"}, {"title": "Human-centered XAI: Developing design patterns for explanations of clinical decision support system", "author": "TAJ Schoonderwoerd"}, {"title": "Explainable artificial intelligence for developing smart cities solution", "author": "D Thakker"}, {"title": "\u201cIt cannot do all of my work\u201d: community health worker perceptions of AI-enabled mobile health applications in rural Indi", "author": "CT Okolo"}, {"title": "Monsters, metaphors, and machine learnin", "author": "G Dove"}, {"title": "Clinician involvement in research on machine learning\u2013based predictive clinical decision support for the hospital setting: A scoping revie", "author": "JM Schwartz"}, {"title": "Better metrics for evaluating explainable artificial intelligenc", "author": "A Rosenfeld", "abstract": "This paper presents objective metrics for how explainable artificial intelligence (XAI) can be quantified. Through an overview of current trends, we show that many explanations are generated post-hoc and independent of the agent's logical process, which in turn creates explanations with limited meaning as they lack transparency and fidelity. While user studies are a known basis for evaluating XAI, studies that do not consider objective metrics for evaluating XAI may have limited meaning and may suffer from confirmation bias, particularly", "keywords": [["explainable", 0.4231], ["explanations", 0.4105], ["intelligence", 0.3746], ["xai", 0.2986], ["artificial", 0.2732], ["agent", 0.2658], ["logical", 0.2441], ["quantified", 0.2317], ["bias", 0.2026], ["metrics", 0.1971]]}, {"title": "Introduction to explainable A", "author": "QV Liao"}, {"title": "Carla: a python library to benchmark algorithmic recourse and counterfactual explanation algorithm", "author": "M Pawelczyk"}, {"title": "Human-Centered A", "author": "B Shneiderman"}, {"title": "Distal explanations for explainable reinforcement learning agent", "author": "P Madumal"}, {"title": "Crowdsourcing the perception of machine teachin", "author": "J Hong"}, {"title": "Visual, textual or hybrid: the effect of user expertise on different explanation", "author": "M Szymanski"}, {"title": "Studying the effects of cognitive biases in evaluation of conversational agent", "author": "S Santhanam"}, {"title": "Designing interactions with intention-aware gaze-enabled artificial agent", "author": "J Newn"}, {"title": "Machine Learning Uncertainty as a Design Material: A Post-Phenomenological Inquir", "author": "JJ Benjamin"}, {"title": "Requirements for trustworthy artificial intelligence\u2013a revie", "author": "D Kaur", "abstract": "The field of algorithmic decision-making, particularly Artificial Intelligence (AI), has been drastically changing. With the availability of a massive amount of data and an increase in the processing power, AI systems have been used in a vast number of high-stake applications. So, it becomes vital to make these systems reliable and trustworthy. Different approaches have been proposed to make theses systems trustworthy. In this paper, we have reviewed these approaches and summarized them based on the principles proposed by the European", "keywords": {"keywords": ["trustworthy artificial intelligence", "algorithmic decision-making", "data", "processing power", "high-stake applications"], "research type": ["review"], "methodology": [], "purposes": [], "discipline": ["computer science"], "content": ["methodological"], "application area": ["algorithmic decision-making"]}}, {"title": "Human-ai collaboration in a cooperative game setting: Measuring social perception and outcome", "author": "Z Ashktorab"}, {"title": "Explainable AI in manufacturing: a predictive maintenance case stud", "author": "B Hrnjica"}, {"title": "Player-AI interaction: What neural network games reveal about AI as pla", "author": "J Zhu"}, {"title": "The grammar of interactive explanatory model analysi", "author": "H Baniecki"}, {"title": "Data-driven game development: ethical consideration", "author": "M Seif El-Nasr"}, {"title": "Ethical machines: The human-centric use of artificial intelligenc", "author": "B Lepri"}, {"title": "Evaluating the quality of learning resources: A learnersourcing approac", "author": "S Abdi"}, {"title": "Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistan", "author": "Q Wang"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experience", "author": "QV Liao"}, {"title": "Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognitio", "author": "M Nourani", "abstract": "Explainable machine learning and artificial intelligence models have been used to justify a model's decision-making process. This added transparency aims to help improve user performance and understanding of the underlying model. However, in practice, explainable systems face many open questions and challenges. Specifically, designers might reduce the complexity of deep learning models in order to provide interpretability. The explanations generated by these simplified models, however, might not accurately justify and be truthful to", "keywords": {"keywords": ["Explainable AI", "Verifying Veracity", "Video Activity Recognition", "Transparency", "Interpretability"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Application", "application area": "Machine Learning"}}, {"title": "Tailoring heuristics and timing AI interventions for supporting news veracity assessment", "author": "BD Horne"}, {"title": "Investigating searchers' mental models to inform search explanation", "author": "P Thomas"}, {"title": "Conceptualising contestability: Perspectives on contesting algorithmic decision", "author": "H Lyons"}, {"title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologie", "author": "O Vereschak"}, {"title": "ProtoAI: Model-Informed Prototyping for AI-Powered Interface", "author": "H Subramonyam"}, {"title": "After-action review for AI (AAR/AI", "author": "J Dodge"}, {"title": "Interpretable Machine Learning: A brief survey from the predictive maintenance perspectiv", "author": "S Vollert"}, {"title": "Toward personalized XAI: A case study in intelligent tutoring system", "author": "C Conati"}, {"title": "Designing visual markers for continuous artificial intelligence support: A colonoscopy case stud", "author": "N van Berkel"}, {"title": "Lessons learned from designing an AI-enabled diagnosis tool for pathologist", "author": "H Gu", "abstract": "Despite the promises of data-driven artificial intelligence (AI), little is known about how we can bridge the gulf between traditional physician-driven diagnosis and a plausible future of medicine automated by AI. Specifically, how can we involve AI usefully in physicians' diagnosis workflow given that most AI is still nascent and error-prone (\\eg in digital pathology)? To explore this question, we first propose a series of collaborative techniques to engage human pathologists with AI given AI's capabilities and limitations, based on which", "keywords": {"keywords": ["AI-enabled diagnosis tool", "physician-driven diagnosis", "medicine automated by AI", "collaborative techniques", "human pathologists"], "research type": ["Case study"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["medical diagnosis"]}}, {"title": "VisTA: Integrating machine intelligence with visualization to support the investigation of think-aloud session", "author": "M Fan"}, {"title": "Virtual reality in biology: could we become virtual naturalists", "author": "J Morimoto"}, {"title": "Deciding fast and slow: The role of cognitive biases in ai-assisted decision-makin", "author": "C Rastogi"}, {"title": "The effect of explanation styles on user's trus", "author": "R Larasati"}, {"title": "Investigating the intelligibility of a computer vision system for blind user", "author": "S Ahmed"}, {"title": "Mediating community-AI interaction through situated explanation: the case of AI-Led moderatio", "author": "Y Kou"}, {"title": "Users & machine learning-based curation system", "author": "H Heuer"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI system", "author": "M Riveiro"}, {"title": "Question-driven design process for explainable ai user experience", "author": "QV Liao"}, {"title": "Towards Semantic Integration for Explainable Artificial Intelligence in the Biomedical Domain", "author": "C Pesquita", "abstract": "Explainable artificial intelligence typically focuses on data-based explanations, lacking the semantic context needed to produce human-centric explanations. This is especially relevant in healthcare and life sciences where the heterogeneity in both data sources and user expertise, and the underlying complexity of the domain and applications poses serious challenges. The Semantic Web represents an unparalleled opportunity in this area: it provides large amounts of freely available data in the form of Knowledge Graphs, which link", "keywords": {"keywords": ["Explainable Artificial Intelligence", "Semantic Integration", "Biomedical Domain", "Healthcare", "Life sciences"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Biomedical Informatics"]}}, {"title": "From reflection to action: Combining machine learning with expert knowledge for nutrition goal recommendation", "author": "E G. Mitchell"}, {"title": "Nudging users towards better security decisions in password creation using whitebox-based multidimensional visualisation", "author": "K Hartwig"}, {"title": "Incorporating Explainable Artificial Intelligence (XAI) to aid the Understanding of Machine Learning in the Healthcare Domain", "author": "U Pawar"}, {"title": "Towards a process model for co-creating AI experience", "author": "H Subramonyam"}, {"title": "Explain yourself! transparency for positive ux in autonomous drivin", "author": "T Schneider"}, {"title": "Investigating the importance of first impressions and explainable ai with interactive video analysi", "author": "M Nourani"}, {"title": "Semantic modeling for food recommendation explanation", "author": "I Padhiar"}, {"title": "On the relation of trust and explainability: Why to engineer for trustworthines", "author": "L K\u00e4stner"}, {"title": "Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interfac", "author": "M Desmond"}, {"title": "Cogito ergo quid? The effect of cognitive style in a transparent mobile music recommender syste", "author": "M Millecamp", "abstract": "An increasing body of research indicates that transparency in recommender systems affects trust of users. Additionally, a vast amount of studies already showed that personality impacts the way users perceive a recommender system. However, only recently, research has begun to investigate the effects of cognitive style on the perception of recommender systems. Furthermore, it is still unclear whether this cognitive style also affects the interaction strategies of users, and whether the reason why and when users want transparency is", "keywords": {"keywords": ["transparency", "recommender system", "trust", "personality", "cognitive style"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Psychology"], "content": ["Application"], "application area": ["Mobile music recommendation"]}}, {"title": "Show or suppress? Managing input uncertainty in machine learning model explanation", "author": "D Wang"}, {"title": "Designing real-time teacher augmentation to combine strengths of human and AI instructio", "author": "K Holstein"}, {"title": "Distal explanations for model-free explainable reinforcement learnin", "author": "P Madumal"}, {"title": "Chasing Luck: Data-driven Prediction, Faith, Hunch, and Cultural Norms in Rural Betting Practice", "author": "S Sultana"}, {"title": "What's up with Requirements Engineering for Artificial Intelligence Systems", "author": "K Ahmad"}, {"title": "I Don't Get IT, but IT seems Valid! The Connection between Explainability and Comprehensibility in (X) AI Research", "author": "LV Herm"}, {"title": "Can explainable AI explain unfairness? A framework for evaluating explainable A", "author": "K Alikhademi"}, {"title": "XAlgo: A design probe of explaining Algorithms' internal states via question-answerin", "author": "J Rebanal"}, {"title": "Sibyl: Understanding and addressing the usability challenges of machine learning in high-stakes decision makin", "author": "A Zytek"}, {"title": "Fifty Shades of Grey: In Praise of a Nuanced Approach Towards Trustworthy Desig", "author": "L Thornton", "abstract": "Environmental data science is uniquely placed to respond to essentially complex and fantastically worthy challenges related to arresting planetary destruction. Trust is needed for facilitating collaboration between scientists who may share datasets and algorithms, and for crafting appropriate science-based policies. Achieving this trust is particularly challenging because of the numerous complexities, multi-scale variables, interdependencies and multi-level uncertainties inherent in environmental data science. Virtual Labs---easily accessible", "keywords": {"keywords": ["trustworthy design", "environmental data science", "collaboration", "multi-scale variables", "uncertainty"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["environmental science"], "content": ["methodological"], "application area": ["environmental conservation"]}}, {"title": "Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burden", "author": "H Park"}, {"title": "Machine reasoning explainabilit", "author": "K Cyras"}, {"title": "Fostering human agency: a process for the design of user-centric XAI system", "author": "M F\u00f6rster"}, {"title": "Finding AI's faults with AAR/AI: An empirical stud", "author": "R Khanna"}, {"title": "Learning Artificial Intelligence: Insights into How Youth Encounter and Build Understanding of AI Concept", "author": "E Greenwald"}, {"title": "Managing the tension between opposing effects of explainability of artificial intelligence: a contingency theory perspectiv", "author": "B Abedin"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligenc", "author": "W Jin"}, {"title": "The impact of explanations on layperson trust in Artificial Intelligence\u2013driven symptom checker apps: Experimental stud", "author": "C Woodcock"}, {"title": "Position: We Can Measure XAI Explanations Better with Templates", "author": "J Dodge"}, {"title": "Explainable artificial intelligence for tabular data: A surve", "author": "M Sahakyan", "abstract": "Machine learning techniques are increasingly gaining attention due to their widespread use in various disciplines across academia and industry. Despite their tremendous success, many such techniques suffer from the \u201cblack-box\u201d problem, which refers to situations where the data analyst is unable to explain why such techniques arrive at certain decisions. This problem has fuelled interest in Explainable Artificial Intelligence (XAI), which refers to techniques that can easily be interpreted by humans. Unfortunately, many of these", "keywords": {"keywords": ["Explainable Artificial Intelligence", "Tabular data", "Machine learning techniques", "Black-box problem", "Interpretability"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Data analysis"]}}, {"title": "High dimensional model explanations: An axiomatic approac", "author": "N Patel"}, {"title": "Parental acceptance of children's storytelling robots: A projection of the uncanny valley of A", "author": "C Lin"}, {"title": "Do ML experts discuss explainability for AI systems? a discussion case in the industry for a domain-specific solutio", "author": "JJ Ferreira"}, {"title": "Why not explain? effects of explanations on human perceptions of autonomous drivin", "author": "D Omeiza"}, {"title": "Towards Relatable Explainable AI with the Perceptual Proces", "author": "W Zhang"}, {"title": "Design decision framework for ai explanation", "author": "O Anuyah"}, {"title": "Exploiting explanations for model inversion attack", "author": "X Zhao"}, {"title": "Are you sure? Prediction revision in automated decision\u2010makin", "author": "N Burkart"}, {"title": "Privacy-Enabled Smart Home Framework with Voice Assistan", "author": "D Singh"}, {"title": "An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainabilit", "author": "F Sovrano", "abstract": "Numerous government initiatives (eg the EU with GDPR) are coming to the conclusion that the increasing complexity of modern software systems must be contrasted with some Rights to Explanation and metrics for the Impact Assessment of these tools, that allow humans to understand and oversee the output of Automated Decision Making systems. Explainable AI was born as a pathway to allow humans to explore and understand the inner working of complex systems. But establishing what is an explanation and objectively evaluating", "keywords": {"keywords": ["Explainable AI", "Objective Metric", "Automated Decision Making", "Rights to Explanation", "Impact Assessment"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Explainability"]}}, {"title": "The Difference between Explainable and Explaining: Requirements and Challenges under the GDPR", "author": "F Sovrano"}, {"title": "Subplex: Towards a better understanding of black box model explanations at the subpopulation leve", "author": "GYY Chan"}, {"title": "Characterizing Student Engagement Moods for Dropout Prediction in Question Pool Website", "author": "RH Mogavi"}, {"title": "Designing co-creative ai for virtual environment", "author": "J Urban Davis"}, {"title": "Experience matters: design and evaluation of an anesthesia support tool guided by user experience theor", "author": "S Kl\u00fcber"}, {"title": "Analyzing description, user understanding and expectations of AI in mobile health application", "author": "Z Su"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interface", "author": "M Chromik"}, {"title": "Vbridge: Connecting the dots between features and data to explain healthcare model", "author": "F Cheng"}, {"title": "Decision explanation: applying contextual importance and contextual utility in affect detectio", "author": "N Fouladgar"}, {"title": "ML for UX?-An Inventory and Predictions on the Use of Machine Learning Techniques for UX Researc", "author": "M Chromik", "abstract": "Machine learning (ML) techniques have successfully been applied to many complex domains. Yet, applying it to UX research (UXR) received little academic attention so far. To better understand how UX practitioners envision the synergies between empathy-focused UX work and data-driven ML techniques, we surveyed 49 practitioners experienced in UX, ML, or both and conducted 13 semi-structured interviews with UX experts. We derived an inventory of ML's impact on current UXR activities and practitioners' predictions about its", "keywords": {"keywords": ["Machine learning", "UX research", "Empathy", "Data-driven", "Predictions"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["User experience"]}}, {"title": "User trust and understanding of explainable AI: exploring algorithm visualisations and user biase", "author": "D Branley-Bell"}, {"title": "\" How advertiser-friendly is my video?\": YouTuber's Socioeconomic Interactions with Algorithmic Content Moderatio", "author": "R Ma"}, {"title": "Explainable Artificial Intelligence for Predictive Analytics on Customer Turnover: A User-Friendly Interface for Non-expert User", "author": "J Souza"}, {"title": "Applying artificial intelligence to glioma imaging: Advances and challenge", "author": "W Jin"}, {"title": "On the interaction of belief bias and explanation", "author": "AV Gonzalez"}, {"title": "Big data e conoscenz", "author": "M Palmirani"}, {"title": "Quantus: an explainable AI toolkit for responsible evaluation of neural network explanation", "author": "A Hedstr\u00f6m"}, {"title": "Explainable Ontology-Based Intelligent Decision Support System for Business Model Design and Sustainabilit", "author": "B Hamrouni"}, {"title": "To explain or not to explain?\u2014Artificial intelligence explainability in clinical decision support system", "author": "J Amann"}, {"title": "Explaining the behavior of remote robots to humans: an agent-based approac", "author": "Y Mualla", "abstract": "With the widespread use of Artificial Intelligence (AI) systems, understanding the behavior of intelligent agents and robots is crucial to guarantee smooth human-agent collaboration since it is not straightforward for humans to understand the agent's state of mind. Recent studies in the goal-driven Explainable AI (XAI) domain have confirmed that explaining the agent's behavior to humans fosters the latter's understandability of the agent and increases its acceptability. However, providing overwhelming or unnecessary information may also", "keywords": {"keywords": ["Artificial Intelligence", "Explainable AI", "Human-Agent Collaboration", "Intelligent Agents", "Robots"], "research type": ["Empirical"], "methodology": ["Mixed Methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-Robot Interaction"]}}, {"title": "A real-time prescriptive solution for explainable cyber-fraud detection within the iGaming industr", "author": "D Farrugia"}, {"title": "AI-driven interface design for intelligent tutoring system improves student engagemen", "author": "B Kim"}, {"title": "MTV: Visual Analytics for Detecting, Investigating, and Annotating Anomalies in Multivariate Time Serie", "author": "D Liu"}, {"title": "The quest of parsimonious XAI: A human-agent architecture for explanation formulatio", "author": "Y Mualla"}, {"title": "A systematic review of explainable artificial intelligence in terms of different application domains and task", "author": "MR Islam"}, {"title": "Human-Computer Synergies in Prosthetic Interactions", "author": "JM Carroll"}, {"title": "The evaluation of the black box problem for AI-based recommendations: An interview-based stud", "author": "J Ochmann"}, {"title": "What does it mean to explain? A user-centered study on AI explainabilit", "author": "L Yang"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluation", "author": "F Sperrle"}, {"title": "Data-driven vs knowledge-driven inference of health outcomes in the ageing population: a case stud", "author": "D Ferrari", "abstract": "Preventive, Predictive, Personalised and Participative (P4) medicine has the potential to not only vastly improve people's quality of life, but also to significantly reduce healthcare costs and improve its efficiency. Our research focuses on age-related diseases and explores the opportunities offered by a data-driven approach to predict wellness states of ageing individuals, in contrast to the commonly adopted knowledge-driven approach that relies on easy-to-interpret metrics manually introduced by clinical experts. This is done by", "keywords": [["wellness", 0.4338], ["predictive", 0.4031], ["ageing", 0.3467], ["healthcare", 0.3326], ["medicine", 0.3176], ["data", 0.2861], ["predict", 0.2845], ["knowledge", 0.2789], ["diseases", 0.2498], ["preventive", 0.2443]]}, {"title": "Research on human\u2013AI co-creation based on reflective design practic", "author": "Z Fu"}, {"title": "A patient journey map to improve the home isolation experience of persons with mild COVID-19: design research for service touchpoints of artificial ", "author": "Q He"}, {"title": "Towards design principles for user-centric explainable AI in fraud detectio", "author": "D Cirqueira"}, {"title": "\u201cNow, I Want to Teach It for Real!\u201d: Introducing Machine Learning as a Scientific Discovery Tool for K-12 Teacher", "author": "X Zhou"}, {"title": "Capturing the Trends, Applications, Issues, and Potential Strategies of Designing Transparent AI Agent", "author": "L Sun"}, {"title": "Using Explainability to Help Children UnderstandGender Bias in A", "author": "GI Melsi\u00f3n"}, {"title": "The explainability paradox: Challenges for xAI in digital patholog", "author": "T Evans"}, {"title": "Comprehensible counterfactual explanation on Kolmogorov-Smirnov tes", "author": "Z Cong"}, {"title": "Ensuring patient and public involvement in the transition to AI\u2010assisted mental health care: A systematic scoping review and agenda for design justic", "author": "T Zidaru"}, {"title": "Design and Evaluation of User-Centered Explanations for Machine Learning Model Predictions in Healthcar", "author": "AJ Barda", "abstract": "Challenges in interpreting some high-performing models present complications in applying machine learning (ML) techniques to healthcare problems. Recently, there has been rapid growth in research on model interpretability; however, approaches to explaining complex ML models are rarely informed by end-user needs and user evaluations of model interpretability are lacking, especially in healthcare. This makes it challenging to determine what explanation approaches might enable providers to understand model predictions in a", "keywords": {"keywords": ["model interpretability", "machine learning", "healthcare", "user evaluations", "explanation approaches"], "research type": ["Empirical"], "methodology": ["Quantitative", "qualitative", "mixed-methods"], "purposes": ["Exploratory", "explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Healthcare"]}}, {"title": "Explainable artificial intelligence in educatio", "author": "H Khosravi"}, {"title": "Detecting and explaining potential financial fraud cases in invoice data with Machine Learnin", "author": "LH Hamelers"}, {"title": "Prototyping machine learning through diffractive art practic", "author": "H Scurto"}, {"title": "Assessing Gender Bias in Predictive Algorithms using eXplainable A", "author": "C Manresa-Yee"}, {"title": "Explainable artificial intelligence in forensics: Realistic explanations for number of contributor predictions of DNA profile", "author": "MS Veldhuis"}, {"title": "\" A cold, technical decision-maker\": Can AI provide explainability, negotiability, and humanity", "author": "A Woodruff"}, {"title": "Towards participatory design spaces for explainable ai interfaces in expert domain", "author": "H Mucha"}, {"title": "Making things explainable vs explaining: Requirements and challenges under the GDP", "author": "F Sovrano"}, {"title": "Exploring contextual importance and utility in explaining affect detectio", "author": "N Fouladgar"}, {"title": "Not All AI are Equal: Exploring the Accessibility of AI-Mediated Communication Technolog", "author": "E Goldenthal", "abstract": "While AI technologies and tools offer various potential benefits to their users, it is not clear whether opportunities to access these benefits are equally accessible to all. We examine this gap between availability and accessibility as it relates to the adoption of AI-Mediated Communication (AI-MC) tools, which enable interpersonal communication where an intelligent agent operates on behalf of a communicator. Upon defining six functional AI-MC types (voice-assisted communication, language correction, predictive text suggestion", "keywords": {"keywords": ["AI-Mediated Communication", "accessibility", "technologies", "communication tools", "interpersonal communication"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Communication"]}}, {"title": "Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Direction", "author": "S Atakishiyev"}, {"title": "Trouncing in dota 2: An investigation of blowout matche", "author": "M Viggiato"}, {"title": "Design Methods for Artificial Intelligence Fairness and Transparenc", "author": "S Stumpf"}, {"title": "Can You Trust the Black Box? The Effect of Personality Traits on Trust in AI-Enabled User Interface", "author": "M B\u00f6ckle"}, {"title": "EUCA: the End-User-Centered Explainable AI Framewor", "author": "W Jin"}, {"title": "Resisting out-of-distribution data problem in perturbation of xa", "author": "L Qiu"}, {"title": "Rapid assisted visual search: Supporting digital pathologists with imperfect A", "author": "M Lindvall"}, {"title": "Accurate, Fair, and Explainable: Building Human-Centered A", "author": "A Springer"}, {"title": "Co-adaptive visual data analysis and guidance processe", "author": "F Sperrle"}, {"title": "Fast TreeSHAP: Accelerating SHAP Value Computation for Tree", "author": "J Yang", "abstract": "SHAP (SHapley Additive exPlanation) values are one of the leading tools for interpreting machine learning models, with strong theoretical guarantees (consistency, local accuracy) and a wide availability of implementations and use cases. Even though computing SHAP values takes exponential time in general, TreeSHAP takes polynomial time on tree-based models. While the speedup is significant, TreeSHAP can still dominate the computation time of industry-level machine learning solutions on datasets with millions or more entries", "keywords": {"keywords": ["SHAP values", "interpretability", "TreeSHAP", "computation time", "machine learning models"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Towards a domain-customized automated machine learning framework for networks and system", "author": "B Arzani"}, {"title": "Explainable AI for Cultural Mind", "author": "H Kopeck\u00e1"}, {"title": "Failure of One, Fall of Many: An Exploratory Study of Software Features for Defect Predictio", "author": "GE dos Santos"}, {"title": "Design Patterns of Investing Apps and Their Effects on Investing Behavior", "author": "S Chaudhry"}, {"title": "A survey on artificial intelligence (ai) and explainable ai in air traffic management: Current trends and development with future research trajector", "author": "A Degas"}, {"title": "Towards a User Integration Framework for Personal Health Decision Support and Recommender System", "author": "K Herrmanny"}, {"title": "\u4eba\u673a\u667a\u80fd\u534f\u540c\u7814\u7a76\u7efc", "author": "\u5b59\u6548\u534e\uff0c \u5f20\u4e49\u6587\uff0c \u79e6\u89c9\u6653\uff0c \u674e\u749f\u7490\uff0c \u738b\u8212\u8d85"}, {"title": "Dark patterns, electronic medical records, and the opioid epidemi", "author": "D Capurro"}, {"title": "Explainable artificial intelligence for human-friendly explanations to predictive analytics on big dat", "author": "J do Nascimento Souza"}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trus", "author": "J Ooge", "abstract": "In the scope of explainable artificial intelligence, explanation techniques are heavily studied to increase trust in recommender systems. However, studies on explaining recommendations typically target adults in e-commerce or media contexts; e-learning has received less research attention. To address these limits, we investigated how explanations affect adolescents' initial trust in an e-learning platform that recommends mathematics exercises with collaborative filtering. In a randomized controlled experiment with 37", "keywords": {"keywords": ["explanation techniques", "recommender systems", "e-learning", "trust", "adolescents"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Education", "content": "Application", "application area": "E-learning platforms"}}, {"title": "Towards a theory of adoption and design for clinical decision support system", "author": "B Eapen"}, {"title": "Storytelling With Learner Data: Guiding Student Reflection on Multimodal Team Dat", "author": "GM Fernandez-Nieto"}, {"title": "Quantitative intersectional data (QUINTA): a# metoo case stud", "author": "A Boyd"}, {"title": "Forward reasoning decision support: toward a more complete view of the human-AI interaction design spac", "author": "ZT Zhang"}, {"title": "Factors Impacting K-12 Teachers in Understanding Explanations of Machine Learning Model on Students' Performanc", "author": "H Guo"}, {"title": "Model explanations via the axiomatic causal len", "author": "V Viswanathan"}, {"title": "You should not control what you do not understand: the risks of controllability in A", "author": "GDJ Barbosa"}, {"title": "Explainable artificial intelligence for data science on customer chur", "author": "CK Leung"}, {"title": "Engaging end users in an ai-enabled smart service design-the application of the smart service blueprint scape (SSBS) framewor", "author": "F Li"}, {"title": "Explanation ontology in action: a clinical use-cas", "author": "S Chari", "abstract": "We addressed the problem of a lack of semantic representation for user-centric explanations and different explanation types in our Explanation Ontology (https://purl. org/heals/eo). Such a representation is increasingly necessary as explainability has become an important problem in Artificial Intelligence with the emergence of complex methods and an uptake in high-precision and user-facing settings. In this submission, we provide step-by-step guidance for system designers to utilize our ontology, introduced in our resource track", "keywords": {"keywords": ["Explanation Ontology", "semantic representation", "user-centric explanations", "explainability", "Artificial Intelligence"], "research type": ["Case study"], "methodology": ["Qualitative"], "purposes": ["Prescriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Clinical use-case"]}}, {"title": "Algorithmic and HCI Aspects for Explaining Recommendations of Artistic Image", "author": "V Dominguez"}, {"title": "Using psychological characteristics of situations for social situation comprehension in support agent", "author": "I Kola"}, {"title": "Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional network", "author": "W Zhang"}, {"title": "More Similar Values, More Trust?-the Effect of Value Similarity on Trust in Human-Agent Interactio", "author": "S Mehrotra"}, {"title": "A Meta Survey of Quality Evaluation Criteria in Explanation Method", "author": "H L\u00f6fstr\u00f6m"}, {"title": "Towards Human-Centred Explainable AI: A Systematic Literature Revie", "author": "O Williams"}, {"title": "Developing Temporal Machine Learning Approaches to Support Modeling, Explaining, and Sensemaking of Academic Success and Risk of Undergraduate Student", "author": "N Nur"}, {"title": "Designing feedback for collocated teams using multimodal learning analytic", "author": "VI Echeverria Barzola"}, {"title": "Let's Go to the Alien Zoo: Introducing an Experimental Framework to Study Usability of Counterfactual Explanations for Machine Learnin", "author": "U Kuhl"}, {"title": "Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI System", "author": "M Nourani", "abstract": "EXplainable Artificial Intelligence (XAI) approaches are used to bring transparency to machine learning and artificial intelligence models, and hence, improve the decision-making process for their end-users. While these methods aim to improve human understanding and their mental models, cognitive biases can still influence a user's mental model and decision-making in ways that system designers do not anticipate. This paper presents research on cognitive biases due to ordering effects in intelligent systems. We", "keywords": {"keywords": ["Anchoring Bias", "Mental Model Formation", "User Reliance", "Explainable AI System", "Cognitive Biases"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Application", "application area": "Machine Learning"}}, {"title": "Generating Purpose-Driven Explanations: The Case of Process Predictive Model Inspectio", "author": "B Wickramanayake"}, {"title": "From heatmaps to structured explanations of image classifier", "author": "L Fuxin"}, {"title": "Explainable artificial intelligence for digital forensic", "author": "SW Hall"}, {"title": "Marcelle: Composing Interactive Machine Learning Workflows and Interface", "author": "J Fran\u00e7oise"}, {"title": "Interpretable conditional recurrent neural network for weight change prediction: algorithm development and validation stud", "author": "HH Kim"}, {"title": "Mental Models of Adversarial Machine Learnin", "author": "L Bieringer"}, {"title": "Commentary: critique of contesting control: how an excellent exemplar of artist-led HCI research surfaces unresolved issue", "author": "J Zimmerman"}, {"title": "How to explain AI systems to end users: a systematic literature review and research agend", "author": "S Laato"}, {"title": "Knowledge Management in the Era of Artificial Intelligence-Developing an Integrative Framewor", "author": "N Fteimi"}, {"title": "Supporting users in understanding intelligent everyday system", "author": "M Eiband", "abstract": "Intelligent systems have permeated many areas of daily life like communication, search, decision-making, and navigation, and thus present an important meeting point of people and arti\ufb01cial intelligence in practice. These intelligent everyday systems are in focus of this thesis. Intelligent everyday systems exhibit the characteristics of so-called complex systems as de\ufb01ned in cognitive science: They serve ill-de\ufb01ned user goals, change dynamically over time, and comprise a large number of interrelated variables whose dependencies are not", "keywords": {"keywords": ["Intelligent everyday systems", "complex systems", "user understanding", "artificial intelligence", "cognitive science"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["User support for intelligent everyday systems"]}}, {"title": "Cognitive Differences in Human and AI Explanatio", "author": "RA Kaufman"}, {"title": "Designing Chatbots with Black Americans with Chronic Conditions: Overcoming Challenges against COVID-1", "author": "J Kim"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Makin", "author": "G Bansal"}, {"title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imagin", "author": "R Fogliato"}, {"title": "Human-AI Interaction Paradigm for Evaluating Explainable Artificial Intelligenc", "author": "M Franklin"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboratio", "author": "H VASCONCELOS"}, {"title": "User centric explanations: a breakthrough for explainable model", "author": "A Hassan"}, {"title": "Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendation", "author": "R Shang"}, {"title": "Comparing Human Reasoning and Explainable A", "author": "CJ Helgstrand"}, {"title": "Teaching-Learning Interaction: A New Concept for Interaction Design to Support Reflective User Agency in Intelligent System", "author": "H Kim", "abstract": "Intelligent systems in everyday lives learn about their users to tailor services over time. However, these systems are often designed with little consideration of user agency on their learning processes, hindering users from taking full advantage of the systems. In this paper, we propose Teaching-Learning Interaction (TLI) as a new form of interaction that affords user agency by letting users reflectively shepherd an intelligent system's manner of learning. Given such agency, users will be able to better personalize services for themselves. We first", "keywords": {"keywords": ["Teaching-Learning Interaction", "Reflective User Agency", "Intelligent System", "User Personalization", "Interaction Design"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Prescriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Intelligent Systems"]}}, {"title": "Explainable artificial intelligence for autonomous driving: An overview and guide for future research direction", "author": "S Atakishiyev"}, {"title": "Learning Explainable Templated Graphical Model", "author": "VR Embar"}, {"title": "A Conceptual View on the Design and Properties of Explainable AI Systems for Legal Setting", "author": "M Otterlo"}, {"title": "Generating Purpose-Driven Explanations: The Case of Process Predictive Model Inspectio", "author": "C Moreira"}, {"title": "Human-Centered AI: Challenges and Opportunitie", "author": "V Sarakiotis"}, {"title": "1Department of Computer Science, Aarhus University, Aarhus, Denmark 2National University of Singapore, Singapore, Singapore [jo. vermeulen, miavd18] ", "author": "J Vermeulen"}, {"title": "Enriching teachers' assessments of rhythmic Forr\u00f3 dance skills by modelling motion sensor dat", "author": "ADP dos Santos"}, {"title": "Towards Human-Centered Natural Language Processin", "author": "AV Gonzalez"}, {"title": "Fifty Shades of Gre", "author": "L Thornton"}, {"title": "Exploring folk theories of algorithmic news curation for explainable desig", "author": "T Ngo", "abstract": "Algorithmic news curation determines users' news exposure in the online environment. Despite its usefulness, it also comes along with the problem of algorithmic opacity. To combat this, explainable algorithmic news curation systems are necessary. One user-centered solution to design these systems can be achieved through the systematic exploration of user folk theories. For this, we conducted twelve in-depth semi-structured interviews to explore (1) the user preferences for explainable system design, and (2) folk", "keywords": {"keywords": ["algorithmic news curation", "explainable design", "folk theories", "user preferences", "semi-structured interviews"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["News curation"]}}, {"title": "Learner Models for Learnersourced Adaptive Educational System", "author": "S Abdi"}, {"title": "Conversion rate optimization in e-commerce: using machine learning to identify website satisfaction in clickstream pattern", "author": "D Tomescu"}, {"title": "The Role of Explanations in Human-Machine Learnin", "author": "L Holmberg"}, {"title": "Co-Designing AI Literacy Exhibits for Informal Learning Space", "author": "D Long"}, {"title": "From Spoken Thoughts to Automated Driving Commentary: Predicting and Explaining Intelligent Vehicles' Action", "author": "D Omeiza"}, {"title": "Computational ethic", "author": "E Awad"}, {"title": "Explaining Artificial Intelligence with Tailored Interactive Visualisation", "author": "J Ooge"}, {"title": "Opening the AI Black Box for a better HC", "author": "FM Cau"}, {"title": "Translating Predictive Models for Alzheimer's Disease to Clinical Practice: User Research, Adoption Opportunities, and Conceptual Design of a Decision Support Too", "author": "M Bellio"}, {"title": "A Case Study Investigating a User-Centred and Expert Informed'Companion Guide'for a Complex Sensor-based Platfor", "author": "R Eardley", "abstract": "We present a case study that informs the creation of a'companion guide'providing transparency to potential non-expert users of a ubiquitous machine learning (ML) platform during the initial onboarding. Ubiquitous platforms (eg, smart home systems, including smart meters and conversational agents) are increasingly commonplace and increasingly apply complex ML methods. Understanding how non-ML experts comprehend these platforms is important in supporting participants in making an informed choice about if and how they", "keywords": {"keywords": ["companion guide", "sensor-based platform", "user-centred", "expert informed", "ubiquitous machine learning"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["smart home systems"]}}, {"title": "The Past, Present, and Prospective Future of XAI: A Comprehensive Revie", "author": "MU Islam"}, {"title": "Using human-in-the-loop and explainable AI to envisage new future work practice", "author": "K Tsiakas"}, {"title": "Black-box systems, multi-level explanation, and cognitive architectures", "author": "NT Lam"}, {"title": "Decrypting the Black Boxing of Artificial Intelligence Using Explainable Artificial Intelligence in Smart Healthcar", "author": "TA Shaikh"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.", "author": "JP Wanner"}, {"title": "Enabling Automated, Conversational Health Coaching with Human-Centered Artificial Intelligenc", "author": "EG Mitchell"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learnin", "author": "W Zhang"}, {"title": "Illuminating the black bo", "author": "K EKICI"}, {"title": "Designing for Contestation: Insights from Administrative La", "author": "H Lyons"}, {"title": "User Experience Design Using Machine Learning: A Systematic Revie", "author": "AMH Abbas", "abstract": "review, we present and analyze case studies which developed innovative applications and  systems through the use of AR in combination with deep  and enhanced user experience.", "keywords": {"keywords": ["User Experience Design", "Machine Learning", "AR", "Deep Learning", "Enhanced User Experience"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Augmented Reality"]}}, {"title": "Why Not Explain? Effects of Explanations on Human Perceptions of Autonomous Drivin", "author": "K Kollnig"}, {"title": "Individuality in Human-Centered A", "author": "AHC Hwang"}, {"title": "Building Trust by Supporting Situation Awareness: Exploring Pilots' Design Requirements for Decision Support Tool", "author": "C STORATH"}, {"title": "Visually Explaining Uncertain Price Predictions in Agrifood: A User-Centred Case-Stud", "author": "J Ooge"}, {"title": "Sibyl: Explaining Machine Learning Models for High-Stakes Decision Makin", "author": "A Zytek"}, {"title": "Contextualising local explanations for non-expert users: an XAI pricing interface for insurance", "author": "C Bove"}, {"title": "Toward Accountable and Explainable Artificial Intelligence Part Two: The Framework Implementatio", "author": "M Khan"}, {"title": "Rotterdam University of Applied Sciences 3011WN, Rotterdam, The Netherlands n. stembert@ hr. n", "author": "N Stembert"}, {"title": "E-Blob: Explaining Student Engagement in Online Teachin", "author": "K Bekker"}, {"title": "Explainable AI; Are we there yet", "author": "N Tanwar", "abstract": "these issues and came up with explainable artificial intelligence (XAI) that helps in justifying  the output of such models, but with the advancement, there will be some counter challenges", "keywords": {"keywords": ["explainable AI", "output justification", "advancement", "counter challenges"], "research type": ["review"], "methodology": ["not applicable"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "\u201cI See You!\u201d: A Design Framework for Interface Cues about Agent Visual Perception from a Thematic Analysis of Videogame", "author": "M Rueben"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Propertie", "author": "JE Dodge"}, {"title": "Technological Pedagogical Content Design (TPCD) for a User-centered website: a case study in Finlan", "author": "Z Hosseini"}, {"title": "Transparent-AI Blueprint: Developing a Conceptual Tool to Support the Design of Transparent AI Agent", "author": "Z Zhou"}, {"title": "Quantifying the Demand for Explainabilit", "author": "T Weber"}, {"title": "Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solution", "author": "H Park"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learnin", "author": "W Zhang"}, {"title": "Enriching teachers' assessments of rhythmic Forr\u00f3 dance skills by modelling motion sensor dat", "author": "A Dias Pereira dos Santos"}, {"title": "Assessing Clinicians' Reliance on Computational Aids for Acute Stroke Diagnosi", "author": "V Ramesh"}, {"title": "Towards Human-Centred Explainable A", "author": "MSM ten Koppel", "abstract": "One major goal of Explainable Artificial Intelligence (XAI) in order to enhance trust in technology  is to enable the user to enquire information and explanation directly from an intelligent", "keywords": {"keywords": ["Explainable Artificial Intelligence", "trust", "technology", "enquiry", "intelligent"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Usability Study of an Explainable Machine Learning Risk Model for Predicting Illegal Shipbreakin", "author": "C Haas"}, {"title": "Explainable reinforcement learning and rule reduction for advanced building contro", "author": "\uc870\uc131\uad8c"}, {"title": "Computational ethic", "author": "A Edmond"}, {"title": "Visualization Guidelines for Model Performance Communication Between Data Scientists and Subject Matter Expert", "author": "A Suh"}, {"title": "Making Conversational Robots Learn from Children in a Collaborative Museum Exhibitio", "author": "H Candello"}, {"title": "How Accurate Does It Feel?\u2013Human Perception of Different Types of Classification Mistake", "author": "A Papenmeier"}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert User", "author": "C Bove"}, {"title": "Responsible cognitive digital clones as decision-makers: a design science research stud", "author": "M Golovianko"}, {"title": "Making SHAP Rap: Bridging Local and Global Insights Through Interaction and Narrative", "author": "M Chromik"}, {"title": "Towards Explainable Artificial Intelligence in Banking and Financial Service", "author": "A Hanif", "abstract": "In recent years, machine learning (ML) has become a key enabling technology for the sciences and industry. Especially through improvements in methodology, the availability of large databases and increased computational power, today's ML algorithms are able to achieve excellent performance (at times even exceeding the human level) on an increasing number of complex tasks. Deep learning models are at the forefront of this development. However, due to their nested non-linear structure, these powerful models have been", "keywords": {"keywords": ["explainable artificial intelligence", "banking", "financial service", "machine learning", "deep learning"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["banking and financial service"]}}, {"title": "Deep learning techniques for computer auditio", "author": "Z Ren"}, {"title": "Towards Involving End-users in Interactive Human-in-the-loop AI Fairnes", "author": "Y Nakao"}, {"title": "IISM-IS-I-Teaching-Thesis & Practical Seminar Projects-Counterfactual explanations for a tensor flow neural network using Microsoft DICE library: An Experimental ", "author": "A M\u00e4dche"}, {"title": "Ensemble image explainable AI (XAI) algorithm for severe community-acquired pneumonia and COVID-19 respiratory infection", "author": "L Zou"}, {"title": "Explainable artificial intelligence and social science: Further insights for qualitative investigatio", "author": "AJ Johs"}, {"title": "Calibrating Trust in AI-Assisted Decision Makin", "author": "A Turner"}, {"title": "SUPERVISED MACHINE LEARNING IN LA", "author": "W De Mulder"}, {"title": "Play for Real (ism)-Using Games to Predict Human-AI interactions in the Real Worl", "author": "RD Guttman"}, {"title": "From explanations to shared understandings of A", "author": "I Nicenboim"}, {"title": "Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanation", "author": "J Dai", "abstract": "As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across various population subgroups including the minority groups. For instance, it should not be the case that explanations associated with instances belonging to a particular gender subgroup (eg, female) are less accurate than those associated with other genders. However, there is little to no research that assesses if there", "keywords": {"keywords": ["fairness", "explanation quality", "disparities", "post hoc explanation", "minority groups"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "How to Handle Health-Related Small Imbalanced Data in Machine Learning", "author": "M Rauschenberger"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimizatio", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysi", "author": "N Xing"}, {"title": "Toward Accountable and Explainable Artificial Intelligence Part Two: The Framework Implementatio", "author": "J Vice"}, {"title": "Similarity-Based Explanations meet Matrix Factorization via Structure-Preserving Embedding", "author": "LB Marinho"}, {"title": "Perceive, Understand & Predict-Empirical Indication for Facets in Subjective Information Processing Awarenes", "author": "TPP Schrills"}, {"title": "Human-Centered Artificial Intelligence, a revie", "author": "E Adjei Domfeh"}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertis", "author": "Q Zhang"}, {"title": "Information Resilience: the nexus of responsible and agile approaches to information us", "author": "S Sadiq"}, {"title": "Proverb representation using semantic technologies: a case study of Nigerian Yoruba proverb", "author": "VA Omolaoye", "abstract": "Indigenous Knowledge (IK) is the unique, traditional, local knowledge existing within and developed around the specific condition of women and men indigenous to a particular geographic area. Forms of expressing IK include folklores, songs, stories, festivals, outfit and proverbs. Proverbs are grounded upon years of experience and close observation of life and natural phenomena. Some research posed proverbs to be the bedrock of civilization of a society. By implication, almost all societies have proverbs or a form of it. The Nigerian", "keywords": {"keywords": ["proverb representation", "semantic technologies", "Nigerian Yoruba proverb", "Indigenous Knowledge", "proverbs"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["cultural heritage"]}}, {"title": "Explanation Ontology: A General-Purpose, Semantic Representation for Supporting User-Centered Explanation", "author": "S Chari"}, {"title": "ProtoTEx: Explaining Model Decisions with Prototype Tensor", "author": "A Das"}, {"title": "A User-Centered Design of Explainable AI for Clinical Decision Support", "author": "M Salimiparsa"}, {"title": "Explanation Oracles and the architecture of explainabilit", "author": "R Pappu"}, {"title": "The Alchemy of Trust: The Creative Act of Designing Trustworthy Socio-Technical System", "author": "L Thornton"}, {"title": "Designing Investing Apps to Promote Healthier Trading Habits in Retail Investor", "author": "S Chaudhry"}, {"title": "Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideatio", "author": "Y Wang"}, {"title": "Explainable AI for Constraint-Based Expert System", "author": "J Bode"}, {"title": "Capable but Amoral? Comparing AI and Human Expert Collaboration in Ethical Decision Makin", "author": "S Tolmeijer"}, {"title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in Indi", "author": "D Ramesh", "abstract": "Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a'high-risk'AI system", "keywords": {"keywords": ["algorithmic accountability", "platform-user power relations", "instant loan platforms", "financially stressed users", "India"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["sociology"], "content": ["application"], "application area": ["fintech"]}}, {"title": "Towards Scenario-Based and Question-Driven Explanations in Autonomous Vehicle", "author": "Y Zhang"}, {"title": "An experimental intervention with an eye-tracking device to promote social inclusion of user of augmented communication technologies", "author": "S Bhurtel"}, {"title": "Making AI Explainable in the Global South: A Systematic Revie", "author": "CT Okolo"}, {"title": "Explanation Interfaces for Sales Forecastin", "author": "TB Fahse"}, {"title": "Playful Probes for Design Interaction with Machine Learning: A Tool for Aircraft Condition-Based Maintenance Planning and Visualisatio", "author": "J Ribeiro"}, {"title": "The What, Why, What-If and How-To for Designing Architecture, Explainability for Auto-Completion of Computer-Aided Architectural Design of Floor Plan Layouting ", "author": "J Bielski"}, {"title": "EXPLAINING DEVELOPMENTAL NEUROTOXICITY BY XA", "author": "N TANWAR"}, {"title": "Designing for emotion regulation interventions: an agenda for HCI theory and researc", "author": "P Slovak"}, {"title": "Publications-MIMBCD-UI/meta Wik", "author": "FM Calisto"}, {"title": "A Taxonomy of User-centered Explainable AI Studie", "author": "LV Herm", "abstract": "The progress in the research field of machine learning is fast-paced and it is most noticeable in terms of prediction performance. However, there seems to be a lack of understanding of the explanatory value for the actual user. As only a user-appropriate implementation realizes effective human-machine cooperation, this must be the goal for any intended intelligent system development. Accordingly, some studies have addressed the problem. However, their aims and methods vary, and a meta synthesis of the results is missing. To address", "keywords": {"keywords": ["Explainable AI", "User-centered", "Taxonomy", "Machine learning", "Human-machine cooperation"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Usability, User Comprehension, and Perceptions of Explanations for Complex Decision Support Systems in Finance: A Robo-Advisory Use Cas", "author": "S Deo"}, {"title": "Empowering educators to be AI-read", "author": "R Luckin"}, {"title": "Perspectives on Incorporating Expert Feedback into Model Update", "author": "V Chen"}, {"title": "PAPER 3-THE ROLE OF EXPLANATIONS IN HUMAN-MACHINE LEARNIN", "author": "L Holmberg"}, {"title": "Explaining the Suspicion: Design of an XAI-Based User-Focused Anti-Phishing Measur", "author": "K Kluge"}, {"title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theor", "author": "H Kaur"}, {"title": "Considerations for applying logical reasoning to explain neural network output", "author": "FM Cau"}, {"title": "IISM-IS-I-Research-Research Departments-Intelligent Enterprise Systems-Explainable Interactive Machine Learnin", "author": "A M\u00e4dche"}, {"title": "SalienTrack: providing salient information for semi-automated self-tracking feedback with model explanation", "author": "Y Wang"}, {"title": "Explainable Reinforcement Learning Through a Causal Len", "author": "PMMB Appuhamilage", "abstract": "Prominent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen by referring to counterfactuals\u2014things that did not happen. In this paper, we use causal models to derive causal explanations of the behaviour of model-free reinforcement learning agents. We present an approach that learns a structural causal", "keywords": {"keywords": ["explainable reinforcement learning", "causal relationships", "causal models", "counterfactuals", "model-free reinforcement learning"], "research type": ["theoretical"], "methodology": ["Quantitative"], "purposes": ["explanatory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["Artificial Intelligence"]}}, {"title": "Making Things Explainable vs Explaining: Requirements and Challenges under the GDP", "author": "M Palmirani"}, {"title": "Improving Trustworthiness of AI Solutions: A Qualitative Approach to Support Ethically-Grounded AI Desig", "author": "A Vianello"}, {"title": "'We can't find fault with a friend': The Mediation of Accountability on Instant Loan Platforms in Indi", "author": "D Ramesh"}, {"title": "Clinician Preimplementation Perspectives of a Decision-Support Tool for the Prediction of Cardiac Arrhythmia Based on Machine Learning: Near-Live ", "author": "S Matthiesen"}, {"title": "EXAIT: A Symbiotic Explanation Learning Syste", "author": "B FLANAGAN"}, {"title": "Factors Influencing Clinician Trust in Predictive Clinical Decision Support Systems for In-Hospital Deterioration: Qualitative Descriptive Stud", "author": "JM Schwartz"}, {"title": "Assessing and Finding Faults in AI: Two Empirical Studie", "author": "R Khanna"}, {"title": "Advances in XAI: Explanation Interfaces in Healthcar", "author": "C Manresa-Yee"}, {"title": "To explain or not to explain?\u2014Artificial intelligence explainability in clinical decision support system", "author": "J AmannID"}, {"title": "The Roles and Modes of Human Interactions with Automated Machine Learning System", "author": "TT Khuat", "abstract": "As automated machine learning (AutoML) systems continue to progress in both sophistication and performance, it becomes important to understand thehow'andwhy'of human-computer interaction (HCI) within these frameworks, both current and expected. Such a discussion is necessary for optimal system design, leveraging advanced data-processing capabilities to support decision-making involving humans, but it is also key to identifying the opportunities and risks presented by ever-increasing levels of machine", "keywords": {"keywords": ["automated machine learning", "human-computer interaction", "system design", "data-processing capabilities", "decision-making"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["machine learning"]}}, {"title": "Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretabilit", "author": "JJ Benjamin"}, {"title": "Heading Toward Trusted ATCO-AI Systems: A Literature Revie", "author": "JY Zhong"}, {"title": "How Transparency Measures Can Attenuate Initial Failures of Intelligent Decision Support System", "author": "C Matt"}, {"title": "Predicting and Understanding Human Action Decisions during Skillful Joint-Action via Machine Learning and Explainable-A", "author": "F Auletta"}, {"title": "Family as a Third Space for AI Literacies: How do children and parents learn about AI together", "author": "S Druga"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Revie", "author": "A Bertrand"}, {"title": "Basic Issues and Challenges on Explainable Artificial Intelligence (XAI) in Healthcare System", "author": "OI Dauda"}, {"title": "Exploring How Anomalous Model Input and Output Alerts Affect Decision-Making in Healthcar", "author": "M Radensky"}, {"title": "Interactive Visualization Interface of Text Exploration and Annotatio", "author": "D Stachnik"}, {"title": "Human-Centered Artificial Intelligence: The Superlative Approach to Achieve Sustainable Development Goals in the Fourth Industrial Revolutio", "author": "D Mhlanga", "abstract": "Artificial intelligence (AI) is currently being developed by large corporations, and governments all over the world are yearning for it. AI isn'ta futuristic concept; it is already here, and it is being implemented in a range of industries. Finance, national security, health care, criminal justice, transportation, and smart cities are all examples of this. There are countless examples of AI having a substantial impact on the world and complementing human abilities. However, due to the immense societal ramifications of these technologies", "keywords": {"keywords": ["Artificial intelligence", "Sustainable development goals", "Fourth Industrial Revolution", "Human-centered", "Superlative approach"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Interdisciplinary"], "content": ["Application"], "application area": ["Various industries"]}}, {"title": "Evaluation of XAI as an Enabler for Fairness, Accountability and Transparenc", "author": "E Ruttkamp-Bloem"}, {"title": "Too Late to be Creative? AI-Empowered Tools in Creative Processe", "author": "AHC Hwang"}, {"title": "Opening the black-box of artificial intelligence predictions on clinical decision support system", "author": "MIL Neves"}, {"title": "Design Patterns of Trading Apps and Their Effects on Investing Behaviors Authors' preprint version-do not distribut", "author": "S CHAUDHRY"}, {"title": "Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirement", "author": "J Bogner"}, {"title": "Gaze-Based Intention Recognition for Human-Agent Collaboratio", "author": "J Newn"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XA", "author": "Q Wang"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI System", "author": "S Mohseni"}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Stud", "author": "L Chazette"}, {"title": "Uncertainty Communication by AI Assistants: The Effects on User Trus", "author": "LB Siegling", "abstract": "As artificial intelligence (AI) rapidly spreads across multiple domains and becomes increasingly integrated into everyday life, user trust is vital to consider. Inappropriate user trust has resulted in fatal accidents and significantly stunts the opportunities that AI can offer. Given the uncertainty involved in AI inputs, processing and outputs, this study investigated the effects of communicating system uncertainty on users' trust in AI assistants. Trust development was repeatedly assessed whilst 64 participants completed an online search", "keywords": {"keywords": ["uncertainty communication", "AI assistants", "user trust", "online search", "system uncertainty"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["AI assistants"]}}, {"title": "Delegating Agency? The Effects of XAI, Personality Traits, and the Moral Significance of the Application on the Reliance on Autonomous Systems: A User Stud", "author": "E Kallina"}, {"title": "Strategies for Selecting and Adapting Machine Learning Systems to Support Different Types of Expert", "author": "A Okeson"}, {"title": "Towards Diverse AI: Can an AI-Human Hybrid Council Prevent Future Apartheids", "author": "GDJ Barbosa"}, {"title": "Teaching Conversational Robots in a Museum Exhibition with Interactive Surface", "author": "H Candello"}, {"title": "Predicting the Financial Impact of the CEO's Comments in Quarterly Report", "author": "L Westerdahl"}, {"title": "\u201d Mirror, Mirror, on the Wall\u201d-Promoting Self-Regulated Learning using Affective States Recognition via Facial Movement", "author": "S Chen"}, {"title": "\u30b9\u30ad\u30fc\u521d\u5b66\u8005\u306e\u305f\u3081\u306e\u8377\u91cd\u79fb\u52d5\u306e\u63d0\u793a\u306b\u3088\u308b\u30b9\u30ad\u30fc\u6280\u80fd\u5b66\u7fd2\u652f\u63f4:\u300e\u5185\u9762\u5316\u53ef\u80fd AI\u300f \u306e\u5b9f\u73fe\u306b\u5411\u3051\u305f\u521d\u671f\u7684\u8a66", "author": "\u5c0f\u91ce\u91cd\u9065\uff0c \u91d1\u4e95\u79c0\u660e\uff0c \u897f\u672c\u4e00\u5fd7\uff0c \u6e25\u7f8e\u4eae\u7950"}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare System", "author": "M Balci"}, {"title": "Big Data, Explanations and Knowability Ragion pratic", "author": "M Palmirani"}, {"title": "Social Machines Zusammenfassun", "author": "V Projekte", "abstract": "life is and must be full of all kinds of social constraint \u2013 the very processes from which \u201a social  machines on the Web: processes in which the people do the creative work and the machine", "keywords": {"keywords": ["social machines", "Web processes", "creative work", "social constraint", "machine"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["Web processes"]}}, {"title": "Diskriminierung im Machine Learning und Erkl\u00e4rbarkeit von Algorithme", "author": "A Jackszis"}, {"title": "Social Machine", "author": "C Draude"}, {"title": "DIREITO & CI\u00caNCIA DE DADOS: TEND\u00caNCIAS E IMPACTOS DA QUARTA REVOLU\u00c7\u00c3O INDUSTRIA", "author": "GL Fernandes"}], "explaining decision-making algorithms through ui: strategies to help non-expert stakeholders": [{"title": "Manipulating and measuring model interpretability", "author": "F Poursabzi-Sangdeh", "abstract": "As such, we argue that to understand interpretability, it is necessary to directly manipulate  diferent factors and measure their efects. What is or is not interpretable must be defned by", "keywords": {"keywords": ["interpretability", "manipulating", "measuring", "factors", "effects"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao"}, {"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making", "author": "Y Zhang"}, {"title": "Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI systems", "author": "B Shneiderman"}, {"title": "What do we want from Explainable Artificial Intelligence (XAI)?\u2013A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research", "author": "M Langer"}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "Keeping community in the loop: Understanding wikipedia stakeholder values for machine learning-based systems", "author": "CE Smith"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "Mental models of AI agents in a cooperative game setting", "author": "KI Gero"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "Keeping designers in the loop: Communicating inherent algorithmic trade-offs across multiple objectives", "author": "B Yu", "abstract": "Artificial intelligence algorithms have been used to enhance a wide variety of products and services, including assisting human decision making in high-stake contexts. However, these algorithms are complex and have trade-offs, notably between prediction accuracy and fairness to population subgroups. This makes it hard for designers to understand algorithms and design products or services in a way that respects users' goals, values, and needs. We proposed a method to help designers and users explore algorithms, visualize their trade", "keywords": {"keywords": ["algorithmic trade-offs", "communication", "designers", "artificial intelligence", "fairness"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["product design"]}}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "What are people doing about XAI user experience? A survey on AI explainability research and practice", "author": "JJ Ferreira"}, {"title": "\u201cBrilliant AI Doctor\u201d in Rural Clinics: Challenges in AI-Powered Clinical Decision Support System Deployment", "author": "D Wang"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks", "author": "Z Lu"}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable AI", "author": "M Chromik"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Effect of information presentation on fairness perceptions of machine learning predictors", "author": "N Van Berkel"}, {"title": "Designing alternative representations of confusion matrices to support non-expert public understanding of algorithm performance", "author": "H Shen", "abstract": "Ensuring effective public understanding of algorithmic decisions that are powered by machine learning techniques has become an urgent task with the increasing deployment of AI systems into our society. In this work, we present a concrete step toward this goal by redesigning confusion matrices for binary classification to support non-experts in understanding the performance of machine learning models. Through interviews (n= 7) and a survey (n= 102), we mapped out two major sets of challenges lay people have in", "keywords": {"keywords": ["confusion matrices", "algorithm performance", "non-expert understanding", "machine learning models", "public understanding"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Algorithm performance"]}}, {"title": "Human-Centered AI", "author": "B Shneiderman"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "Reviewing the need for explainable artificial intelligence (xAI)", "author": "J Gerlings"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "Human-ai collaboration in a cooperative game setting: Measuring social perception and outcomes", "author": "Z Ashktorab"}, {"title": "Modeling assumptions clash with the real world: Transparency, equity, and community challenges for student assignment algorithms", "author": "S Robertson"}, {"title": "Psychological foundations of explainability and interpretability in artificial intelligence", "author": "DA Broniatowski"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "A Survey of Human\u2010Centered Evaluations in Human\u2010Centered Machine Learning", "author": "F Sperrle"}, {"title": "You'd better stop! Understanding human reliance on machine learning models under covariate shift", "author": "CW Chiang", "abstract": "Decision-making aids powered by machine learning models become increasingly prevalent on the web today. However, when applied to a new distribution of data that is different from the training data (ie, when covariate shift occurs), machine learning models often suffer from performance degradation and may provide misleading recommendations to human decision-makers. In this paper, we conduct a randomized experiment to investigate how people rely on machine learning models to make decisions under covariate shift. Surprisingly, we find", "keywords": {"keywords": ["machine learning models", "covariate shift", "performance degradation", "misleading recommendations", "decision-making aids"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Decision-making"]}}, {"title": "Introduction to this special issue on unifying human computer interaction and artificial intelligence", "author": "MD Choudhury"}, {"title": "Mediating community-AI interaction through situated explanation: the case of AI-Led moderation", "author": "Y Kou"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "Nudging users towards better security decisions in password creation using whitebox-based multidimensional visualisations", "author": "K Hartwig"}, {"title": "The legitimacy gap of algorithmic decision-making in the public sector: Why it arises and how to address it", "author": "PD K\u00f6nig"}, {"title": "Machine Learning: The Basics", "author": "A Jung"}, {"title": "On the relation of trust and explainability: Why to engineer for trustworthiness", "author": "L K\u00e4stner"}, {"title": "Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface", "author": "M Desmond"}, {"title": "Show or suppress? Managing input uncertainty in machine learning model explanations", "author": "D Wang"}, {"title": "Cody: An AI-based system to semi-automate coding for qualitative research", "author": "T Rietz", "abstract": "Qualitative research can produce a rich understanding of a phenomenon but requires an essential and strenuous data annotation process known as coding. Coding can be repetitive and time-consuming, particularly for large datasets. Existing AI-based approaches for partially automating coding, like supervised machine learning (ML) or explicit knowledge represented in code rules, require high technical literacy and lack transparency. Further, little is known about the interaction of researchers with AI-based coding assistance. We", "keywords": {"keywords": ["AI-based system", "automating coding", "qualitative research", "data annotation", "supervised machine learning"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Qualitative research coding"]}}, {"title": "Outcome-explorer: A causality guided interactive visual interface for interpretable algorithmic decision making", "author": "MN Hoque"}, {"title": "Effects of interactivity and presentation on review-based explanations for recommendations", "author": "DC Hernandez-Bocanegra"}, {"title": "Standardizing reporting of participant compensation in HCI: A systematic literature review and recommendations for the field", "author": "J Pater"}, {"title": "FORTNIoT: Intelligible Predictions to Improve User Understanding of Smart Home Behavior", "author": "S Coppers"}, {"title": "Sociotechnical Approach for Accountability by Design in AI Systems.", "author": "P Vassilakopoulou"}, {"title": "Revealing neural network bias to non-experts through interactive counterfactual examples", "author": "CM Myers"}, {"title": "Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approach", "author": "H Mucha"}, {"title": "Narrative maps: An algorithmic approach to represent and extract information narratives", "author": "BF Keith Norambuena"}, {"title": "Do ML experts discuss explainability for AI systems? a discussion case in the industry for a domain-specific solution", "author": "JJ Ferreira"}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang", "abstract": "Machine learning models need to provide contrastive explanations, since people often seek to understand why a puzzling prediction occurred instead of some expected outcome. Current contrastive explanations are rudimentary comparisons between examples or raw features, which remain difficult to interpret, since they lack semantic meaning. We argue that explanations must be more relatable to other concepts, hypotheticals, and associations. Inspired by the perceptual process from cognitive psychology, we propose the XAI", "keywords": {"keywords": ["contrastive explanations", "relatable", "explainable AI", "perceptual process", "XAI"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explainability"]}}, {"title": "AI-Assisted Human Labeling: Batching for Efficiency without Overreliance", "author": "Z Ashktorab"}, {"title": "Analyzing description, user understanding and expectations of AI in mobile health applications", "author": "Z Su"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "Towards the Design of an Interactive Machine Learning System for Qualitative Coding.", "author": "T Rietz"}, {"title": "AI-Assisted and explainable hate speech detection for social media moderators\u2013A design science approach", "author": "E Bunde"}, {"title": "Controllability and explainability in a hybrid social recommender system", "author": "CH Tsai"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "Explainable AI for tailored electricity consumption feedback\u2013An experimental evaluation of visualizations", "author": "J Wastensteiner"}, {"title": "Capturing the Trends, Applications, Issues, and Potential Strategies of Designing Transparent AI Agents", "author": "L Sun"}, {"title": "ChatrEx: Designing explainable chatbot interfaces for enhancing usefulness, transparency, and trust", "author": "A Khurana", "abstract": "When breakdowns occur during a human-chatbot conversation, the lack of transparency and the \u201cblack-box\u201d nature of task-oriented chatbots can make it difficult for end users to understand what went wrong and why. Inspired by recent HCI research on explainable AI solutions, we explored the design of in-application explainable chatbot interfaces (ChatrEx) that explain the underlying working of a chatbot during a breakdown. ChatrEx-VINC provides visual example-based step-by-step explanations in-context of the chat window", "keywords": {"keywords": ["chatbot interfaces", "explainable AI", "transparency", "trust", "breakdowns"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["HCI research"]}}, {"title": "Causal perception in question-answering systems", "author": "PM Law"}, {"title": "When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models", "author": "A Rechkemmer"}, {"title": "Explaining Interpretable Machine Learning: Theory, Methods and Applications", "author": "M Benk"}, {"title": "On the visualization of semantic-based mappings", "author": "P Vago"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainable", "author": "A Simkute"}, {"title": "How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI.", "author": "ZT Zhang"}, {"title": "Design principles for user interfaces in AI-Based decision support systems: The case of explainable hate speech detection", "author": "C Meske"}, {"title": "Identifying Group-Specific Mental Models of Recommender Systems: A Novel Quantitative Approach", "author": "J Kunkel"}, {"title": "Towards personalized explanations for AI systems: designing a role model for explainable AI in auditing", "author": "J Rebstadt"}, {"title": "Empowering local communities using artificial intelligence", "author": "YC Hsu", "abstract": "We define social impact as how a project influences the society and local communities   community empowerment as a process of yielding agency to communities so that they can use", "keywords": {"keywords": ["social impact", "local communities", "empowerment", "artificial intelligence", "agency"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["social science"], "content": ["application"], "application area": ["community development"]}}, {"title": "How to Train a (Bad) Algorithmic Caseworker: A Quantitative Deconstruction of Risk Assessments in Child Welfare", "author": "D Saxena"}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization", "author": "M Fan"}, {"title": "Forward reasoning decision support: toward a more complete view of the human-AI interaction design space", "author": "ZT Zhang"}, {"title": "People underestimate the errors made by algorithms for credit scoring and recidivism prediction but accept even fewer errors", "author": "FG Rebitschek"}, {"title": "Attitudes and Folk Theories of Data Subjects on Transparency and Accuracy in Emotion Recognition", "author": "G Grill"}, {"title": "Personalised Recommendations in Mental Health Apps: The Impact of Autonomy and Data Sharing", "author": "S Pieritz"}, {"title": "Pilot attitudes toward AI in the cockpit: implications for design", "author": "ZT Zhang"}, {"title": "Narrative Maps: An Algorithmic Approach to Represent and Extract Information Narratives", "author": "B Keith"}, {"title": "The effect of explanations on trust in an assistance system for public transport users and the role of the propensity to trust", "author": "AK Faulhaber"}, {"title": "How to explain AI systems to end users: a systematic literature review and research agenda", "author": "S Laato", "abstract": "Purpose Inscrutable machine learning (ML) models are part of increasingly many information systems. Understanding how these models behave, and what their output is based on, is a challenge for developers let alone non-technical end users. Design/methodology/approach The authors investigate how AI systems and their decisions ought to be explained for end users through a systematic literature review. Findings The authors' synthesis of the literature suggests that AI system communication for end users has", "keywords": {"keywords": ["AI systems", "machine learning", "explanation", "end users", "systematic literature review"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["information systems"]}}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation", "author": "V Lai"}, {"title": "Understanding Science and Policy Making in Agriculture: A Machine Learning Application for India", "author": "S Balaji"}, {"title": "\" It Feels Like Being Locked in A Cage\": Understanding Blind or Low Vision Streamers' Perceptions of Content Curation Algorithms", "author": "EZ Rong"}, {"title": "Fairness in computational decision making", "author": "T Saarinen"}, {"title": "It'sa Disaster! Factors Affecting Trust Development and Repair Following Agent Task Failure", "author": "M SharifHeravi"}, {"title": "Explainable Predictive Process Monitoring: A User Evaluation", "author": "W Rizzi"}, {"title": "Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations", "author": "R Shang"}, {"title": "Teaching-Learning Interaction: A New Concept for Interaction Design to Support Reflective User Agency in Intelligent Systems", "author": "H Kim"}, {"title": "Designing AI-Based Systems for Qualitative Data Collection and Analysis", "author": "T Rietz"}, {"title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?", "author": "A Balayn", "abstract": "Deep learning models for image classification suffer from dangerous issues often discovered after deployment. The process of identifying bugs that cause these issues remains limited and understudied. Especially, explainability methods are often presented as obvious tools for bug identification. Yet, the current practice lacks an understanding of what kind of explanations can best support the different steps of the bug identification process, and how practitioners could interact with those explanations. Through a formative study and", "keywords": {"keywords": ["bug identification", "computer vision models", "explainability methods", "deep learning", "image classification"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Computer Vision"]}}, {"title": "Explaining Automated Decisions in Practice: Insights from the Swedish Credit Scoring Industry", "author": "F Matz"}, {"title": "for Evaluating XAI with Non-Experts. In AP Rocha, L. Steels, & J. van den Herik (Eds.), Proceedings of the 14th International Conference on Agents and Artificial \u2026", "author": "K McAreavey"}, {"title": "Empowering Local Communities Using Artificial Intelligence", "author": "AB Nourbakhsh"}, {"title": "Data Work and Data Tracking Technologies in Fertility Care: A Holistic Approach", "author": "MC Figueiredo"}, {"title": "Facets of algorithmic literacy: Information, experience, and individual factors predict attitudes toward algorithmic systems", "author": "DE Silva"}, {"title": "Exploring the Effects of Interactive Dialogue in Improving User Control for Explainable Online Symptom Checkers", "author": "Y Sun"}, {"title": "Contextualising local explanations for non-expert users: an XAI pricing interface for insurance.", "author": "C Bove"}, {"title": "It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI", "author": "A Papenmeier"}, {"title": "Comparing the Perceived Legitimacy of Content Moderation Processes: Contractors, Algorithms, Expert Panels, and Digital Juries", "author": "CA Pan"}, {"title": "\u201cI See You!\u201d: A Design Framework for Interface Cues about Agent Visual Perception from a Thematic Analysis of Videogames", "author": "M Rueben", "abstract": "As artificial agents proliferate, there will be more and more situations in which they must communicate their capabilities to humans, including what they can \u201csee.\u201d Artificial agents have existed for decades in the form of computer-controlled agents in videogames. We analyze videogames in order to not only inspire the design of better agents, but to stop agent designers from replicating research that has already been theorized, designed, and tested in-depth. We present a qualitative thematic analysis of sight cues in videogames and develop a", "keywords": {"keywords": ["artificial agents", "interface cues", "visual perception", "thematic analysis", "videogames"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["agent design"]}}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk"}, {"title": "An empirical investigation on user understanding of algorithms", "author": "PT Ngo"}, {"title": "Transparent-AI Blueprint: Developing a Conceptual Tool to Support the Design of Transparent AI Agents", "author": "Z Zhou"}, {"title": "Transparent and Explainable ML", "author": "A Jung"}, {"title": "Investigating Explanations that Target Training Data.", "author": "AI Anik"}, {"title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems", "author": "C Panigutti"}, {"title": "Trust and Reliance in XAI--Distinguishing Between Attitudinal and Behavioral Measures", "author": "N Scharowski"}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert Users", "author": "C Bove"}, {"title": "Making SHAP Rap: Bridging Local and Global Insights Through Interaction and Narratives", "author": "M Chromik"}, {"title": "A Smart Home Testbed for Evaluating XAI with Non-experts.", "author": "K McAreavey", "abstract": "", "keywords": {"keywords": ["Smart Home", "Testbed", "XAI", "Non-experts", "Evaluation"], "research type": ["Case study"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Smart Home"]}}, {"title": "Exploring User Perception of Causality in Automated Data Insights", "author": "PM Law"}, {"title": "IISM-IS-I-Teaching-Thesis & Practical Seminar Projects-Counterfactual explanations for a tensor flow neural network using Microsoft DICE library: An Experimental \u2026", "author": "A M\u00e4dche"}, {"title": "Journal of Responsible Technology", "author": "A Simkute"}, {"title": "Understanding Science and Policy Making in Agriculture: A Machine Learning Application for India", "author": "B Sj"}, {"title": "Exploring design practices for explaining music recommendations", "author": "H Ryu"}, {"title": "Hybrid Recommender System for a Context Aware Recommendation in the Film Domain", "author": "NP Hern\u00e1ndez L\u00f3pez"}, {"title": "Walking on Eggshells: Using Analogies to Promote Appropriate Reliance in Human-AI Decision Making", "author": "G HE"}, {"title": "Calibrating Trust in AI-Assisted Decision Making", "author": "A Turner"}, {"title": "People underestimate the errors by algorithms for credit scoring and recidivism but tolerate even fewer errors", "author": "F Rebitschek"}, {"title": "Augmented Cross-Selling Through Explainable AI\u2014A Case From Energy Retailing", "author": "F Haag", "abstract": "The advance of Machine Learning (ML) has led to a strong interest in this technology to support decision making. While complex ML models provide predictions that are often more accurate than those of traditional tools, such models often hide the reasoning behind the prediction from their users, which can lead to lower adoption and lack of insight. Motivated by this tension, research has put forth Explainable Artificial Intelligence (XAI) techniques that uncover patterns discovered by ML. Despite the high hopes in both ML and", "keywords": {"keywords": ["Explainable AI", "Machine Learning", "Cross-selling", "Energy Retailing", "Decision making"], "research type": ["Case study"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Energy Retailing"]}}, {"title": "Applications of machine learning and deep learning methods for climate change mitigation and adaptation", "author": "T Ladi"}, {"title": "Algorithmic nudge to make better choices: Evaluating effectiveness of XAI frameworks to reveal biases in algorithmic decision making to users", "author": "P Juneja"}, {"title": "Designing Intelligent Interfaces to Facilitate Communication in Large-Scale Live Streaming", "author": "J Guo"}, {"title": "Perceive, Understand & Predict-Empirical Indication for Facets in Subjective Information Processing Awareness", "author": "TPP Schrills"}, {"title": "Glancee: An Adaptable System for Instructors to Grasp Student Learning Status in Synchronous Online Classes", "author": "S Ma"}, {"title": "Energy democracy: A digital future?", "author": "E Judson"}, {"title": "Evaluating human understanding in XAI systems", "author": "D Gentile"}, {"title": "The effect of explaining algorithmic decisions on the perception of fairness", "author": "BW van der Meulen"}, {"title": "Transparent but incomprehensible: Investigating the relation between transparency, explanations, and usability in automated decision-making", "author": "J Dexe"}, {"title": "Empowering local communities using artificial intelligence", "author": "A Bozzon", "abstract": "We define social impact as how a project influences the society and local communities   community empowerment as a process of yielding agency to communities so that they can use", "keywords": {"keywords": ["social impact", "local communities", "artificial intelligence", "community empowerment", "agency"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["sociology"], "content": ["application"], "application area": ["social services"]}}, {"title": "Making AI Explainable in the Global South: A Systematic Review", "author": "CT Okolo"}, {"title": "A Game-Based Approach for Helping Designers Learn Machine Learning Concepts", "author": "CM Myers"}, {"title": "Explanation Interfaces for Sales Forecasting", "author": "TB Fahse"}, {"title": "Visual and Textual Common Semantic Spaces for the Analysis of Multimodal Content", "author": "LVB Beltr\u00e1n"}, {"title": "Participatory Design of a Machine Learning Driven Visualization System for Non-Technical Stakeholders", "author": "JJ Benjamin"}, {"title": "Self-Tracking Data for Professional Use", "author": "T van den Driesschen"}, {"title": "Designing and Building Evidence-Based Intelligent Algorithmic Systems in Online Communities", "author": "B Yu"}, {"title": "A quarter century of usable security and privacy research: transparency, tailorability, and the road ahead", "author": "C Reuter"}, {"title": "Situated Case Studies for a Human-Centered Design of Explanation User Interfaces", "author": "C M\u00fcller-Birn"}, {"title": "IISM-IS-I-Research-Research Departments-Intelligent Enterprise Systems-Explainable Interactive Machine Learning", "author": "A M\u00e4dche", "abstract": "ML is now increasingly entering the mainstream and supporting human decision-making (Wang et al., 2019). Because of their ability to learn, ML systems are changing how complex decisions are made (\u00c5gerfalk, 2020; Kellogg, Valentine, and Christin, 2020). However, the effectiveness of these systems is limited by the machine's inability to explain its thoughts and actions to human users (Wang et al., 2019). The high complexity of some ML models leads to a lack of understandability of their decisions, which is a major problem (Dosilovic, Brcic", "keywords": {"keywords": ["machine learning", "explainable", "interactive", "decision-making", "complexity"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["intelligent enterprise systems"]}}, {"title": "A study of UX practitioners roles in designing real-world, enterprise ML systems", "author": "S Zdanowska"}, {"title": "A Comparative User Study of Human Predictions in Algorithm-Supported Recidivism Risk Assessment", "author": "M Portela"}, {"title": "Human-Centered Evaluation of Explanations", "author": "J Boyd-Graber"}, {"title": "Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretability", "author": "JJ Benjamin"}, {"title": "policy linkages: Empirical evidence from agroeconomic research in India", "author": "SJ Balaji"}, {"title": "Towards a multi-stakeholder value-based assessment framework for algorithmic systems", "author": "M Yurrita"}, {"title": "SELF-DIAGNOSIS THROUGH CHATBOT-BASED SYMPTOM CHECKERS: USER EXPERIENCES AND DESIGN CONSIDERATIONS", "author": "Y You"}, {"title": "Design guidelines for narrative maps in sensemaking tasks", "author": "BF Keith Norambuena"}, {"title": "DeepShovel: An Online Collaborative Platform for Data Extraction in Geoscience Literature with AI Assistance", "author": "S Zhang"}, {"title": "Tell Me Something That Will Help Me Trust You: A Survey of Trust Calibration in Human-Agent Interaction", "author": "GJ Cancro", "abstract": "When a human receives a prediction or recommended course of action from an intelligent agent, what additional information, beyond the prediction or recommendation itself, does the human require from the agent to decide whether to trust or reject the prediction or recommendation? In this paper we survey literature in the area of trust between a single human supervisor and a single agent subordinate to determine the nature and extent of this additional information and to characterize it into a taxonomy that can be leveraged by future", "keywords": {"keywords": ["trust calibration", "human-agent interaction", "prediction", "recommendation", "taxonomy"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI trust"]}}, {"title": "Investigating Algorithmic Decision-Making with Applications to Student Diversity at Illinois Tech", "author": "A Khan"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XAI", "author": "Q Wang"}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study", "author": "L Chazette"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Beyond the\" Black Box\": Enabling Meaningful Transparency of Algorithmic Decision-Making Systems through Public Registers", "author": "M Murad"}, {"title": "Towards Diverse AI: Can an AI-Human Hybrid Council Prevent Future Apartheids?", "author": "GDJ Barbosa"}, {"title": "Tensions in transparent urban AI: designing a smart electric vehicle charge point", "author": "K Alfrink"}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare Systeme", "author": "M Balci"}, {"title": "\u53ef\u89c6\u5316\u4e0e\u4eba\u5de5\u667a\u80fd\u4ea4\u53c9\u7814\u7a76\u7efc\u8ff0", "author": "\u590f\u4f73\u5fd7\uff0c \u674e\u6770\uff0c \u9648\u601d\u660e\uff0c \u79e6\u7ea2\u661f\uff0c \u5218\u4e16\u971e"}, {"title": "Vers un outil s\u00e9mantique d'autocodage qualitatif pour l'\u00e9valuation de l'acceptabilit\u00e9 des innovations", "author": "D Simonnet", "abstract": "Les techniques du Traitement Automatique des Langues (TAL) et les m\u00e9thodes d'analyse qualitative des donn\u00e9es textuelles entretiennent une certaine affinit\u00e9 \u00e9pist\u00e9mologique. Malgr\u00e9 cela, l'analyse qualitative ne profite pas pleinement des apports potentiels du TAL. En particulier, les travaux visant \u00e0 une r\u00e9elle automatisation du codage qualitatif des donn\u00e9es restent somme toute assez rares. Cette th\u00e8se se donne pour ambition d'investiguer le potentiel de diff\u00e9rentes techniques du TAL dans cet objectif et pour une t\u00e2che qui", "keywords": {"keywords": ["Traitement Automatique des Langues", "analyse qualitative", "acceptabilit\u00e9 des innovations", "codage qualitatif des donn\u00e9es", "automatisation"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Innovation evaluation"]}}], "gamut: a design probe to understand how data scientists understand machine learning models": [{"title": "Manipulating and measuring model interpretability", "author": "F Poursabzi-Sangdeh", "abstract": "As such, we argue that to understand interpretability, it is necessary to directly manipulate  diferent factors and measure their efects. What is or is not interpretable must be defned by", "keywords": {"keywords": ["interpretability", "manipulating", "measuring", "factors", "effects"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao"}, {"title": "The what-if tool: Interactive probing of machine learning models", "author": "J Wexler"}, {"title": "One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques", "author": "V Arya"}, {"title": "Interpreting interpretability: understanding data scientists' use of interpretability tools for machine learning", "author": "H Kaur"}, {"title": "Interpretml: A unified framework for machine learning interpretability", "author": "H Nori"}, {"title": "Toward trustworthy AI development: mechanisms for supporting verifiable claims", "author": "M Brundage"}, {"title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations", "author": "F Hohman"}, {"title": "Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI systems", "author": "B Shneiderman"}, {"title": "The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and \u2026", "author": "AF Markus"}, {"title": "What do we want from Explainable Artificial Intelligence (XAI)?\u2013A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research", "author": "M Langer", "abstract": "Previous research in Explainable Artificial Intelligence (XAI) suggests that a main aim of explainability approaches is to satisfy specific interests, goals, expectations, needs, and demands regarding artificial systems (we call these \u201cstakeholders' desiderata\u201d) in a variety of contexts. However, the literature on XAI is vast, spreads out across multiple largely disconnected disciplines, and it often remains unclear how explainability approaches are supposed to achieve the goal of satisfying stakeholders' desiderata. This paper discusses", "keywords": {"keywords": ["Explainable Artificial Intelligence", "XAI", "stakeholders", "conceptual model", "interdisciplinary research"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Human factors in model interpretability: Industry practices, challenges, and needs", "author": "SR Hong"}, {"title": "The state of the art in enhancing trust in machine learning models with the use of visualizations", "author": "A Chatzimparmpas"}, {"title": "COGAM: measuring and moderating cognitive load in machine learning model explanations", "author": "A Abdul"}, {"title": "Interactive machine teaching: a human-centered approach to building machine-learned models", "author": "G Ramos"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "A human-centered agenda for intelligible machine learning", "author": "JW Vaughan"}, {"title": "Dece: Decision explorer with counterfactual explanations for machine learning models", "author": "F Cheng"}, {"title": "Focused or stuck together: multimodal patterns reveal triads' performance in collaborative problem solving", "author": "H Vrzakova"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "Understanding client support strategies to improve clinical outcomes in an online mental health intervention", "author": "P Chikersal", "abstract": "Online mental health interventions are increasingly important in providing access to, and supporting the effectiveness of, mental health treatment. While these technologies are effective, user attrition and early disengagement are key challenges. Evidence suggests that integrating a human supporter into such services mitigates these challenges, however, it remains under-studied how supporter involvement benefits client outcomes, and how to maximize such effects. We present our analysis of 234,735 supporter messages to discover", "keywords": {"keywords": ["online mental health interventions", "client support strategies", "clinical outcomes", "human supporter", "user attrition"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["sociology"], "content": ["Application"], "application area": ["mental health"]}}, {"title": "Operationalizing human-centered perspectives in explainable AI", "author": "U Ehsan"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "What are people doing about XAI user experience? A survey on AI explainability research and practice", "author": "JJ Ferreira"}, {"title": "Big data visualization and analytics: Future research challenges and emerging applications", "author": "G Andrienko"}, {"title": "TeleGam: Combining visualization and verbalization for interpretable machine learning", "author": "F Hohman"}, {"title": "Mental models of mere mortals with explanations of reinforcement learning", "author": "A Anderson"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Designing AI for trust and collaboration in time-constrained medical decisions: a sociotechnical lens", "author": "M Jacobs"}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "ExplainExplore: Visual exploration of machine learning explanations", "author": "D Collaris", "abstract": "Machine learning models often exhibit complex behavior that is difficult to understand. Recent research in explainable AI has produced promising techniques to explain the inner workings of such models using feature contribution vectors. These vectors are helpful in a wide variety of applications. However, there are many parameters involved in this process and determining which settings are best is difficult due to the subjective nature of evaluating interpretability. To this end, we introduce EXPLAINEXPLORE: an interactive explanation", "keywords": {"keywords": ["machine learning explanations", "interpretable AI", "feature contribution vectors", "visual exploration", "interactive explanation"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "Reliable post hoc explanations: Modeling uncertainty in explainability", "author": "D Slack"}, {"title": "Exploring explainability: a definition, a model, and a knowledge catalogue", "author": "L Chazette"}, {"title": "Learning machine learning with personal data helps stakeholders ground advocacy arguments in model mechanics", "author": "Y Register"}, {"title": "Attention flows: Analyzing and comparing attention mechanisms in language models", "author": "JF DeRose"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Effect of information presentation on fairness perceptions of machine learning predictors", "author": "N Van Berkel"}, {"title": "Human-Centered AI", "author": "B Shneiderman"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "Visual, textual or hybrid: the effect of user expertise on different explanations", "author": "M Szymanski", "abstract": "As the use of AI algorithms keeps rising continuously, so does the need for their transparency and accountability. However, literature often adopts a one-size-fits-all approach for developing explanations when in practice, the type of explanations needed depends on the type of end-user. This research will look at user expertise as a variable to see how different levels of expertise influence the understanding of explanations. The first iteration consists of developing two common types of explanations (visual and textual", "keywords": {"keywords": ["user expertise", "explanations", "transparency", "accountability", "AI algorithms"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["AI explanations"]}}, {"title": "Machine Learning Uncertainty as a Design Material: A Post-Phenomenological Inquiry", "author": "JJ Benjamin"}, {"title": "Embedding comparator: Visualizing differences in global structure and local neighborhoods via small multiples", "author": "A Boggust"}, {"title": "Crosscheck: Rapid, reproducible, and interpretable model evaluation", "author": "D Arendt"}, {"title": "Facilitating knowledge sharing from domain experts to data scientists for building nlp models", "author": "S Park"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Vine: visualizing statistical interactions in black box models", "author": "M Britton"}, {"title": "Epistemic values in feature importance methods: Lessons from feminist epistemology", "author": "L Hancox-Li"}, {"title": "A Survey of Human\u2010Centered Evaluations in Human\u2010Centered Machine Learning", "author": "F Sperrle"}, {"title": "You'd better stop! Understanding human reliance on machine learning models under covariate shift", "author": "CW Chiang"}, {"title": "DFSeer: A visual analytics approach to facilitate model selection for demand forecasting", "author": "D Sun", "abstract": "Selecting an appropriate model to forecast product demand is critical to the manufacturing industry. However, due to the data complexity, market uncertainty and users' demanding requirements for the model, it is challenging for demand analysts to select a proper model. Although existing model selection methods can reduce the manual burden to some extent, they often fail to present model performance details on individual products and reveal the potential risk of the selected model. This paper presents DFSeer, an interactive visualization", "keywords": {"keywords": ["demand forecasting", "model selection", "visual analytics", "product demand", "risk assessment"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Manufacturing industry"]}}, {"title": "Phonetic lessons from automatic phonemic transcription: preliminary reflections on Na (Sino-Tibetan) and Tsuut'ina (Dene) data", "author": "A Michaud"}, {"title": "A visual analytics approach for exploratory causal analysis: Exploration, validation, and applications", "author": "X Xie"}, {"title": "A visual analytics framework for explaining and diagnosing transfer learning processes", "author": "Y Ma"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "Melody: generating and visualizing machine learning model summary to understand data and classifiers together", "author": "GYY Chan"}, {"title": "Show or suppress? Managing input uncertainty in machine learning model explanations", "author": "D Wang"}, {"title": "Outcome-explorer: A causality guided interactive visual interface for interpretable algorithmic decision making", "author": "MN Hoque"}, {"title": "Machine learning practices outside big tech: How resource constraints challenge responsible development", "author": "A Hopkins"}, {"title": "Sibyl: Understanding and addressing the usability challenges of machine learning in high-stakes decision making", "author": "A Zytek"}, {"title": "Umlaut: Debugging deep learning programs using program structure and model behavior", "author": "E Schoop", "abstract": "Training deep neural networks can generate non-descriptive error messages or produce unusual output without any explicit errors at all. While experts rely on tacit knowledge to apply debugging strategies, non-experts lack the experience required to interpret model output and correct Deep Learning (DL) programs. In this work, we identify DL debugging heuristics and strategies used by experts, andIn this work, we categorize the types of errors novices run into when writing ML code, and map them onto opportunities where tools could", "keywords": {"keywords": ["debugging", "deep learning", "program structure", "model behavior", "error messages"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Deep Learning"]}}, {"title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspective", "author": "H Lakkaraju"}, {"title": "The human-AI relationship in decision-making: AI explanation to support people on justifying their decisions", "author": "JJ Ferreira"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence", "author": "W Jin"}, {"title": "Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approach", "author": "H Mucha"}, {"title": "Do ML experts discuss explainability for AI systems? a discussion case in the industry for a domain-specific solution", "author": "JJ Ferreira"}, {"title": "Are you sure? Prediction revision in automated decision\u2010making", "author": "N Burkart"}, {"title": "Subplex: Towards a better understanding of black box model explanations at the subpopulation level", "author": "GYY Chan"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "HypoML: Visual analysis for hypothesis-based evaluation of machine learning models", "author": "Q Wang"}, {"title": "Explainable Artificial Intelligence for Predictive Analytics on Customer Turnover: A User-Friendly Interface for Non-expert Users", "author": "J Souza", "abstract": "Nowadays, machine learning techniques have become critical for decision-making mechanisms in numerous real-life applications in areas like healthcare, justice, transportation, and finance. However, recommendations made by machine learning techniques, as well as their logical reasoning behind these recommendation decisions, are often not easy to be comprehended by humans. In this chapter, we present an explainable artificial intelligence web interface that integrates and enhances the state-of-the-art", "keywords": {"keywords": ["Explainable Artificial Intelligence", "Predictive Analytics", "Customer Turnover", "User-Friendly Interface", "Non-expert Users"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["business"]}}, {"title": "Gam changer: Editing generalized additive models with interactive visualization", "author": "ZJ Wang"}, {"title": "From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence", "author": "DA Melis"}, {"title": "Geosmartness for personalized and sustainable future urban mobility", "author": "M Raubal"}, {"title": "Threshy: supporting safe usage of intelligent web services", "author": "A Cummaudo"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "Vaine: Visualization and ai for natural experiments", "author": "G Guo"}, {"title": "Explaining Interpretable Machine Learning: Theory, Methods and Applications", "author": "M Benk"}, {"title": "Evaluating neural model robustness for machine comprehension", "author": "W Wu"}, {"title": "EUCA: the End-User-Centered Explainable AI Framework", "author": "W Jin"}, {"title": "Fairness, explainability, privacy, and robustness for trustworthy algorithmic decision making", "author": "S Majumdar", "abstract": "With the rapid increase in use and deployment of machine learning (ML) systems in the world, concomitant concerns on the ethical implications of their downstream effect have surfaced in recent years. Responding to this challenge, the field of trustworthy ML has grown rapidly, and resulted in a large body of methods and algorithms that embody desirable qualities such as fairness, transparency, privacy and robustness. In this chapter, we survey the current landscape of trustworthy ML methods, introduce fundamental concepts, and", "keywords": {"keywords": ["fairness", "explainability", "privacy", "robustness", "trustworthy algorithmic decision making"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Intuitively assessing ml model reliability through example-based explanations and editing model inputs", "author": "H Suresh"}, {"title": "Analyse d'erreurs de transcriptions phon\u00e9miques automatiques d'une langue \u00abrare\u00bb: le na (mosuo)", "author": "A Michaud"}, {"title": "La transcription du linguiste au miroir de l'intelligence artificielle: r\u00e9flexions \u00e0 partir de la transcription phon\u00e9mique automatique", "author": "A Michaud"}, {"title": "Explainable artificial intelligence for human-friendly explanations to predictive analytics on big data", "author": "J do Nascimento Souza"}, {"title": "From human explanation to model interpretability: A framework based on weight of evidence", "author": "D Alvarez-Melis"}, {"title": "Explainable artificial intelligence for data science on customer churn", "author": "CK Leung"}, {"title": "LEGION: Visually compare modeling techniques for regression", "author": "S Das"}, {"title": "Visually analyzing and steering zero shot learning", "author": "S Sahoo"}, {"title": "Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional networks", "author": "W Zhang"}, {"title": "Model-Centric Verification of Artificial Intelligence", "author": "N Gisolfi", "abstract": "This work shows how provable guarantees can be used to supplement probabilistic estimates in the context of Artificial Intelligence (AI) systems. Statistical techniques measure the expected performance of a model, but low error rates say nothing about the ways in which errors manifest. Formal verification of model adherence to design specifications can yield certificates which explicitly detail the operational conditions under which violations occur. These certificates enable developers and users of AI systems to reason about their", "keywords": {"keywords": ["Model-Centric Verification", "Artificial Intelligence", "provable guarantees", "probabilistic estimates", "formal verification"], "research type": ["theoretical"], "methodology": ["Quantitative"], "purposes": ["Prescriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI systems"]}}, {"title": "How Do People Rank Multiple Mutant Agents?", "author": "J Dodge"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky"}, {"title": "Explaining by Conversing: The Argument for Conversational Xai Systems", "author": "W Marrakchi"}, {"title": "Evaluating Neural Machine Comprehension Model Robustness to Noisy Inputs and Adversarial Attacks", "author": "W Wu"}, {"title": "Towards trustworthy deception detection: Benchmarking model robustness across domains, modalities, and languages", "author": "M Glenski"}, {"title": "Mediators: Conversational Agents Explaining NLP Model Behavior", "author": "N Feldhus"}, {"title": "Community Acknowledgment: Engaging Community Members in Volunteer Acknowledgment", "author": "F Gui"}, {"title": "Data Summaries for Scalable Visual Analysis", "author": "GYY Chan"}, {"title": "StrategyAtlas: Strategy Analysis for Machine Learning Interpretability", "author": "D Collaris"}, {"title": "Big Data Visualization and Analytics: Future Research Challenges and Emerging Applications\u2013Part 2", "author": "G Li", "abstract": "In the context of data visualization and analytics, this report outlines some of the  challenges and emerging applications that arise in the Big Data era. In particularly, fourteen", "keywords": {"keywords": ["Big Data", "visualization", "analytics", "emerging applications", "research challenges"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["information technology"]}}, {"title": "Improving Data Explainability in Analysis of Designed Computer Simulation Experiments", "author": "S Xie"}, {"title": "The Role of Human Knowledge in Explainable AI", "author": "A Tocchetti"}, {"title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?", "author": "A Balayn"}, {"title": "Building Trust in Interactive Machine Learning via User Contributed Interpretable Rules", "author": "L Guo"}, {"title": "Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values", "author": "ZJ Wang"}, {"title": "HypperSteer: Hypothetical Steering and Data Perturbation in Sequence Prediction with Deep Learning", "author": "C Wang"}, {"title": "Interactive Scalable Interfaces for Machine Learning Interpretability", "author": "F Hohman"}, {"title": "Visual Exploration of Machine Learning Model Behavior with Hierarchical Surrogate Rule Sets", "author": "J Yuan"}, {"title": "How Christian School Teachers in Grades 6\u20138 Learn Classroom Management to Lead: A Qualitative Descriptive Study", "author": "T Vandervort"}, {"title": "GAM (e) changer or not? An evaluation of interpretable machine learning models based on additive model constraints", "author": "P Zschech", "abstract": "The number of information systems (IS) studies dealing with explainable artificial intelligence (XAI) is currently exploding as the field demands more transparency about the internal decision logic of machine learning (ML) models. However, most techniques subsumed under XAI provide post-hoc-analytical explanations, which have to be considered with caution as they only use approximations of the underlying ML model. Therefore, our paper investigates a series of intrinsically interpretable ML models and discusses their", "keywords": {"keywords": ["explainable artificial intelligence", "interpretable machine learning", "internal decision logic", "post-hoc-analytical explanations", "approximations"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["information systems"]}}, {"title": "Focused or stuck together", "author": "H Vrzakova"}, {"title": "Augmenting Decision Making via Interactive What-If Analysis", "author": "S Gathani"}, {"title": "Active learning for medical code assignment", "author": "MD Ferreira"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "Visualization Guidelines for Model Performance Communication Between Data Scientists and Subject Matter Experts", "author": "A Suh"}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert Users", "author": "C Bove"}, {"title": "Math Augmentation: How Authors Enhance the Readability of Formulas using Novel Visual Design Practices", "author": "A Head"}, {"title": "Big Data Visualization and Analytics: Future Research Challenges and Emerging Applications\u2013Part", "author": "S Drucker"}, {"title": "How Teams Communicate about the Quality of ML Models: A Case Study at an International Technology Company", "author": "J Almahmoud"}, {"title": "Understanding Diversity in Human-AI Data: What Cognitive Style Disaggregation Reveals", "author": "A Anderson", "abstract": "Artificial Intelligence (AI) is becoming more pervasive through all levels of society, trying to help us be more productive. Research like Amershi et al.'s 18 guidelines for human-AI interaction aim to provide high-level design advice, yet little remains known about how people react to Applications or Violations of the guidelines. This leaves a gap for designers of human-AI systems applying such guidelines, where AI-powered systems might be working better for certain sets of users than for others, inadvertently introducing", "keywords": {"keywords": ["human-AI data", "cognitive style disaggregation", "diversity", "guidelines for human-AI interaction", "AI-powered systems"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["application"], "application area": ["human-AI interaction"]}}, {"title": "Using Visual Analytics to Support Artificial Intelligence Development", "author": "NC Chen"}, {"title": "Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness", "author": "Y Nakao"}, {"title": "Algorithmic nudge to make better choices: Evaluating effectiveness of XAI frameworks to reveal biases in algorithmic decision making to users", "author": "P Juneja"}, {"title": "Interpretive Temporal Sequence Visualization", "author": "C Wang"}, {"title": "Optimal number of clusters in explainable data analysis of agent-based simulation experiments", "author": "S Xie"}, {"title": "Generating visualizations of analytical causal graphs", "author": "F Du"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Institutional Knowledge at Singapore Management Universit y", "author": "SH TAN"}, {"title": "Improving the Reliability of Cloud-Based Pre-Trained Machine Learning Models", "author": "A Cummaudo"}, {"title": "ForSense: Accelerating Online Research Through Sensemaking Integration and Machine Research Support", "author": "G Ramos", "abstract": "Online research is a frequent and important activity people perform on the Internet, yet current support for this task is basic, fragmented and not well integrated into web browser experiences. Guided by sensemaking theory, we present ForSense, a browser extension for accelerating people's online research experience. The two primary sources of novelty of ForSense are the integration of multiple stages of online research and providing machine assistance to the user by leveraging recent advances in neural-driven machine reading. We", "keywords": {"keywords": ["online research", "sensemaking", "web browser", "machine assistance", "neural-driven machine reading"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive", "Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Online research acceleration"]}}, {"title": "Participatory Design of a Machine Learning Driven Visualization System for Non-Technical Stakeholders", "author": "JJ Benjamin"}, {"title": "Epistemic values in feature importance methods", "author": "L Hancox-Li"}, {"title": "Understanding and Scaffolding Family Literacy with Voice-Based Technology in Rural C\u00f4te d'Ivoire", "author": "M Madaio"}, {"title": "A Taxonomy of User-centered Explainable AI Studies", "author": "LV Herm"}, {"title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "author": "H Kaur"}, {"title": "Situated Case Studies for a Human-Centered Design of Explanation User Interfaces", "author": "C M\u00fcller-Birn"}, {"title": "Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Compliance", "author": "A Bell"}, {"title": "Visual Analytics for Data Science: A Critical View", "author": "D Fekete"}, {"title": "Visualization Processing", "author": "G Li"}, {"title": "Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretability", "author": "JJ Benjamin", "abstract": "During a research project in which we developed a machine learning (ML) driven visualization system for non-ML experts, we reflected on interpretability research in ML, computer-supported collaborative work and human-computer interaction. We found that while there are manifold technical approaches, these often focus on ML experts and are evaluated in decontextualized empirical studies. We hypothesized that participatory design research may support the understanding of stakeholders' situated sense-making in our", "keywords": {"keywords": ["machine learning", "interpretability", "visualization system", "participatory design research", "socio-technical contextualization"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["ML interpretability"]}}, {"title": "Visual Analysis of the Product Supply Chain in Smart Manufacturing and Industry 4.0", "author": "D Sun"}, {"title": "Mental Models and Interpretability in AI Fairness Tools and Code Environments", "author": "J Thompson"}, {"title": "Summarize with Caution: Comparing Global Feature Attributions.", "author": "A Okeson"}, {"title": "Learning-From-Disagreement: A Model Comparison and Visual Analytics Framework", "author": "J Wang"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XAI", "author": "Q Wang"}, {"title": "Human-centered explainability for life sciences, healthcare, and medical informatics", "author": "S Dey"}, {"title": "A Design Case Study of Artificial Intelligence Pipeline Visualization", "author": "Y Park"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Strategies for Selecting and Adapting Machine Learning Systems to Support Different Types of Experts", "author": "A Okeson"}, {"title": "SUMMIT: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations", "author": "B Class", "abstract": "present SUMMIT, an interactive system that scalably and systematically summarizes and  visualizes what features a deep learning  SUMMIT introduces two new scalable summarization", "keywords": {"keywords": ["deep learning", "interpretability", "visualization", "activation", "attribution"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": "Artificial intelligence"}}, {"title": "\u53ef\u89c6\u5316\u4e0e\u4eba\u5de5\u667a\u80fd\u4ea4\u53c9\u7814\u7a76\u7efc\u8ff0", "author": "\u590f\u4f73\u5fd7\uff0c \u674e\u6770\uff0c \u9648\u601d\u660e\uff0c \u79e6\u7ea2\u661f\uff0c \u5218\u4e16\u971e"}, {"title": "\uc778\uacf5\uc9c0\ub2a5 \uc218\ud589\uacfc\uc815 \uc2dc\uac01\ud654\uc5d0 \ub300\ud55c \ub514\uc790\uc778 \uc0ac\ub840 \uc5f0\uad6c", "author": "\ubc15\uc724\ud558\uff0c \uc724\uc7ac\uc601"}], "let me explain: impact of personal and impersonal explanations on trust in recommender systems": [{"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making", "author": "Y Zhang", "abstract": "Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to", "keywords": {"keywords": ["confidence", "explanation", "accuracy", "trust calibration", "AI-assisted decision making"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Decision making"]}}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal"}, {"title": "A survey of contrastive and counterfactual explanation generation methods for explainable artificial intelligence", "author": "I Stepin"}, {"title": "Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai", "author": "A Jacovi"}, {"title": "\" Why is' Chicago'deceptive?\" Towards Building Model-Driven Tutorials for Humans", "author": "V Lai"}, {"title": "Generate neural template explanations for recommendation", "author": "L Li"}, {"title": "PRINCE: Provider-side interpretability with counterfactual explanations in recommender systems", "author": "A Ghazimatin"}, {"title": "Mental models of AI agents in a cooperative game setting", "author": "KI Gero"}, {"title": "Exploring mental models for transparent and controllable recommender systems: a qualitative study", "author": "T Ngo"}, {"title": "What's in a user? Towards personalising transparency for music recommender interfaces", "author": "M Millecamp"}, {"title": "Charting the design and analytics agenda of learnersourcing systems", "author": "H Khosravi", "abstract": "Learnersourcing is emerging as a viable learner-centred and pedagogically justified approach for harnessing the creativity and evaluation power of learners as experts-in-training. Despite the increasing adoption of learnersourcing in higher education, understanding students' behaviour while engaged in learnersourcing and best practices for the design and development of learnersourcing systems are still largely under-researched. This paper offers data-driven reflections and lessons learned from the development and", "keywords": {"keywords": ["learnersourcing", "higher education", "students behavior", "design and development", "data-driven reflections"], "research type": ["case study"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["education"], "content": ["application"], "application area": ["learner-centered pedagogy"]}}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Understanding user behavior for document recommendation", "author": "X Xu"}, {"title": "Explaining recommendations by means of aspect-based transparent memories", "author": "T Donkers"}, {"title": "Generation and evaluation of factual and counterfactual explanations for decision trees and fuzzy rule-based classifiers", "author": "I Stepin"}, {"title": "Interacting with explanations through critiquing", "author": "D Antognini"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Argumentative explanations for interactive recommendations", "author": "A Rago"}, {"title": "Combined Cognitive-Motivational Modules Delivered via an LMS Increase Undergraduate Biology Grades.", "author": "JG Cromley"}, {"title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations", "author": "S Arora"}, {"title": "Graph neural networks for recommender system", "author": "C Gao", "abstract": "introduces the preliminaries for recommender systems and graph neural networks. Then, it   in recommender systems and categorizes the existing GNN-based recommendation models.", "keywords": {"keywords": ["Recommender systems", "Graph neural networks", "GNN-based recommendation models"], "research type": ["Review"], "methodology": ["N/A"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Recommendation systems"]}}, {"title": "Newsviz: depicting and controlling preference profiles using interactive treemaps in news recommender systems", "author": "J Kunkel"}, {"title": "Cogito ergo quid? The effect of cognitive style in a transparent mobile music recommender system", "author": "M Millecamp"}, {"title": "Exploring the impact of simple explanations and agency on batch deep reinforcement learning induced pedagogical policies", "author": "M Sanz Ausin"}, {"title": "\u201cKnowing me, knowing you\u201d: personalized explanations for a music recommender system", "author": "M Martijn"}, {"title": "Dax: Deep argumentative explanation for neural networks", "author": "E Albini"}, {"title": "CourseQ: the impact of visual and interactive course recommendation in university environments", "author": "B Ma"}, {"title": "Factual and counterfactual explanation of fuzzy information granules", "author": "I Stepin"}, {"title": "AI-Assisted Human Labeling: Batching for Efficiency without Overreliance", "author": "Z Ashktorab"}, {"title": "Multi-step critiquing user interface for recommender systems", "author": "DA Petrescu"}, {"title": "Leveraging arguments in user reviews for generating and explaining recommendations", "author": "T Donkers", "abstract": "Review texts constitute a valuable source for making system-generated recommendations both more accurate and more transparent. Reviews typically contain statements providing argumentative support for a given item rating that can be exploited to explain the recommended items in a personalized manner. We propose a novel method called Aspect-based Transparent Memories (ATM) to model user preferences with respect to relevant aspects and compare them to item properties to predict ratings, and, by the same", "keywords": {"keywords": ["user reviews", "recommendations", "argumentation", "aspect-based modeling", "transparency"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Recommendation Systems"]}}, {"title": "\u201cAn Error Occurred!\u201d-Trust Repair With Virtual Robot Using Levels of Mistake Explanation", "author": "K Hald"}, {"title": "Trkic G00gle: Why and how users game translation algorithms", "author": "S Kim"}, {"title": "When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models", "author": "A Rechkemmer"}, {"title": "Hive: evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "Why or why not? The effect of justification styles on chatbot recommendations", "author": "D Wilkinson"}, {"title": "On the Use of Feature-based Collaborative Explanations: An Empirical Comparison of Explanation Styles", "author": "S Naveed"}, {"title": "Path-based reasoning over heterogeneous networks for recommendation via bidirectional modeling", "author": "J Zhang"}, {"title": "Identifying Group-Specific Mental Models of Recommender Systems: A Novel Quantitative Approach", "author": "J Kunkel"}, {"title": "Key qualities of conversational recommender systems: From users' perspective", "author": "Y Jin"}, {"title": "Recent studies of xai-review", "author": "ZF Hu", "abstract": "Over the past years, there has been an increasing concern regarding the risk of bias and  discrimination in algorithmic systems, which received significant attention amongst the research", "keywords": {"keywords": ["xai", "bias", "discrimination", "algorithmic systems", "research"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["algorithmic systems"]}}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trust", "author": "J Ooge"}, {"title": "A Qualitative Evaluation of User Preference for Link-Based vs. Text-Based Recommendations of Wikipedia Articles", "author": "M Ostendorff"}, {"title": "Bringing Friends into the Loop of Recommender Systems: An Exploratory Study", "author": "J Piao"}, {"title": "Specialized document embeddings for aspect-based similarity of research papers", "author": "M Ostendorff"}, {"title": "On fairness and interpretability", "author": "JM Jose"}, {"title": "Map-based Visualization of Item Spaces for Increasing Transparency and Control in Recommender Systems", "author": "J Kunkel"}, {"title": "Classifeye: classification of personal characteristics based on eye tracking data in a recommender system interface", "author": "M Millecamp"}, {"title": "Towards Human-Centred Explainable AI: A Systematic Literature Review", "author": "O Williams"}, {"title": "Your eyes explain everything: exploring the use of eye tracking to provide explanations on-the-fly", "author": "M Millecamp"}, {"title": "Improving User Experience Through Recommendation Message Design: A Systematic Literature Review of Extant Literature on Recommender Systems and Message \u2026", "author": "A Falconnet", "abstract": "This paper reports the findings of a Systematic Literature Review of extant literature on Recommender Systems (RS) and message design. By identifying, analyzing and synthesizing relevant studies, we aim to generate a contemporary mapping of studies related to user-RS interaction, extend the body of knowledge regarding effective recommendation messages, inform practitioners about the effect of recommendation message design choices on the user's experience, and motivate researchers to conduct", "keywords": {"keywords": ["Recommender Systems", "message design", "user experience", "literature review", "recommendation messages"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["User experience"]}}, {"title": "Social Recommender Systems", "author": "I Guy"}, {"title": "Deep argumentative explanations", "author": "E Albini"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations", "author": "R Shang"}, {"title": "Exploring and Eliciting Needs and Preferences for Wikidata", "author": "KS Alghamdi"}, {"title": "A Transfer Learning Framework for Human-Centric Deep Reinforcement Learning with Reward Engineering", "author": "MS Ausin"}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "Free ranking vs. rank-choosing: New insights on the conjunction fallacy", "author": "T Polakow"}, {"title": "Measuring and Predicting Human Trust in Recommendations from an AI Teammate", "author": "N Gurney", "abstract": "Predicting compliance with AI recommendations and knowing when to intervene are critical facets of human-AI teaming. AIs are typically deployed in settings where their abilities to evaluate decision variables far exceed the abilities of their human counterparts. However, even though AIs excel at weighing multiple issues and computing near optimal solutions with speed and accuracy beyond that of any human, they still make mistakes. Thus, perfect compliance may be undesirable. This means, just as individuals must know when to follow", "keywords": {"keywords": ["trust", "recommendations", "human-AI teaming", "compliance", "intervention"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-AI teaming"]}}, {"title": "\u201cI See You!\u201d: A Design Framework for Interface Cues about Agent Visual Perception from a Thematic Analysis of Videogames", "author": "M Rueben"}, {"title": "An empirical investigation on user understanding of algorithms", "author": "PT Ngo"}, {"title": "A Diary Study of Social Explanations for Recommendations in Daily Life", "author": "Z Zhang"}, {"title": "It's Just a Recipe?\u2014Comparing Expert and Lay User Understanding of Algorithmic Systems", "author": "T Ngo"}, {"title": "HAEM: Obtaining Higher-Quality Classification Task Results with AI Workers", "author": "Y Yamashita"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Collaboration in the Machine Age: Trustworthy Human-AI Collaboration", "author": "L Razmerita"}, {"title": "Comparing Perception of Disclosure of Different Types of Information Related to Automated Tools.", "author": "V Bracamonte"}, {"title": "Feature-Based Interpretation for Black Box Models in a Restaurant Domain", "author": "M Alshammari"}, {"title": "Designing for Responsible Trust in AI Systems: A Communication Perspective", "author": "QV Liao", "abstract": "Current literature and public discourse on\" trust in AI\" are often focused on the principles underlying trustworthy AI, with insufficient attention paid to how people develop trust. Given that AI systems differ in their level of trustworthiness, two open questions come to the fore: how should AI trustworthiness be responsibly communicated to ensure appropriate and equitable trust judgments by different users, and how can we protect users from deceptive attempts to earn their trust? We draw from communication theories and literature on trust in", "keywords": {"keywords": ["trust", "AI systems", "communication", "trustworthiness", "equitable trust judgments"], "research type": "theoretical", "methodology": "none", "purposes": "explanatory", "discipline": "communication", "content": "none", "application area": "AI systems trust communication"}}, {"title": "Impacts of Personal Characteristics on User Trust in Conversational Recommender Systems", "author": "W Cai"}, {"title": "Designing Persuasive Food Conversational Recommender Systems With Nudging and Socially-Aware Conversational Strategies", "author": "F Pecune"}, {"title": "Tell Me Something That Will Help Me Trust You: A Survey of Trust Calibration in Human-Agent Interaction", "author": "GJ Cancro"}, {"title": "Personalized Interfaces for Music Recommender Systems", "author": "M Millecamp"}, {"title": "Interactive Methods for Model-based Collaborative Filtering Recommender Systems", "author": "B Loepp"}, {"title": "Repenser la recommandation pour \u00e9laborer des Territoires Informatiques d'Apprentissage Humain (TIAH)", "author": "G Bergeret"}], "explanations as mechanisms for supporting algorithmic transparency": [{"title": "Guidelines for human-AI interactio", "author": "S Amershi", "abstract": "18 generally applicable design guidelines for human-AI interaction. These guidelines are  vali guidelines against 20 popular AI-infused products. The results verify the relevance of the", "keywords": {"keywords": ["human-AI interaction", "design guidelines", "AI-infused products", "relevance", "verify"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Prescriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-Computer Interaction"]}}, {"title": "Manipulating and measuring model interpretabilit", "author": "F Poursabzi-Sangdeh"}, {"title": "Playing the visibility game: How digital influencers and algorithms negotiate influence on Instagra", "author": "K Cotter"}, {"title": "Questioning the AI: informing design practices for explainable AI user experience", "author": "QV Liao"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI system", "author": "S Mohseni"}, {"title": "Automated rationale generation: a technique for explainable AI and its effects on human perception", "author": "U Ehsan"}, {"title": "Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concern", "author": "H Felzmann"}, {"title": "The effects of example-based explanations in a machine learning interfac", "author": "CJ Cai"}, {"title": "Rise of machine agency: A framework for studying the psychology of human\u2013AI interaction (HAII", "author": "SS Sundar"}, {"title": "Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forwar", "author": "S Lo Piano"}, {"title": "Does transparency in moderation really matter? User behavior after content removal explanations on reddi", "author": "S Jhaver", "abstract": "When posts are removed on a social media platform, users may or may not receive an explanation. What kinds of explanations are provided? Do those explanations matter? Using a sample of 32 million Reddit posts, we characterize the removal explanations that are provided to Redditors, and link them to measures of subsequent user behaviors---including future post submissions and future post removals. Adopting a topic modeling approach, we show that removal explanations often provide information that educate users about the", "keywords": {"keywords": ["transparency", "moderation", "user behavior", "content removal explanations", "Reddit"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Social media"]}}, {"title": "Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediatio", "author": "MK Lee"}, {"title": "The fallacy of inscrutabilit", "author": "JA Kroll"}, {"title": "A survey of evaluation methods and measures for interpretable machine learnin", "author": "S Mohseni"}, {"title": "What do we want from Explainable Artificial Intelligence (XAI)?\u2013A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI researc", "author": "M Langer"}, {"title": "A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic score", "author": "M De-Arteaga"}, {"title": "User attitudes towards algorithmic opacity and transparency in online reviewing platform", "author": "M Eslami"}, {"title": "Expanding explainability: Towards social transparency in ai system", "author": "U Ehsan"}, {"title": "\" Did you suspect the post would be removed?\" Understanding user reactions to content removals on Reddi", "author": "S Jhaver"}, {"title": "Dark Patterns of Explainability, Transparency, and User Control for Intelligent Systems", "author": "M Chromik"}, {"title": "A governance framework for algorithmic accountability and transparenc", "author": "A Koene", "abstract": "Algorithmic systems are increasingly being used as part of decision-making processes in both the public and private sectors, with potentially significant consequences for individuals, organisations and societies as a whole. Algorithmic systems in this context refer to the combination of algorithms, data and the interface process that together determine the outcomes that affect end users. Many types of decisions can be made faster and more efficiently using algorithms. A significant factor in the adoption of algorithmic systems for", "keywords": {"keywords": ["algorithmic accountability", "transparency", "governance framework", "decision-making processes", "algorithmic systems"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["social sciences"], "content": ["methodological"], "application area": ["decision-making"]}}, {"title": "Towards transparency by design for artificial intelligenc", "author": "H Felzmann"}, {"title": "Algorithmic knowledge gaps: A new horizon of (digital) inequalit", "author": "K Cotter"}, {"title": "Crowdsourcing perceptions of fair predictors for machine learning: A recidivism case stud", "author": "N Van Berkel"}, {"title": "Dissonance between human and machine understandin", "author": "Z Zhang"}, {"title": "When people and algorithms meet: User-reported problems in intelligent everyday application", "author": "M Eiband"}, {"title": "A mulching proposal: Analysing and improving an algorithmic system for turning the elderly into high-nutrient slurr", "author": "O Keyes"}, {"title": "Rating reliability and bias in news articles: Does AI assistance help everyone", "author": "BD Horne"}, {"title": "How much automation does a data scientist want", "author": "D Wang"}, {"title": "Ai-generated vs. human artworks. a perception bias towards artificial intelligence", "author": "M Ragot"}, {"title": "Ambiguity-aware ai assistants for medical data analysi", "author": "M Schaekermann", "abstract": "Artificial intelligence (AI) assistants for clinical decision making show increasing promise in medicine. However, medical assessments can be contentious, leading to expert disagreement. This raises the question of how AI assistants should be designed to handle the classification of ambiguous cases. Our study compared two AI assistants that provide classification labels for medical time series data along with quantitative uncertainty estimates: conventional vs. ambiguity-aware. We simulated our ambiguity-aware AI based", "keywords": {"keywords": ["artificial intelligence", "clinical decision making", "medical data analysis", "ambiguity-aware AI assistants", "quantitative uncertainty estimates"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Medicine"], "content": ["Application"], "application area": ["Medical data analysis"]}}, {"title": "Agent, gatekeeper, drug dealer: How content creators craft algorithmic persona", "author": "EY Wu"}, {"title": "Bridging machine learning and mechanism design towards algorithmic fairnes", "author": "J Finocchiaro"}, {"title": "Evaluating the promise of human-algorithm collaborations in everyday work practice", "author": "C Wolf"}, {"title": "Argumentation as a framework for interactive explanations for recommendation", "author": "A Rago"}, {"title": "Middle-Aged Video Consumers' Beliefs About Algorithmic Recommendations on YouTub", "author": "O Alvarado"}, {"title": "Algorithmic bias: review, synthesis, and future research direction", "author": "N Kordzadeh"}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checker", "author": "CH Tsai"}, {"title": "Biometric mirror: Exploring ethical opinions towards facial analysis and automated decision-makin", "author": "N Wouters"}, {"title": "The changing contours of\" participation\" in data-driven, algorithmic ecosystems: Challenges, tactics, and an agend", "author": "CT Wolf"}, {"title": "RADAR: automated task planning for proactive decision suppor", "author": "S Grover", "abstract": "Proactive Decision Support aims at improving the decision making experience of human decision-makers by enhancing the quality of the decisions and the ease of making them. Given that AI techniques are efficient in searching over a potentially large solution space (of decision) and finding good solutions, it can be used for human-in-the-loop scenarios such as disaster response that demand naturalistic decision making. A human decision-maker, in such scenarios, may experience high-cognitive overload leading to a", "keywords": {"keywords": ["proactive decision support", "automated task planning", "AI techniques", "human-in-the-loop scenarios", "cognitive overload"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Disaster response"]}}, {"title": "Outlining traceability: A principle for operationalizing accountability in computing system", "author": "JA Kroll"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparenc", "author": "AI Anik"}, {"title": "Revisiting human-agent communication: The importance of joint co-construction and understanding mental state", "author": "S Kopp"}, {"title": "How to support users in understanding intelligent systems? Structuring the discussio", "author": "M Eiband"}, {"title": "\" At the End of the Day Facebook Does What ItWants\" How Users Experience Contesting Algorithmic Content Moderatio", "author": "K Vaccaro"}, {"title": "Researching AI Legibility through Desig", "author": "J Lindley"}, {"title": "Player-AI interaction: What neural network games reveal about AI as pla", "author": "J Zhu"}, {"title": "I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decision", "author": "M Kasinidou"}, {"title": "Fairness, legitimacy and the regulation of home-sharing platform", "author": "G Newlands"}, {"title": "Street-level realities of data practices in homeless services provisio", "author": "N Karusala", "abstract": "Quantification and standardization of concepts like risk and vulnerability are increasingly being used in high-stakes, client-facing social services, also presenting the potential for data-driven tools for decision-making in this context. These trends necessitate an understanding of the role of quantitative data in the work of street-level decision-makers in social services. We present a qualitative study of existing data practices and perceptions of potential data-driven tools in housing allocation, engaging the perspective of service providers and", "keywords": {"keywords": ["data practices", "homeless services provision", "quantitative data", "street-level decision-makers", "housing allocation"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Social services"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studie", "author": "V Lai"}, {"title": "Investigating searchers' mental models to inform search explanation", "author": "P Thomas"}, {"title": "Combating fake news with interpretable news feed algorithm", "author": "S Mohseni"}, {"title": "Demand-aware career path recommendations: A reinforcement learning approac", "author": "M Kokkodis"}, {"title": "First do no harm: An exploration of researchers' ethics of conduct in Big Data behavioral studie", "author": "M Favaretto"}, {"title": "Ethical dimensions for data qualit", "author": "D Firmani"}, {"title": "Designing conversational agents: A self-determination theory approac", "author": "X Yang"}, {"title": "Combined Cognitive-Motivational Modules Delivered via an LMS Increase Undergraduate Biology Grades", "author": "JG Cromley"}, {"title": "Mediating community-AI interaction through situated explanation: the case of AI-Led moderatio", "author": "Y Kou"}, {"title": "Users & machine learning-based curation system", "author": "H Heuer", "abstract": "Users are increasingly interacting with machine learning (ML)-based curation systems. YouTube and Facebook, two of the most visited websites worldwide, utilize such systems to curate content for billions of users. Contemporary challenges such as fake news, filter bubbles, and biased predictions make the understanding of ML-based curation systems an important and timely concern. Despite their political, social, and cultural importance, practitioners' framing of machine learning and users' understanding of ML-based curation", "keywords": {"keywords": ["users", "machine learning", "curation systems", "fake news", "biased predictions"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Social media"]}}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI system", "author": "M Riveiro"}, {"title": "The right to contest automated decisions under the General Data Protection Regulation: Beyond the so\u2010called \u201cright to explanation", "author": "E Bayaml\u0131o\u011flu"}, {"title": "Drivers of User Engagement in Influencer Brandin", "author": "T Fink"}, {"title": "Adaptive Folk Theorization as a Path to Algorithmic Literacy on Changing Platforms", "author": "MA DeVito"}, {"title": "Understanding the effect that task complexity has on automation potential and opacity: Implications for algorithmic fairnes", "author": "M Vimalkumar"}, {"title": "Fifty Shades of Grey: In Praise of a Nuanced Approach Towards Trustworthy Desig", "author": "L Thornton"}, {"title": "More than accuracy: towards trustworthy machine learning interfaces for object recognitio", "author": "H Heuer"}, {"title": "Algorithm awareness as an important internet skill: the case of voice assistant", "author": "J Gruber"}, {"title": "Better supporting workers in ML workplace", "author": "MF Clarke"}, {"title": "Trustworthy transparency by desig", "author": "V Zieglmeier", "abstract": "Our goal is to enable trustworthy transparency into data usage processes. In order for our  transparency framework to enable data sovereignty, it depends on the acceptance and trust of", "keywords": {"keywords": ["trustworthy", "transparency", "data usage processes", "data sovereignty", "acceptance and trust"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["data governance"]}}, {"title": "Dax: Deep argumentative explanation for neural network", "author": "E Albini"}, {"title": "Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approac", "author": "H Mucha"}, {"title": "Making sense of algorithmic profiling: user perceptions on Faceboo", "author": "M B\u00fcchi"}, {"title": "Minimalistic explanations: capturing the essence of decision", "author": "M Schuessler"}, {"title": "Contestability For Content Moderatio", "author": "K Vaccaro"}, {"title": "Whose walkability? challenges in algorithmically measuring subjective experienc", "author": "M D\u00edaz"}, {"title": "The GIFT of scrutable learner models: why and ho", "author": "J Kay"}, {"title": "Algorithmic Transparency and Contact-tracing Apps-An Empirical Investigation", "author": "T Bitzer"}, {"title": "Took a pic and got declined, vexed and perplexed: facial recognition in algorithmic managemen", "author": "EA Watkins"}, {"title": "Subplex: Towards a better understanding of black box model explanations at the subpopulation leve", "author": "GYY Chan", "abstract": "Understanding the interpretation of machine learning (ML) models has been of paramount importance when making decisions with societal impacts such as transport control, financial activities, and medical diagnosis. While current model interpretation methodologies focus on using locally linear functions to approximate the models or creating self-explanatory models that give explanations to each input instance, they do not focus on model interpretation at the subpopulation level, which is the understanding of model interpretations across different", "keywords": {"keywords": ["black box model explanations", "subpopulation level", "model interpretation", "machine learning models", "societal impacts"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Folk theories of algorithmic operations during Internet use: A mixed methods stud", "author": "L Dogruel"}, {"title": "Understanding Algorithms Through Exploration: Supporting Knowledge Acquisition in Primary Task", "author": "M Eiband"}, {"title": "\" How advertiser-friendly is my video?\": YouTuber's Socioeconomic Interactions with Algorithmic Content Moderatio", "author": "R Ma"}, {"title": "The role of transparency in multi-stakeholder educational recommendation", "author": "Y Zheng"}, {"title": "The agency of the forum: Mechanisms for algorithmic accountability through the lens of agenc", "author": "F Cech"}, {"title": "Identifying opportunities to improve content moderatio", "author": "S Jhaver"}, {"title": "Explainable AI, but explainable to whom", "author": "J Gerlings"}, {"title": "Machine Learning Explanations to Prevent Overtrust in Fake News Detection", "author": "S Mohseni"}, {"title": "Explainable AI, But Explainable to Whom? An Exploratory Case Study of xAI in Healthcar", "author": "J Gerlings"}, {"title": "An Algorithmic Equity Toolkit for Technology Audits by Community Advocates and Activist", "author": "M Katell", "abstract": "A wave of recent scholarship documenting the discriminatory harms of algorithmic systems has spurred widespread interest in algorithmic accountability and regulation. Yet effective accountability and regulation is stymied by a persistent lack of resources supporting public understanding of algorithms and artificial intelligence. Through interactions with a US-based civil rights organization and their coalition of community organizations, we identify a need for (i) heuristics that aid stakeholders in distinguishing between types of analytic and", "keywords": {"keywords": ["algorithmic equity", "technology audits", "community advocates", "algorithmic accountability", "AI"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["discrimination"]}}, {"title": "Critical algorithmic literacy: Power, epistemology, and platform", "author": "KM Cotter"}, {"title": "Reframing ax with critical design: The potentials and limits of algorithmic experience as a critical design concep", "author": "G Klumbyte"}, {"title": "Profiling Online Social Network Platforms: Twitter vs. Instagra", "author": "V Ayora"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluation", "author": "F Sperrle"}, {"title": "Humane anthropomorphic agents: the quest for the outcome measur", "author": "E Andr\u00e9"}, {"title": "Trust in Collaborative Automation in High Stakes Software Engineering Work: A Case Study at NAS", "author": "DG Widder"}, {"title": "Towards Design Principles for Trustworthy Affective Chatbots in Virtual Team", "author": "I Benke"}, {"title": "Understanding How People Reason about Aesthetic Evaluations of Artificial Intelligenc", "author": "C Oh"}, {"title": "Why am I seeing this? Deconstructing algorithm literacy through the lens of user", "author": "D Shin"}, {"title": "Exploring Understandable Algorithms to Suggest Fitness Tracker Goals that Foster Commitmen", "author": "PW Wo\u017aniak", "abstract": "While fitness trackers are gaining popularity, they struggle to offer long-term health benefits, largely due to their inability to offer engaging goals. Understanding how trackers can suggest and update fitness goals can lead to building improved systems that support wellbeing. We investigate how to suggest fitness tracker goals to users and ways to help them commit to those goals. We compared algorithms for step goal setting in a pre-study. Next, we conducted two surveys (a vignette study and a survey using the users' Fitbit data)", "keywords": {"keywords": ["fitness trackers", "goals", "commitment", "algorithms", "health benefits"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Wellbeing"]}}, {"title": "Studying Social Media From an Ego-Centric Perspectiv", "author": "K Hampton"}, {"title": "Face Mis-ID: An Interactive Pedagogical Tool Demonstrating Disparate Accuracy Rates in Facial Recognitio", "author": "D Raz"}, {"title": "Three directions for the design of human-centered machine translatio", "author": "S Robertson"}, {"title": "Signs of the time: Making AI legibl", "author": "JG Lindley"}, {"title": "\" A cold, technical decision-maker\": Can AI provide explainability, negotiability, and humanity", "author": "A Woodruff"}, {"title": "Tinkering: A Way Towards Designing Transparent Algorithmic User Interfaces", "author": "D Showkat"}, {"title": "Perception of fairness in algorithmic decisions: Future developers' perspectiv", "author": "S Kleanthous"}, {"title": "Supporting Responsible Data and Algorithmic Practices in The News Medi", "author": "D Showkat"}, {"title": "Design Patterns of Investing Apps and Their Effects on Investing Behavior", "author": "S Chaudhry"}, {"title": "Values (Mis) alignment: Exploring Tensions Between Platform and LGBTQ+ Community Design Value", "author": "MA DeVito", "abstract": "Social platforms hold great promise for supporting marginalized communities, such as the LGBTQ+ community, yet they are frequently sites of further stigmatization and harm. By engaging a diverse sample of 31 US LGBTQ+ users in five qualitative, design-based value elicitation exercises, we find that misalignments between perceived platform values and the values of the marginalized users they serve are at the heart of this disconnect. We inductively identify two community-based design values for supporting LGBTQ+ users: self", "keywords": {"keywords": ["values", "platform", "LGBTQ+ community", "design", "misalignment"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Social media"]}}, {"title": "Towards a User Integration Framework for Personal Health Decision Support and Recommender System", "author": "K Herrmanny"}, {"title": "\u4eba\u5de5\u667a\u80fd\u4ea7\u54c1\u4e0e\u670d\u52a1\u4f53\u7cfb\u7814\u7a76\u7efc", "author": "\u5b59\u6548\u534e\uff0c \u5f20\u4e49\u6587\uff0c \u4faf\u7490\uff0c \u5468\u96ef\u6d01\uff0c \u5f20\u7ef3\u5bb8"}, {"title": "Addressing cognitive biases in augmented business decision system", "author": "T Baudel"}, {"title": "Designing to Support Sensemaking in Cross-Lingual Computer-Mediated Communication Using NLP Technique", "author": "H Lim"}, {"title": "Effects of agency locus and transparency of artificial intelligence: Uncertainty reduction and emerging min", "author": "B Liu"}, {"title": "Explanations as discourse: Towards ethical big data analytics service", "author": "S Afrashteh"}, {"title": "On Social Medi", "author": "KN Hampton"}, {"title": "How Fake News Affect Trust in the Output of a Machine Learning System for News Curatio", "author": "H Heuer"}, {"title": "How Do People Rank Multiple Mutant Agents", "author": "J Dodge"}, {"title": "Legible AI by design: design research to frame, design, empirically test and evaluate AI iconograph", "author": "F Pilling", "abstract": "Artificial Intelligence (AI) is becoming increasingly ubiquitous. Implemented into a wide range of everyday applications from social media, shopping, media recommendations and is increasingly making decisions about whether we are eligible for a loan, health insurance and potentially if we are worth interviewing for a job. This proliferation of AI brings many design challenges regarding bias, transparency, fairness, accountability and trust etc. It has been proposed that these challenges can be addressed by considering user agency", "keywords": {"keywords": ["Legible AI", "design research", "AI iconograph", "bias", "transparency"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-Computer Interaction"]}}, {"title": "Participating and designing around algorithmic socio-technical system", "author": "M Eslamimehdiabadi"}, {"title": "Algorithm awareness: Why user awareness is critical for personal privacy in the adoption of algorithmic platforms", "author": "D Shin"}, {"title": "Deep argumentative explanation", "author": "E Albini"}, {"title": "The Expert of Oz: A Two-sided Study Paradigm for Intelligent System", "author": "A Butz"}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderatio", "author": "V Lai"}, {"title": "The Five-Dimensional Space of the Futures of Work: A View to 203", "author": "E Carmel"}, {"title": "Biases as Values: Evaluating Algorithms in Contex", "author": "M D\u00edaz"}, {"title": "Examining the Impact of Algorithm Awareness on Wikidata's Recommender System Recoi", "author": "JJ Benjamin"}, {"title": "Transparency and Trust in A", "author": "N Scharowski"}, {"title": "On Clustering and Interpreting with Rules by Means of Mathematical Optimizatio", "author": "E Carrizosa", "abstract": "stimulating in paradigms, clustering and optimization. On the other  of any rigorous tool for  interpreting the clusters found, which  The very existence of the Periodic Law as an empirical", "keywords": {"keywords": ["clustering", "interpreting", "rules", "mathematical optimization", "periodic law"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["descriptive"], "discipline": ["multiple"], "content": ["methodological"], "application area": ["data analysis"]}}, {"title": "Mitigating Bias in Algorithmic Systems: A Fish-Eye View of Problems and Solutions Across Domain", "author": "K Orphanou"}, {"title": "A Method and Analysis to Elicit User-Reported Problems in Intelligent Everyday Application", "author": "M Eiband"}, {"title": "Supporting users in understanding intelligent everyday system", "author": "M Eiband"}, {"title": "Antecedents of privacy concerns and online information disclosure: moderating role of government regulatio", "author": "R Chaudhuri"}, {"title": "Making Data's Economic Value Transparen", "author": "H Li"}, {"title": "Explanations as Discours", "author": "S Afrashteh"}, {"title": "Achieving Transparency Report Privacy in Linear Tim", "author": "CL Chen"}, {"title": "How to Support Users in Understanding Intelligent Systems? Structuring th", "author": "M EIBAND"}, {"title": "Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendation", "author": "R Shang"}, {"title": "Teaching-Learning Interaction: A New Concept for Interaction Design to Support Reflective User Agency in Intelligent System", "author": "H Kim", "abstract": "Intelligent systems in everyday lives learn about their users to tailor services over time. However, these systems are often designed with little consideration of user agency on their learning processes, hindering users from taking full advantage of the systems. In this paper, we propose Teaching-Learning Interaction (TLI) as a new form of interaction that affords user agency by letting users reflectively shepherd an intelligent system's manner of learning. Given such agency, users will be able to better personalize services for themselves. We first", "keywords": {"keywords": ["Teaching-Learning Interaction", "Reflective User Agency", "Intelligent System", "User Personalization", "Interaction Design"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["User Experience Design"]}}, {"title": "Humane Anthropomorphic Agents: The Quest for the Outcome Measur", "author": "AIS SIGPRAG"}, {"title": "I humanize, therefore I understand? Effects of explanations and humanization of intelligent systems on perceived and objective user understandin", "author": "T Ngo"}, {"title": "Human-Centered AI: Challenges and Opportunitie", "author": "V Sarakiotis"}, {"title": "Do explanations increase the effectiveness of AI-crowd generated fake news warnings", "author": "Z Epstein"}, {"title": "Exploring folk theories of algorithmic news curation for explainable desig", "author": "T Ngo"}, {"title": "Effects of Computer-Based Teaching Strategy on the Academic Performance of Financial Accounting students in Nigeri", "author": "OFT Olufunke"}, {"title": "Understanding Algorithms through Exploratio", "author": "M Eiband"}, {"title": "The Changing Contours of \u201cParticipation\u201d in Data-driven, Algorithmic Ecosystems: Challenges, Tactics, and an Agend", "author": "MK Lee"}, {"title": "Design and validation of a comparison methodology for Explainable AI technique", "author": "F VANNI"}, {"title": "Columbia University Doctoral program in Communications New York, NY 10024 EAW2198@ Columbia. ed", "author": "EA Watkins", "abstract": "The rise of biometric security changes how users make decisions about their privacy. As passwords give way to faces and fingerprints, the algorithmic nature of these processes creates new cognitive labor for users. When biometrics are used in spaces of algorithmic management, workers must negotiate tradeoffs between security, privacy, fairness, and their livelihood. A mixed-methods, human-centered research design paired with theory frameworks from algorithmic management, usable security, and algorithmic", "keywords": {"keywords": ["biometric security", "privacy", "cognitive labor", "algorithmic management", "usable security"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Communications"], "content": ["Application"], "application area": ["Algorithmic management"]}}, {"title": "Facets of algorithmic literacy: Information, experience, and individual factors predict attitudes toward algorithmic system", "author": "DE Silva"}, {"title": "Trust in Collaborative Automation in High Stakes Software Engineering Wor", "author": "D Widder"}, {"title": "Os Keyes Department of Human Centered Design & Engineering University of Washington Seattle, WA, US", "author": "M Durbin"}, {"title": "A Mulching Proposa", "author": "HN Slurry"}, {"title": "Exploring the Effects of Interactive Dialogue in Improving User Control for Explainable Online Symptom Checker", "author": "Y Sun"}, {"title": "Journal of Responsible Technolog", "author": "F Cech"}, {"title": "The Role of Explanations of AI Systems: Beyond Trust and Helping to Form Mental Model", "author": "M Norkute"}, {"title": "It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in A", "author": "A Papenmeier"}, {"title": "Designing Curated Conversation-Driven Explanations for Communicating Complex Technical Concept", "author": "AY Wang"}, {"title": "Enhancing explainability and scrutability of recommender system", "author": "A Ghazimatin", "abstract": "Our increasing reliance on complex algorithms for recommendations calls for models and methods for explainable, scrutable, and trustworthy AI. While explainability is required for understanding the relationships between model inputs and outputs, a scrutable system allows us to modify its behavior as desired. These properties help bridge the gap between our expectations and the algorithm's behavior and accordingly boost our trust in AI. Aiming to cope with information overload, recommender systems play a crucial role in filtering", "keywords": {"keywords": ["explainability", "scrutability", "recommender system", "trustworthy AI", "information overload"], "research type": ["review"], "methodology": ["theoretical"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["recommendations"]}}, {"title": "ObjectivAIze: Measuring Performance and Biases in Augmented Business Decision System", "author": "T Baudel"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Propertie", "author": "JE Dodge"}, {"title": "Explicit and implicit theories of students about algorithmic sorting processes of online information intermediarie", "author": "A Gr\u00f6hn"}, {"title": "Investigating Explanations that Target Training Data", "author": "AI Anik"}, {"title": "Usability Study of an Explainable Machine Learning Risk Model for Predicting Illegal Shipbreakin", "author": "C Haas"}, {"title": "Piercing the Veil: Designs to Support Information Literacy on Social Platform", "author": "J Wolff"}, {"title": "From P4 medicine to P5 medicine: transitional times for a more human-centric approach to AI-based tools for hospitals of tomorro", "author": "D Amram"}, {"title": "Accountability-Aware Design of Voice User Interfaces for Home Appliance", "author": "S Kianzad"}, {"title": "Who needs explanation and when? Juggling explainable AI and user epistemic uncertaint", "author": "J Jiang"}, {"title": "Investigating the YouTube Adpocalypse and Augmenting the User Experience to Address Transparency and Systemic Biase", "author": "SA Masadeh", "abstract": "Social media is becoming a primary mode of political discourse around the world. We can see the impact of this through the recent\" fake news\" phenomenon and recent online political movements. This scrutiny and public pressure have prompted a wave of policy changes on most platforms. YouTube, which enables the monetization of content through advertisement alongside user created content, has faced unique pressure from advertisers and the media to prevent the monetization of controversial and hateful content. This has led", "keywords": {"keywords": ["YouTube Adpocalypse", "User Experience", "Transparency", "Systemic Bias", "Monetization"], "research type": ["Case Study"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Social Media"]}}, {"title": "Explaining call recommendations in nursing homes: a user-centered design approach for interacting with knowledge-based health decision support system", "author": "FS Guti\u00e9rrez Hern\u00e1ndez"}, {"title": "Understanding How People Interact with Algorithm-based Systems Using Artificial Intelligenc", "author": "\uc624\ucc3d\ud6c8"}, {"title": "Exploring design practices for explaining music recommendation", "author": "H Ryu"}, {"title": "Can Explainable AI Mitigate Decision-Making Errors Induced by Algorithms in Street-Level Police Work? An Experiment", "author": "FJ Selten"}, {"title": "It's Just a Recipe?\u2014Comparing Expert and Lay User Understanding of Algorithmic System", "author": "T Ngo"}, {"title": "Explaining Call Recommendations in Nursing Homes: a User-Centered Design Approach for Interacting with Knowledge-Based Health Decision Support System", "author": "F Guti\u00e9rrez"}, {"title": "Understanding User Perception of Explainable Algorithmic Decision-Making Systems: A Systematic Literature Revie", "author": "S Bae"}, {"title": "\u201cIt's not wrong, but I'm quite disappointed\u201d: Toward an Inclusive Algorithmic Experience for Content Creators with Disabilitie", "author": "D Choi"}, {"title": "Unpacking a model: An interactive visualization of a text similarity algorithm for legal document", "author": "D Soroko"}, {"title": "Mitigating Bias in Algorithmic Systems-A Fish-Eye Vie", "author": "K Orphanou", "abstract": "Mitigating bias in algorithmic systems is a critical issue drawing attention across communities within the information and computer sciences. Given the complexity of the problem and the involvement of multiple stakeholders\u2013including developers, end users and third-parties\u2013there is a need to understand the landscape of the sources of bias, and the solutions being proposed to address them, from a broad, cross-domain perspective. This survey provides a \u201cfish-eye view,\u201d examining approaches across four areas of research. The", "keywords": {"keywords": ["bias", "algorithmic systems", "mitigating", "fish-eye view", "sources"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["information and computer sciences"], "content": ["methodological"], "application area": ["bias in algorithmic systems"]}}, {"title": "Role-oriented Task Allocation in Human-Machine Collaboration Syste", "author": "J Liu"}, {"title": "\u201cYou might also like\u201d-The technological consumers understanding of transparent A", "author": "EK Haugstulen"}, {"title": "Designing Investing Apps to Promote Healthier Trading Habits in Retail Investor", "author": "S Chaudhry"}, {"title": "An overview of Algorithmic Experience (AX", "author": "D Dang-Pham"}, {"title": "Three Directions for the Design of Human-Centered Machine Translatio", "author": "DJ Liebling"}, {"title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in Indi", "author": "D Ramesh"}, {"title": "Exploring User Experience of Automatic Documentation Tool", "author": "S Wu"}, {"title": "Presenting the Self on Unstable Ground: Adaptive Folk Theorization as a Path to Algorithmic Literacy on Changing Platform", "author": "MA DeVito"}, {"title": "How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement and ", "author": "D Buschek"}, {"title": "Educating Computer Science Students about Algorithmic Fairness, Accountability, Transparency and Ethic", "author": "M Kasinidou", "abstract": "Professionals are increasingly relying on algorithmic systems for decision making however, algorithmic decisions occasionally perceived as biased or not just. Prior work has provided evidences that education can make a difference on the perception of young developers on algorithmic fairness. In this paper, we investigate computer science students' perception of FATE in algorithmic decision-making and whether their views on FATE can be changed by attending a seminar on FATE topics. Participants attended a seminar on FATE in algorithmic", "keywords": {"keywords": ["algorithmic fairness", "accountability", "transparency", "ethics", "computer science students"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Algorithmic decision-making"]}}, {"title": "Explainable Artificial Intelligence in Cybersecurity: A Brief Revie", "author": "S Hariharan"}, {"title": "The multi-dimensional space of the futures of wor", "author": "E Carmel"}, {"title": "'We can't find fault with a friend': The Mediation of Accountability on Instant Loan Platforms in Indi", "author": "D Ramesh"}, {"title": "Questioning the algorithmic transparency of location-based platform", "author": "L Parisi"}, {"title": "Using Game Engines to Design Digital Workshops for AI Legibilit", "author": "F Pilling"}, {"title": "The Roles and Modes of Human Interactions with Automated Machine Learning System", "author": "TT Khuat"}, {"title": "Reinforced Natural Language Interfaces via Entropy Decompositio", "author": "X Wu"}, {"title": "Response Quality in Human-chatbot Collaborative System", "author": "N Ahuja"}, {"title": "When Self-Humanization Leads to Algorithm Aversio", "author": "PO He\u00dfler"}, {"title": "Visualizing Algorithmic Selection in Social Medi", "author": "MD Muralikumar", "abstract": "Social media sites such as Facebook and Twitter use algorithms to filter information in order to reduce overload and selectively pick content for users. These algorithms create unique, individual, and isolated bubbles of information that users are not always aware of. We recommend that algorithmic awareness should be the first step in addressing the pitfalls of the filter bubble effect. We conducted an experimental study to investigate how simple visualizations can be used to achieve algorithmic awareness and to understand how it might", "keywords": {"keywords": ["algorithmic selection", "social media", "filter bubble effect", "algorithmic awareness", "visualizations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Social media"]}}, {"title": "The Explanatory Gap in Algorithmic News Curatio", "author": "H Heuer"}, {"title": "Lost in Explanation: Users' Expectations of News Recommender Systems v. Data Subject's Rights in Practice (ATAP", "author": "P Dewitte"}, {"title": "THE GIFTOFSCRUTABLE LEARNER MODELS: WHYAND HO", "author": "J Kay"}, {"title": "When Machine and Bandwagon Heuristics Compete: Understanding Users' Response to Conflicting AI and Crowdsourced Fact-Checkin", "author": "JA Banas"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI System", "author": "S Mohseni"}, {"title": "Algorithmic Transparency in Action: How and Why Do Companies Disclose Information on Algorithms", "author": "T Bitzer"}, {"title": "Technological Mediation of Politic", "author": "M Ba\u015f"}, {"title": "Human Factors in Business Technology Managemen", "author": "T Gazit"}, {"title": "New Digital Inequalities. Algorithms Divid", "author": "M Ragnedda"}, {"title": "Tensions in transparent urban AI: designing a smart electric vehicle charge poin", "author": "K Alfrink", "abstract": "The increasing use of artificial intelligence (AI) by public actors has led to a push for more transparency. Previous research has conceptualized AI transparency as knowledge that empowers citizens and experts to make informed choices about the use and governance of AI. Conversely, in this paper, we critically examine if transparency-as-knowledge is an appropriate concept for a public realm where private interests intersect with democratic concerns. We conduct a practice-based design research study in which we prototype and", "keywords": {"keywords": ["transparent urban AI", "smart electric vehicle", "charge point", "transparency-as-knowledge", "practice-based design research"], "research type": "Empirical", "methodology": "Qualitative", "purpose": "Exploratory", "discipline": "Computer science", "content": "Application", "application area": "Urban transportation"}}, {"title": "T\u1ed5ng quan v\u1ec1 tr\u1ea3i nghi\u1ec7m thu\u1eadt to\u00e1n (AX", "author": "D Dang-Pham"}, {"title": "Konzept und Implementierung einer visuellen Methode zur Verbesserung der Interpretierbarkeit der automatisierten Qualitatsbewertung mit ORES in Wikidat", "author": "S Gnanasegaram"}, {"title": "Everyday engagemen", "author": "J Niess"}, {"title": "Social Machines Zusammenfassun", "author": "V Projekte"}, {"title": "Social Machine", "author": "C Draude"}, {"title": "Explizite und implizite Theorien von Sch\u00fclerinnenund Sch\u00fclern in Deutschland \u00fcber die algorithmischenSortierprozesse von Informationsintermedi\u00e4re", "author": "AC Gr\u00f6hn"}, {"title": "Apresenta\u00e7\u00e3o do \u00abeu\u00bb e empoderamento feminino no Instagram: os casos de@ taniiagraca,@ omeuutero e@ catarinacoruj", "author": "ECM Alem\u00e3o"}], "answering questions about charts and generating visual explanations": [{"title": "NL4DV: A toolkit for generating analytic specifications for data visualization from natural language querie", "author": "A Narechania", "abstract": "Natural language interfaces (NLls) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural", "keywords": {"keywords": ["natural language interfaces", "visual data analysis", "visualization design", "tabular dataset", "Python package"], "research type": ["toolkit"], "methodology": ["technical"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["data visualization"]}}, {"title": "Chart mining: A survey of methods for automated chart analysi", "author": "K Davila"}, {"title": "Accessible visualization: Design space, opportunities, and challenge", "author": "NW Kim"}, {"title": "Ai4vis: Survey on artificial intelligence approaches for data visualizatio", "author": "A Wu"}, {"title": "Applying machine learning advances to data visualization: A survey on ml4vi", "author": "Q Wang"}, {"title": "Chart-to-text: Generating natural language descriptions for charts by adapting the transformer mode", "author": "J Obeid"}, {"title": "WSL-DS: Weakly supervised learning with distant supervision for query focused multi-document abstractive summarizatio", "author": "MTR Laskar"}, {"title": "Collecting and characterizing natural language utterances for specifying data visualization", "author": "A Srinivasan"}, {"title": "Towards understanding how readers integrate charts and captions: A case study with line chart", "author": "DH Kim"}, {"title": "Agqa: A benchmark for compositional spatio-temporal reasonin", "author": "M Grunde-McLaughlin"}, {"title": "What questions reveal about novices' attempts to make sense of data visualizations: Patterns and misconception", "author": "AMB Rodrigues", "abstract": "Data visualization literacy has attracted widespread interest due to the urgent need to analyze unprecedented volumes of data we have nowadays. Much work on visualization literacy focuses on asking people to answer specific questions about the data depicted in a visual representation, in an attempt to try to understand how people make sense of the underlying data. In this work, we investigate, through a user survey, the initial questions people pose when first encountering a visualization. We analyzed a set of 1058 questions", "keywords": {"keywords": ["data visualization literacy", "novices", "sense-making", "questions", "misconceptions"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Human-Computer Interaction"], "content": ["Methodological"], "application area": ["Data Analysis"]}}, {"title": "Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairment", "author": "C Jung"}, {"title": "Accessible visualization via natural language descriptions: A four-level model of semantic conten", "author": "A Lundgard"}, {"title": "A Survey on ML4VIS: Applying MachineLearning Advances to Data Visualizatio", "author": "Q Wang"}, {"title": "Utilizing bidirectional encoder representations from transformers for answer selectio", "author": "MTR Laskar"}, {"title": "DIY: Assessing the correctness of natural language to sql system", "author": "A Narechania"}, {"title": "Designing and evaluating multimodal interactions for facilitating visual analysis with dashboard", "author": "I Chowdhury"}, {"title": "Advisor: Automatic visualization answer for natural-language question on tabular dat", "author": "C Liu"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interface", "author": "M Chromik"}, {"title": "Kori: Interactive Synthesis of Text and Charts in Data Document", "author": "S Latif"}, {"title": "Post-wimp interaction for information visualizatio", "author": "B Lee", "abstract": "Well-designed interactions for visualization systems hold great promise both for empowering people to effectively complete their tasks and for providing more natural and engaging user experiences. Leveraging advancements in hardware and software technology, the visualization research community has made considerable progress providing novel input and interaction experiences. These technologies continue to advance at a fast pace, and thus it seems timely to look back at what has been achieved so far and contemplate what", "keywords": {"keywords": ["post-WIMP interaction", "information visualization", "interactions", "visualization systems", "user experiences"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["information technology"]}}, {"title": "Towards natural language interfaces for data visualization: A surve", "author": "L Shen"}, {"title": "Towards Assisting the Visually Impaired: A Review on Techniques for Decoding the Visual Data from Chart Image", "author": "KC Shahira"}, {"title": "Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarizatio", "author": "MTR Laskar"}, {"title": "DT2VIS: A focus+ context answer generation system to facilitate visual exploration of tabular dat", "author": "Q Jiang"}, {"title": "Integrating Image Data Extraction and Table Parsing Methods for Chart Question answerin", "author": "A Masry"}, {"title": "ConVisQA: A Natural Language Interface for Visually Exploring Online Conversation", "author": "N Siddiqui"}, {"title": "Utilizing the Transformer Architecture for Question Answerin", "author": "MTR Laskar"}, {"title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasonin", "author": "A Masry"}, {"title": "VoxLens: Making Online Data Visualizations Accessible with an Interactive JavaScript Plug-I", "author": "A Sharif"}, {"title": "Exploring User Perception of Causality in Automated Data Insight", "author": "PM Law", "abstract": "To facilitate data exploration and analysis, researchers have studied and developed systems that aim to automatically communicate data insights to users. For a data set of cars, these systems may proactively recommend a bar chart that depicts interesting data relationships and generate a description of the chart such as``US cars have a higher average horsepower than Japanese cars.''Such automated insight functionality has emerged in commercial visualization platforms, enabling thousands of analysts to tap into its", "keywords": {"keywords": ["Automated data insight", "User perception", "Data exploration", "Analysis", "Causality"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Data visualization"]}}, {"title": "Uncovering factors that influence how data visualizations are interpreted by non-expert", "author": "SDJ Barbosa"}, {"title": "Facilitating Conversational Interaction in Natural Language Interfaces for Visualizatio", "author": "R Mitra"}, {"title": "Text2Chart: A Multi-staged Chart Generator from Natural Language Tex", "author": "M Rashid"}, {"title": "Visualization question answering using introspective program synthesi", "author": "Y Chen"}, {"title": "ExVis: Explainable Visual Decision Support System for Risk Managemen", "author": "MR Islam"}, {"title": "VividGraph: Learning to Extract and Redesign Network Graphs from Visualization Image", "author": "S Song"}, {"title": "From Data to Knowledge Graphs: A Multi-Layered Method to Model User's Visual Analytics Workflow for Analytical Purpose", "author": "L Christino"}, {"title": "CrossData: Leveraging Text-Data Connections for Authoring Data Document", "author": "Z Chen"}], "understanding the effect of accuracy on trust in machine learning models": [{"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao", "abstract": "A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm", "keywords": {"keywords": ["explainable AI", "user experiences", "design practices", "real-world user needs", "algorithmic work"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Human-computer Interaction"]}}, {"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making", "author": "Y Zhang"}, {"title": "\" Hello AI\": uncovering the onboarding needs of medical practitioners for human-AI collaborative decision-making", "author": "CJ Cai"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "Beyond accuracy: The role of mental models in human-AI team performance", "author": "G Bansal"}, {"title": "A survey on the explainability of supervised machine learning", "author": "N Burkart"}, {"title": "Evaluating saliency map explanations for convolutional neural networks: a user study", "author": "A Alqaraawi"}, {"title": "A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores", "author": "M De-Arteaga"}, {"title": "Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems", "author": "Z Bu\u00e7inca"}, {"title": "Machine learning in mental health: A systematic review of the HCI literature to support the development of effective and implementable ML systems", "author": "A Thieme"}, {"title": "Trust in AutoML: exploring information needs for establishing trust in automated machine learning systems", "author": "J Drozdal", "abstract": "We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists' trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from", "keywords": {"keywords": ["Trust", "AutoML", "Information needs", "Machine learning models", "Data scientists"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Automated Machine Learning"]}}, {"title": "No explainability without accountability: An empirical study of explanations and feedback in interactive ml", "author": "A Smith-Renner"}, {"title": "The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems", "author": "M Nourani"}, {"title": "To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making", "author": "Z Bu\u00e7inca"}, {"title": "\" Why is' Chicago'deceptive?\" Towards Building Model-Driven Tutorials for Humans", "author": "V Lai"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "A human-centered agenda for intelligible machine learning", "author": "JW Vaughan"}, {"title": "Visual interaction with deep learning models through collaborative semantic inference", "author": "S Gehrmann"}, {"title": "Mental models of AI agents in a cooperative game setting", "author": "KI Gero"}, {"title": "How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection", "author": "M Jacobs"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh", "abstract": "To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise-and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders' knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in", "keywords": {"keywords": ["interpretable machine learning", "stakeholders", "accountability", "harm mitigation", "knowledge decoupling"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": "automated systems"}}, {"title": "Soliciting human-in-the-loop user feedback for interactive machine learning reduces user trust and impressions of model accuracy", "author": "D Honeycutt"}, {"title": "The role of domain expertise in user trust and the impact of first impressions with intelligent systems", "author": "M Nourani"}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "\" An Ideal Human\" Expectations of AI Teammates in Human-AI Teaming", "author": "R Zhang"}, {"title": "Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols", "author": "A Head"}, {"title": "Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks", "author": "Z Lu"}, {"title": "Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle", "author": "S Dhanorkar"}, {"title": "Designing AI for trust and collaboration in time-constrained medical decisions: a sociotechnical lens", "author": "M Jacobs"}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "Impact of algorithmic decision making on human behavior: Evidence from ultimatum bargaining", "author": "A Erlei", "abstract": "Recent advances in machine learning have led to the widespread adoption of ML models for decision support systems. However, little is known about how the introduction of such systems affects the behavior of human stakeholders. This pertains both to the people using the system, as well as those who are affected by its decisions. To address this knowledge gap, we present a series of ultimatum bargaining game experiments comprising 1178 participants. We find that users are willing to use a black-box decision support system and", "keywords": {"keywords": ["algorithmic decision making", "human behavior", "ultimatum bargaining", "machine learning", "decision support systems"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Economics"], "content": ["Application"], "application area": ["Decision-making behavior"]}}, {"title": "Misplaced trust: measuring the interference of machine learning in human decision-making", "author": "H Suresh"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Assessing the impact of automated suggestions on decision making: Domain experts mediate model errors but take less initiative", "author": "A Levy"}, {"title": "A review of trust in artificial intelligence: Challenges, vulnerabilities and future directions", "author": "S Lockey"}, {"title": "Measure utility, gain trust: practical advice for XAI researchers", "author": "B Davis"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "To what extent do human explanations of model behavior align with actual model behavior?", "author": "G Prasad"}, {"title": "A slow algorithm improves users' assessments of the algorithm's accuracy", "author": "JS Park"}, {"title": "Second chance for a first impression? Trust development in intelligent system interaction", "author": "S Tolmeijer"}, {"title": "Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition", "author": "M Nourani", "abstract": "Explainable machine learning and artificial intelligence models have been used to justify a model's decision-making process. This added transparency aims to help improve user performance and understanding of the underlying model. However, in practice, explainable systems face many open questions and challenges. Specifically, designers might reduce the complexity of deep learning models in order to provide interpretability. The explanations generated by these simplified models, however, might not accurately justify and be truthful to", "keywords": {"keywords": ["Explainable AI", "Verifying Veracity", "Video Activity Recognition", "Transparency", "Deep Learning Models"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Activity Recognition"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies", "author": "O Vereschak"}, {"title": "You'd better stop! Understanding human reliance on machine learning models under covariate shift", "author": "CW Chiang"}, {"title": "Recent advances in trustworthy explainable artificial intelligence: Status, challenges and perspectives", "author": "A Rawal"}, {"title": "The effect of explanation styles on user's trust", "author": "R Larasati"}, {"title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations", "author": "S Arora"}, {"title": "Human confidence in artificial intelligence and in themselves: The evolution and impact of confidence on adoption of AI advice", "author": "L Chong"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro"}, {"title": "When does uncertainty matter?: Understanding the impact of predictive uncertainty in ML assisted decision making", "author": "S McGrath"}, {"title": "The impact of algorithmic risk assessments on human predictions and its analysis via crowdsourcing studies", "author": "R Fogliato", "abstract": "As algorithmic risk assessment instruments (RAIs) are increasingly adopted to assist decision makers, their predictive performance and potential to promote inequity have come under scrutiny. However, while most studies examine these tools in isolation, researchers have come to recognize that assessing their impact requires understanding the behavior of their human interactants. In this paper, building off of several recent crowdsourcing works focused on criminal justice, we conduct a vignette study in which laypersons are tasked with", "keywords": {"keywords": ["algorithmic risk assessments", "human predictions", "crowdsourcing studies", "predictive performance", "inequity"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Sociology", "content": "Application", "application area": "Criminal justice"}}, {"title": "Show or suppress? Managing input uncertainty in machine learning model explanations", "author": "D Wang"}, {"title": "Understanding user attitudes towards negative side effects of AI systems", "author": "S Saisubramanian"}, {"title": "Can explainable AI explain unfairness? A framework for evaluating explainable AI", "author": "K Alikhademi"}, {"title": "Fifty Shades of Grey: In Praise of a Nuanced Approach Towards Trustworthy Design", "author": "L Thornton"}, {"title": "The messiness of the menstruator: assessing personas and functionalities of menstrual tracking apps", "author": "A Pichon"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence", "author": "W Jin"}, {"title": "Effects of algorithmic flagging on fairness: quasi-experimental evidence from Wikipedia", "author": "N TeBlunthuis"}, {"title": "Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approach", "author": "H Mucha"}, {"title": "Knowledge-to-information translation training (KITT): an adaptive approach to explainable artificial intelligence", "author": "R Thomson"}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang", "abstract": "Machine learning models need to provide contrastive explanations, since people often seek to understand why a puzzling prediction occurred instead of some expected outcome. Current contrastive explanations are rudimentary comparisons between examples or raw features, which remain difficult to interpret, since they lack semantic meaning. We argue that explanations must be more relatable to other concepts, hypotheticals, and associations. Inspired by the perceptual process from cognitive psychology, we propose the XAI", "keywords": {"keywords": ["Explainable AI", "Perceptual Process", "Contrastive explanations", "Semantic meaning", "Relatable"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Are you sure? Prediction revision in automated decision\u2010making", "author": "N Burkart"}, {"title": "Typing Efficiency and Suggestion Accuracy Influence the Benefits and Adoption of Word Suggestions", "author": "Q Roy"}, {"title": "AI-Assisted Human Labeling: Batching for Efficiency without Overreliance", "author": "Z Ashktorab"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "Vis ex machina: An analysis of trust in human versus algorithmically generated visualization recommendations", "author": "R Zehrung"}, {"title": "Human-AI collaboration for natural language generation with interpretable neural networks", "author": "S Gehrmann"}, {"title": "Perceived and Measured Task Effectiveness in Human-AI Collaboration", "author": "RM Jacobsen"}, {"title": "Beyond cross-validation\u2014Accuracy estimation for incremental and active learning models", "author": "C Limberg"}, {"title": "Towards a Trust Reliance Paradox? Exploring the Gap Between Perceived Trust in and Reliance on Algorithmic Advice", "author": "A Schmitt"}, {"title": "When Algorithms Err: Differential Impact of Early vs. Late Errors on Users' Reliance on Algorithms", "author": "A Kim", "abstract": "Errors are a natural part of the development and use of predictive algorithms, but they could discourage people from relying on algorithms even when doing so could lead to better decisions. In this paper, we conduct two experiments to demonstrate that people's reliance on a predictive algorithm following a substantial error depends on when the error occurs and how the algorithm is used in decision making. We find that, when the prediction tasks are fully delegated to an algorithm, the impact of an error on reliance is different if the error", "keywords": {"keywords": ["algorithm errors", "reliance", "predictive algorithms", "error impact", "decision making"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Decision making"]}}, {"title": "Shapley values and meta-explanations for probabilistic graphical model inference", "author": "Y Liu"}, {"title": "Machine Learning Explanations to Prevent Overtrust in Fake News Detection.", "author": "S Mohseni"}, {"title": "ALEEDSA: Augmented reality for interactive machine learning", "author": "A Ferrario"}, {"title": "The effects of warmth and competence perceptions on users' choice of an AI system", "author": "Z Gilad"}, {"title": "Effect of ai explanations on human perceptions of patient-facing ai-powered healthcare systems", "author": "Z Zhang"}, {"title": "Explainable AI and Adoption of Financial Algorithmic Advisors: An Experimental Study", "author": "D Ben David"}, {"title": "Alert override patterns with a medication clinical decision support system in an academic emergency department: retrospective descriptive study", "author": "J Yoo"}, {"title": "Trust in Prediction Models: a Mixed-Methods Pilot Study on the Impact of Domain Expertise", "author": "J Ooge"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "Goal-Directed Self-Tracking in the Management of Chronic Health Conditions", "author": "J Schroeder", "abstract": "Health technologies are increasingly prevalent and important to support the management of chronic health conditions. Such health technologies often rely on self-tracking, or the practice of collecting and reflecting on personal data, to help people better understand their symptoms and learn how their habits and behaviors affect those symptoms. Many people with chronic conditions and their health providers believe that self-tracking offers a potential for a more complete and accurate understanding of an individual's personal health", "keywords": {"keywords": ["self-tracking", "chronic health conditions", "health technologies", "personal data", "symptoms"], "research type": "Review", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Health", "content": "Application", "application area": "Self-management of chronic health conditions"}}, {"title": "Influence of pedagogical beliefs and perceived trust on teachers' acceptance of educational artificial intelligence tools", "author": "S Choi"}, {"title": "When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models", "author": "A Rechkemmer"}, {"title": "Investigations of Performance and Bias in Human-AI Teamwork in Hiring", "author": "A Peng"}, {"title": "Hive: evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "Towards a Human-Centric Design Framework for AI Assisted Music Production", "author": "A Tsiros"}, {"title": "Accelerating Text-as-Data Research in Computational Social Science", "author": "D Card"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainable", "author": "A Simkute"}, {"title": "EUCA: the End-User-Centered Explainable AI Framework", "author": "W Jin"}, {"title": "No explainability without accountability: an empirical study of explanations and feedback in interactive ML", "author": "A Smith"}, {"title": "Accurate, Fair, and Explainable: Building Human-Centered AI", "author": "A Springer", "abstract": "Artificially intelligent systems play an increasingly large role in our everyday lives. These intelligent systems make decisions big and small\u2014from who gets a mortgage to what articles we read online. However, these systems are often designed out of convenience; they are built using data that is available, they do not explain the decisions they make, and sometimes they are simply inaccurate. At best, these problems result in systems that are difficult to use; at worst, these problems result in reinforcing societal biases at scale. Rather", "keywords": {"keywords": ["Artificially intelligent systems", "Accurate", "Fair", "Explainable", "Human-Centered AI"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer science", "content": "Application", "application area": "Ethical AI"}}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "Attitudes surrounding an imperfect AI autograder", "author": "S Hsu"}, {"title": "Expl (AI) ned: The impact of explainable artificial intelligence on cognitive processes", "author": "K Bauer"}, {"title": "Amplifying domain expertise in clinical data pipelines", "author": "P Rahman"}, {"title": "Algorithmic Appreciation in Creative Tasks.", "author": "E Bogert"}, {"title": "Evaluation of Feature Selection Methods on Psychosocial Education Data Using Additive Ratio Assessment", "author": "F Muttakin"}, {"title": "Let's Think Together! Assessing Shared Mental Models, Performance, and Trust in Human-Agent Teams", "author": "BG Schelble"}, {"title": "The Human (s) in the Loop\u2014Bringing AI and HCI Together", "author": "T Gross"}, {"title": "Towards the Human-Centered Design of Intelligent Agents in Medical Imaging Diagnosis", "author": "FM Calisto"}, {"title": "Heterogeneity in Algorithm-Assisted Decision-Making: A Case Study in Child Abuse Hotline Screening", "author": "L Cheng", "abstract": "Algorithmic risk assessment tools are now commonplace in public sector domains such as criminal justice and human services. These tools are intended to aid decision makers in systematically using rich and complex data captured in administrative systems. In this study we investigate sources of heterogeneity in the alignment between worker decisions and algorithmic risk scores in the context of a real world child abuse hotline screening use case. Specifically, we focus on heterogeneity related to worker experience. We find that senior", "keywords": {"keywords": ["algorithm-assisted decision-making", "heterogeneity", "child abuse hotline screening", "worker experience", "algorithmic risk assessment tools"], "research type": ["case study"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["Social sciences"], "content": ["Application"], "application area": ["Child protection"]}}, {"title": "People underestimate the errors made by algorithms for credit scoring and recidivism prediction but accept even fewer errors", "author": "FG Rebitschek"}, {"title": "Designing for the human in the loop: Transparency and control in interactive machine learning", "author": "A Renner"}, {"title": "Can an Algorithm Be My Healthcare Proxy?", "author": "DC McElfresh"}, {"title": "Provably Improving Expert Predictions with Conformal Prediction", "author": "E Straitouri"}, {"title": "A Survey of Machine Learning Techniques for Video Quality Prediction from Quality of Delivery Metrics", "author": "O Izima"}, {"title": "How good is good enough? Quantifying the impact of benefits, accuracy, and privacy on willingness to adopt COVID-19 decision aids", "author": "G Kaptchuk"}, {"title": "Attitudes and Folk Theories of Data Subjects on Transparency and Accuracy in Emotion Recognition", "author": "G Grill"}, {"title": "XAutoML: A Visual Analytics Tool for Establishing Trust in Automated Machine Learning", "author": "MA Z\u00f6ller"}, {"title": "Analysis of Trustworthiness in Machine Learning and Deep Learning", "author": "M Kentour"}, {"title": "A MULTI-LAYERS ARCHITECTURE FOR AN E-LEARNING HYBRID RECOMMENDER SYSTEM", "author": "L Roux", "abstract": "Distance computer-assisted learning is increasingly common, owing largely to the expansion and development of etechnology. Nevertheless, the available tools of the learning platforms have demonstrated their limits during the pandemic context, since many students, who were used to\" face-to-face\" education, got discouraged and dropped out of school. In this context, a main issue is to conceive tools that would allow the teachers to supervise their students at a distance, by monitoring their progress and ensuring follow-up", "keywords": {"keywords": ["e-learning", "hybrid recommender system", "distance computer-assisted learning", "learning platforms", "teacher supervision"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["education"], "content": ["methodological"], "application area": ["distance education"]}}, {"title": "Teachers' trust in AI\u2010powered educational technology and a professional development program to improve it", "author": "T Nazaretsky"}, {"title": "Recognition Assistant Framework Based on Deep Learning for Autonomous Driving: Restoring Damaged Road Sign Information", "author": "J Park"}, {"title": "How to explain AI systems to end users: a systematic literature review and research agenda", "author": "S Laato"}, {"title": "Transparency and Trust in AI", "author": "N Scharowski"}, {"title": "Understanding User Sensemaking in Machine Learning Fairness Assessment Systems", "author": "Z Gu"}, {"title": "An Algorithmic Approach to Explaining Why the Underserved Feel More Pain", "author": "E Pierson"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Will You Accept the AI Recommendation? Predicting Human Behavior in AI-Assisted Decision Making", "author": "X Wang"}, {"title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging", "author": "R Fogliato"}, {"title": "A machine learning approach to evaluate variables of math anxiety in STEM students", "author": "D Soysal", "abstract": "Accepted: 29 Mar. 2022 The relationships between math anxiety and other variables such as students' motivation and confidence have been extensively studied. The main purpose of the present study was to employ a machine learning approach to provide a deeper understanding of variables associated with math anxiety. Specifically, we applied classification and regression tree models to weekly survey data of science, technology, engineering, and mathematics (STEM) students enrolled in calculus. The tree models", "keywords": {"keywords": ["math anxiety", "machine learning", "STEM students", "classification", "regression tree"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Education"], "content": ["Methodological"], "application area": ["Academic performance"]}}, {"title": "Re-examining the role of time in human-algorithm interaction", "author": "JS Park"}, {"title": "Trade-offs in Sampling and Search for Early-stage Interactive Text Classification", "author": "Z Levonian"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "What should we reasonably expect from artificial intelligence?", "author": "L PARENTONI"}, {"title": "Homophily and Incentive Effects in Use of Algorithms", "author": "R Fogliato"}, {"title": "Prediction for Retrospection: Integrating Algorithmic Stress Prediction into Personal Informatics Systems for College Students' Mental Health", "author": "T Kim"}, {"title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?", "author": "RS Zimmermann"}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "Explaining Artificial Intelligence with Tailored Interactive Visualisations", "author": "J Ooge", "abstract": "Artificial intelligence (AI) is becoming ubiquitous in the lives of both researchers and non-researchers, but AI models often lack transparency. To make well-informed and trustworthy decisions based on these models, people require explanations that indicate how to interpret the model outcomes. This paper presents our ongoing research in explainable AI, which investigates how visual analytics interfaces and visual explanations, tailored to the target audience and application domain, can make AI models more transparent and allow", "keywords": {"keywords": ["Artificial Intelligence", "Explainable AI", "Visual Analytics Interfaces", "Visual Explanations", "Transparency"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "AI transparency"}}, {"title": "Pushing the Boundaries of Participatory Design", "author": "J Korte"}, {"title": "Research Needs for Trusted Analytics in National Security Settings", "author": "A Speed"}, {"title": "User Trust on an Explainable AI-based Medical Diagnosis Support System", "author": "Y Rong"}, {"title": "How Validation Methodology Influences Human Activity Recognition Mobile Systems", "author": "H Bragan\u00e7a"}, {"title": "Fair Governance with Humans and Machines", "author": "Y Hermstr\u00fcwer"}, {"title": "A Spectrogram Based Novel Approach for Arrhythmia Detection with Convolutional Neural Networks", "author": "S Rao"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.0", "author": "JP Wanner"}, {"title": "Beyond Cross Validation-Classifier's Accuracy Prediction for Benchmarking Incremental and Active Learning Models", "author": "C Limberg"}, {"title": "Measuring and Predicting Human Trust in Recommendations from an AI Teammate", "author": "N Gurney"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang", "abstract": "Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias). Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary", "keywords": {"keywords": ["model explanations", "saliency maps", "systematic error", "faithful visual explanations", "Debiased-CAM"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Technical"], "application area": ["Machine learning explanations"]}}, {"title": "harmank@ umich. edu Computer Science & Eng. School of Information University of Michigan", "author": "H Kaur"}, {"title": "Designing for Conversational System Trustworthiness: The Impact of Model Transparency on Trust and Task Performance", "author": "A Schmitt"}, {"title": "Should a machine be trusted? A study on the effect of explainable artificial intelligence on trust in automated machine learning", "author": "C Jenytin"}, {"title": "Visually Explaining Uncertain Price Predictions in Agrifood: A User-Centred Case-Study", "author": "J Ooge"}, {"title": "Interactive Machine Learning: A State of the Art Review", "author": "NA Wondimu"}, {"title": "It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI", "author": "A Papenmeier"}, {"title": "Explainable AI and Adoption of Algorithmic Advisors: an Experimental Study", "author": "DB Davida"}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk"}, {"title": "\" It's our fault!\": insights into users' understanding and interaction with an explanatory collaborative dialog system", "author": "K Weitz"}, {"title": "Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions", "author": "H Park", "abstract": "Enterprises have recently adopted AI to human resource management (HRM) to evaluate employees' work performance evaluation. However, in such an HRM context where multiple stakeholders are complexly intertwined with different incentives, it is problematic to design AI reflecting one stakeholder group's needs (eg, enterprises, HR managers). Our research aims to investigate what tensions surrounding AI in HRM exist among stakeholders and explore design solutions to balance the tensions. By conducting stakeholder-centered", "keywords": {"keywords": ["AI in Human Resource Management", "Algorithmic Evaluation", "Stakeholders", "Design solutions", "Tensions"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Human Resource Management"]}}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "A Case for Humans-in-the-Loop: Decisions in the Presence of Misestimated Algorithmic Scores", "author": "R Fogliato"}, {"title": "A Machine Learning Approach for Personality Type Identification using MBTI Framework", "author": "H Shafi"}, {"title": "How Accurate Does It Feel?\u2013Human Perception of Different Types of Classification Mistakes", "author": "A Papenmeier"}, {"title": "Monitoring Variables Influence on Random Forest Models to Forecast Injuries in Short-Track Speed Skating", "author": "J Briand"}, {"title": "Kynurenine and Hemoglobin as Sex-Specific Variables in COVID-19 Patients: A Machine Learning and Genetic Algorithms Approach. Diagnostics 2021, 11 \u2026", "author": "JM Celaya-Padilla"}, {"title": "Exploring the impact of classification probabilities on users' trust in ambiguous instances", "author": "DDS Ribeiro"}, {"title": "Kynurenine and hemoglobin as sex-specific variables in COVID-19 patients: A machine learning and genetic algorithms approach", "author": "JM Celaya-Padilla"}, {"title": "Journal of Responsible Technology", "author": "A Simkute"}, {"title": "Walking on Eggshells: Using Analogies to Promote Appropriate Reliance in Human-AI Decision Making", "author": "G HE", "abstract": "Although AI systems have proved to be powerful in supporting decision making in critical domains, the underlying complexity and their poor explainability pose great challenges for humans to understand their working mechanisms. When humans make the final decisions supported by AI assistance, optimal team performance can be achieved if humans and AI can complement each other. Without appropriate interpretation about AI systems, users tend to overestimate or underestimate the effectiveness of AI systems when making decisions", "keywords": {"keywords": ["human-AI decision making", "analogies", "reliance", "explainability", "team performance"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": "decision making"}}, {"title": "Interpretable Machine Learning", "author": "T Martin"}, {"title": "Play for Real (ism)-Using Games to Predict Human-AI interactions in the Real World", "author": "RD Guttman"}, {"title": "People underestimate the errors by algorithms for credit scoring and recidivism but tolerate even fewer errors", "author": "F Rebitschek"}, {"title": "PhisherCop: Developing an NLP-Based Automated Tool for Phishing Detection", "author": "N Noah"}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang"}, {"title": "Developing Self-Advocacy Skills through Machine Learning Education: The Case of Ad Recommendation on Facebook", "author": "Y Register"}, {"title": "Evaluating the Impact of Algorithm Confidence Ratings on Human Decision Making in Visual Search", "author": "AP Jones"}, {"title": "Reliable Decision-Making with Imprecise Models", "author": "S Saisubramanian"}, {"title": "Trustworthy AI and Robotics and the Implications for the AEC Industry: A Systematic Literature Review and Future Potentials", "author": "N Emaminejad"}, {"title": "Publications-MIMBCD-UI/meta Wiki", "author": "FM Calisto", "abstract": "We are concerned about communicating our progress across the scientific community.  Therefore, when we achieve a potential contribution, we start to communicating our findings", "keywords": {"keywords": ["communications", "progress", "scientific community", "contribution", "findings"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Information Science"], "content": ["Application"], "application area": ["Scientific Communication"]}}, {"title": "Designing for Confidence: The Impact of Visualizing Artificial Intelligence Decisions", "author": "AJ Karran"}, {"title": "Identifying the Most Explainable Classifier", "author": "B Mullins"}, {"title": "A Comparative User Study of Human Predictions in Algorithm-Supported Recidivism Risk Assessment", "author": "M Portela"}, {"title": "Machine Learning for Humans: Building Models that Adapt to Behavior", "author": "AS Hilgard"}, {"title": "Multi-Agent Systems: Technical & Ethical Challenges of Functioning in a Mixed Group", "author": "K Gal"}, {"title": "Clinician Preimplementation Perspectives of a Decision-Support Tool for the Prediction of Cardiac Arrhythmia Based on Machine Learning: Near-Live \u2026", "author": "S Matthiesen"}, {"title": "An enhanced personality detection system through user's digital footprints", "author": "M Mobasher"}, {"title": "A Survey of Machine Learning Techniques for Video Quality Prediction from Quality of Delivery Metrics. Electronics 2021, 10, 2851", "author": "O Izima"}, {"title": "Workshop on Trust and Reliance in AI-Human Teams (TRAIT)", "author": "G Bansal"}, {"title": "TRUSTING MACHINE LEARNING ALGORITHMS IN PREDICTING MALICIOUS NODES ATTACKS", "author": "B Mahbooba", "abstract": "Identifying network attacks is a very crucial task for Internet of things (IoT) security. The increasing amount of IoT devices is creating a massive amount of data and opening new security vulnerabilities that malicious users can exploit to gain access. Recently, the research community in IoT Security has been using a data-driven approach to detect anomaly, intrusion, and cyber-attacks. However, getting accurate IoT attack data is time-consuming and expensive. On the other hand, evaluating complex security systems requires", "keywords": {"keywords": ["machine learning", "IoT security", "network attacks", "malicious nodes", "data-driven approach"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["IoT security"]}}, {"title": "Using a Machine Learning Approach to Predict the Thailand Underground Train's Passenger", "author": "W Kusonkhum"}, {"title": "Appropriate Trust in AI The Influence of Presenting Different Computer-Aided Diagnosis Variations on the Advice Utilization, Decision Certainty, and Diagnostic \u2026", "author": "R Roumans"}, {"title": "Ecology of Online Communities", "author": "N TeBlunthuis"}, {"title": "Exploring the Effects of Machine Learning Literacy Interventions on Laypeople's Reliance on Machine Learning Models", "author": "CW Chiang"}, {"title": "Trust and AI in Clinical Decision Support", "author": "B Wilson"}, {"title": "Interactive and Explainable Advising Dashboard Opens the Black Box of Student Success Prediction", "author": "H Scheers"}, {"title": "Do papers (really) match journals'\u201caims and scope\u201d? A computational assessment of innovation studies", "author": "AT Santos"}, {"title": "Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias", "author": "D Solans"}, {"title": "Mental Models and Interpretability in AI Fairness Tools and Code Environments", "author": "J Thompson"}, {"title": "A Case Study of Using Analytic Provenance to Reconstruct User Trust in a Guided Visual Analytics System", "author": "N Boukhelifa", "abstract": "In this paper, we demonstrate how analytic provenance can be exploited to re-construct user trust in a guided Visual Analytics (VA) system, and suggest that interaction log data analysis can be a valuable tool for on-line trust monitoring. Our approach explores objective trust measures that can be continuously tracked and updated during the exploration, and reflect both the confidence of the user in system suggestions, and the uncertainty of the system with regards to user goals. We argue that this approach is more suitable for guided VA systems", "keywords": {"keywords": ["analytic provenance", "user trust", "guided Visual Analytics system", "interaction log data analysis", "trust monitoring"], "research type": ["Case study"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Visual Analytics"]}}, {"title": "Machine learning in concrete science: applications, challenges, and best practices", "author": "Z Li"}, {"title": "Human and artificial intelligence in decision systems for social development", "author": "A Noriega Campero"}, {"title": "Trust in Clinical AI: Expanding the Unit of Analysis", "author": "JT Brownea"}, {"title": "Trust Cues in Artificial Intelligence: A Multilevel Case Study in a Service Organization", "author": "L van der Werff"}, {"title": "A Comparative Study of Machine Learning Algorithms for Anomaly-Based Network Intrusion Detection System", "author": "V Tripathi"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Employee Turnover Prediction-A Comparative Study of Supervised Machine Learning Models", "author": "SR Kovvuri"}, {"title": "A longitudinal approach for understanding algorithm use", "author": "A Chacon"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Delegating Agency? The Effects of XAI, Personality Traits, and the Moral Significance of the Application on the Reliance on Autonomous Systems: A User Study", "author": "E Kallina", "abstract": "As autonomous systems are becoming more prevalent, it is essential that the users' willingness to delegate agency to them is calibrated to match the system's abilities, as well as the application context. The current study investigates the influence of three factors on such willingness: The availability of XAI, the moral significance of the application context, and the users' personality traits; in particular their dispositional trust and locus of control. In an online study, participants monitored an autonomous system that classified satellite", "keywords": {"keywords": ["autonomous systems", "XAI", "personality traits", "moral significance", "willingness to delegate agency"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["User Study"]}}, {"title": "Designing predictive relations in more-than-human partnerships", "author": "I Smit"}, {"title": "An Investigation of Drivers' Dynamic Situational Trust in Conditionally Automated Driving", "author": "J Ayoub"}, {"title": "Provably improving expert predictions with prediction sets", "author": "E Straitouri"}, {"title": "Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance", "author": "GBB Nushi"}, {"title": "\u067e\u06cc\u0634\u0628\u06cc\u0646\u06cc \u0627\u0628\u062a\u0644\u0627 \u0628\u0647 \u0628\u06cc\u0645\u0627\u0631\u064a \u0645\u0632\u0645\u0646 \u06a9\u0644\u06cc\u0648\u064a \u062f\u0631 \u0634\u0647\u0631 \u0627\u0635\u0641\u0647\u0627\u0646 \u0628\u0627 \u0627\u0633\u062a\u062e\u0631\u0627\u062c \u0642\u0648\u0627\u0639\u062f \u0627\u0646\u062c\u0645\u0646\u06cc \u062a\u0648\u0633\u0637 \u062a\u06a9\u0646\u06cc\u06a9\u0647\u0627\u064a \u062f\u0627\u062f\u0647\u06a9\u0627\u0648\u064a.\u200e", "author": "\u0641\u06cc\u0631\u0648\u0632\u0647 \u0645\u0639\u06cc\u0646\u0632\u0627\u062f\u0647"}, {"title": "LEARNING MOTIVATION: PREDICTIVE MODEL WITH MACHINE LEARNING ANALYSIS", "author": "E Marsico"}], "on human predictions with explanations and predictions of machine learning models: a case study on deception detection": [{"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao", "abstract": "A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm", "keywords": {"keywords": ["explainable AI", "user experiences", "design practices", "algorithmic work", "real-world user needs"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["AI products"]}}, {"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making", "author": "Y Zhang"}, {"title": "Beyond accuracy: The role of mental models in human-AI team performance", "author": "G Bansal"}, {"title": "The principles and limits of algorithm-in-the-loop decision making", "author": "B Green"}, {"title": "Learning to deceive with attention-based explanations", "author": "D Pruthi"}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal"}, {"title": "Evaluating saliency map explanations for convolutional neural networks: a user study", "author": "A Alqaraawi"}, {"title": "Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems", "author": "Z Bu\u00e7inca"}, {"title": "What can ai do for me? evaluating machine learning interpretations in cooperative play", "author": "S Feng"}, {"title": "Debugging tests for model explanations", "author": "J Adebayo"}, {"title": "Tree space prototypes: Another look at making tree ensembles interpretable", "author": "S Tan", "abstract": "Ensembles of decision trees perform well on many problems, but are not interpretable. In contrast to existing approaches in interpretability that focus on explaining relationships between features and predictions, we propose an alternative approach to interpret tree ensemble classifiers by surfacing representative points for each class--prototypes. We introduce a new distance for Gradient Boosted Tree models, and propose new, adaptive prototype selection methods with theoretical guarantees, with the flexibility to choose a", "keywords": {"keywords": ["tree ensemble", "interpretable", "prototypes", "distance", "adaptive prototype selection"], "research type": ["theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making", "author": "Z Bu\u00e7inca"}, {"title": "\" Why is' Chicago'deceptive?\" Towards Building Model-Driven Tutorials for Humans", "author": "V Lai"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "Combat COVID-19 infodemic using explainable natural language processing models", "author": "J Ayoub"}, {"title": "Visual interaction with deep learning models through collaborative semantic inference", "author": "S Gehrmann"}, {"title": "Should we trust algorithms?", "author": "D Spiegelhalter"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI", "author": "A F\u00fcgener"}, {"title": "Does explainable artificial intelligence improve human decision-making?", "author": "Y Alufaisan", "abstract": "Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model. Using real datasets, we", "keywords": {"keywords": ["explainable AI", "human decision-making", "interpretability", "trust", "usability"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Application", "application area": "Artificial Intelligence"}}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "Ask not what AI can do, but what AI should do: Towards a framework of task delegability", "author": "B Lubars"}, {"title": "Towards unifying feature attribution and counterfactual explanations: Different means to the same end", "author": "R Kommiya Mothilal"}, {"title": "Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks", "author": "Z Lu"}, {"title": "Evaluating and characterizing human rationales", "author": "S Carton"}, {"title": "Gradient-based analysis of NLP models is manipulable", "author": "J Wang"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "Feature-Based Explanations Don't Help People Detect Misclassifications of Online Toxicity", "author": "S Carton"}, {"title": "Misplaced trust: measuring the interference of machine learning in human decision-making", "author": "H Suresh"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh", "abstract": "With the increase in data volume, velocity and types, intelligent human-agent systems have become popular and adopted in different application domains, including critical and sensitive areas such as health and security. Humans' trust, their consent and receptiveness to recommendations are the main requirement for the success of such services. Recently, the demand on explaining the recommendations to humans has increased both from humans interacting with these systems so that they make an informed decision and, also", "keywords": {"keywords": ["Explainable recommendations", "Intelligent systems", "Delivery methods", "Modalities", "Risks"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Intelligent human-agent systems"]}}, {"title": "Soliciting stakeholders' fairness notions in child maltreatment predictive systems", "author": "HF Cheng"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "How can I choose an explainer? An application-grounded evaluation of post-hoc explanations", "author": "S Jesus"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "Deceptive ai explanations: Creation and detection", "author": "J Schneider"}, {"title": "Human-ai collaboration in a cooperative game setting: Measuring social perception and outcomes", "author": "Z Ashktorab"}, {"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer"}, {"title": "Algorithmic risk assessments can alter human decision-making processes in high-stakes government contexts", "author": "B Green"}, {"title": "What Gets Echoed? Understanding the\" Pointers\" in Explanations of Persuasive Arguments", "author": "D Atkinson"}, {"title": "Many faces of feature importance: Comparing built-in and post-hoc feature importance in text classification", "author": "V Lai", "abstract": "Feature importance is commonly used to explain machine predictions. While feature importance can be derived from a machine learning model with a variety of methods, the consistency of feature importance via different methods remains understudied. In this work, we systematically compare feature importance from built-in mechanisms in a model such as attention values and post-hoc methods that approximate model behavior such as LIME. Using text classification as a testbed, we find that 1) no matter which method we use", "keywords": {"keywords": ["feature importance", "text classification", "built-in mechanisms", "post-hoc methods", "machine learning"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Natural language processing"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies", "author": "O Vereschak"}, {"title": "Fake review detection on online E-commerce platforms: a systematic literature review", "author": "H Paul"}, {"title": "Zero-shot learning with common sense knowledge graphs", "author": "NV Nayak"}, {"title": "Deciding fast and slow: The role of cognitive biases in ai-assisted decision-making", "author": "C Rastogi"}, {"title": "The flaws of policies requiring human oversight of government algorithms", "author": "B Green"}, {"title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations", "author": "S Arora"}, {"title": "The impact of algorithmic risk assessments on human predictions and its analysis via crowdsourcing studies", "author": "R Fogliato"}, {"title": "Challenging common interpretability assumptions in feature attribution explanations", "author": "J Dinu"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen", "abstract": "Explaining the decisions of an Artificial Intelligence (AI) model is increasingly critical in many real-world, high-stake applications. Hundreds of papers have either proposed new feature attribution methods, discussed or harnessed these tools in their work. However, despite humans being the target end-users, most attribution methods were only evaluated on proxy automatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et al. 2018). In this paper, we conduct the first user study to measure attribution map effectiveness in assisting", "keywords": {"keywords": ["feature attribution methods", "effectiveness", "correlation", "automatic evaluation scores", "user study"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "The utility of explainable ai in ad hoc human-machine teaming", "author": "R Paleja"}, {"title": "Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface", "author": "M Desmond"}, {"title": "Evaluating the Effect of Justification and Confidence Information on User Perception of a Privacy Policy Summarization Tool.", "author": "V Bracamonte"}, {"title": "When AIs Say Yes and I Say No: On the Tension between AI's Decision and Human's Decision from the Epistemological Perspectives", "author": "CY Ku"}, {"title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspective", "author": "H Lakkaraju"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence", "author": "W Jin"}, {"title": "Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approach", "author": "H Mucha"}, {"title": "Harnessing explanations to bridge ai and humans", "author": "V Lai"}, {"title": "Interpreting deep learning models in natural language processing: A review", "author": "X Sun"}, {"title": "Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning", "author": "KZ Gajos", "abstract": "When people receive advice while making difficult decisions, they often make better decisions in the moment and also increase their knowledge in the process. However, such incidental learning can only occur when people cognitively engage with the information they receive and process this information thoughtfully. How do people process the information and advice they receive from AI, and do they engage with it deeply enough to enable learning? To answer these questions, we conducted three experiments in which individuals", "keywords": {"keywords": ["AI assistance", "incidental learning", "cognitive engagement", "advice", "decision-making"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Decision-making"]}}, {"title": "Networked Trust: Computational Understanding of Interpersonal Trust Online", "author": "X Ma"}, {"title": "Human-AI collaboration for natural language generation with interpretable neural networks", "author": "S Gehrmann"}, {"title": "How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions", "author": "HF Cheng"}, {"title": "Sage advice? The impacts of explanations for machine learning models on human decision-making in spam detection", "author": "MC Stites"}, {"title": "Effect of ai explanations on human perceptions of patient-facing ai-powered healthcare systems", "author": "Z Zhang"}, {"title": "The impact of explanations on AI competency prediction in VQA", "author": "K Alipour"}, {"title": "Understanding How People Reason about Aesthetic Evaluations of Artificial Intelligence", "author": "C Oh"}, {"title": "When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models", "author": "A Rechkemmer"}, {"title": "Improving users' mental model with attention\u2010directed counterfactual edits", "author": "K Alipour"}, {"title": "Hive: evaluating the human interpretability of visual explanations", "author": "SSY Kim", "abstract": "As machine learning is increasingly applied to high-impact, high-risk domains, there have been a number of new methods aimed at making AI models more human interpretable. Despite the recent growth of interpretability work, there is a lack of systematic evaluation of proposed techniques. In this work, we propose a novel human evaluation framework HIVE (Human Interpretability of Visual Explanations) for diverse interpretability methods in computer vision; to the best of our knowledge, this is the first work of its kind. We argue that", "keywords": {"keywords": ["human interpretability", "visual explanations", "machine learning", "interpretability methods", "computer vision"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "AI interpretability"}}, {"title": "Conceptualizing Digital Resilience for AI-based Information Systems.", "author": "M Schemmer"}, {"title": "Three directions for the design of human-centered machine translation", "author": "S Robertson"}, {"title": "Introducing a multi-stakeholder perspective on opacity, transparency and strategies to reduce opacity in algorithm-based human resource management", "author": "M Langer"}, {"title": "Decision-Focused Summarization", "author": "CC Hsu"}, {"title": "How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI.", "author": "ZT Zhang"}, {"title": "Adoption Barriers of AI: a Context-Specific Acceptance Model for Industrial Maintenance.", "author": "J Wanner"}, {"title": "Double Trouble: How to not explain a text classifier's decisions using counterfactuals synthesized by masked language models?", "author": "TM Pham"}, {"title": "Intelligent Decision Assistance Versus Automated Decision-Making: Enhancing Knowledge Work Through Explainable Artificial Intelligence", "author": "M Schemmer"}, {"title": "EUCA: the End-User-Centered Explainable AI Framework", "author": "W Jin"}, {"title": "On the Influence of Explainable AI on Automation Bias", "author": "M Schemmer", "abstract": "Artificial intelligence (AI) is gaining momentum, and its importance for the future of work in many areas, such as medicine and banking, is continuously rising. However, insights on the effective collaboration of humans and AI are still rare. Typically, AI supports humans in decision-making by addressing human limitations. However, it may also evoke human bias, especially in the form of automation bias as an over-reliance on AI advice. We aim to shed light on the potential to influence automation bias by explainable AI (XAI). In this pre-test, we", "keywords": {"keywords": ["Explainable AI", "automation bias", "human-AI collaboration", "decision-making", "AI advice"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer science", "content": "Application", "application area": "Human-AI collaboration"}}, {"title": "Intuitively assessing ml model reliability through example-based explanations and editing model inputs", "author": "H Suresh"}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "Bayesian modeling of human\u2013AI complementarity", "author": "M Steyvers"}, {"title": "Validation Methods to Promote Real-world Applicability of Machine Learning in Medicine", "author": "R Bin Rafiq"}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization", "author": "M Fan"}, {"title": "Forward reasoning decision support: toward a more complete view of the human-AI interaction design space", "author": "ZT Zhang"}, {"title": "Justifying Social-Choice Mechanism Outcome for Improving Participant Satisfaction", "author": "SA Suryanarayana"}, {"title": "People underestimate the errors made by algorithms for credit scoring and recidivism prediction but accept even fewer errors", "author": "FG Rebitschek"}, {"title": "How transparency modulates trust in artificial intelligence", "author": "J Zerilli"}, {"title": "The Role of Algorithmic Transparency in Contact-tracing App Adoption", "author": "T Bitzer", "abstract": "Contact-tracing apps represent an algorithm-based technology with significant potential to help address the COVID-19 pandemic by facilitating swift case isolation. However, low adoption rates have prevented these apps from tapping their full potential. A key barrier to adoption has been citizens' uncertainty surrounding contact-tracing apps. In this regard, algorithmic transparency may be a critical factor in fostering app adoption. In this study, we focus on one central aspect of algorithmic transparency\u2014namely, transformation algorithmic", "keywords": {"keywords": ["algorithmic transparency", "contact-tracing apps", "adoption rates", "COVID-19", "transformation algorithmic"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Risky Reforms: A Sociotechnical Analysis of Algorithms as Tools for Social Change", "author": "B Green"}, {"title": "Better together? an evaluation of ai-supported code translation", "author": "JD Weisz"}, {"title": "Explainable NLP for Human-AI Collaboration", "author": "P Lertvittayakumjorn"}, {"title": "On the Relationship Between Explanations, Fairness Perceptions, and Decisions", "author": "J Schoeffer"}, {"title": "The influences of task design on crowdsourced judgement: A case study of recidivism risk evaluation", "author": "X Duan"}, {"title": "A MULTI-LAYERS ARCHITECTURE FOR AN E-LEARNING HYBRID RECOMMENDER SYSTEM", "author": "L Roux"}, {"title": "Pilot attitudes toward AI in the cockpit: implications for design", "author": "ZT Zhang"}, {"title": "Semi-automated data labeling", "author": "M Desmond"}, {"title": "Four years of FAccT: A reflexive, mixed-methods analysis of research contributions, shortcomings, and future prospects", "author": "B Laufer"}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation", "author": "V Lai", "abstract": "Despite impressive performance in many benchmark datasets, AI models can still make mistakes, especially among out-of-distribution examples. It remains an open question how such imperfect models can be used effectively in collaboration with humans. Prior work has focused on AI assistance that helps people make individual high-stakes decisions, which is not scalable for a large amount of relatively low-stakes decisions, eg, moderating social media comments. Instead, we propose conditional delegation as an alternative paradigm for", "keywords": {"keywords": ["Human-AI Collaboration", "Conditional Delegation", "Content Moderation", "AI models", "Benchmark datasets"], "research type": ["Case study"], "methodology": ["Mixed-methods"], "purposes": ["Prescriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Social media"]}}, {"title": "Explainability auditing for intelligent systems: a rationale for multi-disciplinary perspectives", "author": "M Langer"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Will You Accept the AI Recommendation? Predicting Human Behavior in AI-Assisted Decision Making", "author": "X Wang"}, {"title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging", "author": "R Fogliato"}, {"title": "Effective Task Allocation in Ad Hoc Human-Agent Teams", "author": "S ABUHAIMED"}, {"title": "Under-reliance or misalignment? How proxy outcomes limit measurement of appropriate reliance in AI-assisted decision-making", "author": "L GUERDAN"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "Hierarchical Interpretation of Neural Text Classification", "author": "H Yan"}, {"title": "Generating argumentation frameworks from text", "author": "O Cocarascu"}, {"title": "Aid of a Machine Learning Algorithm Can Improve Clinician Predictions of Patient Quality of Life During Breast Cancer Treatments", "author": "M Nuutinen", "abstract": "Proper and well-timed interventions may improve breast cancer patient adaptation and quality of life (QoL) through treatment and recovery. The challenge is to identify those patients who would benefit most from a particular intervention. The aim of this study was to measure whether the machine learning prediction incorporated in the clinical decision support system (CDSS) improves clinicians' performance to predict patients' QoL during treatment process. We conducted two user experiments in which clinicians used a CDSS to", "keywords": {"keywords": ["machine learning", "clinician predictions", "patient quality of life", "breast cancer treatments", "clinical decision support system"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Medical Science", "content": "Application", "application area": "Healthcare"}}, {"title": "On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods", "author": "K Amarasinghe"}, {"title": "Homophily and Incentive Effects in Use of Algorithms", "author": "R Fogliato"}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "Do explanations increase the effectiveness of AI-crowd generated fake news warnings?", "author": "Z Epstein"}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "Designing with Machine Learning in Digital Pathology: Augmenting Medical Specialists Through Interaction Design", "author": "M Lindvall"}, {"title": "On the Effect of Information Asymmetry in Human-AI Teams", "author": "P Hemmer"}, {"title": "Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study", "author": "K Sengupta"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.0", "author": "JP Wanner"}, {"title": "SUPPORTING CREW MEDICAL DECISIONS ON DEEP SPACE MISSIONS: A REAL-TIME PERFORMANCE MONITORING CAPABILITY", "author": "BL Beard", "abstract": "Crewed missions into deep space will require astronauts to respond autonomously to safety-and time-critical anomalies/events. Not only does this require expertise in many areas, but their ability to respond quickly and appropriately may be compromised from exposure to the stressors of spaceflight. A potential solution is to provide decision support for anomaly diagnosis and resolution. A complementary solution that could be integrated into the decision support system would be to build a system that spot-checks and/or continuously", "keywords": {"keywords": ["crew medical decisions", "deep space missions", "real-time performance monitoring", "anomaly diagnosis", "decision support system"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["space medicine"], "content": ["application"], "application area": ["space exploration"]}}, {"title": "Robustness and Usefulness in AI Explanation Methods", "author": "E Galinkin"}, {"title": "Metal-free C-3 alkylation of imidazopyridines with xanthates and convenient access to alpidem and zolpidem", "author": "S Wang"}, {"title": "Building Trust by Supporting Situation Awareness: Exploring Pilots' Design Requirements for Decision Support Tools", "author": "C STORATH"}, {"title": "Towards Human-Centred Explainable AI", "author": "MSM ten Koppel"}, {"title": "A Study of Explainability Features to Scrutinize Faceted Filtering Results", "author": "J Qu"}, {"title": "Trust and Reliance in XAI--Distinguishing Between Attitudinal and Behavioral Measures", "author": "N Scharowski"}, {"title": "Investigating Trust in Human-Machine Learning Collaboration: A Pilot Study on Estimating Public Anxiety from Speech", "author": "AA Tutul"}, {"title": "SAGE Intrusion Detection System: Sensitivity Analysis Guided Explainability for Machine Learning.", "author": "M Smith"}, {"title": "Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Questions", "author": "A Parrish"}, {"title": "When AIs Say Yes and I Say No", "author": "CYUN KU", "abstract": "fields show that the Artificial Intelligence (AI/AIs), or say the algorithm, is not only an idea  in the Sci-Fi but also a reality in our daily lives. Not even mention those AI applications in the", "keywords": {"keywords": ["Artificial Intelligence", "Sci-Fi", "algorithm", "AI applications", "daily lives"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["daily life"]}}, {"title": "Explainability in Mechanism Design: Recent Advances and the Road Ahead", "author": "SA Suryanarayana"}, {"title": "Markus Langera & Cornelius J. K\u00f6niga Universit\u00e4t des Saarlandes", "author": "M Langer"}, {"title": "Play for Real (ism)-Using Games to Predict Human-AI interactions in the Real World", "author": "RD Guttman"}, {"title": "People underestimate the errors by algorithms for credit scoring and recidivism but tolerate even fewer errors", "author": "F Rebitschek"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "\" If it didn't happen, why would I change my decision?\": How Judges Respond to Counterfactual Explanations for the Public Safety Assessment", "author": "Y Yacoby"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimization", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "The role of explainable AI in the research field of AI ethics: systematic mapping study", "author": "H Vainio-Pekka"}, {"title": "Stakeholder perceptions of the safety and assurance of artificial intelligence in healthcare", "author": "MA Sujan"}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang", "abstract": "People consider recommendations from AI systems in diverse domains ranging from recognizing tumors in medical images to deciding which shoes look cute with an outfit. Implicit in the decision process is the perceived expertise of the AI system. In this paper, we investigate how people trust and rely on an AI assistant that performs with different levels of expertise relative to the person, ranging from completely overlapping expertise to perfectly complementary expertise. Through a series of controlled online lab studies where", "keywords": {"keywords": ["AI systems", "trust", "expertise", "human-AI teams", "complementary expertise"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-AI Interaction"]}}, {"title": "The Algorithmic Imprint", "author": "U Ehsan"}, {"title": "Three Directions for the Design of Human-Centered Machine Translation", "author": "DJ Liebling"}, {"title": "The Response Shift Paradigm to Quantify Human Trust in AI Recommendations", "author": "A Shafti"}, {"title": "Effective Task Allocation in Ad Hoc Human-Agent Teams (Full)", "author": "S Abuhaimed"}, {"title": "Is attention explanation? A look at the case of sequential recommender systems", "author": "T Baumhauer"}, {"title": "The Design and Evaluation of Neural Attention Mechanisms for Explaining Text Classifiers", "author": "S Carton"}, {"title": "Machine Learning for Humans: Building Models that Adapt to Behavior", "author": "AS Hilgard"}, {"title": "Effects of Explanatory Information on Privacy Policy Summarization Tool Perception", "author": "S Kiyomoto"}, {"title": "Human-Centered Evaluation of Explanations", "author": "J Boyd-Graber"}, {"title": "Effects of Explanatory Information on Privacy Policy Summarization Tool Perception", "author": "V Bracamonte", "abstract": "Privacy policies summarization tools can provide information about the contents of a privacy policy in a short and usable format. Although these automated tools can support users in understanding the information in privacy policies, they rely on machine learning techniques for the analysis of textual data to generate these summaries and can therefore contain errors which may affect the reliability of their results. A few of the existing privacy policy summarization tools provide some explanatory information about their performance, but the", "keywords": {"keywords": ["privacy policy summarization", "explanatory information", "tools", "machine learning techniques", "reliability"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Technology"]}}, {"title": "Algorithm Reliance Under Pressure: The Effect of Customer Load on Service Workers", "author": "C Snyder"}, {"title": "An Exploration of Post-Editing Effectiveness in Text Summarization", "author": "V Lai"}, {"title": "Machine-in-the-Loop Process in Project Risk Management", "author": "T Mori"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand"}, {"title": "Attention-based and Causal Explanations in Computer Vision", "author": "K Alipour"}, {"title": "\u201cRather Solve the Problem from Scratch\u201d: Gamesploring Human-Machine Collaboration for Optimizing the Debris Collection Problem", "author": "A Ulusan"}, {"title": "Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias", "author": "D Solans"}, {"title": "Human detection of political deepfakes across transcripts, audio, and video", "author": "M Groh"}, {"title": "Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language", "author": "R Sevastjanova"}, {"title": "Tell Me Something That Will Help Me Trust You: A Survey of Trust Calibration in Human-Agent Interaction", "author": "GJ Cancro", "abstract": "When a human receives a prediction or recommended course of action from an intelligent agent, what additional information, beyond the prediction or recommendation itself, does the human require from the agent to decide whether to trust or reject the prediction or recommendation? In this paper we survey literature in the area of trust between a single human supervisor and a single agent subordinate to determine the nature and extent of this additional information and to characterize it into a taxonomy that can be leveraged by future", "keywords": {"keywords": ["trust calibration", "human-agent interaction", "literature survey", "additional information", "trust taxonomy"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Deep Learning-Based Text Emotion Analysis for Legal Anomie", "author": "B She"}, {"title": "DETECTING DECEPTION IN THE WILDERNESS: A PROPOSAL OF WARFARE INTEGRATED LIE DETECTION SYSTEM (WILDS)", "author": "A Ahmad"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Argumentative Explanations for Pattern-Based Text Classifiers", "author": "P Lertvittayakumjorn"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Delegating Agency? The Effects of XAI, Personality Traits, and the Moral Significance of the Application on the Reliance on Autonomous Systems: A User Study", "author": "E Kallina"}, {"title": "Empowerment Effects in Human-machine Collaboration-A Systematic Literature Review and Directions on Hybrid Intelligence Behavior Patterns", "author": "E Akmeikina"}, {"title": "Automatically Explaining Machine Learning Predictions on Severe Chronic Obstructive Pulmonary Disease Exacerbations: Retrospective Cohort Study", "author": "S Zeng"}, {"title": "Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance", "author": "GBB Nushi"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen", "abstract": "Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human", "keywords": {"keywords": ["machine explanations", "human understanding", "empirical studies", "causal diagrams", "model debugging"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}], "there is not enough information: on the effects of explanations on perceptions of informational fairness and trustworthiness in automated decision-making": [{"title": "Training Novices: The Role of Human-AI Collaboration and Knowledge Transfer", "author": "P Spitzer", "abstract": "Across a multitude of work environments, expert knowledge is imperative for humans to conduct tasks with high performance and ensure business success. These humans possess task-specific expert knowledge (TSEK) and hence, represent subject matter experts (SMEs). However, not only demographic changes but also personnel downsizing strategies lead and will continue to lead to departures of SMEs within organizations, which constitutes the challenge of how to retain that expert knowledge and train novices to keep the competitive", "keywords": {"keywords": ["human-AI collaboration", "knowledge transfer", "novices", "task-specific expert knowledge", "SMEs"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["sociology"], "content": ["application"], "application area": ["work environments"]}}], "why am i not seeing it? understanding users' needs for counterfactual explanations in everyday recommendations": [], "the utility of explainable ai in ad hoc human-machine teaming": [{"title": "Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming", "author": "S Konan", "abstract": "Information sharing is key in building team cognition and enables coordination and cooperation. High-performing human teams also benefit from acting strategically with hierarchical levels of iterated communication and rationalizability, meaning a human agent can reason about the actions of their teammates in their decision-making. Yet, the majority of prior work in Multi-Agent Reinforcement Learning (MARL) does not support iterated rationalizability and only encourage inter-agent communication, resulting in a suboptimal", "keywords": [["cognition", 0.3069], ["teammates", 0.2903], ["cooperation", 0.2778], ["communication", 0.2709], ["rationalizability", 0.2665], ["reinforcement", 0.2556], ["hierarchical", 0.2537], ["information", 0.2455], ["coordination", 0.2394], ["learning", 0.2266]]}, {"title": "Adaptable and Scalable Multi-Agent Graph-Attention Communication", "author": "Y Niu", "abstract": "High-performing teams learn effective communication strategies to judiciously share information and reduce the cost of communication overhead. Within multi-agent reinforcement learning, synthesizing effective policies requires reasoning about when to communicate, whom to communicate with, and how to process messages. Meanwhile, in real-world problems, training policies and communication strategies that are able to generalize to multiple tasks, and adapt to unseen tasks, can improve the learning efficiency", "keywords": {"keywords": ["multi-agent reinforcement learning", "graph-attention", "communication strategies", "generalization", "adaptability"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["Artificial Intelligence"]}}, {"title": "Mutual Understanding in Human-Machine Teaming", "author": "R Paleja", "abstract": "Collaborative robots (ie,\u201ccobots\u201d) and machine learningbased virtual agents are increasingly entering the human workspace with the aim of increasing productivity, enhancing safety, and improving the quality of our lives. These agents will dynamically interact with a wide variety of people in dynamic and novel contexts, increasing the prevalence of human-machine teams in healthcare, manufacturing, and search-and-rescue. In this research, we enhance the mutual understanding within a human-machine team by", "keywords": {"keywords": ["mutual understanding", "human-machine teaming", "collaborative robots", "machine learning-based virtual agents", "dynamic interaction"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Collaborative workspaces"]}}, {"title": "Learning Performance Graphs from Demonstrations via Task-Based Evaluations", "author": "AG Puranic", "abstract": "In the learning from demonstration (LfD) paradigm, understanding and evaluating the demonstrated behaviors plays a critical role in extracting control policies for robots. Without this knowledge, a robot may infer incorrect reward functions that lead to undesirable or unsafe control policies. Recent work has proposed an LfD framework where a user provides a set of formal task specifications to guide LfD, to address the challenge of reward shaping. However, in this framework, specifications are manually ordered in a performance graph (a", "keywords": {"keywords": ["learning from demonstration", "task-based evaluations", "performance graphs", "reward shaping", "control policies"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["robotics"]}}, {"title": "i-Sim2Real: Reinforcement Learning of Robotic Policies in Tight Human-Robot Interaction Loops", "author": "S Abeyruwan", "abstract": "Sim-to-real transfer is a powerful paradigm for robotic reinforcement learning. The ability to train policies in simulation enables safe exploration and large-scale data collection quickly at low cost. However, prior works in sim-to-real transfer of robotic policies typically do not involve any human-robot interaction because accurately simulating human behavior is an open problem. In this work, our goal is to leverage the power of simulation to train robotic policies that are proficient at interacting with humans upon deployment. But there is a", "keywords": {"keywords": ["sim-to-real transfer", "reinforcement learning", "human-robot interaction", "simulation", "robotic policies"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Robotics"]}}, {"title": "Empirically Evaluating Meta Learning of Robot Explainability with Humans", "author": "A Silva", "abstract": "As physically-embodied robots and digital assistants are deployed in the real world, these agents must be able to communicate their decision-making criteria to build trust, improve human-robot teaming, and enable collaboration. While the field of explainable machine learning has made great strides in building a set of mechanisms to enable such communication, these advancements often assume that one approach is ideally suited to one problem (eg, decision trees are best for explaining how to triage patients in an", "keywords": {"keywords": ["robot explainability", "meta learning", "human-robot teaming", "explanation mechanisms", "decision-making criteria"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer science", "content": "Application", "application area": "Human-robot interaction"}}, {"title": "Learning Interpretable, High-Performing Policies for Autonomous Driving", "author": "R Paleja", "abstract": "Gradient-based approaches in reinforcement learning have achieved tremendous success in learning policies for autonomous vehicles. While the performance of these approaches warrants real-world adoption, these policies lack interpretability, limiting deployability in the safety-critical and legally-regulated domain of autonomous driving (AD). AD requires interpretable and verifiable control policies that maintain high performance. We propose Interpretable Continuous Control Trees (ICCTs), a tree-based model that can be optimized", "keywords": {"keywords": ["autonomous driving", "interpretable policies", "gradient-based approaches", "continuous control trees", "high-performance policies"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Technical"], "application area": ["Autonomous driving"]}}, {"title": "Strategy Discovery and Mixture in Lifelong Learning from Heterogeneous Demonstration", "author": "S Jayanthi", "abstract": "Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. A key challenge in LfD research is that users tend to provide heterogeneous demonstrations for the same task due to various strategies and preferences. Therefore, it is essential to develop LfD algorithms that ensure\\textit {flexibility}(the robot adapts to personalized strategies),\\textit {efficiency}(the robot achieves sample-efficient adaptation), and\\textit {scalability}(robot", "keywords": {"keywords": ["learning from demonstration", "heterogeneous demonstrations", "lifelong learning", "strategy discovery", "mixture"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Robotics"]}}], "debugging tests for model explanations": [{"title": "Getting a clue: A method for explaining uncertainty estimates", "author": "J Antor\u00e1n", "abstract": "Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the", "keywords": {"keywords": ["uncertainty estimation", "interpretability", "probabilistic models", "Bayesian Neural Networks", "counterfactual explanations"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning"}}, {"title": "Benchmarking and survey of explanation methods for black box models", "author": "F Bodria"}, {"title": "Do feature attribution methods correctly attribute features?", "author": "Y Zhou"}, {"title": "Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond", "author": "X Li"}, {"title": "Evaluating saliency methods for neural language models", "author": "S Ding"}, {"title": "Leveraging sparse linear layers for debuggable deep networks", "author": "E Wong"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "3db: A framework for debugging computer vision models", "author": "G Leclerc"}, {"title": "CLEVR-XAI: a benchmark dataset for the ground truth evaluation of neural network explanations", "author": "L Arras"}, {"title": "Ground truth evaluation of neural network explanations with clevr-xai", "author": "L Arras"}, {"title": "Towards benchmarking the utility of explanations for model debugging", "author": "M Idahl", "abstract": "Post-hoc explanation methods are an important class of approaches that help understand the rationale underlying a trained model's decision. But how useful are they for an end-user towards accomplishing a given task? In this vision paper, we argue the need for a benchmark to facilitate evaluations of the utility of post-hoc explanation methods. As a first step to this end, we enumerate desirable properties that such a benchmark should possess for the task of debugging text classifiers. Additionally, we highlight that such a benchmark", "keywords": {"keywords": ["explanations", "model debugging", "benchmarking", "post-hoc explanation methods", "text classifiers"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning/Artificial Intelligence"]}}, {"title": "Acquisition of chess knowledge in alphazero", "author": "T McGrath"}, {"title": "Do Input Gradients Highlight Discriminative Features?", "author": "H Shah"}, {"title": "Characterizing the risk of fairwashing", "author": "U A\u00efvodji"}, {"title": "Fooling Partial Dependence via Data Poisoning", "author": "H Baniecki"}, {"title": "Sanity simulations for saliency methods", "author": "JS Kim"}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta"}, {"title": "Edge: Explaining deep reinforcement learning policies", "author": "W Guo"}, {"title": "Fully Automated Colorimetric Analysis of the Optic Nerve Aided by Deep Learning and Its Association with Perimetry and OCT for the Study of Glaucoma", "author": "M Gonzalez-Hernandez"}, {"title": "Quantus: an explainable AI toolkit for responsible evaluation of neural network explanations", "author": "A Hedstr\u00f6m"}, {"title": "Quality metrics for transparent machine learning with and without humans in the loop are not correlated", "author": "F Biessmann", "abstract": "The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality", "keywords": {"keywords": ["explainable artificial intelligence", "transparent machine learning", "interpretable computer vision algorithms", "psychophysics", "annotation tasks"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Machine Learning"]}}, {"title": "Finding and fixing spurious patterns with explanations", "author": "G Plumb"}, {"title": "Improving deep learning interpretability by saliency guided training", "author": "AA Ismail"}, {"title": "Selective Ensembles for Consistent Predictions", "author": "E Black"}, {"title": "An empirical study on explanations in out-of-domain settings", "author": "G Chrysostomou"}, {"title": "Hive: evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "Shared Interest: Large-Scale Visual Analysis of Model Behavior by Measuring Human-AI Alignment", "author": "A Boggust"}, {"title": "Towards Black-Box Explainability with Gaussian Discriminant Knowledge Distillation", "author": "A Haselhoff"}, {"title": "Shared Interest: Measuring Human-AI Alignment to Identify Recurring Patterns in Model Behavior", "author": "A Boggust"}, {"title": "Resource Reservation in Sliced Networks: An Explainable Artificial Intelligence (XAI) Approach", "author": "P Barnard"}, {"title": "Post hoc explanations may be ineffective for detecting unknown spurious correlation", "author": "J Adebayo", "abstract": "We investigate whether three types of post hoc model explanations\u2013feature attribution, concept activation, and training point ranking\u2013are effective for detecting a model's reliance on spurious signals in the training data. Specifically, we consider the scenario where the spurious signal to be detected is unknown, at test-time, to the user of the explanation method. We design an empirical methodology that uses semi-synthetic datasets along with pre-specified spurious artifacts to obtain models that verifiably rely on these spurious", "keywords": {"keywords": ["post hoc explanations", "spurious correlation", "unknown spurious signals", "feature attribution", "concept activation"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Evaluating feature attribution: An information-theoretic perspective", "author": "Y Rong"}, {"title": "Simulated User Studies for Explanation Evaluation", "author": "V Chen"}, {"title": "Diverse, Global and Amortised Counterfactual Explanations for Uncertainty Estimates", "author": "D Ley"}, {"title": "Revisiting Sanity Checks for Saliency Maps", "author": "G Yona"}, {"title": "A psychological theory of explainability", "author": "SCH Yang"}, {"title": "Neural Activation Patterns (NAPs): Visual Explainability of Learned Concepts", "author": "A B\u00e4uerle"}, {"title": "Right for the Right Reasons: Training Neural Networks to Be Interpretable, Robust, and Consistent with Expert Knowledge", "author": "AS Ross"}, {"title": "Use-Case-Grounded Simulations for Explanation Evaluation", "author": "V Chen"}, {"title": "More Than Words: Towards Better Quality Interpretations of Text Classifiers", "author": "MB Zafar"}, {"title": "A Unified Study of Machine Learning Explanation Evaluation Metrics", "author": "Y Wang", "abstract": "The growing need for trustworthy machine learning has led to the blossom of interpretability research. Numerous explanation methods have been developed to serve this purpose. However, these methods are deficiently and inappropriately evaluated. Many existing metrics for explanations are introduced by researchers as by-products of their proposed explanation techniques to demonstrate the advantages of their methods. Although widely used, they are more or less accused of problems. We claim that the lack of acknowledged", "keywords": {"keywords": ["machine learning", "explanation", "evaluation metrics", "interpretability research", "trustworthy"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning explanation evaluation"]}}, {"title": "On Generating Symbolic Explanations for Recurrent Neural Networks", "author": "M de Sousa Ribeiro"}, {"title": "Deletion and Insertion Tests in Regression Models", "author": "N Hama"}, {"title": "Path Integrals for the Attribution of Model Uncertainties", "author": "IP Lopez"}, {"title": "The Need for Empirical Evaluation of Explanation Quality", "author": "N Halliwell"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimization", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "Do not explain without context: addressing the blind spot of model explanations", "author": "K Wo\u017anica"}, {"title": "xxAI-Beyond Explainable Artificial Intelligence", "author": "A Holzinger"}, {"title": "Concept-based Explanations for Out-Of-Distribution Detectors", "author": "J Choi"}, {"title": "Concept Activation Vectors for Generating User-Defined 3D Shapes", "author": "S Druc", "abstract": "We explore the interpretability of 3D geometric deep learning models in the context of Computer-Aided Design (CAD). The field of parametric CAD can be limited by the difficulty of expressing high-level design concepts in terms of a few numeric parameters. In this paper, we use a deep learning architectures to encode high dimensional 3D shapes into a vectorized latent representation that can be used to describe arbitrary concepts. Specifically, we train a simple auto-encoder to parameterize a dataset of complex shapes. To understand", "keywords": {"keywords": ["3D geometric deep learning", "Computer-Aided Design (CAD)", "parametric CAD", "auto-encoder", "latent representation"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Design Optimization"]}}, {"title": "Subgoal-Based Explanations for Unreliable Intelligent Decision Support Systems", "author": "D Das"}, {"title": "Perspectives on Incorporating Expert Feedback into Model Updates", "author": "V Chen"}, {"title": "Path Integrals for the Attribution of Model Uncertainties", "author": "I Perez"}, {"title": "The Manifold Hypothesis for Gradient-Based Explanations", "author": "S Bordt"}, {"title": "Interpreting models interpreting brain dynamics", "author": "M Rahman"}, {"title": "Towards ML Methods for Biodiversity: A Novel Wild Bee Dataset and Evaluations of XAI Methods for ML-Assisted Rare Species Annotations", "author": "T Chiaburu"}, {"title": "Attribution of Predictive Uncertainties in Classification Models", "author": "I Perez"}, {"title": "\" Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification", "author": "J Bastings"}, {"title": "Using Expert Interpretation and Reasoning to Guide Model Selection in Machine Learning", "author": "J Wang"}, {"title": "Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection", "author": "T Bose", "abstract": "Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source. This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. Previous work has attempted to mitigate this problem by regularizing specific terms from pre-defined static dictionaries. While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the", "keywords": {"keywords": ["Hate speech detection", "Regularization", "Generalizability", "Dynamically refined", "Cross-corpora"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Hate speech detection"]}}, {"title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset", "author": "L Sixt"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XAI", "author": "Q Wang"}, {"title": "Expert-Informed, User-Centric Explanations for Machine Learning", "author": "M Pazzani"}], "metrics for explainable ai: challenges and prospects": [{"title": "Causability and explainability of artificial intelligence in medicine", "author": "A Holzinger", "abstract": "Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black\u2010box machine learning", "keywords": {"keywords": ["Explainable artificial intelligence", "Causability", "Medicine", "Transparency", "Traceability"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purpose": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Healthcare"]}}, {"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao"}, {"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making", "author": "Y Zhang"}, {"title": "Explainable reinforcement learning through a causal lens", "author": "P Madumal"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "Interpretable machine learning\u2013a brief history, state-of-the-art and challenges", "author": "C Molnar"}, {"title": "A survey on the explainability of supervised machine learning", "author": "N Burkart"}, {"title": "Explainability in deep reinforcement learning", "author": "A Heuillet"}, {"title": "What do we want from Explainable Artificial Intelligence (XAI)?\u2013A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research", "author": "M Langer"}, {"title": "Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems", "author": "Z Bu\u00e7inca"}, {"title": "Evaluating the quality of machine learning explanations: A survey on methods and metrics", "author": "J Zhou", "abstract": "The most successful Machine Learning (ML) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of ML explanation has experienced a surge in interest from the research community to application domains. While numerous explanation", "keywords": {"keywords": ["machine learning", "explanations", "quality", "methods", "metrics"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["ML explanation"]}}, {"title": "A practical tutorial on bagging and boosting based ensembles for machine learning: Algorithms, software tools, performance study, practical perspectives and \u2026", "author": "S Gonz\u00e1lez"}, {"title": "Evaluating XAI: A comparison of rule-based and example-based explanations", "author": "J van der Waa"}, {"title": "A historical perspective of explainable Artificial Intelligence", "author": "R Confalonieri"}, {"title": "A survey of data-driven and knowledge-aware explainable ai", "author": "XH Li"}, {"title": "Trust in AutoML: exploring information needs for establishing trust in automated machine learning systems", "author": "J Drozdal"}, {"title": "The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems", "author": "M Nourani"}, {"title": "Cocox: Generating conceptual and counterfactual explanations via fault-lines", "author": "A Akula"}, {"title": "The need to separate the wheat from the chaff in medical informatics: Introducing a comprehensive checklist for the (self)-assessment of medical AI studies", "author": "F Cabitza"}, {"title": "Explaining reinforcement learning to mere mortals: An empirical study", "author": "A Anderson"}, {"title": "Four principles of explainable artificial intelligence", "author": "PJ Phillips", "abstract": "four principles for explainable artificial intelligence (AI) that comprise fundamental properties  for explainable AI  In this paper, we introduce four principles that we believe comprise", "keywords": {"keywords": ["explainable artificial intelligence", "principles", "fundamental properties", "AI"], "research type": ["theoretical"], "methodology": [], "purposes": ["explanatory"], "discipline": ["computer science"], "content": [], "application area": []}}, {"title": "Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies", "author": "EM Kenny"}, {"title": "Explainable AI (xAI) for anatomic pathology", "author": "AB Tosun"}, {"title": "Operationalizing human-centered perspectives in explainable AI", "author": "U Ehsan"}, {"title": "How much automation does a data scientist want?", "author": "D Wang"}, {"title": "Counterfactuals and causability in explainable artificial intelligence: Theory, algorithms, and applications", "author": "YL Chou"}, {"title": "Using ontologies to enhance human understandability of global post-hoc explanations of black-box models", "author": "R Confalonieri"}, {"title": "In AI we trust? Factors that influence trustworthiness of AI-infused decision-making processes", "author": "M Ashoori"}, {"title": "The role of domain expertise in user trust and the impact of first impressions with intelligent systems", "author": "M Nourani"}, {"title": "Does explainable artificial intelligence improve human decision-making?", "author": "Y Alufaisan"}, {"title": "Local and global explanations of agent behavior: Integrating strategy summaries with saliency maps", "author": "T Huber", "abstract": "With advances in reinforcement learning (RL), agents are now being developed in high-stakes application domains such as healthcare and transportation. Explaining the behavior of these agents is challenging, as the environments in which they act have large state spaces, and their decision-making can be affected by delayed rewards, making it difficult to analyze their behavior. To address this problem, several approaches have been developed. Some approaches attempt to convey the global behavior of the agent, describing the actions", "keywords": {"keywords": ["agent behavior", "explanation", "strategy summaries", "saliency maps", "reinforcement learning"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI"]}}, {"title": "Quantifying explainers of graph neural networks in computational pathology", "author": "G Jaume"}, {"title": "Artificial cognition: How experimental psychology can help generate explainable artificial intelligence", "author": "JET Taylor"}, {"title": "Mental models of mere mortals with explanations of reinforcement learning", "author": "A Anderson"}, {"title": "Trepan reloaded: A knowledge-driven approach to explaining artificial neural networks", "author": "R Confalonieri"}, {"title": "Autods: Towards human-centered automation of data science", "author": "D Wang"}, {"title": "Principles of explanation in human-AI systems", "author": "ST Mueller"}, {"title": "Interpretable confidence measures for decision support systems", "author": "J van der Waa"}, {"title": "A situation awareness-based framework for design and evaluation of explainable AI", "author": "L Sanneman"}, {"title": "Explanations in autonomous driving: A survey", "author": "D Omeiza"}, {"title": "Causality learning: a new perspective for interpretable machine learning", "author": "G Xu", "abstract": "Recent years have witnessed the rapid growth of machine learning in a wide range of fields such as image recognition, text classification, credit scoring prediction, recommendation system, etc. In spite of their great performance in different sectors, researchers still concern about the mechanism under any machine learning (ML) techniques that are inherently black-box and becoming more complex to achieve higher accuracy. Therefore, interpreting machine learning model is currently a mainstream topic in the research community", "keywords": {"keywords": ["causality learning", "interpretable machine learning", "machine learning", "black-box model", "complexity"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["interpretability in machine learning"]}}, {"title": "Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations", "author": "S Sreedharan"}, {"title": "Better metrics for evaluating explainable artificial intelligence", "author": "A Rosenfeld"}, {"title": "Explaining anomalies detected by autoencoders using Shapley Additive Explanations", "author": "L Antwarg"}, {"title": "Accessible cultural heritage through explainable artificial intelligence", "author": "N D\u00edaz-Rodr\u00edguez"}, {"title": "Distal explanations for explainable reinforcement learning agents", "author": "P Madumal"}, {"title": "From contrastive to abductive explanations and back again", "author": "A Ignatiev"}, {"title": "Explainable ai for interpretable credit scoring", "author": "LM Demajo"}, {"title": "Measure utility, gain trust: practical advice for XAI researchers", "author": "B Davis"}, {"title": "Reviewing the need for explainable artificial intelligence (xAI)", "author": "J Gerlings"}, {"title": "Designing interactions with intention-aware gaze-enabled artificial agents", "author": "J Newn", "abstract": "As it becomes more common for humans to work alongside artificial agents on everyday tasks, it is increasingly important to design artificial agents that can understand and interact with their human counterparts naturally. We posit that an effective way to do this is to harness nonverbal cues used in human-human interaction. We, therefore, leverage knowledge from existing work on gaze-based intention recognition, where the awareness of gaze can provide insights into the future actions of an observed human subject. In this", "keywords": {"keywords": ["artificial agents", "gaze-enabled", "intention recognition", "human-computer interaction", "nonverbal cues"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["human-computer interaction"]}}, {"title": "Human-Centered Artificial Intelligence for Designing Accessible Cultural Heritage", "author": "G Pisoni"}, {"title": "Data-driven remaining useful life estimation for milling process: sensors, algorithms, datasets, and future directions", "author": "S Sayyad"}, {"title": "based school reform: The Clinton Administration's Agenda", "author": "MS SMITH"}, {"title": "How much is the black box? The value of explainability in machine learning models", "author": "J Wanner"}, {"title": "Explaining convolutional neural networks through attribution-based input sampling and block-wise feature aggregation", "author": "S Sattarzadeh"}, {"title": "Human-agent Explainability: An Experimental Case Study on the Filtering of Explanations.", "author": "Y Mualla"}, {"title": "Explainable artificial intelligence: a comprehensive review", "author": "D Minh"}, {"title": "Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition", "author": "M Nourani"}, {"title": "Psychological foundations of explainability and interpretability in artificial intelligence", "author": "DA Broniatowski"}, {"title": "An ontology-based approach to explaining artificial neural networks", "author": "R Confalonieri", "abstract": "Explainability in Artificial Intelligence has been revived as a topic of active research by the need of conveying safety and trust to users in the 'how'and 'why'of automated decision-making. Whilst a plethora of approaches have been developed for post-hoc explainability, only a few focus on how to use domain knowledge, and how this influences the understandability of an explanation from the users' perspective. In this paper we show how ontologies help the understandability of interpretable machine learning models, such as", "keywords": {"keywords": ["explainability", "artificial neural networks", "ontology", "interpretability", "understandability"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Explainability and interpretability: keys to deep medicine", "author": "A Shaban-Nejad"}, {"title": "After-action review for AI (AAR/AI)", "author": "J Dodge"}, {"title": "What makes a good explanation? Cognitive dimensions of explaining intelligent machines.", "author": "R Confalonieri"}, {"title": "Explainable deep learning for video recognition tasks: a framework & recommendations", "author": "L Hiley"}, {"title": "ELVIRA: An explainable agent for value and utility-driven multiuser privacy", "author": "F Mosca"}, {"title": "HistoMapr\u2122: An Explainable AI (xAI) Platform for Computational Pathology Solutions", "author": "AB Tosun"}, {"title": "On Relating'Why?'and'Why Not?'Explanations", "author": "A Ignatiev"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro"}, {"title": "Combining machine learning with visual analytics for explainable forecasting of energy demand in prosumer scenarios", "author": "AI Grimaldo"}, {"title": "Teach me to explain: A review of datasets for explainable natural language processing", "author": "S Wiegreffe", "abstract": "Explainable Natural Language Processing (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 65 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize", "keywords": {"keywords": ["Explainable Natural Language Processing", "datasets", "textual explanations", "data augmentation", "model-generated explanations"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["Application"], "application area": ["Natural Language Processing"]}}, {"title": "Applying AI in practice: key challenges and lessons learned", "author": "L Fischer"}, {"title": "Ai system engineering\u2014key challenges and lessons learned", "author": "L Fischer"}, {"title": "CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models", "author": "AR Akula"}, {"title": "Some insights Towards a Unified Semantic Representation of Explanation for eXplainable Artificial Intelligence (XAI)", "author": "I Baaj"}, {"title": "Recent advances in trustworthy explainable artificial intelligence: Status, challenges and perspectives", "author": "A Rawal"}, {"title": "Explainable AI for domain experts: a post Hoc analysis of deep learning for defect classification of TFT\u2013LCD panels", "author": "M Lee"}, {"title": "\" i don't think so\": Disagreement-based policy summaries for comparing agents", "author": "Y Amitai"}, {"title": "Distal explanations for model-free explainable reinforcement learning", "author": "P Madumal"}, {"title": "Introducing peripheral awareness as a neurological state for human-computer integration", "author": "J Andres"}, {"title": "Human in the Loop: Industry 4.0 technologies and scenarios for worker mediation of automated manufacturing", "author": "CJ Turner", "abstract": "Industry 4.0 derived technologies have the potential to enable a new wave of digital manufacturing solutions for semi and fully automated production. In addition, this paradigm encompasses the use of communication technologies to transmit data to processing stations as well as the utilization of cloud based computational resources for data mining. Despite the rise in automation, future manufacturing systems will initially still require humans in the loop to provide supervisory level mediation for even the most autonomous production", "keywords": {"keywords": ["Industry 4.0", "automated manufacturing", "worker mediation", "digital manufacturing solutions", "human in the loop"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer science", "content": "Application", "application area": "Manufacturing"}}, {"title": "Trust considerations for explainable robots: A human factors perspective", "author": "L Sanneman"}, {"title": "Towards trustworthiness in the context of explainable search", "author": "S Polley"}, {"title": "I Don't Get IT, but IT seems Valid! The Connection between Explainability and Comprehensibility in (X) AI Research.", "author": "LV Herm"}, {"title": "Data representing ground-truth explanations to evaluate xai methods", "author": "SS Amiri"}, {"title": "Towards explainability for a civilian UAV fleet management using an agent-based approach", "author": "Y Mualla"}, {"title": "Investigating the impact of explanation on repairing trust in AI diagnostic systems for re-diagnosis", "author": "L Alam"}, {"title": "From shallow to deep interactions between knowledge representation, reasoning and machine learning", "author": "KR Amel"}, {"title": "Quantitative evaluation of machine learning explanations: A human-grounded benchmark", "author": "S Mohseni"}, {"title": "Enhanced well-being assessment as basis for the practical implementation of ethical and rights-based normative principles for AI", "author": "M Havrda"}, {"title": "Trusting the X in XAI: Effects of different types of explanations by a self-driving car on trust, explanation satisfaction and mental models", "author": "JM Schraagen", "abstract": "There is an increasing demand for opaque intelligent systems to explain themselves to humans, in order to increase user trust and the formation of adequate mental models. Previous research has shown effects of different types of explanations on user preferences and performance. However, this research has not addressed the differential effects of intentional and causal explanations on both users' trust and mental models, nor has it employed multiple trust measurement scales at multiple points in time. In the current", "keywords": {"keywords": ["XAI", "explanations", "trust", "explanation satisfaction", "mental models"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Autonomous vehicles"]}}, {"title": "Explaining citation recommendations: Abstracts or full texts", "author": "RO Weber"}, {"title": "Finding AI's faults with AAR/AI: An empirical study", "author": "R Khanna"}, {"title": "Sequential explanations with mental model-based policies", "author": "A Yeung"}, {"title": "Governmental Transparency in the Era of Artificial Intelligence.", "author": "TM van Engers"}, {"title": "The national academies board on human system integration (BOHSI) panel: Explainable AI, system transparency, and human machine teaming", "author": "T Warden"}, {"title": "From shallow to deep interactions between knowledge representation, reasoning and machine learning (Kay R. Amel group)", "author": "Z Bouraoui"}, {"title": "The impact of explanations on layperson trust in Artificial Intelligence\u2013driven symptom checker apps: Experimental study", "author": "C Woodcock"}, {"title": "Unifying Principles and Metrics for Safe and Assistive AI", "author": "S Srivastava"}, {"title": "Invertible concept-based explanations for cnn models with non-negative concept activation vectors", "author": "R Zhang"}, {"title": "Why not explain? effects of explanations on human perceptions of autonomous driving", "author": "D Omeiza", "abstract": "Autonomous vehicles (AVs) have the potential to change the way we commute, travel, and transport our goods. The deployment of AVs in society, however, requires that people understand, accept, and trust them. Intelligible explanations can help different AV stakeholders to assess AVs' behaviours, and in turn, increase their confidence and foster trust. In a user study (N= 101), we examined different explanation types (based on investigatory queries) provided by an AV and their effect on people using the trust", "keywords": {"keywords": ["autonomous driving", "explanations", "perceptions", "trust", "user study"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Autonomous Vehicles"]}}, {"title": "Explainable agency in intelligent agents: Doctoral consortium", "author": "P Madumal"}, {"title": "Quantitative evaluations on saliency methods: An experimental study", "author": "XH Li"}, {"title": "GANterfactual\u2014Counterfactual Explanations for Medical Non-experts Using Generative Adversarial Learning", "author": "S Mertes"}, {"title": "An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability", "author": "F Sovrano"}, {"title": "Gaining Justified Human Trust by Improving Explainability in Vision and Language Reasoning Models", "author": "AR Akula"}, {"title": "The Difference between Explainable and Explaining: Requirements and Challenges under the GDPR.", "author": "F Sovrano"}, {"title": "Determining System Requirements for Human-Machine Integration in Cyber Security Incident Response", "author": "MM Nyre-Yu"}, {"title": "A framework for step-wise explaining how to solve constraint satisfaction problems", "author": "B Bogaerts"}, {"title": "Towards Sonification in Multimodal and User-friendlyExplainable Artificial Intelligence", "author": "BW Schuller"}, {"title": "CaSE: Explaining text classifications by fusion of local surrogate explanation models with contextual and semantic knowledge", "author": "S Kiefer", "abstract": "Generating explanations within a local and model-agnostic explanation scenario for text classification is often accompanied by a local approximation task. In order to create a local neighborhood for a document, whose classification shall be explained, sampling techniques are used that most often treat the according features at least semantically independent from each other. Hence, contextual as well as semantic information is lost and therefore cannot be used to update a human's mental model within the according explanation task. In case of", "keywords": {"keywords": ["text classification", "explanation", "local surrogate explanation models", "contextual information", "semantic knowledge"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Natural Language Processing"]}}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta"}, {"title": "Towards accountability: providing intelligible explanations in autonomous driving", "author": "D Omeiza"}, {"title": "Explainable AI for B5G/6G: Technical Aspects, Use Cases, and Research Challenges", "author": "S Wang"}, {"title": "Contrastive explanations of plans through model restrictions", "author": "B Krarup"}, {"title": "Brain-inspired search engine assistant based on knowledge graph", "author": "X Zhao"}, {"title": "Big data e conoscenza", "author": "M Palmirani"}, {"title": "Improved explanatory efficacy on human affect and workload through interactive process in artificial intelligence", "author": "BH Kim"}, {"title": "Play MNIST for me! User studies on the effects of post-hoc, example-based explanations & error rates on debugging a deep learning, black-box classifier", "author": "C Ford"}, {"title": "Explainable AI, but explainable to whom?", "author": "J Gerlings"}, {"title": "Applying artificial intelligence in the science & technology cycle", "author": "R Weber", "abstract": "The science and technology (S&T) cycle starts with Congress identifying areas that demand scientific advancements. The cycle continues with funding agencies distributing funds to researchers to investigate and advance these areas. These novel advancements are then reported in scholarly publications. Scientific advancements are ultimately expected to be converted into wealth, promoting economic activity, and benefiting society. It is not, however, guaranteed that the desired advances targeted by Congress are met. Scholarly publications", "keywords": {"keywords": ["artificial intelligence", "science and technology", "cycle", "funding agencies", "scholarly publications"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["interdisciplinary"], "content": ["application"], "application area": ["science and technology advancements"]}}, {"title": "Explainable AI methods-a brief overview", "author": "A Holzinger"}, {"title": "Machine Learning Explanations to Prevent Overtrust in Fake News Detection.", "author": "S Mohseni"}, {"title": "Explaining the behavior of remote robots to humans: an agent-based approach", "author": "Y Mualla"}, {"title": "Sage advice? The impacts of explanations for machine learning models on human decision-making in spam detection", "author": "MC Stites"}, {"title": "Evaluating explainability methods intended for multiple stakeholders", "author": "K Martin"}, {"title": "Micro-entries: Encouraging deeper evaluation of mental models over time for interactive data systems", "author": "JE Block"}, {"title": "An explainable assistant for multiuser privacy", "author": "F Mosca"}, {"title": "\u201cAn Error Occurred!\u201d-Trust Repair With Virtual Robot Using Levels of Mistake Explanation", "author": "K Hald"}, {"title": "Explainable AI and Adoption of Financial Algorithmic Advisors: An Experimental Study", "author": "D Ben David"}, {"title": "To trust or not to trust a regressor: Estimating and explaining trustworthiness of regression predictions", "author": "K de Bie", "abstract": "In hybrid human-AI systems, users need to decide whether or not to trust an algorithmic prediction while the true error in the prediction is unknown. To accommodate such settings, we introduce RETRO-VIZ, a method for (i) estimating and (ii) explaining trustworthiness of regression predictions. It consists of RETRO, a quantitative estimate of the trustworthiness of a prediction, and VIZ, a visual explanation that helps users identify the reasons for the (lack of) trustworthiness of a prediction. We find that RETRO-scores negatively correlate with", "keywords": {"keywords": ["trustworthiness", "regression predictions", "hybrid human-AI systems", "estimation, explaining", "RETRO-VIZ"], "research type": ["Empirical"], "methodology": ["Quantitative", "Qualitative"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Trustworthiness"]}}, {"title": "Explainable AI for fighting COVID-19 pandemic: opportunities, challenges, and future prospects", "author": "KM Abiodun"}, {"title": "The quest of parsimonious XAI: A human-agent architecture for explanation formulation", "author": "Y Mualla"}, {"title": "From classification to definition: The changing nature of human adjudication", "author": "R Singh"}, {"title": "Qualitative investigation in explainable artificial intelligence: A bit more insight from social science", "author": "AJ Johs"}, {"title": "Explaining Deep Learning using examples: Optimal feature weighting methods for twin systems using post-hoc, explanation-by-example in XAI", "author": "EM Kenny"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "Multifactorial Evolutionary Prediction of Phenology and Pests: Can Machine Learning Help?", "author": "FJ Lacueva-P\u00e9rez"}, {"title": "How to Answer Why--Evaluating the Explanations of AI Through Mental Model Analysis", "author": "T Schrills"}, {"title": "Trepan Reloaded: A Knowledge-driven Approach to Explaining Artificial Neural Networks", "author": "R Confalonieri"}, {"title": "Stop ordering machine learning algorithms by their explainability! An empirical investigation of the tradeoff between performance and explainability", "author": "J Wanner", "abstract": "Numerous machine learning algorithms have been developed and applied in the field. Their application indicates that there seems to be a tradeoff between their model performance and explainability. That is, machine learning models with higher performance are often based on more complex algorithms and therefore lack interpretability or explainability and vice versa. The true extent of this tradeoff remains unclear while some theoretical assumptions exist. With our research, we aim to explore this gap empirically with a user study. Using four", "keywords": {"keywords": ["machine learning algorithms", "explainability", "performance", "tradeoff", "user study"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Machine Learning"]}}, {"title": "Towards Knowledge-driven Distillation and Explanation of Black-box Models", "author": "R Confalonieri"}, {"title": "Detecting and explaining potential financial fraud cases in invoice data with Machine Learning", "author": "LH Hamelers"}, {"title": "On Evaluating Correctness of Explainable AI Algorithms: an Empirical Study on Local Explanations for Classification", "author": "MO Yalcin"}, {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "author": "L Sixt"}, {"title": "Critical empirical study on black-box explanations in AI", "author": "JM John-Mathews"}, {"title": "Color for characters-effects of visual explanations of AI on trust and observability", "author": "T Schrills"}, {"title": "Assessing collaborative explanations of ai using explanation goodness criteria", "author": "TI Mamun"}, {"title": "A social evaluation of the perceived goodness of explainability in machine learning", "author": "J Wanner"}, {"title": "DIVINE: Diverse Influential Training Points for Data Visualization and Model Refinement", "author": "U Bhatt"}, {"title": "A negotiation support system for defining utility functions for multi-stakeholder self-adaptive systems", "author": "R Wohlrab", "abstract": "For realistic self-adaptive systems, multiple quality attributes need to be considered and traded off against each other. These quality attributes are commonly encoded in a utility function, for instance, a weighted sum of relevant objectives. Utility functions are typically subject to a set of constraints, ie, hard requirements that should not be violated by the system. The research agenda for requirements engineering for self-adaptive systems has raised the need for decision-making techniques that consider the trade-offs and priorities of", "keywords": [["adaptive", 0.3183], ["requirements", 0.2445], ["attributes", 0.2127], ["objectives", 0.2075], ["utility", 0.2054], ["constraints", 0.1996], ["self", 0.1933], ["quality", 0.1811], ["priorities", 0.1688], ["instance", 0.1554]]}, {"title": "Iterative planning with plan-space explanations: A tool and user study", "author": "R Eifler"}, {"title": "On the Influence of Explainable AI on Automation Bias", "author": "M Schemmer"}, {"title": "Recommender systems: an explainable AI perspective", "author": "A Vultureanu-Albi\u015fi"}, {"title": "Resisting out-of-distribution data problem in perturbation of xai", "author": "L Qiu"}, {"title": "Explaining Reward Functions to Humans for Better Human-Robot Collaboration", "author": "L Sanneman"}, {"title": "Towards harnessing natural language generation to explain black-box models", "author": "E Mariotti"}, {"title": "A multistakeholder approach towards evaluating ai transparency mechanisms", "author": "A Lucic"}, {"title": "The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending", "author": "M Dikmen"}, {"title": "Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "A comparative approach to explainable artificial intelligence methods in application to high-dimensional electronic health records: Examining the usability of xai", "author": "JA Duell", "abstract": "Explainable Artificial Intelligence (XAI) is a rising field in AI. It aims to produce a demonstrative factor of trust, which for human subjects is achieved through communicative means, which Machine Learning (ML) algorithms cannot solely produce, illustrating the necessity of an extra layer producing support to the model output. When approaching the medical field, we can see challenges arise when dealing with the involvement of human-subjects, the ideology behind trusting a machine to tend towards the livelihood of a human", "keywords": [["ai", 0.4644], ["trust", 0.3979], ["explainable", 0.3796], ["trusting", 0.3678], ["intelligence", 0.3419], ["ml", 0.3222], ["medical", 0.3207], ["human", 0.3129], ["machine", 0.2969], ["xai", 0.2862]]}, {"title": "K\u00fcnstliche Intelligenz\u2013eine Einf\u00fchrung", "author": "J Reinhart"}, {"title": "Applied Affective Computing", "author": "L Tian"}, {"title": "Explainability in Multi-Agent Path/Motion Planning: User-study-driven Taxonomy and Requirements", "author": "M Brandao"}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trust", "author": "J Ooge"}, {"title": "Culture Organism or Techno-Feudalism: How Growing Addictions and Artificial Intelligence Shape Contemporary Society", "author": "L Boji\u0107"}, {"title": "Explaining Dnn Based Facial Expression Classifications", "author": "K Burg"}, {"title": "Explainable, interactive c ontent\u2010based image retrieval", "author": "B Vasu"}, {"title": "Enhancing human robot interaction through explanation", "author": "E Bagheri"}, {"title": "A multidisciplinary definition of privacy labels: the story of princess privacy and the seven helpers", "author": "J Johansen"}, {"title": "AI: To interpret or to explain?", "author": "J Zhong", "abstract": ": interpret and explain. In this paper, we seek to disambiguate the use of interpret and explain  in the context of AI  methods for interpretability and explainability in AI systems respectively.", "keywords": {"keywords": ["interpret", "explain", "AI systems", "methods", "interpretability"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Artificial Intelligence"}}, {"title": "Who wants to use an augmented reality shopping assistant application?", "author": "D Mora"}, {"title": "Towards explainable decision-making strategies of deep convolutional neural networks: An exploration into explainable ai and potential applications within cancer \u2026", "author": "T Hammarstr\u00f6m"}, {"title": "Explainable Agency in Reinforcement Learning Agents", "author": "P Madumal"}, {"title": "Towards Explainable Evaluation Metrics for Natural Language Generation", "author": "C Leiter"}, {"title": "Human-Centered AI", "author": "H Barmer"}, {"title": "A Survey on Explainable Artificial Intelligence Techniques and Challenges", "author": "A Hanif"}, {"title": "The automated Laplacean demon: How ML challenges our views on prediction and explanation", "author": "S Sre\u0107kovi\u0107"}, {"title": "Memento hominibus: on the fundamental role of end users in real-world interactions with neuromorphic systems", "author": "S Thill"}, {"title": "ModelSpeX: Model specification using explainable artificial intelligence methods", "author": "U Schlegel"}, {"title": "ExplainEx: an explainable artificial intelligence framework for interpreting predictive models", "author": "NE Udenwagu", "abstract": "Artificial Intelligence (AI) systems are increasingly dependent on machine learning models which lack interpretability and algorithmic transparency, and hence may not be trusted by its users. The fear of failure in these systems is driving many governments to demand more explanation and accountability. Take, for example, the \u201cRight of Explanation\u201d rule proposed in the European Union in 2019, which gives citizens the right to demand an explanation from AI-based predictions. Explainable Artificial Intelligence (XAI) is an attempt to open up the", "keywords": {"keywords": ["explainable artificial intelligence", "interpreting predictive models", "machine learning", "algorithmic transparency", "right of explanation"], "research type": "theoretical", "methodology": "mixed-methods", "purposes": "explanatory", "discipline": "computer science", "content": "methodological", "application area": "AI interpretability"}}, {"title": "Using psychological characteristics of situations for social situation comprehension in support agents", "author": "I Kola"}, {"title": "Explainable NLP for Human-AI Collaboration", "author": "P Lertvittayakumjorn"}, {"title": "Specifying and Interpreting Reinforcement Learning Policies through Simulatable Machine Learning", "author": "P Tambwekar"}, {"title": "A Meta Survey of Quality Evaluation Criteria in Explanation Methods", "author": "H L\u00f6fstr\u00f6m"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky"}, {"title": "How knowledge-driven class generalization affects classical machine learning algorithms for mono-label supervised classification", "author": "H Turki"}, {"title": "The role of transparency and explainability in automated systems", "author": "J Maarten Schraagen"}, {"title": "Discern: Discovering counterfactual explanations using relevance features from neighbourhoods", "author": "N Wiratunga"}, {"title": "An Explanation Framework for Interpretable Credit Scoring", "author": "LM Demajo"}, {"title": "Metrics, Explainability and the European AI Act Proposal", "author": "F Sovrano", "abstract": "On 21 April 2021, the European Commission proposed the first legal framework on Artificial Intelligence (AI) to address the risks posed by this emerging method of computation. The Commission proposed a Regulation known as the AI Act. The proposed AI Act considers not only machine learning, but expert systems and statistical models long in place. Under the proposed AI Act, new obligations are set to ensure transparency, lawfulness, and fairness. Their goal is to establish mechanisms to ensure quality at launch and throughout the whole", "keywords": {"keywords": ["Artificial Intelligence", "European AI Act Proposal", "Metrics", "Explainability", "Regulation"], "research type": ["Review"], "methodology": ["Not Applicable"], "purposes": ["Descriptive"], "discipline": ["Law and Policy"], "content": ["Application"], "application area": ["AI Governance"]}}, {"title": "SVEA: A Small-scale Benchmark for Validating the Usability of Post-hoc Explainable AI Solutions in Image and Signal Recognition", "author": "S Sattarzadeh"}, {"title": "Let's Go to the Alien Zoo: Introducing an Experimental Framework to Study Usability of Counterfactual Explanations for Machine Learning", "author": "U Kuhl"}, {"title": "Towards Explainable Recommendations of Resource Allocation Mechanisms in On-Demand Transport Fleets", "author": "A Daoud"}, {"title": "On Deep Research Problems in Deep Learning", "author": "A Ulhaq"}, {"title": "Extending System Performance Past the Boundaries of Technical Maturity: Human-Agent Teamwork Perspective for Industrial Inspection", "author": "G Cabour"}, {"title": "\u201cI Don't Think So\u201d: Summarizing Policy Disagreements for Agent Comparison", "author": "Y Amitai"}, {"title": "A survey on effects of adding explanations to recommender systems", "author": "A Vultureanu\u2010Albi\u015fi"}, {"title": "Toward XAI & Human Synergies to Explain the History of Art: The Smart Photobooth Project", "author": "E Peijl"}, {"title": "Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems", "author": "M Nourani"}, {"title": "Machine Learning Verification and Safety for Unmanned Aircraft-A Literature Study", "author": "C Torens", "abstract": "View Video Presentation: https://doi. org/10.2514/6.2022-1133. vid Machine learning (ML) has proven to be the tool of choice for achieving human-like or even super-human performance with automation on specific tasks. As a result, this data-driven approach is currently experiencing massive interest in all industry domains. This increased use also applies for the safety critical aviation domain. With no human pilot on board, the potential use cases of ML for unmanned aircraft are particularly promising. Even upcoming Urban Air", "keywords": {"keywords": ["machine learning", "verification", "safety", "unmanned aircraft", "literature study"], "research type": "review", "methodology": "qualitative", "purposes": "descriptive", "discipline": "computer science", "content": "application", "application area": "aviation"}}, {"title": "Brittle AI, Causal Confusion, and Bad Mental Models: Challenges and Successes in the XAI Program", "author": "J Druce"}, {"title": "SHION: Towards An Interactive Digital Twin Supporting Shopfloor Operations on Real Time", "author": "FJ Lacueva-Perez"}, {"title": "Descriptive and Prescriptive Visual Guidance to Improve Shared Situational Awareness in Human-Robot Teaming", "author": "A Tabrez"}, {"title": "Systemic Oversimplification Limits the Potential for Human-AI Partnership", "author": "JS Metcalfe"}, {"title": "Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability", "author": "LV Herm"}, {"title": "Exploratory Team Cognition and Resilience in Human Agent Teaming", "author": "G Lematta"}, {"title": "Information theory and player archetype choice in Hearthstone", "author": "M Zuparic"}, {"title": "A hybrid machine learning process framework for data-driven direct product marketing", "author": "RM Petersen"}, {"title": "Increasing the Trustworthiness ofAI-based In-Vehicle IDS usingeXplainable AI", "author": "H Lundberg"}, {"title": "Explaining Any ML Model?--On Goals and Capabilities of XAI", "author": "M Renftle", "abstract": "An increasing ubiquity of machine learning (ML) motivates research on algorithms to explain ML models and their predictions--so-called eXplainable Artificial Intelligence (XAI). Despite many survey papers and discussions, the goals and capabilities of XAI algorithms are far from being well understood. We argue that this is because of a problematic reasoning scheme in XAI literature: XAI algorithms are said to complement ML models with desired properties, such as\" interpretability\", or\" explainability\". These properties are in turn assumed", "keywords": {"keywords": ["eXplainable Artificial Intelligence", "machine learning", "ML models", "interpretability", "explainability"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["XAI"]}}, {"title": "The Use of Responsible Artificial Intelligence Techniques in the Context of Loan Approval Processes", "author": "E Purificato"}, {"title": "Human-AI Interaction Paradigm for Evaluating Explainable Artificial Intelligence", "author": "M Franklin"}, {"title": "Features of Explainability: How users understand counterfactual and causal explanations for categorical and continuous features in XAI", "author": "G Warren"}, {"title": "Towards Semantic Description of Explainable Machine Learning Workflows", "author": "PI Nakagawa"}, {"title": "Know Your Model (KYM): Increasing Trust in AI and Machine Learning", "author": "M Roszel"}, {"title": "platform for computational pathology", "author": "JL Fine"}, {"title": "Examining Physicians' Explanatory Reasoning in Re-Diagnosis Scenarios for Improving AI Diagnostic Systems", "author": "L Alam"}, {"title": "eMorpheus: The unconscious human labour of producing commercial data in educational settings", "author": "JA Arantes"}, {"title": "Towards Human-Centered Natural Language Processing", "author": "AV Gonzalez"}, {"title": "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Challenges and Solutions", "author": "E Petersen", "abstract": "Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning systems must be developed responsibly. Many high-level declarations of ethical principles have been put forth for this purpose, but there is a severe lack of technical guidelines explicating the practical consequences for medical machine learning. Similarly, there is currently", "keywords": {"keywords": ["machine learning", "medicine", "challenges", "solutions", "responsible"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["application"], "application area": ["medical care"]}}, {"title": "An Interactive Graphical Visualization Approach to CNNs and RNNs", "author": "A Vyas"}, {"title": "The Situation Awareness Framework for Explainable AI (SAFE-AI) and Human Factors Considerations for XAI Systems", "author": "L Sanneman"}, {"title": "Do Users Understand AI as Good as They Think? Experiencing Information Processing Awareness in Intelligent Medical Systems", "author": "T Schrills"}, {"title": "Explaining Automated Decisions in Practice: Insights from the Swedish Credit Scoring Industry", "author": "F Matz"}, {"title": "The Myth of Diagnosis as Classification: Examining the Effect of Explanation on Patient Satisfaction and Trust in AI Diagnostic Systems", "author": "L Alam"}, {"title": "for Evaluating XAI with Non-Experts. In AP Rocha, L. Steels, & J. van den Herik (Eds.), Proceedings of the 14th International Conference on Agents and Artificial \u2026", "author": "K McAreavey"}, {"title": "Enhancing the Transparency of Personal Data Access through Semantic Web Technologies", "author": "RRG Hamed"}, {"title": "How to Guide Task-oriented Chatbot Users, and When: A Mixed-methods Study of Combinations of Chatbot Guidance Types and Timings", "author": "SF Yeh"}, {"title": "Explainable Model Fusion for Customer Journey Mapping", "author": "K Okazaki"}, {"title": "Improving Model Understanding and Trust with Counterfactual Explanations of Model Confidence", "author": "T Le", "abstract": "In this paper, we show that counterfactual explanations of confidence scores help users better understand and better trust an AI model's prediction in human-subject studies. Showing confidence scores in human-agent interaction systems can help build trust between humans and AI systems. However, most existing research only used the confidence score as a form of communication, and we still lack ways to explain why the algorithm is confident. This paper also presents two methods for understanding model", "keywords": {"keywords": ["counterfactual explanations", "model understanding", "model trust", "confidence scores", "human-agent interaction"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "An Empirical Study of Reward Explanations with Human-Robot Interaction Applications", "author": "L Sanneman"}, {"title": "A Cognitive Work Analysis Approach to Explainable Artificial Intelligence in Non-Expert Financial Decision-Making", "author": "M Dikmen"}, {"title": "Are contrastive explanations useful?", "author": "J Forrest"}, {"title": "Developing and Experimenting on Approaches to Explainability in AI Systems.", "author": "Y Zhang"}, {"title": "PIP: Pictorial Interpretable Prototype Learning for Time Series Classification", "author": "A Ghods"}, {"title": "Design and validation of a comparison methodology for Explainable AI techniques", "author": "F VANNI"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.0", "author": "JP Wanner"}, {"title": "Risk and Exposure of XAI in Persuasion and Argumentation: The case of Manipulation", "author": "D Calvaresi"}, {"title": "Context sensitive or rule compliant: the effect of a (semi-) autonomous system's behaviour on trust", "author": "C Peksen"}, {"title": "SHION (Smart tHermoplastic InjectiON): An Interactive Digital Twin Supporting Real-Time Shopfloor Operations", "author": "FJ Lacueva-Perez", "abstract": "Injection molding is widely used to produce plastic components with large lot size. However, quality failures occur during molding cycles. These can be minimized through real-time process monitoring. This article reports on a cloud-based digital twin (DT) that is supported by A-based control of process parameters and can be used to help companies detect product failures in real time. Process parameters and their interrelationship with quality failure were studied and used to generate models for real-time prediction of part quality. Two", "keywords": {"keywords": ["Injection molding", "Digital twin", "Real-time process monitoring", "Quality failures", "Process parameters"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Engineering", "content": "Application", "application area": "Manufacturing"}}, {"title": "I don't know, is AI also used in airbags?", "author": "F Alizadeh"}, {"title": "BRIEFING NOTES", "author": "M Taheri"}, {"title": "Why Not Explain? Effects of Explanations on Human Perceptions of Autonomous Driving", "author": "K Kollnig"}, {"title": "From Blackbox to Explainable AI in Healthcare: Existing Tools and Case Studies", "author": "PN Srinivasu"}, {"title": "Contextualising local explanations for non-expert users: an XAI pricing interface for insurance.", "author": "C Bove"}, {"title": "A Novel Machine Learning Scheme for mmWave Path Loss Modeling for 5G Communications in Dense Urban Scenarios", "author": "W Jin"}, {"title": "The role of shared mental models in human-AI teams: a theoretical review", "author": "RW Andrews"}, {"title": "Trustworthy Predictive Algorithms for Complex Forest System Decision-Making", "author": "P Rana"}, {"title": "Enhancing brick-and-mortar store shopping experience with an augmented reality shopping assistant application using personalized recommendations and \u2026", "author": "R Zimmermann"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge", "abstract": "Assessing AI systems is difficult. Humans rely on AI systems in increasing ways, both visible and invisible, meaning a variety of stakeholders need a variety of assessment tools (eg, a professional auditor, a developer, and an end user all have different needs). We posit that it is possible to provide explanations and assessment processes that enable AI non-experts observing multiple intelligent agents in sequential domains to differentiate the agents with respect to a property (eg, quality or fairness), as well as articulate justification for their", "keywords": {"keywords": ["AI assessment", "manipulable properties", "explanations", "processes", "assessment tools"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI systems"]}}, {"title": "Self-labeling Grey-box Model: An Interpretable Semi-supervised Classifier", "author": "ICG Garc\u00eda"}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk"}, {"title": "Impact of artificial intelligence on pathologists' decisions: an experiment", "author": "J Meyer"}, {"title": "\" It's our fault!\": insights into users' understanding and interaction with an explanatory collaborative dialog system", "author": "K Weitz"}, {"title": "Transparent-AI Blueprint: Developing a Conceptual Tool to Support the Design of Transparent AI Agents", "author": "Z Zhou"}, {"title": "Quantifying the Demand for Explainability", "author": "T Weber"}, {"title": "Exploratory Study on Accuracy of Students' Mental Models of a Singly Linked List", "author": "E Almadhoun"}, {"title": "Explaining Decision-Tree Predictions by Addressing Potential Conflicts between Predictions and Plausible Expectations", "author": "S Maruf"}, {"title": "Hybrid intelligence decision making: successful human-AI integration in optical diagnosis", "author": "C Reverberi"}, {"title": "Trust in medical artificial intelligence: a discretionary account", "author": "PJ Nickel", "abstract": "This paper sets out an account of trust in AI as a relationship between clinicians, AI applications, and AI practitioners in which AI is given discretionary authority over medical questions by clinicians. Compared to other accounts in recent literature, this account more adequately explains the normative commitments created by practitioners when inviting clinicians' trust in AI. To avoid committing to an account of trust in AI applications themselves, I sketch a reductive view on which discretionary authority is exercised by AI practitioners", "keywords": {"keywords": ["trust", "medical artificial intelligence", "discretionary authority", "clinicians", "AI practitioners"], "research type": ["theoretical"], "methodology": [], "purposes": ["exploratory"], "discipline": [], "content": ["application"], "application area": ["healthcare"]}}, {"title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems", "author": "C Panigutti"}, {"title": "Model Agnostic Explainability Techniques in Ultrasound Image Analysis", "author": "N Prentzas"}, {"title": "Explainable Retinal Screening with Self-Management Support to Improve Eye-Health of Diabetic Population via Telemedicine", "author": "JF Tumpa"}, {"title": "Communicating Performance of Regression Models Using Visualization in Pharmacovigilance", "author": "A Suh"}, {"title": "Extendable and explainable deep learning for pan-cancer radiogenomics research", "author": "Q Liu"}, {"title": "Interpretability of Deep Learning for Medical Image Classification: Improved Understandability and Generalization", "author": "M Graziani"}, {"title": "Visualization Guidelines for Model Performance Communication Between Data Scientists and Subject Matter Experts", "author": "A Suh"}, {"title": "Toward Policy Explanations for Multi-Agent Reinforcement Learning", "author": "K Boggess"}, {"title": "Explanation Strategies for Trustworthy AI Diagnostic Systems: Examin-ing Physicians' Explanatory Reasoning in Re-diagnosis Scenarios", "author": "L Alam"}, {"title": "Methods and Measures to Evaluate Technologies that Influence Aviator Decision Making and Situation Awareness", "author": "E Roth", "abstract": "This report details the results of the Aviation Decision Making and Situation Awareness study. The objective of this study was to recommend measures and methods to evaluate future technologies that influence pilot decision making and situation awareness SA in the context of Future Vertical Lift FVL. With regard to evaluation design, we offer two high-level recommendations. First, we recommend the use of scenario-based methods to test and evaluate technologies, with an emphasis on exploring a range of realistic scenarios", "keywords": {"keywords": ["aviation", "decision making", "situation awareness", "Future Vertical Lift", "scenario-based methods"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["prescriptive"], "discipline": ["aviation"], "content": ["methodological"], "application area": ["technology evaluation"]}}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert Users", "author": "C Bove"}, {"title": "Exploratory Study to Uncover Student Mental Models of Singly Linked Lists in the C Programming Language", "author": "E Almadhoun"}, {"title": "A Smart Home Testbed for Evaluating XAI with Non-experts.", "author": "K McAreavey"}, {"title": "SAGE Intrusion Detection System: Sensitivity Analysis Guided Explainability for Machine Learning.", "author": "M Smith"}, {"title": "Towards Involving End-users in Interactive Human-in-the-loop AI Fairness", "author": "Y Nakao"}, {"title": "Understanding Is a Process", "author": "LM Blaha"}, {"title": "Towards Autonomous PErceptual Systems", "author": "S Herbin"}, {"title": "Explaining Reinforcement Learning Policies through Counterfactual Trajectories", "author": "J Frost"}, {"title": "Explainable artificial intelligence and social science: Further insights for qualitative investigation", "author": "AJ Johs"}, {"title": "Calibrating Trust in AI-Assisted Decision Making", "author": "A Turner", "abstract": "With the proliferation of AI products, humans and AI are increasingly working in partnership with each other to make decisions. For this type of collaboration to be successful, humans need to understand AI capability in order to effectively calibrate their trust. In these partnerships, it's critical to explain decisions and predictions in a manner that can be understood by humans in order to encourage trust calibration. The field of explainable AI is focused on integrating explainability into AI, but is geared towards making AI models more", "keywords": {"keywords": ["trust calibration", "AI-assisted decision making", "explainable AI", "human-AI collaboration", "decision explanations"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI ethics"]}}, {"title": "One step further: evaluating interpreters using metamorphic testing", "author": "M Fan"}, {"title": "INTELLIGENT TUTORING SYSTEMS", "author": "R Mullins"}, {"title": "Increasing Trust and Fairness in Machine Learning Applications within the Mortgage Industry", "author": "W van Zetten"}, {"title": "Metrics for Robot Proficiency Self-assessment and Communication of Proficiency in Human-robot Teams", "author": "A Norton"}, {"title": "Essential Features in a Theory of Context for Enabling Artificial General Intelligence", "author": "M Kejriwal"}, {"title": "Algorithmic nudge to make better choices: Evaluating effectiveness of XAI frameworks to reveal biases in algorithmic decision making to users", "author": "P Juneja"}, {"title": "Exploring preferences on chatbot's guidance type and timing", "author": "SF Yeh"}, {"title": "How Can Automated Vehicles Explain Their Driving Decisions? Generating Clarifying Summaries Automatically", "author": "F Henze"}, {"title": "Explainable Classification and Annotation through Relation Learning and Reasoning", "author": "R Pierrard"}, {"title": "Esthetic Evaluation of Decision tree Visualization in XAI", "author": "CY Ahn", "abstract": "AI \uc758 \uacb0\uacfc\ub97c \uc774\ud574\ud558\uae30 \uc704\ud574\uc11c XAI (eXplainable Artificial Intelligence) \uc758 \uc5f0\uad6c\ub294 \ub9e4\uc6b0 \uc911\uc694\ud558\ub2e4. \uc138\uacc4\uc801\uc73c\ub85c XAI \uac1c\ubc1c \uc5f0\uad6c\ub294 \ub9ce\uc774 \uc9c4\ud589\ub418\uace0 \uc788\uc9c0\ub9cc \uac1c\ubc1c\ub41c XAI \ub97c \ud3c9\uac00\ud558\ub294 \uc5f0\uad6c\ub294 \ub9e4\uc6b0 \uc801\ub2e4. \ubcf8 \ub17c\ubb38\uc740 \uc0ac\uc6a9\uc131 \uce21\uba74\uc5d0\uc11c XAI \ub97c \ud3c9\uac00\ud558\uae30 \uc704\ud574 AI \uc0ac\uc6a9\uc131 \uc694\uc18c, \uacfc\ud559\uc801 \uc124\uba85\uc758 \uc694\uc18c, \ud734\ub9ac\uc2a4\ud2f1 \ud3c9\uac00 \uc694\uc18c\ub97c \ubd84\ub958\ud558\uace0 \uc758\uc0ac\uacb0\uc815 \ub098\ubb34\ub97c \uc2dc\uac01\ud654\uc5ec \uc2ec\ubbf8\ub3c4\ub97c \ud3c9\uac00\ud55c\ub2e4.", "keywords": {"keywords": ["XAI", "esthetic evaluation", "decision tree", "visualization", "AI"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["artificial intelligence"]}}, {"title": "Perceive, Understand & Predict-Empirical Indication for Facets in Subjective Information Processing Awareness", "author": "TPP Schrills"}, {"title": "The challenges of providing explanations of AI systems when they do not behave like users expect", "author": "M Riveiro"}, {"title": "Empirically Evaluating Meta Learning of Robot Explainability with Humans", "author": "A Silva"}, {"title": "Affective Response Categories\u2014Toward Personalized Reactions in Affect-Adaptive Tutoring Systems", "author": "A Schmitz-H\u00fcbsch"}, {"title": "Exploring Users' Preferences for Chatbot's Guidance Type and Timing", "author": "MH Wu"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Measuring the Usability and Quality of Explanations of a Machine Learning Web-Based Tool for Oral Tongue Cancer Prognostication", "author": "RO Alabi"}, {"title": "Evaluating human understanding in XAI systems", "author": "D Gentile"}, {"title": "Towards an Explanation Space to Align Humans and Explainable-AI Teamwork", "author": "G Cabour"}, {"title": "The ethical implications of using digital traces: studying explainability and trust during a pandemic", "author": "N Dwyer", "abstract": "Digital technologies give researchers new opportunities to access the most personal thoughts of those who use them. The ethics and implications of using data from peoples' everyday interactions have recently become a mainstream topic of concern (Lucivero, 2020). In some contexts, such as governance, it can be argued that algorithmically-generated decisions are valued over individuals' and communities' expertise (Danaher, 2016). As with other projects described in this book, our work is being carried out during the COVID-19", "keywords": {"keywords": ["digital traces", "explainability", "trust", "pandemic", "ethical implications"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["sociology"], "content": ["methodological"], "application area": ["data analysis"]}}, {"title": "Enriching Artificial Intelligence Explanations with Knowledge Fragments", "author": "J Ro\u017eanec"}, {"title": "A Taxonomy of User-centered Explainable AI Studies", "author": "LV Herm"}, {"title": "Usability, User Comprehension, and Perceptions of Explanations for Complex Decision Support Systems in Finance: A Robo-Advisory Use Case", "author": "S Deo"}, {"title": "Human-AI Partnerships in Generative Music", "author": "J Smith"}, {"title": "Transparent Interaction Based Learning for Human-Robot Collaboration", "author": "E Bagheri"}, {"title": "The Explainable Business Process (XBP)-An Exploratory Research", "author": "R Alhomsi"}, {"title": "Developing Metrics and Evaluation Methods for Assessing AI-Enabled Robots in Manufacturing", "author": "A Norton"}, {"title": "Investigating the Impact of Online Human Collaboration in Explanation of AI Systems", "author": "TI Mamun"}, {"title": "Evaluating Human-like Explanations for Robot Actions in Reinforcement Learning Scenarios", "author": "F Cruz"}, {"title": "Designing for Responsible Trust in AI Systems: A Communication Perspective", "author": "QV Liao", "abstract": "Current literature and public discourse on\" trust in AI\" are often focused on the principles underlying trustworthy AI, with insufficient attention paid to how people develop trust. Given that AI systems differ in their level of trustworthiness, two open questions come to the fore: how should AI trustworthiness be responsibly communicated to ensure appropriate and equitable trust judgments by different users, and how can we protect users from deceptive attempts to earn their trust? We draw from communication theories and literature on trust in", "keywords": {"keywords": ["trust", "AI systems", "communication", "trustworthiness", "deceptive attempts"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI systems"]}}, {"title": "The Effects of Artificial Intelligence on Crisis Communication Rebuild Strategies", "author": "RR Ehlers"}, {"title": "Towards an Accountability Framework for AI: Ethical and Legal Considerations", "author": "A Boch"}, {"title": "Evaluating Explainable Artificial Intelligence for X-ray Image Analysis", "author": "M Mir\u00f3-Nicolau"}, {"title": "Dynamic Voting based Explainable Intrusion Detection System for In-vehicle Network", "author": "NI Mowla"}, {"title": "Assessing and Finding Faults in AI: Two Empirical Studies", "author": "R Khanna"}, {"title": "SHION (Smart tHermoplastic InjectiON): An Interactive Digital Twin Supporting Real-Time Shopfloor Operations", "author": "S Hermawati"}, {"title": "Scalable Interpretability via Polynomials", "author": "A Dubey"}, {"title": "Opportunities for machine learning to accelerate halide-perovskite commercialization and scale-up", "author": "RE Kumar"}, {"title": "Explainability in multi-agent path/motion planning", "author": "M Brandao"}, {"title": "Taking down malicious webshops: designing Explainable AI against growing e-commerce fraud", "author": "N Ekhart", "abstract": "The amount of internet fraud in the Netherlands is increasing to the extent where the Landel\u0133k Meldpunt Internet Oplichting (LMIO) cannot keep up with the number of reports [2, 39, 42]. As a result, the process of evaluating webshops requires a complete redesign. Odekerken and Bex [40] designed a solution to the problem: the WEbsite Evaluation Tool (WEET). This tool utilizes Explainable AI (XAI) to extract bona fide and mala fide features of webshops and presents them to analysts of the LMIO, thereby removing manual searching", "keywords": [["ai", 0.379], ["webshops", 0.3362], ["fraud", 0.3053], ["website", 0.2864], ["internet", 0.2712], ["evaluation", 0.2621], ["xai", 0.2535], ["evaluating", 0.2514], ["reports", 0.2368], ["analysts", 0.2295]]}, {"title": "Cross-Loss Influence Functions to Explain Deep Network Representations", "author": "A Silva"}, {"title": "ACIPS: A Framework for Evaluating Patient Perception in the Introduction of AI-Enabled Healthcare", "author": "CT Okolo"}, {"title": "Interpretable Machine Learning Models for Predicting and Explaining Vehicle Fuel Consumption Anomalies", "author": "A Barbado"}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications", "author": "M Nourani"}, {"title": "Enriching Artificial Intelligence Explanations with Knowledge Fragments. Preprints 2022, 1, 0", "author": "JM Ro\u017eanec"}, {"title": "Investigating Explanations in Conditional and Highly Automated Driving: The Effects of Situation Awareness and Modality", "author": "L Avetisyan"}, {"title": "Semantic Description of Explainable Machine Learning Workflows", "author": "P Inoue Nakagawa"}, {"title": "Exploration of smart infrastructure for drivers of autonomous vehicles: design space, out-of-view visualization and explanations", "author": "G Wiegand"}, {"title": "A Human-Centered Systematic Literature Review of the Computational Approaches for Online Sexual Risk Detection", "author": "A Razi"}, {"title": "Interpretation of SVM to Build an Explainable AI via Granular Computing", "author": "SS Samuel", "abstract": "Machine learning (ML) is known to be one of the chief tools used for extracting information from data and predict output with exceptional accuracy. However, this accuracy comes with a lack of explainability. This especially becomes a serious problem when it comes to analyzing and making diagnosis with medical data. These ML models are usually built by expert coders without any incorporation or feedback of contextual information from subject matter experts (physicians in our case). This in turn leads the physicians to be", "keywords": {"keywords": ["Machine learning", "Explainable AI", "SVM", "Granular computing", "Medical data"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Medical diagnosis and analysis"]}}, {"title": "Designing a pragmatic explanation for the XAI system based on the user's context and background knowledge", "author": "S Kim"}, {"title": "Trust and AI in Clinical Decision Support", "author": "B Wilson"}, {"title": "Metrics for Robot Proficiency Self-Assessment and Communication of Proficiency in Human-Robot Teams", "author": "S MATTHIAS"}, {"title": "A Human-Agent Architecture for Explanation Formulation", "author": "Y Mualla"}, {"title": "Towards FAIR Explainable AI: a standardized ontology for mapping XAI solutions to use cases, explanations, and AI systems", "author": "A Adhikari"}, {"title": "Human Factors in Interactive Machine Learning: A Cybersecurity Case Study", "author": "MH Chignell"}, {"title": "Integrating Policy Summaries with Reward Decomposition Explanations", "author": "Y Septon"}, {"title": "A Computational Cognitive Model of Informative and Persuasive Explanations of Artificial Intelligence Systems", "author": "ST Mueller"}, {"title": "Metrics and Evaluations of Time Series Explanations: An Application in Affect Computing", "author": "N Fouladgar"}, {"title": "New-Generation AIs Reasoning about Norms and Values", "author": "R Markovich", "abstract": "The recent rapid evolution of artificial intelligence and its widespread application in a multitude of domains led to the emergence of new heterogeneous systems where humans cohabit with software agents. The logics needed in these new-generation systems with intelligent behavior (AIs) have to accommodate reasoning about norms and values: these considerations drive humans' everyday life and decisions, we expect artificial intelligence tools to operate in our society taking these considerations very much into account. Some of", "keywords": {"keywords": ["artificial intelligence", "norms", "values", "reasoning", "intelligent behavior"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["Application"], "application area": ["ethics"]}}, {"title": "Interactive Constrained Learning from Demonstration Using Visual Robot Behavior Counterfactuals", "author": "C Mueller"}, {"title": "XAI Approach to Improved and Informed Detection of Burnt Scar", "author": "T Maneerat"}, {"title": "Filling the gap between principle and practice: building an ethical and human rights-based tool-kit for AI development.", "author": "N Palladino"}, {"title": "Attention cannot be an Explanation", "author": "AR Akula"}, {"title": "Gaze-Based Intention Recognition for Human-Agent Collaboration", "author": "J Newn"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Explainable Artificial Intelligence (Xai) in Insurance: A Systematic Review", "author": "E Owens"}, {"title": "Explainable AI for Security of Human-Interactive Robots", "author": "A Roque"}, {"title": "Provenance as a Substrate for Human Sensemaking and Explanation of Machine Collaborators", "author": "SE Friedman"}, {"title": "Tutorial# 4 eXplainable AI", "author": "R Weber", "abstract": "In this tutorial, we will present an overview of model interpretability and explainability in AI  [4], key regulations/laws, and techniques/tools for providing explainability as part of AI/ML sys", "keywords": {"keywords": ["explainable AI", "model interpretability", "AI", "ML sys", "techniques/tools"], "research type": ["tutorial"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI/ML"]}}, {"title": "Human-Centered Explainable AI (HCXAI): beyond opening the black-box of AI", "author": "U Ehsan"}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare Systeme", "author": "M Balci"}, {"title": "Une terminologie pour une IA explicable contextualis\u00e9e", "author": "M Bellucci"}, {"title": "Exploration of smart infrastructure for drivers of autonomous vehicles", "author": "G Wiegand"}, {"title": "XAI \uc5d0\uc11c \uc758\uc0ac\uacb0\uc815 \ub098\ubb34 \uc2dc\uac01\ud654\uc758 \uc2ec\ubbf8\ub3c4 \ud3c9\uac00", "author": "\uc548\ucca0\uc6a9\uff0c \ubc15\uc9c0\uc218\uff0c \uc190\uc9c4\uace4"}, {"title": "D\u00e9veloppement d'une interface web pour l'\u00e9valuation du diagnostic assist\u00e9 par ordinateur (CADeval)", "author": "N Constantin"}], "getting a clue: a method for explaining uncertainty estimates": [{"title": "Interpretable machine learning: Fundamental principles and 10 grand challenges", "author": "C Rudin", "abstract": "Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are:(1)", "keywords": {"keywords": ["interpretable machine learning", "fundamental principles", "grand challenges", "high stakes decisions", "troubleshooting"], "research type": "review", "methodology": [], "purposes": [], "discipline": "computer science", "content": "methodological", "application area": "machine learning"}}, {"title": "A survey of uncertainty in deep neural networks", "author": "J Gawlikowski"}, {"title": "Sample-efficient optimization in the latent space of deep generative models via weighted retraining", "author": "A Tripp"}, {"title": "Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty", "author": "U Bhatt"}, {"title": "Machine learning explainability for external stakeholders", "author": "U Bhatt"}, {"title": "Explaining in style: Training a gan to explain a classifier in stylespace", "author": "O Lang"}, {"title": "Deep neural networks and tabular data: A survey", "author": "V Borisov"}, {"title": "Carla: a python library to benchmark algorithmic recourse and counterfactual explanation algorithms", "author": "M Pawelczyk"}, {"title": "Bayes-trex: a bayesian sampling approach to model transparency by example", "author": "S Booth"}, {"title": "To trust or not to trust a regressor: Estimating and explaining trustworthiness of regression predictions", "author": "K de Bie"}, {"title": "{\\delta}-CLUE: Diverse Sets of Explanations for Uncertainty Estimates", "author": "D Ley", "abstract": "To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating Counterfactual Latent Uncertainty Explanations (CLUEs). However, for a single input, such approaches could output a variety of explanations due to the lack of constraints placed on the explanation. Here we augment the original CLUE approach, to provide what we call $\\delta $-CLUE. CLUE indicates $\\it {one} $ way to change an input, while remaining on the data manifold, such that the model becomes more confident about its", "keywords": {"keywords": ["Uncertainty estimates", "Probabilistic models", "Counterfactual Latent Uncertainty Explanations", "Data manifold", "Explanations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Algorithmic Recourse in the Face of Noisy Human Responses", "author": "M Pawelczyk"}, {"title": "Effects of Uncertainty on the Quality of Feature Importance Explanations", "author": "T Shaikhina"}, {"title": "Shapley variable importance cloud for interpretable machine learning", "author": "Y Ning"}, {"title": "Diverse, Global and Amortised Counterfactual Explanations for Uncertainty Estimates", "author": "D Ley"}, {"title": "McXai: Local model-agnostic explanation as two games", "author": "Y Huang"}, {"title": "Titre du sujet de stage", "author": "A Leite"}, {"title": "Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers", "author": "S Tsuei"}, {"title": "Uncertainty Interpretation of the Machine Learning Survival Model Predictions", "author": "LV Utkin"}, {"title": "RELAX: Representation Learning Explainability", "author": "KK Wickstr\u00f8m"}, {"title": "Improving Model Understanding and Trust with Counterfactual Explanations of Model Confidence", "author": "T Le", "abstract": "In this paper, we show that counterfactual explanations of confidence scores help users better understand and better trust an AI model's prediction in human-subject studies. Showing confidence scores in human-agent interaction systems can help build trust between humans and AI systems. However, most existing research only used the confidence score as a form of communication, and we still lack ways to explain why the algorithm is confident. This paper also presents two methods for understanding model", "keywords": {"keywords": ["counterfactual explanations", "model understanding", "trust", "confidence scores", "human-subject studies"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "MCCE: Monte Carlo sampling of realistic counterfactual explanations", "author": "A Redelmeier"}, {"title": "Provable concept learning for interpretable predictions using variational inference", "author": "A Taeb"}, {"title": "Diverse and Amortised Counterfactual Explanations for Uncertainty Estimates", "author": "D Ley"}, {"title": "Explaining Prediction Uncertainty of Pre-trained Language Models by Detecting Uncertain Words in Inputs", "author": "H Chen"}, {"title": "Communication and Evaluation of Risk and Uncertainty in Deep Learning: A Case Study in in-Vitro-Fertilization", "author": "JS Lee"}, {"title": "Path Integrals for the Attribution of Model Uncertainties", "author": "IP Lopez"}, {"title": "Uncertainty Quantification of Surrogate Explanations: an Ordinal Consensus Approach", "author": "J Schulz"}, {"title": "Path Integrals for the Attribution of Model Uncertainties", "author": "I Perez"}, {"title": "M2 internship\u20135-6 months 2022\u2013Paris-Saclay Campus", "author": "A Leite"}, {"title": "Beyond Examples: Constructing Explanation Space for Explaining Prototypes", "author": "H Joo", "abstract": "As deep learning has been successfully deployed in diverse applications, there is ever increasing need for explaining its decision. Most of the existing methods produced explanations with a second model that explains the first black-box model, but we propose an inherently interpretable model for more faithful explanations. Our method constructs an explanation space in which similarities in terms of human-interpretable features at images share similar latent representations by using a variational autoencoder. This explanation", "keywords": {"keywords": ["deep learning", "explanation", "interpretable model", "variational autoencoder", "latent representations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image classification"]}}, {"title": "Attribution of Predictive Uncertainties in Classification Models", "author": "I Perez"}, {"title": "DS-UI: Dual-Supervised Mixture of Gaussian Mixture Models for Uncertainty Inference in Image Recognition", "author": "J Xie"}, {"title": "Gradient-based Counterfactual Explanations using Tractable Probabilistic Models", "author": "X Shao"}, {"title": "DISENTANGLING ALGORITHMIC RECOURSE", "author": "M Pawelczyk"}, {"title": "Adapting the Linearised Laplace Model Evidence for Modern Deep Learning", "author": "J Antor\u00e1n"}, {"title": "An Extension of the Neural Additive Model for Uncertainty Explanation of Machine Learning Survival Models", "author": "L Utkin"}], "do humans prefer debiased ai algorithms? a case study in career recommendation": [{"title": "The Use of Responsible Artificial Intelligence Techniques in the Context of Loan Approval Processes", "author": "E Purificato", "abstract": "Despite the existing skepticism about the use of automatic systems in contexts where human knowledge and experience are considered indispensable (eg, the granting of a mortgage, the prediction of stock prices, or the detection of cancers), our work aims to show how the use of explainability and fairness techniques can lead to the growth of a domain expert's trust and reliance on an artificial intelligence (AI) system. This article presents a system, applied to the context of loan approval processes, focusing on the two aforementioned", "keywords": {"keywords": ["Responsible Artificial Intelligence", "Loan Approval Processes", "Explainability", "Fairness", "Trust"], "research type": ["Case Study"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Finance"]}}, {"title": "Beyond-Accuracy Perspectives on Graph Neural Network-Based Models for Behavioural User Profiling", "author": "E Purificato", "abstract": "The presented doctoral research aims to develop a behavioural user profiling framework focusing simultaneously on three beyond-accuracy perspectives: privacy, to study how to intervene on graph data structures of specific contexts and provide methods to make the data available in a meaningful manner without neither exposing personal user information nor corrupting the profiles creation and system performances; fairness, to provide user representations that are free of any inherited discrimination which could affect a downstream", "keywords": {"keywords": ["graph neural network", "behavioural user profiling", "privacy", "fairness", "data structures"], "research type": "theoretical", "methodology": "mixed-methods", "purposes": "explanatory", "discipline": "computer science", "content": "methodological", "application area": "user profiling"}}], "perception of fairness in group music recommender systems": [{"title": "Fairness perceptions of artificial intelligence decision-making", "author": "L JIANG", "abstract": "Inequality is the biggest challenge for global social and economic development, which has the potential to impede the goal of global sustainable development. One way to reduce such inequality is to use artificial intelligence (AI) for decision-making. However, recent research has found that while AI is more accurate and is not influenced by personal bias, people are generally averse to AI decision-making and perceive it as being less fair. Given the theoretical and practical importance of fairness perceptions of AI decision-making, a growing", "keywords": {"keywords": ["fairness perceptions", "artificial intelligence", "decision-making", "inequality", "sustainable development"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["sociology"], "content": ["application"], "application area": ["social sciences"]}}, {"title": "Fairness in Recommender Systems: Research Landscape and Future Directions", "author": "Y Deldjoo", "abstract": "Recommender systems can strongly influence which information we see online, eg, on social media, and thus impact our beliefs, decisions, and actions. At the same time, these systems can create substantial business value for different stakeholders. Given the growing potential impact of such AI-based systems on individuals, organizations, and society, questions of fairness have gained increased attention in recent years. However, research on fairness in recommender systems is still a developing area. In this survey, we first review the", "keywords": {"keywords": ["fairness", "recommender systems", "research landscape", "future directions", "information"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["information retrieval"]}}, {"title": "A Survey of Research on Fair Recommender Systems", "author": "Y Deldjoo", "abstract": "Recommender systems can strongly influence which information we see online, eg, on social media, and thus impact our beliefs, decisions, and actions. At the same time, these systems can create substantial business value for different stakeholders. Given the growing potential impact of such AI-based systems on individuals, organizations, and society, questions of fairness have gained increased attention in recent years. However, research on fairness in recommender systems is still a developing area. In this survey, we first review the", "keywords": {"keywords": ["fair recommender systems", "social media", "beliefs", "decisions", "actions"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Do Humans Prefer Debiased AI Algorithms? A Case Study in Career Recommendation", "author": "C Wang", "abstract": "Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, eg along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when human-fair-AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real", "keywords": {"keywords": ["fair AI algorithms", "discriminatory bias", "AI algorithms", "human preferences", "fairness"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Career Recommendation"]}}, {"title": "\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u7684\u516c\u5e73\u611f\u77e5", "author": "\u848b\u8def\u8fdc\uff0c \u66f9\u674e\u6885\uff0c \u79e6\u6615\uff0c \u8c2d\u73b2\uff0c \u9648\u6668\uff0c \u5f6d\u5c0f\u6590", "abstract": "\u8fd9\u65e2\u4e0d\u5229\u4e8e\u8de8\u5b66\u79d1\u5bf9\u8bdd, \u4e5f\u4e0d \u5229\u4e8e\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u5bf9AI \u51b3\u7b56\u516c\u5e73\u611f\u77e5\u5f62\u6210\u7cfb\u7edf \u6027\u7406\u89e3.\u57fa\u4e8e\u6b64 , \u672c\u6587\u7cfb\u7edf\u5730\u68b3\u7406\u4e86AI \u51b3\u7b56\u516c\u5e73 \u611f\u77e5\u7684\u76f8\u5173\u7814\u7a76, \u603b\u7ed3\u4e86AI \u51b3\u7b56\u516c\u5e73\u611f\u77e5\u7684\u5f71\u54cd \u56e0\u7d20\u548c\u673a\u5236\u7b49,"}], "an empirical study on the perceived fairness of realistic, imperfect machine learning models": [{"title": "Measuring non-expert comprehension of machine learning fairness metrics", "author": "D Saha", "abstract": "Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap", "keywords": {"keywords": ["machine learning fairness metrics", "bias", "comprehension", "definitions of fairness", "non-expert"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning fairness"]}}, {"title": "Do the machine learning models on a crowd sourced platform exhibit bias? an empirical study on model fairness", "author": "S Biswas"}, {"title": "Bridging machine learning and mechanism design towards algorithmic fairness", "author": "J Finocchiaro"}, {"title": "Fairness perceptions of algorithmic decision-making: A systematic review of the empirical literature", "author": "C Starke"}, {"title": "\" Ignorance and Prejudice\" in Software Fairness", "author": "JM Zhang"}, {"title": "I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions", "author": "M Kasinidou"}, {"title": "Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline", "author": "S Biswas"}, {"title": "Artificial Intelligence as an Anti-Corruption Tool (AI-ACT)--Potentials and Pitfalls for Top-down and Bottom-up Approaches", "author": "N K\u00f6bis"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Dimensions of diversity in human perceptions of algorithmic fairness", "author": "N Grgi\u0107-Hla\u010da"}, {"title": "Public understanding and perceptions of data practices: A review of existing research", "author": "H Kennedy", "abstract": "The widespread collection and use of digital data is said to have wide-ranging effects: benefits such as more effective service provision, and harms such as more surveillance, less privacy, and new forms of inequality and injustice. In this report, we use the term 'data practices' to refer to the systematic collection, analysis and sharing of data and the outcomes of these processes (this is sometimes also called 'datafication'1). The data at the centre of such practices is often personal data, and research into public perceptions often focuses on", "keywords": {"keywords": ["public understanding", "perceptions", "data practices", "digital data", "personal data"], "research type": "Review", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Sociology", "content": "Methodological", "application area": "Data practices"}}, {"title": "What is fair? Exploring the artists' perspective on the fairness of music streaming platforms", "author": "A Ferraro"}, {"title": "Breaking taboos in fair machine learning: An experimental study", "author": "J Nyarko"}, {"title": "Fairea: A model behaviour mutation approach to benchmarking bias mitigation methods", "author": "M Hort"}, {"title": "The Role of Accuracy in Algorithmic Process Fairness Across Multiple Domains", "author": "M Albach"}, {"title": "Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation", "author": "J Hannan"}, {"title": "Perception of fairness in algorithmic decisions: Future developers' perspective", "author": "S Kleanthous"}, {"title": "\u4eba\u673a\u667a\u80fd\u534f\u540c\u7814\u7a76\u7efc\u8ff0", "author": "\u5b59\u6548\u534e\uff0c \u5f20\u4e49\u6587\uff0c \u79e6\u89c9\u6653\uff0c \u674e\u749f\u7490\uff0c \u738b\u8212\u8d85"}, {"title": "Artificial Intelligence Project Success Factors: Moral Decision-Making with Algorithms", "author": "GJ Miller"}, {"title": "Opportunities for a More Interdisciplinary Approach to Perceptions of Fairness in Machine Learning", "author": "ST Dasch"}, {"title": "Improving machine learning fairness with sampling and adversarial learning", "author": "JJ Amend", "abstract": "Machine learning approaches learn models based on the statistical properties of training data. Learned models may be unfair due to bias inherent in the training data or because of spurious correlations based on sensitive attributes such as race or sex. This type of bias can lead to detrimental outcomes in important applications, including prison sentencing, credit scoring, and loan approvals. In this work, we perform a comparative study of techniques to increase the fairness of machine learning based classification with respect", "keywords": {"keywords": ["machine learning fairness", "sampling", "adversarial learning", "bias", "classification"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer science", "content": "Methodological", "application area": "Fairness in machine learning"}}, {"title": "Where Is the Normative Proof? Assumptions and Contradictions in ML Fairness Research", "author": "AF Cooper"}, {"title": "The FairCeptron: A framework for measuring human perceptions of algorithmic fairness", "author": "G Ahnert"}, {"title": "Fairness perceptions of artificial intelligence decision-making", "author": "L JIANG"}, {"title": "On Learning and Enforcing Latent Assessment Models using Binary Feedback from Human Auditors Regarding Black-Box Classifiers", "author": "M Telukunta"}, {"title": "Music Recommender Systems", "author": "A Ferraro"}, {"title": "Practical Algorithms for Resource Allocation and Decision Making", "author": "DC McElfresh"}, {"title": "Enhancing Fairness Perception\u2013Towards Human-Centred AI and Personalized Explanations Understanding the Factors Influencing Laypeople's Fairness Perceptions \u2026", "author": "A Shulner-Tal"}, {"title": "Music recommender systems: taking into account the artists' perspective", "author": "A Ferraro"}, {"title": "Fair Governance with Humans and Machines", "author": "Y Hermstr\u00fcwer"}, {"title": "Towards evaluating ethical accountability and trustworthiness in AI systems", "author": "R Dvorak", "abstract": "Intelligent systems, also referred to as Artificial Intelligence, are rapidly expanding with direct consequences and impacts. Given that the media focuses more on reporting\" disasters\" involving these systems, and due to both the complexity and often black-box nature of these systems, there is a need for the non-technical audience to understand the impacts given the speed of their adoption. This paper proposes a process for describing and rating ethical accountability of intelligent systems in order to allow non-experts to contrast and evaluate", "keywords": {"keywords": ["ethical accountability", "trustworthiness", "AI systems", "non-technical audience", "intelligent systems"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives", "author": "A Peng"}, {"title": "Fair Engineering of Machine Learning Systems\u2013Lessons Learned From", "author": "J Sengewald"}, {"title": "Fairness and Explanation in AI-Informed Decision Making", "author": "A Angerschmid"}, {"title": "The promise and perils of using artificial intelligence to fight corruption", "author": "N K\u00f6bis"}, {"title": "Fairness of artificial intelligence in human resources-held to a higher standard?", "author": "SL Fisher"}, {"title": "Toward a decision process of the best machine learning model for multi-stakeholders: a crowdsourcing survey method", "author": "T Yokota"}, {"title": "How Teams Communicate about the Quality of ML Models: A Case Study at an International Technology Company", "author": "J Almahmoud"}, {"title": "Fairness as Equal Concession: Critical Remarks on Fair AI", "author": "R van Nood"}, {"title": "Modeling Disclosive Transparency in NLP Application Descriptions", "author": "M Saxon"}, {"title": "Ethics-based AI auditing core drivers and dimensions: A systematic literature review", "author": "J Laine", "abstract": "This thesis provides a systematic literature review (SLR) of ethics-based AI auditing research. The review's main goals are to report the current status of AI auditing academic literature and provide findings addressing the review objectives. The review incorporated 50 articles presenting ethics-based AI auditing. The SLR findings indicate that the AI auditing field is still new and rising. Most of the studies were conference proceeding published either 2019 or 2020. Therefore, there was a demand for a SLR work as the AI auditing field was", "keywords": {"keywords": ["AI auditing", "ethics-based", "systematic literature review", "AI", "rising field"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Ethics-based AI auditing"]}}, {"title": "Artificial Intelligence Project Success Factors\u2014Beyond the Ethical Principles", "author": "GJ Miller"}, {"title": "Understanding and Reasoning Fairness in Machine Learning Pipelines", "author": "S Biswas"}, {"title": "Understanding Unfairness and Its Mitigation in Open-Source Machine Learning Models", "author": "S Biswas"}, {"title": "Towards a multi-stakeholder value-based assessment framework for algorithmic systems", "author": "M Yurrita"}, {"title": "The multisided complexity of fairness in recommender systems", "author": "N Sonboli"}, {"title": "Opportunities for a More Interdisciplinary Approach to Measuring Perceptions of Fairness in Machine Learning", "author": "CM Boykin"}, {"title": "False positives vs. false negatives: public opinion on the cost ratio in criminal justice risk assessment", "author": "B Kang"}, {"title": "\u5229\u5bb3\u95a2\u4fc2\u8005\u3054\u3068\u306b\u7570\u306a\u308b AI \u306e\u6307\u6a19\u306b\u5bfe\u3059\u308b\u9078\u597d\u306e\u30af\u30e9\u30a6\u30c9\u30bd\u30fc\u30b7\u30f3\u30b0\u306b\u3088\u308b\u7d71\u5408\u65b9\u6cd5\u306e\u691c\u8a0e", "author": "\u6a2a\u7530\u62d3\u4e5f\uff0c \u4e2d\u5c3e\u60a0\u91cc"}, {"title": "\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u7684\u516c\u5e73\u611f\u77e5", "author": "\u848b\u8def\u8fdc\uff0c \u66f9\u674e\u6885\uff0c \u79e6\u6615\uff0c \u8c2d\u73b2\uff0c \u9648\u6668\uff0c \u5f6d\u5c0f\u6590"}], "appropriate fairness perceptions? on the effectiveness of explanations in enabling people to assess the fairness of automated decision systems": [{"title": "Perceptions of fairness and trustworthiness based on explanations in human vs. automated decision-making", "author": "J Schoeffer", "abstract": "Automated decision systems (ADS) have become ubiquitous in many high-stakes domains. Those systems typically involve sophisticated yet opaque artificial intelligence (AI) techniques that seldom allow for full comprehension of their inner workings, particularly for affected individuals. As a result, ADS are prone to deficient oversight and calibration, which can lead to undesirable (eg, unfair) outcomes. In this work, we conduct an online study with 200 participants to examine people's perceptions of fairness and trustworthiness towards", "keywords": {"keywords": ["fairness", "trustworthiness", "explanations", "human vs. automated decision-making", "perceptions"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Automated decision systems"]}}, {"title": "A Human-Centric Perspective on Fairness and Transparency in Algorithmic Decision-Making", "author": "J Schoeffer", "abstract": "Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. This is not only problematic from a legal perspective, but non-transparent systems are also prone to yield unfair outcomes because their sanity is challenging to assess and calibrate in the first place\u2014which is particularly worrisome for human decision-subjects. Based on this observation and", "keywords": {"keywords": ["fairness", "transparency", "algorithmic decision-making", "automated decision systems", "machine learning models"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["algorithmic decision-making"]}}, {"title": "\" There Is Not Enough Information\": On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making", "author": "J Schoeffer", "abstract": "Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people's perceptions of informational fairness (ie, whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying", "keywords": {"keywords": ["automated decision systems", "explanations", "informational fairness", "trustworthiness", "machine learning"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Application", "application area": "Automated decision-making"}}], "improving understandability of feature contributions in model-agnostic explainable ai tools": [{"title": "The Role of Human Knowledge in Explainable AI", "author": "A Tocchetti", "abstract": "As the performance and complexity of machine learning models have grown significantly over the last years, there has been an increasing need to develop methodologies to describe their behaviour. Such a need has mainly arisen due to the widespread use of black-box models, ie, high-performing models whose internal logic is challenging to describe and understand. Therefore, the machine learning and AI field is facing a new challenge: making models more explainable through appropriate techniques. The final goal of an explainability", "keywords": {"keywords": ["explainable AI", "machine learning", "black-box models", "methodologies", "complexity"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explainability"]}}], "interpretable directed diversity: leveraging model explanations for iterative crowd ideation": [{"title": "The Role of Human Knowledge in Explainable AI", "author": "A Tocchetti", "abstract": "As the performance and complexity of machine learning models have grown significantly over the last years, there has been an increasing need to develop methodologies to describe their behaviour. Such a need has mainly arisen due to the widespread use of black-box models, ie, high-performing models whose internal logic is challenging to describe and understand. Therefore, the machine learning and AI field is facing a new challenge: making models more explainable through appropriate techniques. The final goal of an explainability", "keywords": {"keywords": ["Machine learning", "Explainable AI", "Black-box models", "Complexity", "Human knowledge"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}], "evaluating the interpretability of generative models by interactive reconstruction": [{"title": "Understanding instance-based interpretability of variational auto-encoders", "author": "Z Kong", "abstract": "Instance-based interpretation methods have been widely studied for supervised learning methods as they help explain how black box neural networks predict. However, instance-based interpretations remain ill-understood in the context of unsupervised learning. In this paper, we investigate influence functions [Koh and Liang, 2017], a popular instance-based interpretation method, for a class of deep generative models called variational auto-encoders (VAE). We formally frame the counter-factual question answered by influence", "keywords": {"keywords": ["Instance-based interpretability", "Variational Auto-Encoders", "Unsupervised Learning", "Influence Functions", "Counter-factual Question"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Deep Generative Models"]}}, {"title": "Investigating Explainability of Generative AI for Code through Scenario-based Design", "author": "J Sun", "abstract": "What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users' explainability needs", "keywords": {"keywords": ["generative AI", "explainable AI", "scenario-based design", "question-driven XAI design", "software engineering"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["software engineering"]}}, {"title": "Expressive communication: A common framework for evaluating developments in generative models and steering interfaces", "author": "R Louie", "abstract": "There is an increasing interest from ML and HCI communities in empowering creators with better generative models and more intuitive interfaces with which to control them. In music, ML researchers have focused on training models capable of generating pieces with increasing long-range structure and musical coherence, while HCI researchers have separately focused on designing steering interfaces that support user control and ownership. In this study, we investigate through a common framework how developments in", "keywords": {"keywords": ["generative models", "steering interfaces", "expressive communication", "ML", "HCI"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["music"]}}, {"title": "Better together? an evaluation of ai-supported code translation", "author": "JD Weisz", "abstract": "Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced", "keywords": {"keywords": ["generative machine learning models", "source code", "code translation", "Java-to-Python", "software engineers"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Code translation"]}}, {"title": "Expressive Communication: Evaluating Developments in Generative Models and Steering Interfaces for Music Creation", "author": "R Louie", "abstract": "There is an increasing interest from ML and HCI communities in empowering creators with better generative models and more intuitive interfaces with which to control them. In music, ML researchers have focused on training models capable of generating pieces with increasing long-range structure and musical coherence, while HCI researchers have separately focused on designing steering interfaces that support user control and ownership. In this study, we investigate how developments in both models and user", "keywords": {"keywords": ["Generative models", "Steering interfaces", "Music creation", "ML", "HCI"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Music production"]}}, {"title": "Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations", "author": "W Stammer", "abstract": "Learning visual concepts from raw images without strong supervision is a challenging task. In this work, we show the advantages of prototype representations for understanding and revising the latent space of neural concept learners. For this purpose, we introduce interactive Concept Swapping Networks (iCSNs), a novel framework for learning concept-grounded representations via weak supervision and implicit prototype representations. iCSNs learn to bind conceptual information to specific prototype slots by swapping the latent", "keywords": {"keywords": ["disentanglement", "prototype representations", "concept learners", "weak supervision", "latent space"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Machine learning"]}}, {"title": "AI-Moderated Decision-Making: Capturing and Balancing Anchoring Bias in Sequential Decision Tasks", "author": "JM Echterhoff", "abstract": "Decision-making involves biases from past experiences, which are difficult to perceive and eliminate. We investigate a specific type of anchoring bias, in which decision-makers are anchored by their own recent decisions, eg a college admission officer sequentially reviewing students. We propose an algorithm that identifies existing anchored decisions, reduces sequential dependencies to previous decisions, and mitigates decision inaccuracies post-hoc with 2% increased agreement to ground-truth on a large-scale", "keywords": {"keywords": ["AI-Moderated Decision-Making", "Anchoring Bias", "Sequential Decision Tasks", "Algorithms", "Decision Inaccuracies"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Decision Support Systems"]}}, {"title": "GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information", "author": "H Dang", "abstract": "We investigate how multiple sliders with and without feedforward visualizations influence users' control of generative models. In an online study (N= 138), we collected a dataset of people interacting with a generative adversarial network (StyleGAN2) in an image reconstruction task. We found that more control dimensions (sliders) significantly increase task difficulty and user actions. Visual feedforward partly mitigates this by enabling more goal-directed interaction. However, we found no evidence of faster or more accurate task", "keywords": {"keywords": ["generative models", "sliders", "feedforward information", "GANSlider", "image reconstruction"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image synthesis"]}}, {"title": "How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement and \u2026", "author": "D Buschek", "abstract": "The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI", "keywords": {"keywords": ["intelligent systems", "user understanding", "human-computer interaction", "transparency", "explainability"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["user support"]}}, {"title": "Measuring algorithmic interpretability: A human-learning-based framework and the corresponding cognitive complexity score", "author": "JP Lalor", "abstract": "Algorithmic interpretability is necessary to build trust, ensure fairness, and track accountability. However, there is no existing formal measurement method for algorithmic interpretability. In this work, we build upon programming language theory and cognitive load theory to develop a framework for measuring algorithmic interpretability. The proposed measurement framework reflects the process of a human learning an algorithm. We show that the measurement framework and the resulting cognitive complexity score have the", "keywords": {"keywords": ["algorithmic interpretability", "measurement framework", "cognitive complexity score", "programming language theory", "cognitive load theory"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Beyond Examples: Constructing Explanation Space for Explaining Prototypes", "author": "H Joo", "abstract": "As deep learning has been successfully deployed in diverse applications, there is ever increasing need for explaining its decision. Most of the existing methods produced explanations with a second model that explains the first black-box model, but we propose an inherently interpretable model for more faithful explanations. Our method constructs an explanation space in which similarities in terms of human-interpretable features at images share similar latent representations by using a variational autoencoder. This explanation", "keywords": {"keywords": ["deep learning", "explanation", "interpretable model", "latent representations", "variational autoencoder"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["image classification"]}}], "do people engage cognitively with ai? impact of ai assistance on incidental learning": [{"title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging", "author": "R Fogliato", "abstract": "Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a", "keywords": {"keywords": ["human-AI collaboration", "decision making", "clinical imaging", "interaction design", "workflow"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Medical Imaging"]}}, {"title": "Towards Informed Decision-making: Triggering Curiosity in Explanations to Non-expert Users", "author": "A Bertrand", "abstract": "As AI systems gain performance, their adoption expands to areas considered critical. In finance, increasingly sophisticated recommender systems known as\" robo-advisors\" are democratizing online access to life insurance. Their users are confronted with a choice that will affect their quality of life, even though they may not have any financial knowledge. To address this asymmetry of information between advisor and client, the French and European legislation [4] require insurance providers to produce\" clear, precise and nonmisleading\"", "keywords": {"keywords": ["informed decision-making", "curiosity", "explanations", "non-expert users", "robo-advisors"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["finance"], "content": ["application"], "application area": ["insurance"]}}, {"title": "A Case for Humans-in-the-Loop: Decisions in the Presence of Misestimated Algorithmic Scores", "author": "R Fogliato", "abstract": "The increased use of machine learning to assist with decision-making in high-stakes domains has been met with both enthusiasm and concern. One source of ongoing debate is the effect and value of decision makers' discretionary power to override algorithmic recommendations. In this paper, we study the adoption of an algorithmic tool used to help with decisions in child maltreatment hotline screenings. By taking advantage of an implementation glitch, we investigate corrective overrides: whether decision makers are", "keywords": {"keywords": ["machine learning", "decision-making", "algorithmic scores", "discretionary power", "child maltreatment hotline screenings"], "research type": ["Case study"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Child protection"]}}, {"title": "Not Just a Preference: Reducing Biased Decision-making on Dating Websites", "author": "Z Ma", "abstract": "As dating websites are becoming an essential part of how people meet intimate and romantic partners, it is vital to design these systems to be resistant to, or at least do not amplify, bias and discrimination. Instead, the results of our online experiment with a simulated dating website, demonstrate that popular dating website design choices, such as the user of the swipe interface (swiping in one direction to indicate a like and in the other direction to express a dislike) and match scores, resulted in people racially biases choices", "keywords": {"keywords": ["dating websites", "biased decision-making", "bias and discrimination", "swipe interface", "match scores"], "research type": ["Empirical"], "methodology": ["Mixed methods"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Methodological"], "application area": ["Social interactions"]}}], "how do people rank multiple mutant agents?": [{"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge", "abstract": "Assessing AI systems is difficult. Humans rely on AI systems in increasing ways, both visible and invisible, meaning a variety of stakeholders need a variety of assessment tools (eg, a professional auditor, a developer, and an end user all have different needs). We posit that it is possible to provide explanations and assessment processes that enable AI non-experts observing multiple intelligent agents in sequential domains to differentiate the agents with respect to a property (eg, quality or fairness), as well as articulate justification for their", "keywords": {"keywords": ["AI assessment", "manipulable properties", "explanations", "processes", "sequential domains"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI assessment"]}}], "anchoring bias affects mental model formation and user reliance in explainable ai systems": [{"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan", "abstract": "Explainability of AI systems is critical for users to take informed actions and hold systems accountable. While\" opening the opaque box\" is important, understanding who opens the box can govern if the Human-AI interaction is effective. In this paper, we conduct a mixed-methods study of how two different groups of whos--people with and without a background in AI--perceive different types of AI explanations. These groups were chosen to look at how disparities in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively", "keywords": {"keywords": ["explainable AI", "AI background", "perception", "AI explanations", "creator-consumer gap"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao", "abstract": "As a technical sub-field of artificial intelligence (AI), explainable AI (XAI) has produced a vast collection of algorithms in recent years. However, explainability is an inherently human-centric property and the field is starting to embrace inter-disciplinary perspectives and human-centered approaches. As researchers and practitioners begin to leverage XAI algorithms to build XAI applications, explainability has moved beyond a demand by data scientists or researchers to comprehend the models they are developing, to become an", "keywords": {"keywords": ["explainable AI", "human-centered approaches", "algorithms", "user experiences", "XAI applications"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai", "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of", "keywords": {"keywords": ["human-ai decision making", "empirical studies", "predictive performance", "AI assistance", "high-stakes domains"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Criminal justice and healthcare"]}}, {"title": "How level of explanation detail affects human performance in interpretable intelligent systems: A study on explainable fact checking", "author": "R Linder", "abstract": "Explainable artificial intelligence (XAI) systems aim to provide users with information to help them better understand computational models and reason about why outputs were generated. However, there are many different ways an XAI interface might present explanations, which makes designing an appropriate and effective interface an important and challenging task. Our work investigates how different types and amounts of explanatory information affect user ability to utilize explanations to understand system behavior and", "keywords": {"keywords": ["Explainable artificial intelligence", "XAI", "interactable intelligent systems", "explanatory information", "system behavior"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Fact checking"]}}, {"title": "Wizard of Errors: Introducing and Evaluating Machine Learning Errors in Wizard of Oz Studies", "author": "A Jansen", "abstract": "When designing Machine Learning (ML) enabled solutions, designers often need to simulate ML behavior through the Wizard of Oz (WoZ) approach to test the user experience before the ML model is available. Although reproducing ML errors is essential for having a good representation, they are rarely considered. We introduce Wizard of Errors (WoE), a tool for conducting WoZ studies on ML-enabled solutions that allows simulating ML errors during user experience assessment. We explored how this system can be used to", "keywords": {"keywords": ["Machine Learning", "Wizard of Oz", "ML errors", "user experience", "simulation"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["User Experience Assessment"]}}, {"title": "Scim: Intelligent Faceted Highlights for Interactive, Multi-Pass Skimming of Scientific Papers", "author": "R Fok", "abstract": "Researchers are expected to keep up with an immense literature, yet often find it prohibitively time-consuming to do so. This paper explores how intelligent agents can help scaffold in-situ information seeking across scientific papers. Specifically, we present Scim, an AI-augmented reading interface designed to help researchers skim papers by automatically identifying, classifying, and highlighting salient sentences, organized into rhetorical facets rooted in common information needs. Using Scim as a design probe, we", "keywords": {"keywords": ["intelligent agents", "scientific papers", "information seeking", "AI-augmented reading interface", "skimming papers"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["information retrieval"]}}, {"title": "Overreliance on AI: Literature review", "author": "S Passi", "abstract": "This report synthesizes~ 60 research papers about overreliance on AI. The papers originate from a variety of disciplines, including Human-Computer Interaction (HCI); Human Factors; Intelligent User Interfaces (IUI); Computer Supported Cooperative Work (CSCW); Organizational Science; and Fairness, Accountability, and Transparency (FAccT).", "keywords": {"keywords": ["AI", "overreliance", "literature review", "Human-Computer Interaction", "Accountability"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Interdisciplinary"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk", "abstract": "Building trust in AI-based systems is deemed critical for their adoption and appropriate use. Recent research has thus attempted to evaluate how various attributes of these systems affect user trust. However, limitations regarding the definition and measurement of trust in AI have hampered progress in the field, leading to results that are inconsistent or difficult to compare. In this work, we provide an overview of the main limitations in defining and measuring trust in AI. We focus on the attempt of giving trust in AI a numerical value and its", "keywords": {"keywords": ["trust", "AI", "measurement", "socio-technical system", "adoption"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["technology"]}}, {"title": "Capable but Amoral? Comparing AI and Human Expert Collaboration in Ethical Decision Making", "author": "S Tolmeijer", "abstract": "While artificial intelligence (AI) is increasingly applied for decision-making processes, ethical decisions pose challenges for AI applications. Given that humans cannot always agree on the right thing to do, how would ethical decision-making by AI systems be perceived and how would responsibility be ascribed in human-AI collaboration? In this study, we investigate how the expert type (human vs. AI) and level of expert autonomy (adviser vs. decider) influence trust, perceived responsibility, and reliance. We find that participants", "keywords": {"keywords": ["artificial intelligence", "ethical decision-making", "expert collaboration", "trust", "responsibility"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Ethics"]}}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications", "author": "M Nourani", "abstract": "While EXplainable Artificial Intelligence (XAI) approaches aim to improve human-AI collaborative decision-making by improving model transparency and mental model formations, experiential factors associated with human users can cause challenges in ways system designers do not anticipate. In this paper, we first showcase a user study on how anchoring bias can potentially affect mental model formations when users initially interact with an intelligent system and the role of explanations in addressing this bias. Using a video", "keywords": {"keywords": ["Explainable Artificial Intelligence", "User backgrounds", "Impressions", "Anchoring bias", "Explanations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": "Human-AI interaction"}}, {"title": "Do Humans Prefer Debiased AI Algorithms? A Case Study in Career Recommendation", "author": "C Wang", "abstract": "Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, eg along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when human-fair-AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real"}, {"title": "Interactive Model Cards: A Human-Centered Approach to Model Documentation", "author": "A Crisan", "abstract": "Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model's details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves", "keywords": {"keywords": ["deep learning models", "model documentation", "interactive model cards", "natural language processing (NLP)", "human-centered approach"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Model Documentation"]}}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand", "abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI", "keywords": {"keywords": ["cognitive biases", "XAI", "explainability systems", "human explanation methods", "heuristic map"], "research type": "Review", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Computer science", "content": "Methodological", "application area": "Artificial intelligence"}}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen", "abstract": "Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human", "keywords": {"keywords": ["machine explanations", "human understanding", "model debugging", "human decision making", "causal diagrams"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}], "i think i get your point, ai! the illusion of explanatory depth in explainable ai": [{"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan", "abstract": "Explainability of AI systems is critical for users to take informed actions and hold systems accountable. While\" opening the opaque box\" is important, understanding who opens the box can govern if the Human-AI interaction is effective. In this paper, we conduct a mixed-methods study of how two different groups of whos--people with and without a background in AI--perceive different types of AI explanations. These groups were chosen to look at how disparities in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively", "keywords": {"keywords": ["explainable AI", "perceptions", "AI explanations", "background in AI", "creator-consumer gap"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explainability"]}}, {"title": "General pitfalls of model-agnostic interpretation methods for machine learning models", "author": "C Molnar", "abstract": "An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty", "keywords": {"keywords": ["model-agnostic interpretation", "machine learning models", "pitfalls", "partial dependence plots", "permutation feature importance"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning interpretation"]}}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao", "abstract": "As a technical sub-field of artificial intelligence (AI), explainable AI (XAI) has produced a vast collection of algorithms in recent years. However, explainability is an inherently human-centric property and the field is starting to embrace inter-disciplinary perspectives and human-centered approaches. As researchers and practitioners begin to leverage XAI algorithms to build XAI applications, explainability has moved beyond a demand by data scientists or researchers to comprehend the models they are developing, to become an", "keywords": {"keywords": ["explainable AI", "algorithms", "human-centered", "inter-disciplinary perspectives", "XAI applications"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai", "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of", "keywords": {"keywords": ["human-ai decision making", "empirical studies", "augmenting human decision making", "high-stakes domains", "AI assistance"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Finding AI's faults with AAR/AI: An empirical study", "author": "R Khanna", "abstract": "Would you allow an AI agent to make decisions on your behalf? If the answer is \u201cnot always,\u201d the next question becomes \u201cin what circumstances\u201d? Answering this question requires human users to be able to assess an AI agent\u2014and not just with overall pass/fail assessments or statistics. Here users need to be able to localize an agent's bugs so that they can determine when they are willing to rely on the agent and when they are not. After-Action Review for AI (AAR/AI), a new AI assessment process for integration with Explainable AI", "keywords": {"keywords": ["AI agent", "assessment", "bugs", "localize", "reliability"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Explainable AI"]}}, {"title": "Towards personalized explanations for AI systems: designing a role model for explainable AI in auditing", "author": "J Rebstadt", "abstract": "Due to a continuously growing repertoire of available methods and applications, Artificial Intelligence (AI) is becoming an innovation driver for most industries. In the auditing domain, initial approaches of AI have already been discussed in scientific discourse, but practical application is still lagging behind. Caused by a highly regulated environment, the explainability of AI is of particular relevance. Using semi-structured expert interviews, we identified stakeholder specific requirements regarding explainable AI (XAI) in auditing. To", "keywords": {"keywords": ["Explainable AI", "Auditing", "Artificial Intelligence", "Semi-structured expert interviews", "Regulated environment"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Economics"], "content": ["Methodological"], "application area": ["AI in Auditing"]}}, {"title": "The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending", "author": "M Dikmen", "abstract": "Increasingly, artificial intelligence (AI) is being used to assist complex decision-making such as financial investing. However, there are concerns regarding the black-box nature of AI algorithms. The field of explainable AI (XAI) has emerged to address these concerns. XAI techniques can reveal how an AI decision is formed and can be used to understand and appropriately trust an AI system. However, XAI techniques still may not be human-centred and may not support human decision-making adequately. In this work, we explored how", "keywords": {"keywords": ["domain knowledge", "trust", "explainable AI", "task performance", "peer-to-peer lending"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Economics"], "content": ["Methodological"], "application area": ["Financial decision-making"]}}, {"title": "Explaining Dnn Based Facial Expression Classifications", "author": "K Burg", "abstract": "Classifying facial expressions is a vital part of developing systems capable of aptly interacting with users. In this field, the use of deep-learning models has become the standard. However, the inner workings of these models are unintelligible, which is an important issue when deploying them to high-stakes environments. Recent efforts to generate explanations for emotion classification systems has been focused on this type of models. In this study, an alternative way of explaining the decisions of a more conventional", "keywords": {"keywords": ["facial expression classification", "deep-learning models", "explaining", "emotion classification", "conventional model"], "research type": ["empirical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["human-computer interaction"]}}, {"title": "Mediators: Conversational Agents Explaining NLP Model Behavior", "author": "N Feldhus", "abstract": "The human-centric explainable artificial intelligence (HCXAI) community has raised the need for framing the explanation process as a conversation between human and machine. In this position paper, we establish desiderata for Mediators, text-based conversational agents which are capable of explaining the behavior of neural models interactively using natural language. From the perspective of natural language processing (NLP) research, we engineer a blueprint of such a Mediator for the task of sentiment analysis and assess how far", "keywords": {"keywords": ["Mediators", "Conversational Agents", "NLP Model Behavior", "Explainable AI", "Sentiment Analysis"], "research type": "Empirical", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Technical", "application area": "Natural Language Processing"}}, {"title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?", "author": "A Balayn", "abstract": "Deep learning models for image classification suffer from dangerous issues often discovered after deployment. The process of identifying bugs that cause these issues remains limited and understudied. Especially, explainability methods are often presented as obvious tools for bug identification. Yet, the current practice lacks an understanding of what kind of explanations can best support the different steps of the bug identification process, and how practitioners could interact with those explanations. Through a formative study and", "keywords": {"keywords": ["bug identification", "computer vision models", "explainability methods", "deep learning", "image classification"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Computer Vision"]}}, {"title": "Explaining Automated Decisions in Practice: Insights from the Swedish Credit Scoring Industry", "author": "F Matz", "abstract": "The field of explainable artificial intelligence (XAI) has gained momentum in recent years following the increased use of AI systems across industries leading to bias, discrimination, and data security concerns. Several conceptual frameworks for how to reach AI systems that are fair, transparent, and understandable have been proposed, as well as a number of technical solutions improving some of these aspects in a research context. However, there is still a lack of studies examining the implementation of these concepts and techniques in", "keywords": {"keywords": ["explainable artificial intelligence", "automated decisions", "credit scoring industry", "fairness", "transparency"], "research type": ["empirical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI systems"]}}, {"title": "A Cognitive Work Analysis Approach to Explainable Artificial Intelligence in Non-Expert Financial Decision-Making", "author": "M Dikmen", "abstract": "Artificial Intelligence (AI) is being increasingly used to assist complex decision-making such as financial investing. As most AI systems rely on black-box machine learning models, understanding how to support human decision-makers and gaining users' trust becomes important. Explainable Artificial Intelligence (XAI) has been proposed to address these issues by making the decision-making process of AI systems understandable to users. However, existing XAI approaches fail to take into account users' domain experience, and", "keywords": {"keywords": ["Explainable Artificial Intelligence", "Non-Expert Financial Decision-Making", "Cognitive Work Analysis", "Trust", "Domain Experience"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Financial Decision-Making"]}}, {"title": "On the Need for Collaborative Intelligence in Cybersecurity.", "author": "T Martin", "abstract": "The success of artificial intelligence (and particularly data-driven machine learning) in classifying and making predictions from large bodies of data has led to an expectation that autonomous AI systems can be deployed in cybersecurity applications. In this position paper we outline some of the problems facing machine learning in cybersecurity and argue for a collaborative approach where humans contribute insight and understanding, whilst machines are used to gather, filter and process data into a convenient and understandable", "keywords": {"keywords": ["collaborative intelligence", "cybersecurity", "artificial intelligence", "machine learning", "data-driven"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["cybersecurity"]}}, {"title": "Topic for a Bachelor/Master's Thesis", "author": "M Learning", "abstract": "SHORT DESCRIPTION: In the realm of automated machine learning [1], a frequently considered problem is to automatically find a suitable composition of machine learning algorithms and values for their so-called hyperparameters for a given data set. Since the space of possible solutions is extremely large, various heuristic optimization approaches have been proposed in the literature to tackle the problem. However, despite some efforts to warmstart the optimization process by injecting hints on well-performing candidate solutions", "keywords": {"keywords": ["automated machine learning", "optimization", "heuristic approaches", "hyperparameters", "warmstart"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["machine learning"]}}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert Users", "author": "C Bove", "abstract": "The increasing usage of complex Machine Learning models for decision-making has raised interest in explainable artificial intelligence (XAI). In this work, we focus on the effects of providing accessible and useful explanations to non-expert users. More specifically, we propose generic XAI design principles for contextualizing and allowing the exploration of explanations based on local feature importance. To evaluate the effectiveness of these principles for improving users' objective understanding and satisfaction, we conduct a", "keywords": {"keywords": ["explainable artificial intelligence", "local feature importance", "non-expert users", "contextualization", "exploration"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Making SHAP Rap: Bridging Local and Global Insights Through Interaction and Narratives", "author": "M Chromik", "abstract": "The interdisciplinary field of explainable artificial intelligence (XAI) aims to foster human understanding of black-box machine learning models through explanation-generating methods. In practice, Shapley explanations are widely used. However, they are often presented as visualizations and thus leave their interpretation to the user. As such, even ML experts have difficulties interpreting them appropriately. On the other hand, combining visual cues with textual rationales has been shown to facilitate understanding and communicative", "keywords": {"keywords": ["explainable artificial intelligence", "Shapley explanations", "visual cues", "textual rationales", "interpretation"], "research type": "Theoretical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning"}}, {"title": "Towards Involving End-users in Interactive Human-in-the-loop AI Fairness", "author": "Y Nakao", "abstract": "Ensuring fairness in artificial intelligence (AI) is important to counteract bias and discrimination in far-reaching applications. Recent work has started to investigate how humans judge fairness and how to support machine learning (ML) experts in making their AI models fairer. Drawing inspiration from an Explainable AI (XAI) approach called explanatory debugging used in interactive machine learning, our work explores designing interpretable and interactive human-in-the-loop interfaces that allow ordinary end-users without any", "keywords": {"keywords": ["AI fairness", "human-in-the-loop", "interpretability", "interactive", "end-users"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI fairness"]}}, {"title": "Understanding User Perception of Explainable Algorithmic Decision-Making Systems: A Systematic Literature Review", "author": "S Bae", "abstract": "Improving the accuracy of algorithmic prediction has gained attention in Information Systems research in recent decades. Information systems which include algorithmic prediction have been seen to provide organisational value. However, as decisions based on these opaque algorithms become more ubiquitous, public demand for explanations for its output have naturally increased. This review evaluates research that examines the impact of providing explanations for the predictions made by algorithms, on how users respond to the", "keywords": {"keywords": ["algorithmic decision making", "explanation", "user perception", "systematic literature review", "predictive algorithms"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["Information Systems"], "content": ["methodological"], "application area": ["algorithmic decision-making systems"]}}, {"title": "Plant Genotype to Phenotype Prediction Using Machine Learning", "author": "MF Danilevicz", "abstract": "Genomic prediction tools support crop breeding based on statistical methods, such as the genomic best linear unbiased prediction (GBLUP). However, these tools are not designed to capture non-linear relationships within multi-dimensional datasets, or deal with high dimension datasets such as imagery collected by unmanned aerial vehicles. Machine learning (ML) algorithms have the potential to surpass the prediction accuracy of current tools used for genotype to phenotype prediction, due to their capacity to autonomously", "keywords": {"keywords": ["plant genotype", "phenotype prediction", "machine learning", "genomic prediction", "unmanned aerial vehicles"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["biology"], "content": ["methodological"], "application area": ["crop breeding"]}}, {"title": "Explainable Artificial Intelligence\u2013A New Step towards the Trust in Medical Diagnosis with AI Frameworks: A Review", "author": "NM Deshpande", "abstract": ", trust and effectively manage the emerging AI systems, an AI  The conceptual framework of  my proposed approach is  It would fall into the third approach reviewed above \u2014interpretable", "keywords": {"keywords": ["Explainable Artificial Intelligence", "Trust", "Medical Diagnosis", "AI Frameworks", "Review"], "research type": ["Review"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": ["Medical Diagnosis"]}}, {"title": "Defining Explanation and Explanatory Depth in XAI", "author": "S Buijsman", "abstract": "Explainable artificial intelligence (XAI) aims to help people understand black box algorithms, particularly of their outputs. But what are these explanations and when is one explanation better than another? The manipulationist definition of explanation from the philosophy of science offers good answers to these questions, holding that an explanation consists of a generalization that shows what happens in counterfactual cases. Furthermore, when it comes to explanatory depth this account holds that a generalization that has more abstract", "keywords": {"keywords": ["Explainable artificial intelligence", "XAI", "Explanation", "Explanatory depth", "Generalization"], "research type": ["Theoretical"], "methodology": ["Not applicable"], "purposes": ["Explanatory"], "discipline": ["Philosophy of science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand", "abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI", "keywords": {"keywords": ["cognitive biases", "XAI-assisted decision-making", "systematic review", "explainability systems", "heuristic map"], "research type": "Review", "methodology": "Qualitative", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Artificial Intelligence"}}, {"title": "Exploring How Anomalous Model Input and Output Alerts Affect Decision-Making in Healthcare", "author": "M Radensky", "abstract": "An important goal in the field of human-AI interaction is to help users more appropriately trust AI systems' decisions. A situation in which the user may particularly benefit from more appropriate trust is when the AI receives anomalous input or provides anomalous output. To the best of our knowledge, this is the first work towards understanding how anomaly alerts may contribute to appropriate trust of AI. In a formative mixed-methods study with 4 radiologists and 4 other physicians, we explore how AI alerts for anomalous input, very high", "keywords": {"keywords": ["anomaly alerts", "human-AI interaction", "trust", "healthcare", "decision-making"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Medicine"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "On the Need for Collaborative Intelligence in Cybersecurity", "author": "T Martin", "abstract": "The success of artificial intelligence (and particularly data-driven machine learning) in classifying and making predictions from large bodies of data has led to an expectation that autonomous AI systems can be deployed in cybersecurity applications. In this position paper we outline some of the problems facing machine learning in cybersecurity and argue for a collaborative approach where humans contribute insight and understanding, whilst machines are used to gather, filter and process data into a convenient and understandable", "keywords": {"keywords": ["collaborative intelligence", "cybersecurity", "artificial intelligence", "machine learning", "data-driven"], "research type": ["position paper"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["cybersecurity"]}}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang", "abstract": "Recent years have witnessed the growing literature in empirical evaluation of explainable AI (XAI) methods. This study contributes to this ongoing conversation by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy\u2014improve people's understanding of the AI model, help people recognize the model uncertainty, and support people's calibrated trust in the", "keywords": {"keywords": ["explainable AI", "AI-assisted decision making", "XAI methods", "understanding", "trust"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explanations"]}}, {"title": "Do We Need Another Explainable AI Method? Toward Unifying Post-hoc XAI Evaluation Methods into an Interactive and Multi-dimensional Benchmark", "author": "MK Belaid", "abstract": "In recent years, Explainable AI (xAI) attracted a lot of attention as various countries turned explanations into a legal right. xAI allows for improving models beyond the accuracy metric by, eg, debugging the learned pattern and demystifying the AI's behavior. The widespread use of xAI brought new challenges. On the one hand, the number of published xAI algorithms underwent a boom, and it became difficult for practitioners to select the right tool. On the other hand, some experiments did highlight how easy data scientists could misuse", "keywords": {"keywords": ["Explainable AI", "xAI", "Post-hoc XAI", "Evaluation methods", "Benchmark"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen", "abstract": "Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human", "keywords": {"keywords": ["Machine Explanations", "Human Understanding", "Model debugging", "Enhancing human decision making", "Causal diagrams"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Prescriptive", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning"}}], "xalgo: a design probe of explaining algorithms' internal states via question-answering": [{"title": "Explainable artificial intelligence for tabular data: A survey", "author": "M Sahakyan", "abstract": "Machine learning techniques are increasingly gaining attention due to their widespread use in various disciplines across academia and industry. Despite their tremendous success, many such techniques suffer from the \u201cblack-box\u201d problem, which refers to situations where the data analyst is unable to explain why such techniques arrive at certain decisions. This problem has fuelled interest in Explainable Artificial Intelligence (XAI), which refers to techniques that can easily be interpreted by humans. Unfortunately, many of these", "keywords": {"keywords": ["explainable artificial intelligence", "tabular data", "survey", "machine learning techniques", "black-box problem"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability", "author": "F Sovrano", "abstract": "Numerous government initiatives (eg the EU with GDPR) are coming to the conclusion that the increasing complexity of modern software systems must be contrasted with some Rights to Explanation and metrics for the Impact Assessment of these tools, that allow humans to understand and oversee the output of Automated Decision Making systems. Explainable AI was born as a pathway to allow humans to explore and understand the inner working of complex systems. But establishing what is an explanation and objectively evaluating", "keywords": {"keywords": ["Explainable AI", "objective metric", "degree of explainability", "automated decision making", "impact assessment"], "research type": "theoretical", "methodology": "qualitative", "purposes": "exploratory", "discipline": "computer science", "content": "methodological", "application area": "AI explainability"}}, {"title": "StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement", "author": "Z Zhang", "abstract": "Despite its benefits for children's skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD)", "keywords": {"keywords": ["interactive storytelling", "parent-child bonding", "AI", "chatbot", "participatory design"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Parent-child interactive storytelling"]}}, {"title": "Generating User-Centred Explanations via Illocutionary Question Answering: From Philosophy to Interfaces", "author": "F Sovrano", "abstract": "We propose a new method for generating explanations with Artificial Intelligence (AI) and a tool to test its expressive power within a user interface. In order to bridge the gap between philosophy and human-computer interfaces, we show a new approach for the generation of interactive explanations based on a sophisticated pipeline of AI algorithms for structuring natural language documents into knowledge graphs, answering questions effectively and satisfactorily. With this work we aim to prove that the philosophical theory of explanations", "keywords": {"keywords": ["explanations", "user interface", "Artificial Intelligence", "knowledge graphs", "question answering"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Technical"], "application area": ["Natural language processing"]}}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study", "author": "L Chazette", "abstract": "Quality aspects such as ethics, fairness, and transparency have been proven to be essential for trustworthy software systems. Explainability has been identified not only as a means to achieve all these three aspects in systems, but also as a way to foster users' sentiments of trust. Despite this, research has only marginally focused on the activities and practices to develop explainable systems. To close this gap, we recommend six core activities and associated practices for the development of explainable systems based on the results of a", "keywords": {"keywords": ["explainable systems", "trustworthy software systems", "ethics", "fairness", "transparency"], "research type": ["Literature Review", "Interview Study"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Computing Technology"]}}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare Systeme", "author": "M Balci", "abstract": "Zusammenfassung Nachdem die Forschung zur Softwareanforderung Erkl\u00e4rbarkeit sich haupts\u00e4chlich auf das Fachgebiet der k\u00fcnstlichen Intelligenz beschr\u00e4nkt hat, wird dieser aktuell aus der endnutzerzentrierten Perspektive betrachtet. Das Requirements Engineering (RE) sowie die Mensch-Maschine Kommunikation (MMK) befassen sich mit der Erfassung der Bed\u00fcrfnisse, Ziele und des Kontextes um Erkl\u00e4rbarkeitsanforderungen zu definieren und Evaluationsm\u00f6glichkeiten bereitzustellen. Trotz dessen existiert noch keine Darstellung der", "keywords": {"keywords": ["Requirements Engineering", "Explained Systems", "User-centered Perspective", "Mensch-Maschine Kommunikation (MMK)", "Evaluations"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Software Engineering"]}}], "keeping it \"organized and logical\": after-action review for ai (aar/ai)": [{"title": "Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI systems", "author": "B Shneiderman", "abstract": "This article attempts to bridge the gap between widely discussed ethical principles of Human-centered AI (HCAI) and practical steps for effective governance. Since HCAI systems are developed and implemented in multiple organizational structures, I propose 15 recommendations at three levels of governance: team, organization, and industry. The recommendations are intended to increase the reliability, safety, and trustworthiness of HCAI systems:(1) reliable systems based on sound software engineering practices,(2) safety", "keywords": {"keywords": ["ethics", "practice", "Human-centered AI systems", "reliable", "safe", "trustworthy"], "research type": ["theoretical"], "methodology": [], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["technology"]}}, {"title": "After-action review for AI (AAR/AI)", "author": "J Dodge", "abstract": "AI? In this article, we investigate this approach by adapting a technique called After-Action  Review (AAR) for use with AI.  adaptation AAR/AI (pronounced \u201carf-eye,\u201d short for \u201cAAR for AI\u201d).", "keywords": {"keywords": ["After-action Review", "AI", "AAR/AI", "adaptation", "technique"], "research type": ["Review"], "methodology": [], "purposes": [], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "DARPA's explainable AI (XAI) program: A retrospective", "author": "D Gunning", "abstract": "DARPA formulated the Explainable Artificial Intelligence (XAI) program in 2015 with the goal to enable end users to better understand, trust, and effectively manage artificially intelligent systems. In 2017, the four-year XAI research program began. Now, as XAI comes to an end in 2021, it is time to reflect on what succeeded, what failed, and what was learned. This article summarizes the goals, organization, and research progress of the XAI Program.", "keywords": {"keywords": ["DARPA", "Explainable AI", "XAI program", "end users", "artificially intelligent systems"], "research type": ["Retrospective"], "methodology": ["Mixed-methods"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "Finding AI's faults with AAR/AI: An empirical study", "author": "R Khanna", "abstract": "Would you allow an AI agent to make decisions on your behalf? If the answer is \u201cnot always,\u201d the next question becomes \u201cin what circumstances\u201d? Answering this question requires human users to be able to assess an AI agent\u2014and not just with overall pass/fail assessments or statistics. Here users need to be able to localize an agent's bugs so that they can determine when they are willing to rely on the agent and when they are not. After-Action Review for AI (AAR/AI), a new AI assessment process for integration with Explainable AI", "keywords": {"keywords": ["AI agent", "after-action review", "bugs", "assess", "localize"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Technical"], "application area": ["Explainable AI"]}}, {"title": "Position: We Can Measure XAI Explanations Better with Templates.", "author": "J Dodge", "abstract": "This paper argues that the Explainable AI (XAI) research community needs to think harder about how to compare, measure, and describe the quality of XAI explanations. We conclude that one (or a few) explanations can be reasonably assessed with methods of the \u201cExplanation Satisfaction\u201d type, but that scaling up our ability to evaluate explanations requires more development of \u201cExplanation Goodness\u201d methods.", "keywords": {"keywords": ["Explainable AI", "XAI explanations", "Templates", "Comparison", "Measurement"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "From \u201cno clear winner\u201d to an effective Explainable Artificial Intelligence process: An empirical journey", "author": "J Dodge", "abstract": "\u201cIn what circumstances would you want this AI to make decisions on your behalf?\u201d We have been investigating how to enable a user of an Artificial Intelligence\u2010powered system to answer questions like this through a series of empirical studies, a group of which we summarize here. We began the series by (a) comparing four explanation configurations of saliency explanations and/or reward explanations. From this study we learned that, although some configurations had significant strengths, no one configuration was a clear", "keywords": {"keywords": ["Explainable Artificial Intelligence", "empirical studies", "saliency explanations", "reward explanations", "decision-making"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Decision-making"]}}, {"title": "Doing remote controlled studies with humans: Tales from the COVID trenches", "author": "R Dikkala", "abstract": "How should empirical researchers conduct controlled, remote \u201clab\u201d studies in the uncontrolled, noisy conditions of each participant's own home? Volatility in participant home environments, hardware, internet connection, and surrounding distractions takes the \u201ccontrolled\u201d out of controlled studies. This paper recounts our in-the-trenches mitigations for designing and conducting two complex controlled studies under COVID, in which participants, from home, interactively localized faults in an AI system. The studies with our", "keywords": {"keywords": ["remote controlled studies", "uncontrolled conditions", "participant home environments", "AI system", "COVID trenches"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Computer Science"], "content": ["Application"], "application area": "Human Computer Interaction"}}, {"title": "How Do People Rank Multiple Mutant Agents?", "author": "J Dodge", "abstract": "Faced with several AI-powered sequential decision-making systems, how might someone choose on which to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (ie, too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) Explanation Resolution, a quantifiable direct measurement concept; 2) a new", "keywords": {"keywords": ["AI-powered decision-making systems", "self-driving car", "ML model", "Explanation Resolution", "measurement"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Decision-making"]}}, {"title": "\u201cWhy did my AI agent lose?\u201d: Visual Analytics for Scaling Up After-Action Review", "author": "D Tabatabai", "abstract": "How can we help domain-knowledgeable users who do not have expertise in AI analyze why an AI agent failed? Our research team previously developed a new structured process for such users to assess AI, called After-Action Review for AI (AAR/AI), consisting of a series of steps a human takes to assess an AI agent and formalize their understanding. In this paper, we investigate how the AAR/AI process can scale up to support reinforcement learning (RL) agents that operate in complex environments. We augment the AAR/AI", "keywords": {"keywords": ["AI agent", "after-action review", "visual analytics", "scaling up", "reinforcement learning (RL)"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["artificial intelligence"]}}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge", "abstract": "Assessing AI systems is difficult. Humans rely on AI systems in increasing ways, both visible and invisible, meaning a variety of stakeholders need a variety of assessment tools (eg, a professional auditor, a developer, and an end user all have different needs). We posit that it is possible to provide explanations and assessment processes that enable AI non-experts observing multiple intelligent agents in sequential domains to differentiate the agents with respect to a property (eg, quality or fairness), as well as articulate justification for their", "keywords": {"keywords": ["AI systems", "assessment tools", "manipulable properties", "explanations", "assessment processes"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI assessment"]}}, {"title": "Assessing and Finding Faults in AI: Two Empirical Studies", "author": "R Khanna", "abstract": "With the advent of Artificial Intelligence (AI) in every sphere of life in today's day and age, it has become increasingly important for non-AI experts to be able to comprehend the underlying logic of how AI systems work, assess them and find faults in these systems, particularly when they are used in high risk scenarios such as in military strategies and medical applications. Recent developments to address the need to open the black boxes of these AI-powered systems have led to the emergence of AI explanations. There now exist", "keywords": {"keywords": ["AI", "assessing faults", "finding faults", "empirical studies", "AI explanations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["AI systems"]}}], "leveraging rationales to improve human task performance": [{"title": "Aligning faithful interpretations with their social attribution", "author": "A Jacovi", "abstract": "We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful", "keywords": {"keywords": ["faithful interpretations", "social attribution", "causal chain of decisions", "interpretation by textual highlights", "aligned faithfulness"], "research type": ["Case study"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Interpretability in machine learning"]}}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan", "abstract": "Explainability of AI systems is critical for users to take informed actions and hold systems accountable. While\" opening the opaque box\" is important, understanding who opens the box can govern if the Human-AI interaction is effective. In this paper, we conduct a mixed-methods study of how two different groups of whos--people with and without a background in AI--perceive different types of AI explanations. These groups were chosen to look at how disparities in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively", "keywords": {"keywords": ["Explainable AI", "AI explanation", "Human-AI interaction", "AI background", "Creator-consumer gap"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["AI explainability"]}}, {"title": "Explainable ai for robot failures: Generating explanations that improve user assistance in fault recovery", "author": "D Das", "abstract": "With the growing capabilities of intelligent systems, the integration of robots in our everyday life is increasing. However, when interacting in such complex human environments, the occasional failure of robotic systems is inevitable. The field of explainable AI has sought to make complex-decision making systems more interpretable but most existing techniques target domain experts. On the contrary, in many failure cases, robots will require recovery assistance from non-expert users. In this work, we introduce a new type of explanation, \u03b5err", "keywords": {"keywords": ["explainable AI", "robot failures", "user assistance", "fault recovery", "interpretability"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Robotics"]}}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi", "abstract": "After Deep Learning (DL) regained popularity recently, the Artificial Intelligence (AI) or Machine Learning (ML) field is undergoing rapid growth concerning research and real-world application development. Deep Learning has generated complexities in algorithms, and researchers and users have raised concerns regarding the usability and adoptability of Deep Learning systems. These concerns, coupled with the increasing human-AI interactions, have created the emerging field that is Human-Centered Machine Learning", "keywords": {"keywords": ["deep learning", "human-centered machine learning", "artificial intelligence", "machine learning", "usability"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI research"]}}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable AI", "author": "M Chromik", "abstract": "Unintended consequences of deployed AI systems fueled the call for more interpretability in AI systems. Often explainable AI (XAI) systems provide users with simplifying local explanations for individual predictions but leave it up to them to construct a global understanding of the model behavior. In this work, we examine if non-technical users of XAI fall for an illusion of explanatory depth when interpreting additive local explanations. We applied a mixed methods approach consisting of a moderated study with 40 participants and", "keywords": {"keywords": ["explainable AI", "illusion of explanatory depth", "interpretability", "local explanations", "mixed methods approach"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["AI interpretation"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai", "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of", "keywords": {"keywords": ["human-ai decision making", "empirical studies", "AI assistance", "high-stakes domains", "augmented decision making"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["criminal justice and healthcare"]}}, {"title": "Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning", "author": "KZ Gajos", "abstract": "When people receive advice while making difficult decisions, they often make better decisions in the moment and also increase their knowledge in the process. However, such incidental learning can only occur when people cognitively engage with the information they receive and process this information thoughtfully. How do people process the information and advice they receive from AI, and do they engage with it deeply enough to enable learning? To answer these questions, we conducted three experiments in which individuals", "keywords": {"keywords": ["AI assistance", "incidental learning", "cognitive engagement", "information processing", "advice processing"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Psychology"], "content": ["Methodological"], "application area": ["Decision making"]}}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik", "abstract": "The interdisciplinary field of explainable artificial intelligence (XAI) aims to foster human understanding of black-box machine learning models through explanation-generating methods. Although the social sciences suggest that explanation is a social and iterative process between an explainer and an explainee, explanation user interfaces and their user interactions have not been systematically explored in XAI research yet. Therefore, we review prior XAI research containing explanation user interfaces for ML-based intelligent systems", "keywords": {"keywords": ["human-XAI interaction", "explanation user interfaces", "machine learning", "explainable artificial intelligence", "design principles"], "research type": "Review", "methodology": "Qualitative", "purposes": "Descriptive", "discipline": "Computer Science", "content": "Methodological", "application area": "Artificial Intelligence"}}, {"title": "Aid of a Machine Learning Algorithm Can Improve Clinician Predictions of Patient Quality of Life During Breast Cancer Treatments", "author": "M Nuutinen", "abstract": "Proper and well-timed interventions may improve breast cancer patient adaptation and quality of life (QoL) through treatment and recovery. The challenge is to identify those patients who would benefit most from a particular intervention. The aim of this study was to measure whether the machine learning prediction incorporated in the clinical decision support system (CDSS) improves clinicians' performance to predict patients' QoL during treatment process. We conducted two user experiments in which clinicians used a CDSS to", "keywords": {"keywords": ["breast cancer", "machine learning algorithm", "clinician predictions", "patient quality of life", "clinical decision support system"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Medicine"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning", "author": "J Ho", "abstract": "Human-Centered Computing and AI are two fields devoted to several cross-intersecting interests in the modern AI design. They consider human factors and the machine learning algorithms to enhance compatibility and reliability for human-robot interaction and cooperation. In this work, we propose a novel design concept for the challenging issues that have raised ethical dilemmas; an augmented ethical causality with successor representation for policy gradient models Human-Centered AI with environments. The proposed system", "keywords": {"keywords": ["Human-Centered AI", "Ethical Causality", "Learning Representation", "Multi-Agent", "Deep Reinforcement Learning"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Design"]}}, {"title": "Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty", "author": "J Jiang", "abstract": "In recent years, AI explainability (XAI) has received wide attention. Although XAI is expected to play a positive role in decision-making and advice acceptance, various opposing effects have also been found. The opposing effects of XAI highlight the critical role of context, especially human factors, in understanding XAI's impacts. This study investigates the effects of providing three types of post-hoc explanations (alternative advice, prediction confidence scores, and prediction rationale) on two context-specific user decision-making outcomes (AI", "keywords": {"keywords": ["AI explainability", "user decision-making", "context-specific", "post-hoc explanations", "human factors"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["AI"]}}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert Users", "author": "C Bove", "abstract": "The increasing usage of complex Machine Learning models for decision-making has raised interest in explainable artificial intelligence (XAI). In this work, we focus on the effects of providing accessible and useful explanations to non-expert users. More specifically, we propose generic XAI design principles for contextualizing and allowing the exploration of explanations based on local feature importance. To evaluate the effectiveness of these principles for improving users' objective understanding and satisfaction, we conduct a", "keywords": {"keywords": ["explainable artificial intelligence", "local feature importance", "understanding", "satisfaction", "non-expert users"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning Explainability"]}}, {"title": "Making SHAP Rap: Bridging Local and Global Insights Through Interaction and Narratives", "author": "M Chromik", "abstract": "The interdisciplinary field of explainable artificial intelligence (XAI) aims to foster human understanding of black-box machine learning models through explanation-generating methods. In practice, Shapley explanations are widely used. However, they are often presented as visualizations and thus leave their interpretation to the user. As such, even ML experts have difficulties interpreting them appropriately. On the other hand, combining visual cues with textual rationales has been shown to facilitate understanding and communicative", "keywords": {"keywords": ["explainable artificial intelligence", "Shapley explanations", "interpretation", "visual cues", "textual rationales"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideation", "author": "Y Wang", "abstract": "Feedback in creativity support tools can help crowdworkers to improve their ideations. However, current feedback methods require human assessment from facilitators or peers. This is not scalable to large crowds. We propose Interpretable Directed Diversity to automatically predict ideation quality and diversity scores, and provide AI explanations\u2014Attribution, Contrastive Attribution, and Counterfactual Suggestions\u2014to feedback on why ideations were scored (low), and how to get higher scores. These explanations provide multi", "keywords": {"keywords": ["Interpretable Directed Diversity", "model explanations", "iterative crowd ideation", "ideation quality", "diversity scores"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Prescriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Creativity support tools"]}}, {"title": "Subgoal-Based Explanations for Unreliable Intelligent Decision Support Systems", "author": "D Das", "abstract": "Intelligent decision support (IDS) systems leverage artificial intelligence techniques to generate recommendations that guide human users through the decision making phases of a task. However, a key challenge is that IDS systems are not perfect, and in complex real-world scenarios may produce incorrect output or fail to work altogether. The field of explainable AI planning (XAIP) has sought to develop techniques that make the decision making of sequential decision making AI systems more explainable to end-users. Critically", "keywords": {"keywords": ["Intelligent decision support", "Unreliable systems", "Explainable AI", "Sequential decision making", "Subgoal-based explanations"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Decision support"]}}, {"title": "Explainable Neural Claim Verification Using Rationalization", "author": "SC Gurrapu", "abstract": "The dependence on Natural Language Processing (NLP) systems has grown significantly in the last decade. Recent advances in deep learning have enabled language models to generate high-quality text at the same level as human-written text. If this growth continues, it can potentially lead to increased misinformation, which is a significant challenge. Although claim verification techniques exist, they lack proper explainability. Numerical scores such as Attention and Lime and visualization techniques such as saliency heat maps are insufficient", "keywords": {"keywords": ["Explainable", "Neural Claim Verification", "Rationalization", "Natural Language Processing", "Misinformation"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Claim verification"]}}], "njm-vis: interpreting neural joint models in nlp,transparency?": [{"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn", "abstract": "Debugging a machine learning model is hard since the bug usually involves the training data and the learning process. This becomes even harder for an opaque deep learning model if we have no clue about how the model actually works. In this survey, we review papers that exploit explanations to enable humans to give feedback and debug NLP models. We call this problem explanation-based human debugging (EBHD). In particular, we categorize and discuss existing work along three dimensions of EBHD (the bug context", "keywords": {"keywords": ["explangation-based human debugging", "NLP models", "training data", "deep learning models", "bug context"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["natural language processing"]}}, {"title": "T3-Vis: visual analytic for Training and fine-Tuning Transformers in NLP", "author": "R Li", "abstract": "Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the model's intrinsic properties and behaviours. Our framework offers an intuitive overview that allows the user to explore different facets of the model (eg, hidden states, attention) through interactive visualization, and allows a suite of built-in"}, {"title": "Towards Explainable Evaluation Metrics for Natural Language Generation", "author": "C Leiter", "abstract": "Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics (such as BERTScore or MoverScore) are based on black-box language models such as BERT or XLM-R. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are transparent. To foster more widespread acceptance of the novel high-quality metrics, explainability thus becomes crucial. In this", "keywords": {"keywords": ["natural language generation", "explanation of evaluation metrics", "BLEU", "BERTScore", "MoverScore"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Natural Language Processing"]}}, {"title": "T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP", "author": "R Li", "abstract": "Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the model's intrinsic properties and behaviours. Our framework offers an intuitive overview that allows the user to explore different facets of the model (eg, hidden states, attention) through interactive visualization, and allows a suite of built-in", "keywords": {"keywords": ["Transformer", "NLP", "visual analytic framework", "training", "fine-tuning"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Natural Language Processing"]}}, {"title": "Clustering and Multimodal Analysis of Participants in Task-Based Discussions", "author": "D Johnson", "abstract": "Participants in task-based conversational interactions are clustered using outcomes of interest that include task performance, satisfaction ratings, and demographic traits. Each cluster is described in terms of the member participants' common characteristics, and we perform participant outlier detection as well. We extract multimodal features of the conversational interaction and analyze how the participant groups differ in terms of these features.", "keywords": {"keywords": ["task-based discussions", "clustering", "multimodal analysis", "outlier detection", "participant groups"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Conversational analysis"]}}], "parallel embeddings: a visualization technique for contrasting learned representations": [{"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi", "abstract": "After Deep Learning (DL) regained popularity recently, the Artificial Intelligence (AI) or Machine Learning (ML) field is undergoing rapid growth concerning research and real-world application development. Deep Learning has generated complexities in algorithms, and researchers and users have raised concerns regarding the usability and adoptability of Deep Learning systems. These concerns, coupled with the increasing human-AI interactions, have created the emerging field that is Human-Centered Machine Learning", "keywords": [["ai", 0.5182], ["ml", 0.4064], ["intelligence", 0.3283], ["learning", 0.3232], ["human", 0.291], ["machine", 0.2826], ["algorithms", 0.2716], ["systems", 0.2577], ["users", 0.2558], ["artificial", 0.2452]]}, {"title": "Embedding comparator: Visualizing differences in global structure and local neighborhoods via small multiples", "author": "A Boggust", "abstract": "Embeddings mapping high-dimensional discrete input to lower-dimensional continuous vector spaces have been widely adopted in machine learning applications as a way to capture domain semantics. Interviewing 13 embedding users across disciplines, we find comparing embeddings is a key task for deployment or downstream analysis but unfolds in a tedious fashion that poorly supports systematic exploration. In response, we present the Embedding Comparator, an interactive system that presents a global comparison of", "keywords": {"keywords": ["embeddings", "comparator", "global structure", "local neighborhoods", "small multiples"], "research type": "Empirical", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Computer science", "content": "Application", "application area": "Machine learning"}}, {"title": "Evaluating the interpretability of generative models by interactive reconstruction", "author": "A Ross", "abstract": "For machine learning models to be most useful in numerous sociotechnical systems, many have argued that they must be human-interpretable. However, despite increasing interest in interpretability, there remains no firm consensus on how to measure it. This is especially true in representation learning, where interpretability research has focused on \u201cdisentanglement\u201d measures only applicable to synthetic datasets and not grounded in human factors. We introduce a task to quantify the human-interpretability of generative model representations", "keywords": {"keywords": ["generative models", "interpretability", "sociotechnical systems", "disentanglement", "human factors"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "embcomp: Visual interactive comparison of vector embeddings", "author": "F Heimerl", "abstract": "This paper introduces embComp, a novel approach for comparing two embeddings that capture the similarity between objects, such as word and document embeddings. We survey scenarios where comparing these embedding spaces is useful. From those scenarios, we derive common tasks, introduce visual analysis methods that support these tasks, and combine them into a comprehensive system. One of embComp's central features are overview visualizations that are based on metrics for measuring differences in the local", "keywords": {"keywords": ["embComp", "embedding", "visual analysis", "comparison", "metrics"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["natural language processing"]}}, {"title": "Design decision framework for ai explanations", "author": "O Anuyah", "abstract": "Explanations can help users of Artificial Intelligent (AI) systems gain a better understanding of the reasoning behind the model's decision, facilitate their trust in AI, and assist them in making informed decisions. Due to its numerous benefits in improving how users interact and collaborate with AI, this has stirred the AI/ML community towards developing understandable or interpretable models to a larger degree, while design researchers continue to study and research ways to present explanations of these models' decisions in a", "keywords": {"keywords": ["AI explanations", "design decision framework", "understandable models", "interpretable models", "user interaction"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Contrastive identification of covariate shift in image data", "author": "ML Olson", "abstract": "Identifying covariate shift is crucial for making machine learning systems robust in the real world and for detecting training data biases that are not reflected in test data. However, detecting covariate shift is challenging, especially when the data consists of high-dimensional images, and when multiple types of localized covariate shift affect different subspaces of the data. Although automated techniques can be used to detect the existence of covariate shift, our goal is to help human users characterize the extent of covariate shift in", "keywords": {"keywords": ["covariate shift", "machine learning", "real world", "training data biases", "high-dimensional images"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Emblaze: Illuminating machine learning representations through interactive comparison of embedding spaces", "author": "V Sivaraman", "abstract": "Modern machine learning techniques commonly rely on complex, high-dimensional embedding representations to capture underlying structure in the data and improve performance. In order to characterize model flaws and choose a desirable representation, model builders often need to compare across multiple embedding spaces, a challenging analytical task supported by few existing tools. We first interviewed nine embedding experts in a variety of fields to characterize the diverse challenges they face and techniques they", "keywords": {"keywords": ["embedding spaces", "machine learning", "representation", "interactive comparison", "model builders"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Vaine: Visualization and ai for natural experiments", "author": "G Guo", "abstract": "Natural experiments are observational studies where the assignment of treatment conditions to different populations occurs by chance \u201cin the wild\u201d. Researchers from fields such as economics, healthcare, and the social sciences leverage natural experiments to conduct hypothesis testing and causal effect estimation for treatment and outcome variables that would otherwise be costly, infeasible, or unethical. In this paper, we introduce VAINE (Visualization and AI for Natural Experiments), a visual analytics tool for identifying and", "keywords": {"keywords": ["natural experiments", "visualization", "AI", "hypothesis testing", "causal effect estimation"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Multi-disciplinary", "content": "Application", "application area": "Social sciences"}}, {"title": "Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings", "author": "K Eckelt", "abstract": "In this work, we propose an interactive visual approach for the exploration of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users", "keywords": {"keywords": ["visual exploration", "relationships", "structure", "low-dimensional embeddings", "interactive"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Data visualization"]}}, {"title": "Towards Multiple Embeddings for Multivariate Network Analysis", "author": "W Daniel", "abstract": "The study of multivariate networks (MVNs, ie, large data sets where data points have relations to other data points and both these relations and the points themselves can have attributed data) is an important task in many different fields, such as social networks for the humanities, citation networks for bibliometrics and biochemical networks for life sciences. Furthermore, when dealing with visualization and analysis of MVNs, many open challenges still exist regarding both computational aspects (ie, the challenge of computing different", "keywords": {"keywords": ["multivariate networks", "embeddings", "visualization", "analysis", "computational aspects"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["network analysis"]}}, {"title": "CNNC: A Visual Analytics System for Comparative Studies of Deep Convolutional Neural Networks", "author": "X Xuan", "abstract": "The rapid development of Convolutional Neural Networks (CNNs) in recent years has triggered significant breakthroughs in many machine learning (ML) applications. The ability to understand and compare various CNN models available is thus essential. The conventional approach with visualizing each model's quantitative features, such as classification accuracy and computational complexity, is not sufficient for a deeper understanding and comparison of the behaviors of different models. Moreover, most of the", "keywords": {"keywords": ["Convolutional Neural Networks", "Visual Analytics", "Comparative Studies", "Machine Learning", "Deep Learning"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning"}}, {"title": "VAC-CNN: A Visual Analytics System for Comparative Studies of Deep Convolutional Neural Networks", "author": "X Xuan", "abstract": "The rapid development of Convolutional Neural Networks (CNNs) in recent years has triggered significant breakthroughs in many machine learning (ML) applications. The ability to understand and compare various CNN models available is thus essential. The conventional approach with visualizing each model's quantitative features, such as classification accuracy and computational complexity, is not sufficient for a deeper understanding and comparison of the behaviors of different models. Moreover, most of the", "keywords": {"keywords": ["Visual Analytics", "Convolutional Neural Networks", "Comparative Studies", "Quantitative Features", "Behavior Analysis"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning"}}], "decision making strategies difer in the presence of collaborative explanations: two conjoint studies": [{"title": "A historical perspective of explainable Artificial Intelligence", "author": "R Confalonieri", "abstract": "Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the \u201chow\u201d and \u201cwhy\u201d of automated decision\u2010making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge\u2010based) expert systems. Since then, the definition", "keywords": {"keywords": ["Explainable Artificial Intelligence", "Safety", "Trust", "Automated decision-making", "Expert systems"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["AI systems"]}}, {"title": "Measuring the impact of online personalisation: Past, present and future", "author": "M Zanker", "abstract": "Research on understanding, developing and assessing personalisation systems is spread over multiple disciplines and builds on methodologies and findings from several different research fields and traditions, such as Artificial Intelligence (AI), Machine Learning (ML), Human\u2013Computer Interaction (HCI), and User Modelling based on (applied) social and cognitive psychology. The fields of AI and ML primarily focus on the optimisation of personalisation applications, and concentrate on creating ever more accurate algorithmic", "keywords": {"keywords": ["online personalisation", "impact measurement", "Artificial Intelligence", "Machine Learning", "Human\u2013Computer Interaction"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["personalisation"]}}, {"title": "Personalising explainable recommendations: literature and conceptualisation", "author": "M Naiseh", "abstract": "Explanations in intelligent systems aim to enhance a users' understandability of their reasoning process and the resulted decisions and recommendations. Explanations typically increase trust, user acceptance and retention. The need for explanations is on the rise due to the increasing public concerns about AI and the emergence of new laws, such as the General Data Protection Regulation (GDPR) in Europe. However, users are different in their needs for explanations, and such needs can depend on their dynamic context. Explanations", "keywords": {"keywords": ["Personalising", "recommendations", "explanations", "intelligent systems", "user needs"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Personalisation"]}}, {"title": "Recommender systems in tourism", "author": "F Ricci", "abstract": "Recommender Systems (RSs) have been extensively  to substantially enrich tourist  experiences, recommending rich  highly accurate and effective tourist recommendations that", "keywords": {"keywords": ["Recommender Systems", "tourism experiences", "tourist recommendations", "accuracy", "effectiveness"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["tourism"]}}, {"title": "Counteracting the filter bubble in recommender systems: Novelty-aware matrix factorization", "author": "P Symeonidis", "abstract": "The search for unfamiliar experiences and novelty is one of the main drivers behind all human activities, equally important with harm avoidance and reward dependence. A recommender system personalizes suggestions to individuals to support and guide them in their exploration tasks. Personalization mechanisms and recommender systems limit serendipitous encounters by selectively guessing the next item to show to users and potentially leading them into so-called filter bubbles. In the ideal case, these", "keywords": {"keywords": ["filter bubble", "recommender systems", "novelty", "matrix factorization", "personalization"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Information retrieval"]}}, {"title": "Choosing between hotels: impact of bimodal rating summary statistics and maximizing behavioral tendency", "author": "L Coba", "abstract": "Rating summary statistics are basic aggregations that reflect users' assessments of experienced products and services in numerical form. Thus far, scholars primarily investigated textual reviews, but dedicated considerably less time and effort exploring the potential impact of plain rating summary statistics on people's choice behavior. Notwithstanding their fundamental nature, however, rating summary statistics also are relevant to electronic commerce in general, and to e-tourism in particular. In this work, we", "keywords": {"keywords": ["rating summary statistics", "hotels", "aggregations", "choice behavior", "e-tourism"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Economics"], "content": ["Application"], "application area": ["Electronic commerce"]}}, {"title": "Ratings in Recommender Systems: Decision Biases and Explainability", "author": "L Coba", "abstract": "Recommender systems are an application of artificial intelligence techniques where typically past behaviour of users is used to make predictions about their interests and to support them in identifying items they presumably like. Over the past two decades and due to Internet's ever-growing plethora of choices recommender systems have received a lot of interest, both from industry and research, and yielded large progress in the field. However, most of the research is about predicting the user's preferences (ie precision-oriented), while focusing", "keywords": {"keywords": ["Recommender systems", "Decision biases", "Explainability", "Predicting preferences", "User interests"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive", "Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "What's the Appeal? Perceptions of Review Processes for Algorithmic Decisions", "author": "H Lyons", "abstract": "If you were signifcantly impacted by an algorithmic decision, how would you want the decision to be reviewed? In this study, we explore perceptions of review processes for algorithmic decisions that difer across three dimensions: the reviewer, how the review is conducted, and how long the review takes. Using a choice-based conjoint analysis we fnd that people prefer review processes that provide for human review, the ability to participate in the review process, and a timely outcome. Using a survey, we fnd that people also see", "keywords": {"keywords": ["algorithmic decision", "review processes", "perceptions", "choice-based conjoint analysis", "survey"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Algorithmic decision-making"]}}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand", "abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI", "keywords": {"keywords": ["cognitive biases", "explainable artificial intelligence (XAI)", "decision-making", "systematic review", "human explanation methods"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": []}}, {"title": "Designing and evaluating recommender systems with the user in the loop", "author": "M Jugovac", "abstract": "On many of today's most popular Internet service platforms, users are confronted with a seemingly endless number of options to choose from, such as articles to purchase on online shopping sites, music to listen to on online streaming platforms, or posts to read on social media. As a solution to this choice overload problem, recommender systems have been integrated into more and more websites and applications to help users find items that they might like or that could be useful in their current choice situation. In recent decades", "keywords": {"keywords": ["recommender systems", "user in the loop", "choice overload", "internet service platforms", "item recommendation"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommendation systems"]}}], "personalized explanations for hybrid recommender systems": [{"title": "PRINCE: Provider-side interpretability with counterfactual explanations in recommender systems", "author": "A Ghazimatin", "abstract": "Interpretable explanations for recommender systems and other machine learning models are crucial to gain user trust. Prior works that have focused on paths connecting users and items in a heterogeneous network have several limitations, such as discovering relationships rather than true explanations, or disregarding other users' privacy. In this work, we take a fresh perspective, and present PRINCE: a provider-side mechanism to produce tangible explanations for end-users, where an explanation is defined to be a set of minimal", "keywords": {"keywords": ["interpretable explanations", "recommender systems", "counterfactual explanations", "PRINCE", "provider-side"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommendation systems"]}}, {"title": "Personalising explainable recommendations: literature and conceptualisation", "author": "M Naiseh"}, {"title": "Pace my race: recommendations for marathon running", "author": "J Berndsen"}, {"title": "An effective recommendation model based on deep representation learning", "author": "J Ni"}, {"title": "AI-based mobile context-aware recommender systems from an information management perspective: Progress and directions", "author": "M del Carmen Rodr\u00edguez-Hern\u00e1ndez"}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checkers", "author": "CH Tsai"}, {"title": "Explanations in autonomous driving: A survey", "author": "D Omeiza"}, {"title": "An efficient hybrid recommendation model with deep neural networks", "author": "Z Huang"}, {"title": "Explainable recommendation for repeat consumption", "author": "K Tsukuda"}, {"title": "A scalable decision-tree-based method to explain interactions in dyadic data", "author": "C Eiras-Franco"}, {"title": "Visual, textual or hybrid: the effect of user expertise on different explanations", "author": "M Szymanski", "abstract": "As the use of AI algorithms keeps rising continuously, so does the need for their transparency and accountability. However, literature often adopts a one-size-fits-all approach for developing explanations when in practice, the type of explanations needed depends on the type of end-user. This research will look at user expertise as a variable to see how different levels of expertise influence the understanding of explanations. The first iteration consists of developing two common types of explanations (visual and textual", "keywords": {"keywords": ["user expertise", "explanations", "transparency", "accountability", "AI algorithms"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "AI Explanation"}}, {"title": "The seven layers of complexity of recommender systems for children in educational contexts", "author": "E Murgia"}, {"title": "A Bayesian inference based hybrid recommender system", "author": "AN Ngaffo"}, {"title": "Explaining need-based educational recommendations using interactive open learner models", "author": "J Barria-Pineda"}, {"title": "Generating and understanding personalized explanations in hybrid recommender systems", "author": "P Kouki"}, {"title": "Toward personalized XAI: A case study in intelligent tutoring systems", "author": "C Conati"}, {"title": "An algorithm independent case-based explanation approach for recommender systems using interaction graphs", "author": "M Caro-Martinez"}, {"title": "DualDiv: diversifying items and explanation styles in explainable hybrid recommendation", "author": "K Tsukuda"}, {"title": "User and item-aware estimation of review helpfulness", "author": "N Mauro"}, {"title": "Best practices for transparency in machine generated personalization", "author": "L Schelenz"}, {"title": "Effects of interactivity and presentation on review-based explanations for recommendations", "author": "DC Hernandez-Bocanegra", "abstract": "User reviews have become an important source for recommending and explaining products or services. Particularly, providing explanations based on user reviews may improve users' perception of a recommender system (RS). However, little is known about how review-based explanations can be effectively and efficiently presented to users of RS. We investigate the potential of interactive explanations in review-based RS in the domain of hotels, and propose an explanation scheme inspired by dialogue models and formal argument", "keywords": {"keywords": ["user reviews", "recommendations", "explanations", "interactivity", "presentation"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Hotel RS"]}}, {"title": "ChoseAmobile: A Web-based Recommendation System for Mobile Phone Products", "author": "S Akram"}, {"title": "From Philosophy to Interfaces: An Explanatory Method and a Tool Inspired by Achinstein's Theory of Explanation", "author": "F Sovrano"}, {"title": "\u201cKnowing me, knowing you\u201d: personalized explanations for a music recommender system", "author": "M Martijn"}, {"title": "Explaining review-based recommendations: Effects of profile transparency, presentation style and user characteristics", "author": "DC Hernandez-Bocanegra"}, {"title": "Service-aware interactive presentation of items for decision-making", "author": "N Mauro"}, {"title": "Addressing cold start challenges in recommender systems: towards a new hybrid approach", "author": "N Idrissi"}, {"title": "Open, Scrutable and Explainable Interest Models for Transparent Recommendation.", "author": "M Guesmi"}, {"title": "Explainability in Music Recommender Systems", "author": "D Afchar"}, {"title": "Hybrid ecommerce recommendation model incorporating product taxonomy and folksonomy", "author": "M Mao"}, {"title": "Toward XAI for Intelligent Tutoring Systems: a case study", "author": "V Putnam", "abstract": "Our research is a step toward understanding when explanations of AI-driven hints and feedback are useful in Intelligent Tutoring Systems (ITS). We added an explanation functionality for the adaptive hints provided by the Adaptive CSP (ACSP) applet, an intelligent interactive simulation that helps students learn an algorithm for constraint satisfaction problems. We present the design of the explanation functionality and the results of an exploratory study to evaluate how students use it, including an analysis of how", "keywords": {"keywords": ["XAI", "Intelligent Tutoring Systems", "explanation functionality", "adaptive hints", "exploratory study"], "research type": ["case study"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["education"]}}, {"title": "Input or Output: Effects of Explanation Focus on the Perception of Explainable Recommendation with Varying Level of Details.", "author": "M Guesmi"}, {"title": "AI-Assisted and explainable hate speech detection for social media moderators\u2013A design science approach", "author": "E Bunde"}, {"title": "Controllability and explainability in a hybrid social recommender system", "author": "CH Tsai"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "On-demand personalized explanation for transparent recommendation", "author": "M Guesmi"}, {"title": "Recommender system: Towards classification of human intentions in e-shopping using machine learning", "author": "B Kaur"}, {"title": "Do Channels Matter? Illuminating Interpersonal Influence on Music Recommendations", "author": "HJ Kim"}, {"title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis.", "author": "CH Tsai"}, {"title": "Beyond explaining single item recommendations", "author": "N Tintarev"}, {"title": "Explainable artificial intelligence in education", "author": "H Khosravi", "abstract": "There are emerging concerns about the Fairness, Accountability, Transparency, and Ethics (FATE) of educational interventions supported by the use of Artificial Intelligence (AI) algorithms. One of the emerging methods for increasing trust in AI systems is to use eXplainable AI (XAI), which promotes the use of methods that produce transparent explanations and reasons for decisions AI systems make. Considering the existing literature on XAI, this paper argues that XAI in education has commonalities with the broader use of AI", "keywords": {"keywords": ["explainable AI", "education", "fairness", "accountability", "transparency"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["education"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Visualizing Program Genres' Temporal-Based Similarity in Linear TV Recommendations", "author": "V Bogina"}, {"title": "Why or why not? The effect of justification styles on chatbot recommendations", "author": "D Wilkinson"}, {"title": "A literature review of recommendation systems", "author": "K Bhareti"}, {"title": "On the Use of Feature-based Collaborative Explanations: An Empirical Comparison of Explanation Styles", "author": "S Naveed"}, {"title": "Path-based reasoning over heterogeneous networks for recommendation via bidirectional modeling", "author": "J Zhang"}, {"title": "A Mobile Guide to Explore Interconnections between Science, Art and Territory", "author": "N Mauro"}, {"title": "Recent studies of xai-review", "author": "ZF Hu"}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trust", "author": "J Ooge"}, {"title": "Development of an Instrument for Measuring Users' Perception of Transparency in Recommender Systems", "author": "M Hellmann"}, {"title": "Justifying Social-Choice Mechanism Outcome for Improving Participant Satisfaction", "author": "SA Suryanarayana", "abstract": "In many social-choice mechanisms the resulting choice is not the most preferred one for some of the participants, thus the need for methods to justify the choice made in a way that improves the acceptance and satisfaction of said participants. One natural method for providing such explanations is to ask people to provide them, eg, through crowdsourcing, and choosing the most convincing arguments among those received. In this paper we propose the use of an alternative approach, one that automatically generates explanations", "keywords": {"keywords": ["social-choice mechanism", "participant satisfaction", "justification", "explanations", "automatic generation"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["social-choice mechanisms"]}}, {"title": "A Multi-Dimensional Conceptualization Framework for Personalized Explanations in Recommender Systems", "author": "QU Ain"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky"}, {"title": "Natural Language Processing for Recommender Systems", "author": "OS Shalom"}, {"title": "A service-oriented perspective on the summarization of recommendations: Preliminary experiment", "author": "N Mauro"}, {"title": "Improving User Experience Through Recommendation Message Design: A Systematic Literature Review of Extant Literature on Recommender Systems and Message \u2026", "author": "A Falconnet"}, {"title": "Music recommendation systems: A survey", "author": "M Kle\u0107"}, {"title": "Exploring Semantically Interlaced Cultural Heritage Narratives", "author": "N Mauro"}, {"title": "A Survey on Current Recommender Systems", "author": "H Pfaff"}, {"title": "Study and Classification of Recommender Systems: A Survey", "author": "M Sharma"}, {"title": "Conversational review-based explanations for recommender systems: Exploring users' query behavior", "author": "DC Hernandez-Bocanegra", "abstract": "Providing explanations based on user reviews in recommender systems (RS) can increase users' perception of system transparency. While static explanations are dominant, interactive explanatory approaches have emerged in explainable artificial intelligence (XAI), so that users are more likely to examine system decisions and get more arguments supporting system assertions. However, little attention has been paid to conversational approaches for explanations targeting end users. In this paper we explore how to design a conversational", "keywords": {"keywords": ["conversational review-based explanations", "recommender systems", "user reviews", "system transparency", "conversation design"], "research type": ["Exploratory"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Recommendation systems"]}}, {"title": "A Recommender System for Mobility-as-a-Service Plans Selection", "author": "K Arnaoutaki"}, {"title": "Mitigating Bias in Algorithmic Systems: A Fish-Eye View of Problems and Solutions Across Domains", "author": "K Orphanou"}, {"title": "VMI-PSL: Visual Model Inspector for Probabilistic Soft Logic", "author": "A Rodden"}, {"title": "Inferring Local and Global Properties in Knowledge Graphs", "author": "VR Embar"}, {"title": "Extended recommendation-by-explanation", "author": "A Rana"}, {"title": "Service-aware Recommendation and Justification of Results", "author": "ZF Hu"}, {"title": "Learning Explainable Templated Graphical Models", "author": "VR Embar"}, {"title": "Generating Personalized Explanations for Recommender Systems Using a Knowledge Base", "author": "Y Chen"}, {"title": "A Survey of Techniques for the Evaluation of Explanations in Recommender Systems", "author": "M Caro-Martinez"}, {"title": "Rating consistency is consistently underrated: an exploratory analysis of movie-tag rating inconsistency", "author": "D Kotkov", "abstract": "Content-based and hybrid recommender systems rely on item-tag ratings to make recommendations. An example of an item-tag rating is the degree to which the tag\" comedy\" applies to the movie\" Back to the Future (1985)\". Ratings are often generated by human annotators who can be inconsistent with one another. However, many recommender systems take item-tag ratings at face value, assuming them all to be equally valid. In this paper, we investigate the inconsistency of item-tag ratings together with contextual factors", "keywords": {"keywords": ["rating consistency", "movie-tag rating", "recommender systems", "item-tag ratings", "inconsistency"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Recommender Systems"]}}, {"title": "Course Design: Modern Visual Recommender Systems", "author": "KX Thia"}, {"title": "Building effective recommender systems for tourists", "author": "D Massimo"}, {"title": "Service-Aware Personalized Item Recommendation", "author": "N Mauro"}, {"title": "Recommendation System Issues, Approaches and Challenges Based on User Reviews.", "author": "K Benabbes"}, {"title": "Explanation-Based Serendipitous Recommender System (EBSRS)", "author": "C Sharma"}, {"title": "Resolution, Recommendation, and Explanation in Richly Structured Social Networks", "author": "P Kouki"}, {"title": "Recommender System with Explanations", "author": "FPP Rocha"}, {"title": "Enhancing explainability and scrutability of recommender systems", "author": "A Ghazimatin"}, {"title": "ReEx: An integrated architecture for preference model representation and explanation", "author": "F Zafari"}, {"title": "Balancing the trade-off between accuracy and diversity in recommender systems with personalized explanations based on Linked Open Data", "author": "AL Zanon", "abstract": "Collaborative filtering recommendation algorithms generate suggestions based on similar interactions between users. Although it provides accurate recommendations, the approach has two limitations: the popularity bias, which frequently suggests a small set of the most interacted items, and the systems' black box functioning, as they are grounded on complex mathematical models. To improve such aspects in collaborative filtering algorithms, this paper introduces a multi-domain item reordering system based on the best explanation for", "keywords": {"keywords": ["recommendation systems", "accuracy", "diversity", "linked open data", "personalized explanations"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommendation systems"]}}, {"title": "Towards Fast And Accurate Structured Prediction", "author": "S Srinivasan"}, {"title": "A Diary Study of Social Explanations for Recommendations in Daily Life", "author": "Z Zhang"}, {"title": "Categorization of Knowledge Graph based Recommendation Methods and Benchmark Datasets from the Perspectives of Application Scenarios: A Comprehensive \u2026", "author": "N Khan"}, {"title": "Applying Transparency in Artificial Intelligence based Personalization Systems", "author": "L Schelenz"}, {"title": "Mitigating Bias in Algorithmic Systems-A Fish-Eye View", "author": "K Orphanou"}, {"title": "The challenges of providing explanations of AI systems when they do not behave like users expect", "author": "M Riveiro"}, {"title": "Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning", "author": "C Kim"}, {"title": "Similarity and explanation for dynamic telecommunication engineer support.", "author": "K Martin"}, {"title": "Hot Topics in Intelligent User Interfaces", "author": "C Ozcan"}, {"title": "Is More Always Better? The Effects of Personal Characteristics and Level of Detail on the Perception of Explanations in a Recommender System", "author": "MA Chatti", "abstract": "Despite the acknowledgment that the perception of explanations may vary considerably between end-users, explainable recommender systems (RS) have traditionally followed a one-size-fits-all model, whereby the same explanation level of detail is provided to each user, without taking into consideration individual user's context, ie, goals and personal characteristics. To fill this research gap, we aim in this paper at a shift from a one-size-fits-all to a personalized approach to explainable recommendation by giving users agency in", "keywords": {"keywords": ["explainable recommender systems", "perception of explanations", "personal characteristics", "level of detail", "one-size-fits-all model"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Personalization"]}}, {"title": "Argumentative explanations for recommendations-Effect of display style and profile transparency", "author": "DC Hernandez-Bocanegra"}, {"title": "H2E: A Privacy Provisioning Framework for Collaborative Filtering Recommender System", "author": "MU Ashraf"}, {"title": "Institutional Recommendation and Ranking System Based on Integrated Datasets and Analysis", "author": "V Kumar"}, {"title": "Explanation-Based Serendipitous Recommender System (EBSRS)", "author": "CS Richa"}, {"title": "Explaining User Models with Different Levels of Detail for Transparent Recommendation: A User Study", "author": "M Guesmi"}, {"title": "Creating a Personalised Experience for Libraries' Visitors", "author": "A Giannaros"}, {"title": "Effects of Feature-Based Explanation and Its Output Modality on User Satisfaction With Service Recommender Systems", "author": "Z Zhang"}, {"title": "Investigating the efficacy of persuasive strategies on promoting fair recommendations", "author": "SM Mousavifar"}, {"title": "Personalized Interfaces for Music Recommender Systems", "author": "M Millecamp"}, {"title": "A Machine Learning Model for Understanding How Users Value Designs: Applications for Designers and Consumers", "author": "J Bilotti", "abstract": "In this thesis, I demonstrate a number of advances toward developing a machine learning (ML) model of how designs are valued by their users. The model can be used to better understand the implications of furniture design decisions, as well as for commercial strategy. Existing ML systems have been trained on the physical and aesthetic features of completed furniture designs. We consider these methods to be \u201ctop-down\u201d because designers and software engineers alone determine which features are considered important to the value of", "keywords": {"keywords": ["machine learning model", "design valuation", "furniture design", "commercial strategy", "top-down"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Design"]}}, {"title": "Sistema recomendador h\u00edbrido basado en modelos probabil\u00edsticos", "author": "PM Valdiviezo Diaz"}, {"title": "\u7e70\u308a\u8fd4\u3057\u6d88\u8cbb\u3055\u308c\u308b\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u5bfe\u8c61\u3068\u3057\u305f\u63a8\u85a6\u7406\u7531\u306e\u63d0\u793a", "author": "\u4f43\u6d38\u6442\uff0c \u5f8c\u85e4\u771f\u5b5d"}, {"title": "Hybrid Music Recommendation Approach for Heterogeneous Information Network using Factorization Machines/submitted by Majd Azzam", "author": "M Azzam"}, {"title": "\uc74c\uc545 \ucd94\ucc9c\uc5d0\uc11c \ub300\uc778 \uac04 \ucd94\ucc9c\uacfc \uc2dc\uc2a4\ud15c \ucd94\ucc9c\uc5d0 \ub300\ud55c \ube44\uad50 \uc5f0\uad6c", "author": "\uae40\ud604\uc815"}, {"title": "Sistema Recomanador de productes d'hosteleria", "author": "P Renau Larrod\u00e9"}], "fontmatcher: font image paring for harmonious digital graphic design": [{"title": "Emotype: Expressing emotions by changing typeface in mobile messenger texting", "author": "S Choi", "abstract": "Instant messaging is a popular form of text-based communication. However, text-based messaging lacks the ability to communicate nonverbal information such as that conveyed through facial expressions and voice tones, although a multitude of emotions may underlie the text of a conversation between participants. In this paper, we propose an approach that uses typefaces to communicate emotions. We investigated which typefaces are useful for delivering emotions and introduced these typefaces into a mobile chat app. We conducted a", "keywords": {"keywords": ["emotype", "expressing emotions", "typeface", "mobile messenger texting", "nonverbal information"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Mobile communication"]}}, {"title": "Serif or sans: Visual font analytics on book covers and online advertisements", "author": "Y Shinahara", "abstract": "In this paper, we conduct a large-scale study of font statistics in book covers and online advertisements. Through the statistical study, we try to understand how graphic designers relate fonts and content genres and identify the relationship between font styles, colors, and genres. We propose an automatic approach to extract font information from graphic designs by applying a sequence of character detection, style classification, and clustering techniques to the graphic designs. The extracted font information is accumulated together with genre", "keywords": {"keywords": ["font statistics", "book covers", "online advertisements", "visual font analytics", "content genres"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Graphic Design"]}}, {"title": "Assist users' interactions in font search with unexpected but useful concepts generated by multimodal learning", "author": "S Choi", "abstract": "When searching for suitable fonts for a digital graphic, users usually start with an ambiguous thought. For example, they would look for fonts that are suitable for a personal web page or party invitations for children. Their design concept becomes clearer as they interact with external interventions such as exposure to suitable images for use in their web page or the children's preferences regarding the party. Hence, it is important to support users' interactions with unexpected but useful concepts during their search. In this paper, we", "keywords": {"keywords": ["font search", "multimodal learning", "user interactions", "unexpected concepts", "suitable fonts"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Graphic design"]}}, {"title": "Which parts determine the impression of the font?", "author": "M Ueda", "abstract": "Various fonts give different impressions, such as legible, rough, and comic-text. This paper aims to analyze the correlation between the local shapes, or parts, and the impression of fonts. By focusing on local shapes instead of the whole letter shape, we can realize more general analysis independent from letter shapes. The analysis is performed by newly combining SIFT and DeepSets, to extract an arbitrary number of essential parts from a particular font and aggregate them to infer the font impressions by nonlinear regression. Our", "keywords": {"keywords": ["fonts", "impressions", "local shapes", "SIFT", "DeepSets"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Typography"]}}, {"title": "\u201cI Kept Browsing and Browsing, But Still Couldn't Find the One\u201d: Salient Factors and Challenges in Online Typeface Selection", "author": "Y Wayne Wu", "abstract": "Web fonts quickly gained popularity among practitioners. Despite their wide-spread usage and critical role in design, there is a lack of empirical research regarding how practitioners select web fonts and what problems they encounter in the process. To fill this knowledge gap, we took a mixed-method approach to examine the salient factors and common issues in the typeface selection process. To understand the landscape of the problem, we first analyzed adoption data for Google Fonts, a representative online fonts directory. Then, we", "keywords": {"keywords": ["web fonts", "typeface selection", "online typography", "empirical research", "process challenges"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Design"], "content": ["Methodological"], "application area": ["Graphic design"]}}, {"title": "Logo design analysis by ranking", "author": "T Karamatsu", "abstract": "analyze logo designs by using machine learning, as a promising trial of graphic design analysis.   methods for ranking and very suitable for revealing the subtle trends in graphic designs.", "keywords": {"keywords": ["logo design", "analysis", "ranking", "machine learning", "graphic design"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Graphic Design Analysis"]}}, {"title": "Shared Latent Space of Font Shapes and Impressions", "author": "J Kang", "abstract": "We have specific impressions from the style of a typeface (font), suggesting that there are correlations between font shape and its impressions. Based on this hypothesis, we realize a shared latent space where a font shape image and its impression words are embedded in a cross-modal manner. This latent space is useful to understand the style-impression correlation and generate font images by specifying several impression words. Experimental results with a large style-impression dataset prove that it is possible to accurately realize the", "keywords": {"keywords": ["font shapes", "impressions", "shared latent space", "cross-modal embedding", "style-impression correlation"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Typography"]}}, {"title": "Design of fractal art design image based on one-dimensional MFDMA algorithm", "author": "C Shi", "abstract": "There are obvious artificial effects in the constructed art design images such as iterative hard thresholding and other traditional algorithms, especially when the sampling rate is low. These artificial effects not only seriously influence the visual effect of constructed art design images, but also influence the performance of subsequent processing. One-dimensional MFDMA algorithm for art design image in this paper takes the construction results of the traditional construction algorithm as input. One-dimensional MFDMA algorithm is used to", "keywords": {"keywords": ["fractal art design", "MFDMA algorithm", "constructed art design images", "iterative hard thresholding", "sampling rate"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["art and design"]}}, {"title": "WordBlender: principles and tools for generating word blends", "author": "SH Ross", "abstract": "Combining text and images is a powerful strategy in graphic design because images convey meaning faster, but text conveys more precise meaning. Word blends are a technique to combine both elements in a succinct yet expressive image. In a word blend, a letter is replaced by a symbol relevant to the message. This is difficult because the replacement must look blended enough to be readable, yet different enough to recognize the symbol. Currently, there are no known design principles to find aesthetically pleasing word blends", "keywords": {"keywords": ["word blends", "graphic design", "text and images", "design principles", "aesthetically pleasing"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": ["graphic design"], "content": ["methodological"], "application area": ["visual communication"]}}, {"title": "Affect-Aware Word Clouds", "author": "T Kulahcioglu", "abstract": "We present an affect-aware font and color palette selection  both on individual words as well  as in word clouds. Relying on  impact of color choices on word clouds. Our findings suggest", "keywords": {"keywords": ["affect-aware", "word clouds", "font selection", "color palette selection", "impact of color choices"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Data Visualization"]}}, {"title": "Social Font Search by Multimodal Feature Embedding", "author": "S Choi", "abstract": "A typical tag/keyword-based search system retrieves documents where, given a query term q, the query term q occurs in the dataset. However, when applying these systems to a real-world font web community setting, practical challenges arise---font tags are more subjective than other benchmark datasets, which magnify the tag mismatch problem. To address these challenges, we propose a tag dictionary space leveraged by word embedding, which relates undefined words that have a similar meaning. Even if a query is", "keywords": {"keywords": ["social font search", "multimodal feature embedding", "tag mismatch problem", "word embedding", "real-world font web community"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Information retrieval"]}}, {"title": "Primal Heuristics for Branch-and-Price: the assets of diving methods", "author": "R Sadykov", "abstract": "Primal heuristics have become essential components in mixed integer programming (MIP) solvers. Extending MIP-based heuristics, our study outlines generic procedures to build primal solutions in the context of a branch-and-price approach and reports on their performance. Our heuristic decisions carry on variables of the Dantzig\u2013Wolfe reformulation, the motivation being to take advantage of a tighter linear programming relaxation than that of the original compact formulation and to benefit from the combinatorial structure embedded in", "keywords": {"keywords": ["MIP solvers", "primal heuristics", "branch-and-price", "Dantzig-Wolfe reformulation", "combinatorial structure"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": []}}, {"title": "Harmonized Banner Creation from Multimodal Design Assets", "author": "P Vaddamanu", "abstract": "Designing aesthetically pleasing single-page graphic designs (\u201d banners\u201d) that appeal to the target recipients is non-trivial and requires considerable human effort. Designers often start by collecting assets like background images, inspirational banners, and relevant taglines/textual phrases, and iterate over variants of the designs by testing different combinations of these assets for their current task. To expand creative processes beyond professionals, it is crucial to accelerate this tedious process of creating design variants. To", "keywords": {"keywords": ["banner creation", "multimodal design assets", "aesthetically pleasing", "design variants", "creative processes"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["graphic design"]}}, {"title": "Shared Latent Space of Font Shapes and Their Noisy Impressions", "author": "J Kang", "abstract": "Styles of typefaces or fonts are often associated with specific impressions, such as heavy, contemporary, or elegant. This indicates that there are certain correlations between font shapes and their impressions. To understand the correlations, this paper constructs a shared latent space where a font and its impressions are embedded nearby. The difficulty is that the impression words attached to a font are often very noisy. This is because impression words are very subjective and diverse. More importantly, some impression words have no", "keywords": {"keywords": ["font shapes", "impressions", "latent space", "subjective", "correlations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Typography"]}}, {"title": "Font Shape-to-Impression Translation", "author": "M Ueda", "abstract": "Different fonts have different impressions, such as elegant, scary, and cool. This paper tackles part-based shape-impression analysis based on the Transformer architecture, which is able to handle the correlation among local parts by its self-attention mechanism. This ability will reveal how combinations of local parts realize a specific impression of a font. The versatility of Transformer allows us to realize two very different approaches for the analysis, ie, multi-label classification and translation. A quantitative evaluation shows that our", "keywords": {"keywords": ["Font Shape", "Impression", "Transformer", "Part-based Analysis", "Multi-label Classification"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": "Typography Analysis"}}, {"title": "Computational Approaches for Semantics-Aware Typographical Choices", "author": "T Kulahcioglu", "abstract": "Typographic signals carry strong semantic connotations, eg, they may convey excitement, anger or even sweetness, which empowers them to affect almost any aspect of life, from perception of an email, to the perceived sweetness of a cup of coffee. This thesis explores some of the possibilities that can be offered by computational approaches to support users in understanding and taking advantage of this impact. More specifically, the focus is on learning font semantics from crowdsourced and Web data, and using this information to", "keywords": {"keywords": ["typography", "semantics", "computational approaches", "font semantics", "perceived impact"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["usability and user experience"]}}, {"title": "Panduan Pelatihan Perencanaan Studi Lanjut Berbasis Four C's", "author": "PR Pambudi", "abstract": "Increased students' advanced study planning skills required the Four C's guidance base. The purpose of this study is to produce a Four C's-based advanced study planning training guide for Junior Counselors. This research is a development with the ADDIE model. The subject of the study was Marsudisiwi Middle School Counselor. Data collection methods, namely, questionnaire needs, interview guidelines, scale of further study planning, and training guide assessment format. Data analysis using descriptive analysis, and interrater"}], "opportunity team builder for sales teams": [{"title": "Joining together online: the trajectory of CSCW scholarship on group formation", "author": "AM Harris", "abstract": "The field of Computer Supported Cooperative Work (CSCW) has an enduring interest in studying and designing technologies that bring people together in partnerships, teams, crowds, communities, and other collectives. As the technologies enabling group formation have evolved, so too have the guiding questions pursued by CSCW scholars. This review outlines the trajectory of scholarship on group formation with an eye towards the most pressing future questions in this area. To understand how CSCW researchers have studied", "keywords": {"keywords": ["Computer Supported Cooperative Work", "group formation", "trajectory", "technology", "CSCW scholars"], "research type": ["review"], "methodology": [], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["online collaboration"]}}, {"title": "Who would you like to work with?", "author": "D G\u00f3mez-Zar\u00e1", "abstract": "that \u201cmore\u201d might be, however, we have to delve more deeply into money itself, and how it  is that how we would like to pay can encode a host of alternatives for how we shall like to live.", "keywords": {"keywords": ["money", "payment", "alternatives", "living", "work"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["economics"], "content": ["Application"], "application area": ["work life"]}}, {"title": "What data should i protect? Recommender and planning support for data security analysts", "author": "T Li", "abstract": "Major breaches of sensitive company data, as for Facebook's 50 million user accounts in 2018 or Equifax's 143 million user accounts in 2017, are showing the limitations of reactive data security technologies. Companies and government organizations are turning to proactive data security technologies that secure sensitive data at source. However, data security analysts still face two fundamental challenges in data protection decisions: 1) the information overload from the growing number of data repositories and protection"}, {"title": "Discovering collaborators online: Assembling interdisciplinary teams online at an Argentinian University.", "author": "D G\u00f3mez-Zar\u00e1", "abstract": "This study explores how and why scholars find collaborators using team formation systems. Based on theories of teams and human and social capital, we describe how scholars' traits and social networks influence their team formation processes. We conducted a field study in Argentina in which 43 scholars used an online platform and assembled into eight interdisciplinary teams. Our results show that scholars initially tended to invite prior contacts, but, eventually, they assembled cohesive interdisciplinary teams with members they did not", "keywords": {"keywords": ["collaboration", "interdisciplinary teams", "online platforms", "human and social capital", "team formation"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Descriptive"], "discipline": ["Social Sciences"], "content": ["Application"], "application area": ["Team formation"]}}, {"title": "Teammate invitation networks: The roles of recommender systems and prior collaboration in team assembly", "author": "M Twyman", "abstract": "Teammate invitation networks are foundational for team assembly, and recommender systems (similar to dating websites, but for selecting potential teammates) can aid the formation of such networks. This paper extends Hinds et al.'s (2000) influential model of team member selection by incorporating online recommender systems. Exponential random graph modeling of two samples (overall N= 410; 63 teams; 1,048 invitations) shows the invitation network is predicted by online recommendations, beyond previously-established", "keywords": {"keywords": ["teammate invitation networks", "recommender systems", "prior collaboration", "team assembly", "exponential random graph modeling"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Team assembly"]}}, {"title": "CRICTRS: Embeddings based Statistical and Semi Supervised Cricket Team Recommendation System", "author": "P Chhabra", "abstract": "Team Recommendation has always been a challenging aspect in team sports. Such systems aim to recommend a player combination best suited against the opposition players, resulting in an optimal outcome. In this paper, we propose a semi-supervised statistical approach to build a team recommendation system for cricket by modelling players into embeddings. To build these embeddings, we design a qualitative and quantitative rating system which considers the strength of opposition also for evaluating player performance", "keywords": {"keywords": ["Team Recommendation", "Cricket", "Embeddings", "Semi-Supervised", "Statistical"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Sports Analytics"]}}, {"title": "Relational mechanisms in team self-assembly: A network and computational approach", "author": "M Twyman", "abstract": "This dissertation combines perspectives from social networks and teams research to advance understanding of team self-assembly. Across three substantive chapters, I explore team member search behaviors and invitation patterns in contexts where individuals exercise agency to select team members. First, I consider the search for team members in a social network and the resultant team characteristics. During team assembly, how does the prevalence of homophily in a network affect the search for team members and impact team", "keywords": {"keywords": ["team self-assembly", "social networks", "teams research", "homophily", "invitation patterns"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["sociology"], "content": ["methodological"], "application area": ["team formation"]}}, {"title": "Descubriendo colaboradores online: Formando equipos interdisciplinarios en una universidad argentina", "author": "D G\u00f3mez-Zar\u00e1", "abstract": "This study explores how and why scholars find collaborators using team formation systems. Based on theories of teams and human and social capital, we describe how scholars' traits and social networks influence their team formation processes. We conducted a field study in Argentina in which 43 scholars used an online platform and assembled into eight interdisciplinary teams. Our results show that scholars initially tended to invite prior contacts, but, eventually, they assembled cohesive interdisciplinary teams with members they did not", "keywords": {"keywords": ["team formation systems", "interdisciplinary teams", "scholars", "social networks", "human capital"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Education"], "content": ["Application"], "application area": ["Higher education"]}}, {"title": "Understanding Matchmakers' Experiences, Principles and Practices of Assembling Innovation Teams", "author": "S Koivunen", "abstract": "The team composition of a project team is an essential determinant of the success of innovation projects that aim to produce novel solution ideas. Team assembly is essentially complex and sensitive decision-making, yet little supported by information technology (IT). In order to design appropriate digital tools for team assembly, and team formation more broadly, we call for profoundly understanding the practices and principles of matchmakers who manually assemble teams in specific contexts. This paper reports interviews with 13", "keywords": {"keywords": ["innovation teams", "team assembly", "matchmakers", "digital tools", "interviews"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": "Team composition"}}, {"title": "Sistema de Recomenda\u00e7\u00e3o de Recursos Humanos e Gest\u00e3o de informa\u00e7\u00e3o em Projetos de I&D", "author": "TMT Cardoso", "abstract": "A gest\u00e3o de projetos \u00e9 uma tarefa de grande complexidade para muitas organiza\u00e7\u00f5es, pois para l\u00e1 de um grande n\u00famero de projetos que possam existir numa organiza\u00e7\u00e3o cada um destes tem v\u00e1rios aspetos que necessitam de ser abordados. Entre estes aspetos existe a gest\u00e3o dos recursos humanos afetos a cada projeto, a gest\u00e3o das diferentes vari\u00e1veis financeiras, gest\u00e3o de atividades e objetivos envolvidos na realiza\u00e7\u00e3o do projeto em quest\u00e3o e a gest\u00e3o da informa\u00e7\u00e3o gerada durante a sua execu\u00e7\u00e3o.", "keywords": {"keywords": ["gest\u00e3o de projetos", "recursos humanos", "gest\u00e3o da informa\u00e7\u00e3o", "projetos de I&D"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "Joining Together Online", "author": "AM Harris", "abstract": "and six themes in the study of online group formation. These themes  to understand and  enable collectives joining together online.  The expression, \u201cjoining together online,\u201d has evolved", "keywords": {"keywords": ["online group formation", "collectives", "joining together online", "themes", "expression"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["sociology"], "content": ["methodological"], "application area": ["online communities"]}}], "toward foraging for understanding of starcraft agents: an empirical study": [{"title": "Explainable agents and robots: Results from a systematic literature review", "author": "S Anjomshoae", "abstract": "Contribution This paper presents a Systematic Literature Review (SLR) aiming at  on  explainable agency for robots and intelligent agents. This helps understand how these systems", "keywords": {"keywords": ["explainable agents", "robots", "systematic literature review", "intelligent agents", "agency"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI and robotics"]}}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "Explaining reinforcement learning to mere mortals: An empirical study", "author": "A Anderson"}, {"title": "Reinforcement learning interpretation methods: A survey", "author": "A Alharin"}, {"title": "What is\" intelligent\" in intelligent user interfaces? a meta-analysis of 25 years of IUI", "author": "ST V\u00f6lkel"}, {"title": "Mental models of mere mortals with explanations of reinforcement learning", "author": "A Anderson"}, {"title": "Explainable goal-driven agents and robots-a comprehensive review and new framework", "author": "F Sado"}, {"title": "Strategic tasks for explainable reinforcement learning", "author": "R Pocius"}, {"title": "Keeping it\" organized and logical\" after-action review for AI (AAR/AI)", "author": "T Mai"}, {"title": "What Should Be in an XAI Explanation? What IFT Reveals.", "author": "J Dodge"}, {"title": "After-action review for AI (AAR/AI)", "author": "J Dodge", "abstract": "AI? In this article, we investigate this approach by adapting a technique called After-Action  Review (AAR) for use with AI.  adaptation AAR/AI (pronounced \u201carf-eye,\u201d short for \u201cAAR for AI\u201d).", "keywords": {"keywords": ["After-action Review", "AI", "AAR/AI", "adapting", "technique"], "research type": ["Exploratory"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["AI adaptation"]}}, {"title": "A survey of evaluation methods and measures for interpretable machine learning", "author": "S Mohseni"}, {"title": "Recent advances in trustworthy explainable artificial intelligence: Status, challenges and perspectives", "author": "A Rawal"}, {"title": "Finding AI's faults with AAR/AI: An empirical study", "author": "R Khanna"}, {"title": "Demand-driven transparency for monitoring intelligent agents", "author": "M Vered"}, {"title": "Horses for courses: Making the case for persuasive engagement in smart systems", "author": "S Stumpf"}, {"title": "Contrastive explanations of plans through model restrictions", "author": "B Krarup"}, {"title": "The shoutcasters, the game enthusiasts, and the AI: Foraging for explanations of real-time strategy players", "author": "S Penney"}, {"title": "PsyRTS: a web platform for experiments in human decision-making in RTS environments", "author": "DOV Palencia"}, {"title": "How Do People Rank Multiple Mutant Agents?", "author": "J Dodge"}, {"title": "Using Data to\" Git Gud\": A Push for a Player-Centric approach tothe Use of Data in Esports", "author": "E Kleinman", "abstract": "The rapid increase in the availability of player data and the advancement of player modeling technologies have resulted in an abundance of data-driven systems for the domain of esports, both within academia and the industry. However, there is a notable lack of research exploring how players use their data to gain expertise in the context of esports. In this position paper we discuss the current state of the field and argue that there is a need for further research into how players use their data and what they want from datadriven", "keywords": {"keywords": ["data", "player-centric", "esports", "player data", "data-driven systems"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["esports"]}}, {"title": "Using Data to\" Git Gud\": A Push for a Player-Centric approach to the Use of Data in Esports", "author": "E Kleinman"}, {"title": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u72b6\u6001\u6ce8\u610f\u529b\u673a\u5236\u7684\u7814\u7a76", "author": "\u7533\u7fd4\u7fd4\uff0c \u4faf\u65b0\u6587\uff0c \u5c39\u4f20\u73af"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "Variations Foraging", "author": "S Srinivasa Ragavan"}, {"title": "\" What Happened Here!?\" Towards a Taxonomy for User Interaction with Spatio-Temporal Game Data Visualization", "author": "E Kleinman"}, {"title": "Characterizing Human vs Machine Gameplay in StarCraft II", "author": "CA Peters"}, {"title": "\" What Happened Here!?\" A Taxonomy for User Interaction with Spatio-Temporal Game Data Visualization", "author": "E Kleinman"}, {"title": "Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning", "author": "C Kim"}, {"title": "Explainable Reinforcement Learning Through a Causal Lens", "author": "PMMB Appuhamilage"}, {"title": "Assessing and Finding Faults in AI: Two Empirical Studies", "author": "R Khanna", "abstract": "With the advent of Artificial Intelligence (AI) in every sphere of life in today's day and age, it has become increasingly important for non-AI experts to be able to comprehend the underlying logic of how AI systems work, assess them and find faults in these systems, particularly when they are used in high risk scenarios such as in military strategies and medical applications. Recent developments to address the need to open the black boxes of these AI-powered systems have led to the emergence of AI explanations. There now exist", "keywords": {"keywords": ["Artificial Intelligence", "Assessment", "Faults", "AI explanations", "High risk scenarios"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": "AI explanations"}}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Explainable agents and robots: Results from a systematic", "author": "S Anjomshoae"}], "chexplain: enabling physicians to explore and understand data-driven, ai-enabled medical imaging analysis": [{"title": "Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review", "author": "AM Antoniadi", "abstract": "Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted", "keywords": {"keywords": ["explainable AI", "machine learning", "clinical decision support systems", "transparency", "interpretability"], "research type": ["systematic review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["medicine"], "content": ["application"], "application area": ["healthcare"]}}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "A Survey of Human\u2010Centered Evaluations in Human\u2010Centered Machine Learning", "author": "F Sperrle"}, {"title": "What your radiologist might be missing: using machine learning to identify mislabeled instances of X-ray images", "author": "T R\u00e4dsch"}, {"title": "Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI Interactions", "author": "K Vodrahalli"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "Explainable AI for B5G/6G: Technical Aspects, Use Cases, and Research Challenges", "author": "S Wang"}, {"title": "Fair conformal predictors for applications in medical imaging", "author": "C Lu"}, {"title": "Trustworthy machine learning for health care: scalable data valuation with the shapley value", "author": "KD Pandl"}, {"title": "THEMIS: Context-Sensitive Similarity Analysis for Wound Imagery Using Mathematical Model of Meaning", "author": "Y Asayama"}, {"title": "X-MIR: EXplainable Medical Image Retrieval", "author": "B Hu", "abstract": "Despite significant progress in the past few years, machine learning systems are still often viewed as\" black boxes\", which lack the ability to explain their output decisions. In high-stakes situations such as healthcare, there is a need for explainable AI (XAI) tools that can help open up this black box. In contrast to approaches which largely tackle classification problems in the medical imaging domain, we address the less-studied problem of explainable image retrieval. We test our approach on a COVID-19 chest X-ray dataset and", "keywords": {"keywords": ["explainable AI", "medical image retrieval", "black box", "COVID-19", "chest X-ray"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Teaching Humans When To Defer to a Classifier via Exemplars", "author": "H Mozannar"}, {"title": "A Clinical Decision Support System for Sleep Staging Tasks With Explanations From Artificial Intelligence: User-Centered Design and Evaluation Study", "author": "J Hwang"}, {"title": "An Accurate and Explainable Deep Learning System Improves Interobserver Agreement in the Interpretation of Chest Radiograph", "author": "HH Pham"}, {"title": "Human-Centered AI for Medical Imaging", "author": "Y Liang"}, {"title": "BreastScreening-AI: Evaluating medical intelligent agents for human-AI interactions", "author": "FM Calisto"}, {"title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models?", "author": "A Balayn"}, {"title": "A novel explainable neural network for Alzheimer's disease diagnosis", "author": "L Yu"}, {"title": "Explainable Machine Learning for Lung Cancer Screening Models", "author": "K Kobyli\u0144ska"}, {"title": "Use of machine learning to identify patients at risk of sub-optimal adherence: study based on real-world data from 10,929 children using a connected auto \u2026", "author": "A Spataru"}, {"title": "IT-Supported Request Management for Clinical Radiology: Contextual Design and Remote Prototype Testing", "author": "P Krop", "abstract": "Management of radiology requests in larger clinical contexts is characterized by a complex and distributed workflow. In our partner hospital, representing many similar clinics, these processes often still rely on exchanging physical papers and forms, making patient or case data challenging to access. This often leads to phone calls with long waiting queues, which are time-inefficient and result in frequent interrupts. We report on a user-centered design approach based on Rapid Contextual Design with an additional focus group to optimize and", "keywords": {"keywords": ["IT-Supported Request Management", "Clinical radiology", "Contextual design", "Remote prototype testing", "User-centered design"], "research type": ["Case study"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Similarity-Based Explanations meet Matrix Factorization via Structure-Preserving Embeddings", "author": "LB Marinho"}, {"title": "ScrutinAI: A Visual Analytics Approach for the Semantic Analysis of Deep Neural Network Predictions", "author": "E Haedecke"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "A Taxonomy of User-centered Explainable AI Studies", "author": "LV Herm"}, {"title": "Uncalibrated Models Can Improve Human-AI Collaboration", "author": "K Vodrahalli"}, {"title": "General Roadmap and Core Steps for the Development of AI Tools in Digital Pathology", "author": "Y Makhlouf"}, {"title": "General Roadmap and Core Steps for the Development of AI Tools in Digital Pathology. Diagnostics 2022, 12, 1272", "author": "Y Makhlouf"}, {"title": "Investigation of a Web-Based Explainable AI Screening for Prolonged Grief Disorder", "author": "WJ She"}, {"title": "Towards the Use of Saliency Maps for Explaining Low-Quality Electrocardiograms to End Users", "author": "A Lucic"}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare Systeme", "author": "M Balci", "abstract": "Zusammenfassung Nachdem die Forschung zur Softwareanforderung Erkl\u00e4rbarkeit sich haupts\u00e4chlich auf das Fachgebiet der k\u00fcnstlichen Intelligenz beschr\u00e4nkt hat, wird dieser aktuell aus der endnutzerzentrierten Perspektive betrachtet. Das Requirements Engineering (RE) sowie die Mensch-Maschine Kommunikation (MMK) befassen sich mit der Erfassung der Bed\u00fcrfnisse, Ziele und des Kontextes um Erkl\u00e4rbarkeitsanforderungen zu definieren und Evaluationsm\u00f6glichkeiten bereitzustellen. Trotz dessen existiert noch keine Darstellung der", "keywords": {"keywords": ["Requirements Engineering Workflows", "erkl\u00e4rbare Systeme", "Softwareanforderung Erkl\u00e4rbarkeit", "endnutzerzentrierte Perspektive", "Mensch-Maschine Kommunikation"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["Software Engineering"]}}], "how the experts do it: assessing and explaining agent behaviors in real-time strategy games": [{"title": "DARPA's explainable artificial intelligence (XAI) program", "author": "D Gunning", "abstract": "Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA's explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation", "keywords": {"keywords": ["explainable artificial intelligence", "learned models", "decisions", "end users", "explanation"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI applications"]}}, {"title": "Explaining reinforcement learning to mere mortals: An empirical study", "author": "A Anderson"}, {"title": "Toward foraging for understanding of StarCraft agents: An empirical study", "author": "S Penney"}, {"title": "Mental models of mere mortals with explanations of reinforcement learning", "author": "A Anderson"}, {"title": "Designing interactions with intention-aware gaze-enabled artificial agents", "author": "J Newn"}, {"title": "Human-ai collaboration in a cooperative game setting: Measuring social perception and outcomes", "author": "Z Ashktorab"}, {"title": "Player experience extraction from gameplay video", "author": "Z Luo"}, {"title": "Keeping it\" organized and logical\" after-action review for AI (AAR/AI)", "author": "T Mai"}, {"title": "What Should Be in an XAI Explanation? What IFT Reveals.", "author": "J Dodge"}, {"title": "After-action review for AI (AAR/AI)", "author": "J Dodge"}, {"title": "Towards Automated Let's Play Commentary", "author": "M Guzdial", "abstract": "We introduce the problem of generating Let's Play-style commentary of gameplay video via machine learning. We propose an analysis of Let's Play commentary and a framework for building such a system. To test this framework we build an initial, naive implementation, which we use to interrogate the assumptions of the framework. We demonstrate promising results towards future Let's Play commentary generation.", "keywords": {"keywords": ["Let's Play commentary", "gameplay video", "machine learning", "commentary generation", "automated system"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": "Entertainment"}}, {"title": "Wait, but why?: assessing behavior explanation strategies for real-time strategy games", "author": "J Robertson"}, {"title": "Finding AI's faults with AAR/AI: An empirical study", "author": "R Khanna"}, {"title": "Demand-driven transparency for monitoring intelligent agents", "author": "M Vered"}, {"title": "Automated Let's Play Commentary", "author": "S Shah"}, {"title": "XAI for Operations in the Process Industry-Applications, Theses, and Research Directions.", "author": "A Kotriwala"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "Self-attention for deep reinforcement learning", "author": "X Shen"}, {"title": "AutoPreview: A Framework for Autopilot Behavior Understanding", "author": "Y Shen"}, {"title": "The shoutcasters, the game enthusiasts, and the AI: Foraging for explanations of real-time strategy players", "author": "S Penney"}, {"title": "Review of research in the field of developing methods to extract rules from artificial neural networks", "author": "AN Averkin", "abstract": "A large-scale review and analysis of the existing methods and approaches to extract rules from artificial neural networks, including deep learning neural networks, is carried out. A wide range of methods and approaches to extract rules and related approaches to develop explainable artificial intelligence (AI) systems are considered. The taxonomy and several directions in studies of explainable neural networks related to the extraction of rules from neural networks, which allow the user to get an idea of how the neural network uses the", "keywords": {"keywords": ["rules", "artificial neural networks", "methods", "extraction", "explainable AI"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["developing methods to extract rules from artificial neural networks"]}}, {"title": "Interaction design for explainable AI: workshop proposal", "author": "P Madumal"}, {"title": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u72b6\u6001\u6ce8\u610f\u529b\u673a\u5236\u7684\u7814\u7a76", "author": "\u7533\u7fd4\u7fd4\uff0c \u4faf\u65b0\u6587\uff0c \u5c39\u4f20\u73af"}, {"title": "Making Explainable Recommendations Within an Intelligent Information System", "author": "T Goan"}, {"title": "Interaction Design for Explainable AI", "author": "P Madumal"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "Explanatory Artificial Intelligence, Results and Prospects", "author": "A Averkin"}, {"title": "Understanding User Perception of Explainable Algorithmic Decision-Making Systems: A Systematic Literature Review", "author": "S Bae"}, {"title": "Assessing and Finding Faults in AI: Two Empirical Studies", "author": "R Khanna"}, {"title": "Let's Make It Personal, A Challenge in Personalizing Medical Inter-Human Communication", "author": "M Vered"}, {"title": "Gaze-Based Intention Recognition for Human-Agent Collaboration", "author": "J Newn", "abstract": "Human-agent collaboration has repeatedly been proposed over the decades as a way forward to leverage the strengths of arti cial intelligence. As it has become common for humans to work and play alongside intelligent agents, it is increasingly imperative to improve the capacity of agents to interact with their human counterparts socially, naturally and e ectively. However, current agents are still limited in their capacity to recognise nonverbal signals and cues, which in turn, limits their capabilities for natural interaction. This", "keywords": {"keywords": ["gaze-based intention recognition", "human-agent collaboration", "nonverbal signals and cues", "natural interaction", "artificial intelligence"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-Computer Interaction"]}}, {"title": "Nonverbal Communication in Human-AI Interaction: Opportunities and Challenges", "author": "J Newn"}, {"title": "\u041e\u0411\u042a\u042f\u0421\u041d\u0418\u041c\u042b\u0419 \u0418\u0421\u041a\u0423\u0421\u0421\u0422\u0412\u0415\u041d\u041d\u042b\u0419 \u0418\u041d\u0422\u0415\u041b\u041b\u0415\u041a\u0422: \u0418\u0422\u041e\u0413\u0418 \u0418 \u041f\u0415\u0420\u0421\u041f\u0415\u041a\u0422\u0418\u0412\u042b", "author": "\u0410\u041d \u0410\u0432\u0435\u0440\u043a\u0438\u043d"}, {"title": "\u0423\u0447\u0440\u0435\u0434\u0438\u0442\u0435\u043b\u0438: \u0424\u0435\u0434\u0435\u0440\u0430\u043b\u044c\u043d\u043e\u0435 \u0433\u043e\u0441\u0443\u0434\u0430\u0440\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0443\u043d\u0438\u0442\u0430\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u043f\u0440\u0438\u044f\u0442\u0438\u0435\" \u0410\u043a\u0430\u0434\u0435\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043d\u0430\u0443\u0447\u043d\u043e-\u0438\u0437\u0434\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0439, \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u043e-\u043f\u043e\u043b\u0438\u0433\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0438 \u2026", "author": "\u0410\u041d \u0410\u0412\u0415\u0420\u041a\u0418\u041d"}], "improving user confidence in concept maps: exploring data driven explanations": [{"title": "Reliability and inter-rater reliability in qualitative research: Norms and guidelines for CSCW and HCI practice", "author": "N McDonald", "abstract": "What does reliability mean for building a grounded theory? What about when writing an auto-ethnography? When is it appropriate to use measures like inter-rater reliability (IRR)? Reliability is a familiar concept in traditional scientific practice, but how, and even whether to establish reliability in qualitative research is an oft-debated question. For researchers in highly interdisciplinary fields like computer-supported cooperative work (CSCW) and human-computer interaction (HCI), the question is particularly complex as collaborators bring", "keywords": {"keywords": ["reliability", "inter-rater reliability", "qualitative research", "grounded theory", "auto-ethnography"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["CSCW and HCI"]}}, {"title": "The ORCA hub: Explainable offshore robotics through intelligent interfaces", "author": "H Hastie"}, {"title": "Explainable autonomy: A study of explanation styles for building clear mental models", "author": "FJC Garcia"}, {"title": "Visualising covid-19 research", "author": "P Le Bras"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh"}, {"title": "Towards self-explainable cyber-physical systems", "author": "M Blumreiter"}, {"title": "Visualising COVID-19 research", "author": "PL Bras"}, {"title": "Exploring interaction with remote autonomous systems using conversational agents", "author": "DA Robb"}, {"title": "Understanding Algorithms Through Exploration: Supporting Knowledge Acquisition in Primary Tasks", "author": "M Eiband"}, {"title": "Issues Affecting User Confidence in Explanation Systems.", "author": "DA Robb"}, {"title": "Tailoring Motion Recognition Systems to Children's Motions", "author": "A Aloba", "abstract": "Motion-based applications are becoming increasingly popular among children and require accurate motion recognition to ensure meaningful interactive experiences. However, motion recognizers are usually trained on adults' motions. Children and adults differ in terms of their body proportions and development of their neuromuscular systems, so children and adults will likely perform motions differently. Hence, motion recognizers tailored to adults will likely perform poorly for children. My PhD thesis will focus on identifying features that characterize", "keywords": {"keywords": ["motion recognition", "children", "motion-based applications", "body proportions", "neuromuscular systems"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Child motion recognition"]}}, {"title": "Explainable software systems", "author": "A Vogelsang"}, {"title": "Event-driven temporal models for explanations-ETeMoX: explaining reinforcement learning", "author": "JM Parra-Ullauri"}, {"title": "Explainable Autonomy through Natural Language", "author": "J Greenyer"}, {"title": "Understanding Algorithms through Exploration", "author": "M Eiband"}, {"title": "A systematic approach to parameter optimization and its application to flight schedule simulation software", "author": "AEI Brownlee"}, {"title": "Sustainable and Resilient Research", "author": "S Padilla"}, {"title": "Enhancing Reading Strategies by Exploring A Theme-based Approach to Literature Surveys", "author": "T Howden"}, {"title": "Self-Explainable Robots in Remote Environments", "author": "FJ Chiyah Garcia"}, {"title": "Towards explaining metaheuristic solution quality by data mining surrogate fitness models for importance of variables", "author": "A Wallace"}, {"title": "Explainable Autonomy through Natural Language", "author": "FJC Garcia", "abstract": "Robots and autonomous systems are increasingly being deployed remotely in hazardous environments such as in the nuclear or energy sector domains [Ha18, Li17, KY12, Na13, SK16, Wo17]. Typically, these remote robots instil less trust than those co-located [Ba08, HLP17, Li15] and thus it is important to maintain a high level of transparency regarding their behaviour. This is particularly important for robots in remote locations where they cannot be directly observed. The obscure nature of autonomous systems makes understanding them a", "keywords": {"keywords": ["autonomy", "natural language", "remote robots", "transparency", "understanding"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["technical"], "application area": ["robotics"]}}, {"title": "Natural Language Interaction to Facilitate Mental Models of Remote Robots", "author": "FJC Garcia"}, {"title": "ISLAMIC SPIRITUAL CULTURED MANAGERS: A STUDY OF BEHAVIOURAL DISPOSITION THEORY", "author": "RO SALAKO"}, {"title": "Automated strategic visualisations and user confidence", "author": "P Le Bras"}, {"title": "Visualisasi Data Dari Dataset COVID-19 Menggunakan Pemrograman Python", "author": "Y Galahartlambang"}], "questioning the ai: informing design practices for explainable ai user experiences": [{"title": "Manipulating and measuring model interpretabilit", "author": "F Poursabzi-Sangdeh", "abstract": "With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake", "keywords": {"keywords": ["interpretable models", "machine learning", "model interpretability", "experiment", "decision-making"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["machine learning"]}}, {"title": "Drug discovery with explainable artificial intelligenc", "author": "J Jim\u00e9nez-Luna"}, {"title": "Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI system", "author": "B Shneiderman"}, {"title": "How do data science workers collaborate? roles, workflows, and tool", "author": "AX Zhang"}, {"title": "Expanding explainability: Towards social transparency in ai system", "author": "U Ehsan"}, {"title": "Trust in AutoML: exploring information needs for establishing trust in automated machine learning system", "author": "J Drozdal"}, {"title": "Explainable matrix-visualization for global and local interpretability of random forest classification ensemble", "author": "MP Neto"}, {"title": "AutoAIViz: opening the blackbox of automated artificial intelligence with conditional parallel coordinate", "author": "DKI Weidele"}, {"title": "Explanation ontology: a model of explanations for user-centered A", "author": "S Chari"}, {"title": "Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic revie", "author": "AM Antoniadi"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their need", "author": "H Suresh", "abstract": "To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise-and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders' knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in", "keywords": {"keywords": ["interpretable machine learning", "stakeholders", "accountability", "harm mitigation", "granular framework"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Interpretable Machine Learning"]}}, {"title": "Operationalizing human-centered perspectives in explainable A", "author": "U Ehsan"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teacher", "author": "B Ghai"}, {"title": "How much automation does a data scientist want", "author": "D Wang"}, {"title": "\" An Ideal Human\" Expectations of AI Teammates in Human-AI Teamin", "author": "R Zhang"}, {"title": "Towards unifying feature attribution and counterfactual explanations: Different means to the same en", "author": "R Kommiya Mothilal"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanation", "author": "U Ehsan"}, {"title": "To live in their utopia: Why algorithmic systems create absurd outcome", "author": "A Alkhatib"}, {"title": "Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycl", "author": "S Dhanorkar"}, {"title": "Scenario-based requirements elicitation for user-centric explainable A", "author": "D Cirqueira"}, {"title": "A review of recent deep learning approaches in human-centered machine learnin", "author": "T Kaluarachchi", "abstract": "After Deep Learning (DL) regained popularity recently, the Artificial Intelligence (AI) or Machine Learning (ML) field is undergoing rapid growth concerning research and real-world application development. Deep Learning has generated complexities in algorithms, and researchers and users have raised concerns regarding the usability and adoptability of Deep Learning systems. These concerns, coupled with the increasing human-AI interactions, have created the emerging field that is Human-Centered Machine Learning", "keywords": {"keywords": ["Deep Learning", "Artificial Intelligence", "Machine Learning", "Human-Centered Machine Learning", "Usability"], "research type": ["Review"], "methodology": [], "purposes": [], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Research and Development"]}}, {"title": "Survey of explainable machine learning with visual and granular methods beyond quasi-explanation", "author": "B Kovalerchuk"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experienc", "author": "B Ghai"}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checker", "author": "CH Tsai"}, {"title": "Exploring explainability: a definition, a model, and a knowledge catalogu", "author": "L Chazette"}, {"title": "Attention flows: Analyzing and comparing attention mechanisms in language model", "author": "JF DeRose"}, {"title": "Human-centered XAI: Developing design patterns for explanations of clinical decision support system", "author": "TAJ Schoonderwoerd"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparenc", "author": "AI Anik"}, {"title": "\u201cIt cannot do all of my work\u201d: community health worker perceptions of AI-enabled mobile health applications in rural Indi", "author": "CT Okolo"}, {"title": "Effect of information presentation on fairness perceptions of machine learning predictor", "author": "N Van Berkel"}, {"title": "Introduction to explainable A", "author": "QV Liao", "abstract": "As Artificial Intelligence technologies are increasingly used to make important decisions  and perform autonomous tasks, providing explanations to allow users and stakeholders to", "keywords": {"keywords": ["Artificial Intelligence", "explanations", "decision-making", "autonomous tasks", "stakeholders"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["AI technology"]}}, {"title": "Human-Centered A", "author": "B Shneiderman"}, {"title": "Reviewing the need for explainable artificial intelligence (xAI", "author": "J Gerlings"}, {"title": "A framework of high-stakes algorithmic decision-making for the public sector developed through a case study of child-welfar", "author": "D Saxena"}, {"title": "Perfection not required? Human-AI partnerships in code translatio", "author": "JD Weisz"}, {"title": "LINDA-BN: An interpretable probabilistic approach for demystifying black-box predictive model", "author": "C Moreira"}, {"title": "Machine Learning Uncertainty as a Design Material: A Post-Phenomenological Inquir", "author": "JJ Benjamin"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experience", "author": "QV Liao"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studie", "author": "V Lai"}, {"title": "Fits and starts: Enterprise use of automl and the role of humans in the loo", "author": "A Crisan"}, {"title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologie", "author": "O Vereschak", "abstract": "The spread of AI-embedded systems involved in human decision making makes studying human trust in these systems critical. However, empirically investigating trust is challenging. One reason is the lack of standard protocols to design trust experiments. In this paper, we present a survey of existing methods to empirically investigate trust in AI-assisted decision making and analyse the corpus along the constitutive elements of an experimental protocol. We find that the definition of trust is not commonly integrated in experimental protocols", "keywords": {"keywords": ["trust", "AI-assisted decision making", "empirical methodology", "experimental protocols"], "research type": ["Meta-analysis"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive", "Explanatory", "Prescriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-Computer Interaction"]}}, {"title": "Question-driven design process for explainable ai user experience", "author": "QV Liao"}, {"title": "Toward natural language mitigation strategies for cognitive biases in recommender system", "author": "A Rieger"}, {"title": "Explainable artificial intelligence (XAI) in biomedicine: Making AI decisions trustworthy for physicians and patient", "author": "J L\u00f6tsch"}, {"title": "Machine Learning: The Basic", "author": "A Jung"}, {"title": "Accelerating AI adoption with responsible AI signals and employee engagement mechanisms in health car", "author": "W Wang"}, {"title": "Melody: generating and visualizing machine learning model summary to understand data and classifiers togethe", "author": "GYY Chan"}, {"title": "XAlgo: A design probe of explaining Algorithms' internal states via question-answerin", "author": "J Rebanal"}, {"title": "Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burden", "author": "H Park"}, {"title": "Patients' perceptions of using artificial intelligence (AI)-based technology to comprehend radiology imaging dat", "author": "Z Zhang"}, {"title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspectiv", "author": "H Lakkaraju", "abstract": "As practitioners increasingly deploy machine learning models in critical domains such as health care, finance, and policy, it becomes vital to ensure that domain experts function effectively alongside these models. Explainability is one way to bridge the gap between human decision-makers and machine learning models. However, most of the existing work on explainability focuses on one-off, static explanations like feature importances or rule lists. These sorts of explanations may not be sufficient for many use cases that require dynamic", "keywords": {"keywords": ["Explainability", "Dialogue", "Machine learning models", "Critical domains", "Dynamic explanations"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Domain expertise"]}}, {"title": "AI Trust score: A user-centered approach to building, designing, and measuring the success of intelligent workplace feature", "author": "J Wang"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligenc", "author": "W Jin"}, {"title": "Explainable artificial intelligence for tabular data: A surve", "author": "M Sahakyan"}, {"title": "Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approac", "author": "H Mucha"}, {"title": "Contestability For Content Moderatio", "author": "K Vaccaro"}, {"title": "Towards Relatable Explainable AI with the Perceptual Proces", "author": "W Zhang"}, {"title": "Subplex: Towards a better understanding of black box model explanations at the subpopulation leve", "author": "GYY Chan"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interface", "author": "M Chromik"}, {"title": "Requirements engineering for machine learning: A systematic mapping stud", "author": "H Villamizar"}, {"title": "Designer-user communication for XAI: an epistemological approach to discuss XAI desig", "author": "JJ Ferreira", "abstract": "Artificial Intelligence is becoming part of any technology we use nowadays. If the AI informs people's decisions, the explanation about AI's outcomes, results, and behavior becomes a necessary capability. However, the discussion of XAI features with various stakeholders is not a trivial task. Most of the available frameworks and methods for XAI focus on data scientists and ML developers as users. Our research is about XAI for end-users of AI systems. We argue that we need to discuss XAI early in the AI-system design process and", "keywords": {"keywords": ["Designer-user communication", "Explainable Artificial Intelligence", "XAI", "End-users", "AI-system design"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-Computer Interaction"]}}, {"title": "Explainable empirical risk minimizatio", "author": "A Jung"}, {"title": "Educating Software and AI Stakeholders About Algorithmic Fairness, Accountability, Transparency and Ethic", "author": "V Bogina"}, {"title": "Chat-xai: A new chatbot to explain artificial intelligenc", "author": "M Gao"}, {"title": "Interpreting text classifiers by learning context-sensitive influence of word", "author": "S Kumar"}, {"title": "Im receptivity and presentation-type preferences among users of a mobile app with automated receptivity-status adjustmen", "author": "TW Wu"}, {"title": "HANN: Hybrid Attention Neural Network for Detecting Covid-19 Related Rumor", "author": "AM Almars"}, {"title": "Knowledge graph-based rich and confidentiality preserving Explainable Artificial Intelligence (XAI", "author": "JM Ro\u017eanec"}, {"title": "ChatrEx: Designing explainable chatbot interfaces for enhancing usefulness, transparency, and trus", "author": "A Khurana"}, {"title": "The explainability paradox: Challenges for xAI in digital patholog", "author": "T Evans"}, {"title": "Machine learning in governments: Benefits, challenges and future direction", "author": "Y Pi", "abstract": "The unprecedented increase in computing power and data availability has signifi-cantly altered the way and the scope that organizations make decisions relying on technologies. There is a conspicuous trend that organizations are seeking the use of frontier technologies with the purpose of helping the delivery of services and making day-to-day operational deci-sions. Machine learning (ML) is the fastest growing and at the same time, the most debated and controversial of these technologies. Although there is a great deal of research in the", "keywords": {"keywords": ["Machine learning", "Governments", "Benefits", "Challenges", "Future direction"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Government services"]}}, {"title": "Human-Centric Preference Modeling for Virtual Agent", "author": "BP Knijnenburg"}, {"title": "Investigating Explainability of Generative AI for Code through Scenario-based Desig", "author": "J Sun"}, {"title": "The need to move away from agential-AI: Empirical investigations, useful concepts and open issue", "author": "F Cabitza"}, {"title": "Explainable automatic evaluation of the trail making test for dementia screenin", "author": "A Prange"}, {"title": "Explainable artificial intelligence in educatio", "author": "H Khosravi"}, {"title": "Towards Understanding People's Experiences of AI Computer Vision Fitness Instructor App", "author": "A Garbett"}, {"title": "Assessing Gender Bias in Predictive Algorithms using eXplainable A", "author": "C Manresa-Yee"}, {"title": "Posthuman Interaction Design: Designing with, through, and for Human-Nature Interactio", "author": "SY Liu"}, {"title": "Three directions for the design of human-centered machine translatio", "author": "S Robertson"}, {"title": "Introducing a multi-stakeholder perspective on opacity, transparency and strategies to reduce opacity in algorithm-based human resource managemen", "author": "M Langer", "abstract": "Artificial Intelligence and algorithmic technologies support or even automate a large variety of human resource management (HRM) activities. This affects a range of stakeholders with different, partially conflicting perspectives on the opacity and transparency of algorithm-based HRM. In this paper, we explain why opacity is a key characteristic of algorithm-based HRM, describe reasons for opaque algorithm-based HRM, and highlight the implications of opacity from the perspective of the main stakeholders involved (users, affected people", "keywords": {"keywords": ["architecture", "employment", "transparency", "opacity", "stakeholders"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["human resource management"]}}, {"title": "A selective ensemble model for cognitive cybersecurity analysi", "author": "Y Jiang"}, {"title": "Towards participatory design spaces for explainable ai interfaces in expert domain", "author": "H Mucha"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainabl", "author": "A Simkute"}, {"title": "A social evaluation of the perceived goodness of explainability in machine learnin", "author": "J Wanner"}, {"title": "Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Direction", "author": "S Atakishiyev"}, {"title": "Design Methods for Artificial Intelligence Fairness and Transparenc", "author": "S Stumpf"}, {"title": "Can You Trust the Black Box? The Effect of Personality Traits on Trust in AI-Enabled User Interface", "author": "M B\u00f6ckle"}, {"title": "EUCA: the End-User-Centered Explainable AI Framewor", "author": "W Jin"}, {"title": "\u201d Because AI is 100% right and safe\u201d: User Attitudes and Sources of AI Authority in Indi", "author": "S Kapania"}, {"title": "Designing a Multi-Agent Occupant Simulation System to Support Facility Planning and Analysis for COVID-1", "author": "B Lee", "abstract": "The COVID-19 pandemic changed our lives, forcing us to reconsider our built environment, architectural designs, and even behaviours. Multiple stakeholders, including designers, building facility managers, and policy makers, are making decisions to reduce SARS-CoV-2 virus transmission and make our environment safer; however, systems to effectively and interactively evaluate virus transmission in physical spaces are lacking. To help fill this gap, we propose OccSim, a system that automatically generates occupancy behaviours in a 3D", "keywords": {"keywords": ["Multi-Agent", "Occupant Simulation", "Facility Planning", "Analysis", "COVID-19"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Facility Planning and Analysis"]}}, {"title": "Intuitively assessing ml model reliability through example-based explanations and editing model input", "author": "H Suresh"}, {"title": "A multistakeholder approach towards evaluating ai transparency mechanism", "author": "A Lucic"}, {"title": "The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lendin", "author": "M Dikmen"}, {"title": "How to choose an explainability method? towards a methodical implementation of xai in practic", "author": "T Vermeire"}, {"title": "Wikipedia ORES Explorer: Visualizing Trade-offs For Designing Applications With Machine Learning AP", "author": "Z Ye"}, {"title": "Expl (AI) ned: The impact of explainable artificial intelligence on cognitive processe", "author": "K Bauer"}, {"title": "LEx: A Framework for Operationalising Layers of Machine Learning Explanation", "author": "R Singh"}, {"title": "Increasing the Understandability and Explainability of Machine Learning and Artificial Intelligence Solutions: A Design Thinking Approac", "author": "A Kurti"}, {"title": "Towards Explainable Artificial Intelligence and Explanation User Interfaces to Open the 'Black Box'of Automated ECG Interpretatio", "author": "K Rjoob"}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronizatio", "author": "M Fan", "abstract": "Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX", "keywords": {"keywords": ["Human-AI Collaboration", "UX Evaluation", "Explanation", "Synchronization"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer science", "content": "Application", "application area": "User experience"}}, {"title": "Development of an Instrument for Measuring Users' Perception of Transparency in Recommender System", "author": "M Hellmann"}, {"title": "Addressing the ethical principles of the Norwegian National Strategy for AI in a kindergarten allocation syste", "author": "S Botnevik"}, {"title": "\u201d It's like a puppet master\u201d: User Perceptions of Personal Autonomy when Interacting with Intelligent Technologie", "author": "S Sankaran"}, {"title": "XAI: a natural application domain for fuzzy set theor", "author": "B Bouchon-Meunier"}, {"title": "Engaging end users in an ai-enabled smart service design-the application of the smart service blueprint scape (SSBS) framewor", "author": "F Li"}, {"title": "Explanation ontology in action: a clinical use-cas", "author": "S Chari"}, {"title": "Better together? an evaluation of ai-supported code translatio", "author": "JD Weisz"}, {"title": "Quackie: A nlp classification task with ground truth explanation", "author": "Y Rychener"}, {"title": "Artificially Intelligent Technology for the Margins: A Multidisciplinary Design Agend", "author": "F Tachtler"}, {"title": "Specifying and Interpreting Reinforcement Learning Policies through Simulatable Machine Learnin", "author": "P Tambwekar", "abstract": "Human-AI collaborative policy synthesis is a procedure in which (1) a human initializes an autonomous agent's behavior,(2) Reinforcement Learning improves the human specified behavior, and (3) the agent can explain the final optimized policy to the user. This paradigm leverages human expertise and facilitates a greater insight into the learned behaviors of an agent. Existing approaches to enabling collaborative policy specification involve black box methods which are unintelligible and are not catered towards non-expert end-users. In this", "keywords": {"keywords": ["reinforcement learning", "collaborative policy synthesis", "human-AI collaboration", "machine learning", "autonomous agent"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["AI policy synthesis"]}}, {"title": "Explainability in supply chain operational risk management: A systematic literature revie", "author": "SF Nimmy"}, {"title": "Understanding the Design Space of AI-Mediated Social Interaction in Online Learning: Challenges and Opportunitie", "author": "Q Wang"}, {"title": "A\u00e7\u0131klanabilir Evri\u015fimsel Sinir A\u011flar\u0131 ile Beyin T\u00fcm\u00f6r\u00fc Tespit", "author": "A ORMAN"}, {"title": "Explainability using decision trees and Monte Carlo simulation", "author": "I Siddavatam"}, {"title": "How Experienced Designers of Enterprise Applications Engage AI as a Design Materia", "author": "N Yildirim"}, {"title": "Towards Human-Centred Explainable AI: A Systematic Literature Revie", "author": "O Williams"}, {"title": "Metrics, Explainability and the European AI Act Proposa", "author": "F Sovrano"}, {"title": "XAutoML: A Visual Analytics Tool for Establishing Trust in Automated Machine Learnin", "author": "MA Z\u00f6ller"}, {"title": "Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI System", "author": "M Nourani"}, {"title": "From heatmaps to structured explanations of image classifier", "author": "L Fuxin", "abstract": "This paper summarizes our endeavors in the past few years in terms of explaining image classifiers, with the aim of including negative results and insights we have gained. The paper starts with describing the explainable neural network (XNN), which attempts to extract and visualize several high\u2010level concepts purely from the deep network, without relying on human linguistic concepts. This helps users understand network classifications that are less intuitive and substantially improves user performance on a difficult fine\u2010grained", "keywords": {"keywords": ["image classifier", "explanation", "heatmaps", "structured explanations", "XNN"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["image classification"]}}, {"title": "Multivariate Data Explanation by Jumping Emerging Patterns Visualizatio", "author": "MP Neto"}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderatio", "author": "V Lai"}, {"title": "Connectors of smart design and smart system", "author": "I Horv\u00e1th"}, {"title": "Conversational review-based explanations for recommender systems: Exploring users' query behavio", "author": "DC Hernandez-Bocanegra"}, {"title": "Human-Centric Research for NLP: Towards a Definition and Guiding Question", "author": "B Kotnis"}, {"title": "Data Summaries for Scalable Visual Analysi", "author": "GYY Chan"}, {"title": "Tuning into uncertainty: A material exploration of object detection through pla", "author": "J Rukanskait\u0117"}, {"title": "Human-AI Interaction Paradigm for Evaluating Explainable Artificial Intelligenc", "author": "M Franklin"}, {"title": "AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validatio", "author": "O Gomez"}, {"title": "User centric explanations: a breakthrough for explainable model", "author": "A Hassan", "abstract": "Thanks to recent developments in explainable Deep Learning models, researchers have shown that these models can be incredibly successful and provide encouraging results. However, a lack of model interpretability can hinder the efficient implementation of Deep Learning models in real-world applications. This has encouraged researchers to develop and design a large number of algorithms to support transparency. Although studies have raised awareness of the importance of explainable artificial intelligence, the question of how", "keywords": {"keywords": ["explainable Deep Learning models", "model interpretability", "transparency", "explanation algorithms", "explainable artificial intelligence"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explainability"]}}, {"title": "Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendation", "author": "R Shang"}, {"title": "Comparing Human Reasoning and Explainable A", "author": "CJ Helgstrand"}, {"title": "Teaching-Learning Interaction: A New Concept for Interaction Design to Support Reflective User Agency in Intelligent System", "author": "H Kim"}, {"title": "Explainable artificial intelligence for autonomous driving: An overview and guide for future research direction", "author": "S Atakishiyev"}, {"title": "Prediction for Retrospection: Integrating Algorithmic Stress Prediction into Personal Informatics Systems for College Students' Mental Healt", "author": "T Kim"}, {"title": "How can Explainability Methods be Used to Support Bug Identification in Computer Vision Models", "author": "A Balayn"}, {"title": "Square peg, round hole: A case study on using Visual Question & Answering in Game", "author": "P Bala"}, {"title": "Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Challenges and Solution", "author": "E Petersen"}, {"title": "Do explanations increase the effectiveness of AI-crowd generated fake news warnings", "author": "Z Epstein"}, {"title": "Personalizing the explanation extraction in Intelligent Decision Support System", "author": "FRS Oliveira", "abstract": "The use of black-box models compromises the adoption of Decision Support Systems because they tipically do not allow the decision maker to understand how decisions are issued by the system. Even with the growth of the Explanable Artificial Intelligence area, little has been done to personalize the explanations generated in the context of these systems. This article presents an approach to customize explanations of decisions using a graph to mediate inferences. Proofs of concept and analysis of simulated decisions are presented", "keywords": {"keywords": ["explanation extraction", "intelligent decision support system", "personalization", "black-box models", "graph mediation"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Decision support"]}}, {"title": "Moody Man: Improving creative teamwork through dynamic affective recognitio", "author": "M Nguyen"}, {"title": "Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Stud", "author": "K Sengupta"}, {"title": "Analyzing human factor involvement in sustainable hazardous cargo port operation", "author": "RU Khan"}, {"title": "Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanation", "author": "S Hicks"}, {"title": "Textual eXplanations for intuitive machine learnin", "author": "V Torri"}, {"title": "A Cognitive Work Analysis Approach to Explainable Artificial Intelligence in Non-Expert Financial Decision-Makin", "author": "M Dikmen"}, {"title": "Telling Stories from Computational Notebooks: AI-Assisted Presentation Slides Creation for Presenting Data Science Wor", "author": "C Zheng"}, {"title": "Crebot: Exploring Interactive Question Prompts for Critical Paper Readin", "author": "Z Peng"}, {"title": "How Do Users Interact with AI Features in the Workplace? Understanding the AI Feature User Journey in Enterpris", "author": "S Hanses"}, {"title": "Operationalising ethics in artificial intelligence for healthcare: a framework for AI developer", "author": "P Solanki", "abstract": "Artificial intelligence (AI) offers much promise for improving healthcare. However, it runs the looming risk of causing individual and societal harms; for instance, exacerbating inequalities amongst minority groups, or enabling compromises in the confidentiality of patients' sensitive data. As such, there is an expanding, unmet need for ensuring AI for healthcare is developed in concordance with human values and ethics. Augmenting \u201cprinciple-based\u201d guidance that highlight adherence to ethical ideals (without necessarily offering translation", "keywords": {"keywords": ["ethics", "artificial intelligence", "healthcare", "framework", "developer"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["healthcare"]}}, {"title": "Socio-Technical HCI Design in a Wider Contex", "author": "T Clemmensen"}, {"title": "Explainable Artificial Intelligence for Predictive Modeling in Healthcar", "author": "CC Yang"}, {"title": "Facets of algorithmic literacy: Information, experience, and individual factors predict attitudes toward algorithmic system", "author": "DE Silva"}, {"title": "Illuminating the black bo", "author": "K EKICI"}, {"title": "Explainable Deep Neural Network in Recommender System", "author": "MD Zanjania"}, {"title": "Should a machine be trusted? A study on the effect of explainable artificial intelligence on trust in automated machine learnin", "author": "C Jenytin"}, {"title": "Teaching End-User Development in the Time of IoT and A", "author": "F Patern\u00f2"}, {"title": "Governing Artificial Intelligence in Post-Pandemic Societ", "author": "A Arunagiri"}, {"title": "Hofstede's dimensions of culture and gender differences in UI satisfactio", "author": "A Alsswey"}, {"title": "Contextualising local explanations for non-expert users: an XAI pricing interface for insurance", "author": "C Bove", "abstract": "Machine Learning has provided new business opportunities in the insurance industry, but its adoption is for now limited by the difficulty to explain the rationale behind the prediction provided. In this work, we explore how we can enhance local feature importance explanations for non-expert users. We propose design principles to contextualise these explanations with additional information about the Machine Learning system, the domain and external factors that may influence the prediction. These principles are applied to a car", "keywords": {"keywords": ["Explainable Artificial Intelligence", "insurance", "local explanations", "feature importance", "contextualisation"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application_area": ["Insurance"]}}, {"title": "Toward Accountable and Explainable Artificial Intelligence Part Two: The Framework Implementatio", "author": "M Khan"}, {"title": "It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in A", "author": "A Papenmeier"}, {"title": "The Science of Rejection: A Research Area for Human Computatio", "author": "B Sayin"}, {"title": "End-User Creation and Control of Daily Automations in Intelligent Environment", "author": "F Patern\u00f2"}, {"title": "Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solution", "author": "H Park"}, {"title": "Transparent and Explainable M", "author": "A Jung"}, {"title": "Understanding the Role of Explanation Modality in AI-assisted Decision-makin", "author": "V Robbemond"}, {"title": "FCE: Feedback based Counterfactual Explanations for Explainable A", "author": "M Suffian"}, {"title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support System", "author": "C Panigutti"}, {"title": "Trust and Reliance in XAI--Distinguishing Between Attitudinal and Behavioral Measure", "author": "N Scharowski", "abstract": "Trust is often cited as an essential criterion for the effective use and real-world deployment of AI. Researchers argue that AI should be more transparent to increase trust, making transparency one of the main goals of XAI. Nevertheless, empirical research on this topic is inconclusive regarding the effect of transparency on trust. An explanation for this ambiguity could be that trust is operationalized differently within XAI. In this position paper, we advocate for a clear distinction between behavioral (objective) measures of reliance and", "keywords": {"keywords": ["trust", "reliance", "explainable AI", "transparency", "behavioral measures"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Polic", "author": "A Bell"}, {"title": "A Human-centric AI-driven Framework for Exploring Large and Complex Dataset", "author": "MF Costabile"}, {"title": "Making Architecture Optimization Transparent with Tactic-Based Explanation", "author": "JA Diaz-Pace"}, {"title": "Extending a Human-AI Collaboration Framework with Dynamism and Socialit", "author": "M Muller"}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert User", "author": "C Bove"}, {"title": "Making SHAP Rap: Bridging Local and Global Insights Through Interaction and Narrative", "author": "M Chromik"}, {"title": "Random Forest Similarity Maps: A Scalable Visual Representation for Global and Local Interpretatio", "author": "D Mazumdar"}, {"title": "Single-Turn Debate Does Not Help Humans Answer Hard Reading-Comprehension Question", "author": "A Parrish"}, {"title": "Journal of Responsible Technolog", "author": "A Simkute"}, {"title": "Exploring design practices for explaining music recommendation", "author": "H Ryu", "abstract": "ACM Reference Format: This work was submitted in partial fulfilment for a master's degree in Human\u2013Computer Interaction at Uppsala University, Sweden, on 14th June. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author must be honoured.,\u00a9", "keywords": [["interaction", 0.3553], ["computer", 0.3003], ["use", 0.2375], ["classroom", 0.2179], ["components", 0.1812], ["advantage", 0.1787], ["sweden", 0.1771], ["work", 0.1758], ["human", 0.1731], ["uppsala", 0.1684]]}, {"title": "Markus Langera & Cornelius J. K\u00f6niga Universit\u00e4t des Saarlande", "author": "M Langer"}, {"title": "Conversational explanations of Machine Learning models using chatbot", "author": "M Ku\u017aba"}, {"title": "Logic and Pragmatics in AI Explanatio", "author": "CH Tsai"}, {"title": "Calibrating Trust in AI-Assisted Decision Makin", "author": "A Turner"}, {"title": "A Methodological Framework for Facilitating Explainable AI Desig", "author": "J Zhang"}, {"title": "Exploring Peoples' Perception of Autonomy and Reactance in Everyday AI Interaction", "author": "S Sankaran"}, {"title": "Understanding User Perception of Explainable Algorithmic Decision-Making Systems: A Systematic Literature Revie", "author": "S Bae"}, {"title": "Algorithmic nudge to make better choices: Evaluating effectiveness of XAI frameworks to reveal biases in algorithmic decision making to user", "author": "P Juneja"}, {"title": "Designing Intelligent Interfaces to Facilitate Communication in Large-Scale Live Streamin", "author": "J Guo"}, {"title": "Toward Accountable and Explainable Artificial Intelligence Part Two: The Framework Implementatio", "author": "J Vice", "abstract": "Like other Artificial Intelligence (AI) systems, Machine Learning (ML) applications cannot explain decisions, are marred with training-caused biases, and suffer from algorithmic limitations. Their eXplainable Artificial Intelligence (XAI) capabilities are typically measured in a two-dimensional space of explainability and accuracy ignoring the accountability aspects. During system evaluations, measures of comprehensibility, predictive accuracy and accountability remain inseparable. We propose an Accountable eXplainable Artificial", "keywords": {"keywords": ["Artificial Intelligence", "Machine Learning", "eXplainable Artificial Intelligence", "Accountability", "Algorithmic Limitations"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Robust and Interpretable Predictions for Multimodal Sensor System", "author": "JV Jeyakumar"}, {"title": "Algorithmic Accountability for Inverse Transparenc", "author": "A Pretschner"}, {"title": "Co-Designing AI Agents to Support Social Connectedness Among Online Learners: Functionalities, Social Characteristics, and Ethical Challenge", "author": "Q Wang"}, {"title": "How much is too much? Levels of AI Explainability within Decision Support Systems' User Interfaces for improved decision-making performanc", "author": "G Sewnath"}, {"title": "The Algorithmic Imprin", "author": "U Ehsan"}, {"title": "Explanation Ontology: A General-Purpose, Semantic Representation for Supporting User-Centered Explanation", "author": "S Chari"}, {"title": "ProtoTEx: Explaining Model Decisions with Prototype Tensor", "author": "A Das"}, {"title": "Leveraging challenges of an algorithm-based symptom checker on user trust through explainable A", "author": "Y HWANG"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable A", "author": "QV Liao"}, {"title": "Three Directions for the Design of Human-Centered Machine Translatio", "author": "DJ Liebling", "abstract": "\u25cb How can we design systems that help people make use of imperfect translation?  \u25cb  How can machine translation models provide greater control to users over different aspects  of a translation, such as tone? What aspects of a translation would users want control over?", "keywords": {"keywords": ["machine translation", "human-centered design", "imperfect translation", "translation control", "translation aspects"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["language technology"]}}, {"title": "Dimension-Based Interactions with Virtual Assistants: A Co-Design Project with Design Fiction", "author": "HCH Lau"}, {"title": "Explainable AI for Constraint-Based Expert System", "author": "J Bode"}, {"title": "DBias: Predicting attribute effectiveness using biased database", "author": "A Dalvi"}, {"title": "Artificial Intelligence in Urban Plannin", "author": "T Sanchez"}, {"title": "The Stakeholder Playbook for Explaining AI System", "author": "R Hoffman"}, {"title": "Towards Understanding the Transparency of Automations in Daily Environment", "author": "F Patern\u00f2"}, {"title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in Indi", "author": "D Ramesh"}, {"title": "Towards Scenario-Based and Question-Driven Explanations in Autonomous Vehicle", "author": "Y Zhang"}, {"title": "Taking Back Control of Social Media Feeds with Take Back Contro", "author": "J Salminen"}, {"title": "Making AI Explainable in the Global South: A Systematic Revie", "author": "CT Okolo", "abstract": "Artificial intelligence (AI) and machine learning (ML) are quickly becoming pervasive in ways that impact the lives of all humans across the globe. In an effort to make otherwise \u201cblack box\" AI/ML systems more understandable, the field of Explainable AI (XAI) has arisen with the goal of developing algorithms, toolkits, frameworks, and other techniques that enable people to comprehend, trust, and manage AI systems. However, although XAI is a rapidly growing area of research, most of the work has focused on contexts in the Global North, and", "keywords": {"keywords": ["Explainable AI", "Global South", "Artificial Intelligence", "Machine Learning", "Trust"], "research type": "Systematic Review", "methodology": "Mixed-Methods", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Application", "application area": "AI Explainability"}}, {"title": "Explanation Interfaces for Sales Forecastin", "author": "TB Fahse"}, {"title": "Automation Experience at the Workplace\u2013Playing the 80's Record", "author": "V Roto"}, {"title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theor", "author": "H Kaur"}, {"title": "Towards a Roadmap on Software Engineering for Responsible A", "author": "Q Lu"}, {"title": "Situated Case Studies for a Human-Centered Design of Explanation User Interface", "author": "C M\u00fcller-Birn"}, {"title": "Questioning the Bot: Explanaitions for a Conversational Movie Recommende", "author": "BB Heyder"}, {"title": "An Uncommon Task: Participatory Design in Legal A", "author": "F Delgado"}, {"title": "Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Complianc", "author": "A Bell"}, {"title": "Investigating the Impact of Online Human Collaboration in Explanation of AI System", "author": "TI Mamun"}, {"title": "A study of UX practitioners roles in designing real-world, enterprise ML system", "author": "S Zdanowska", "abstract": "Opportunities for AI and machine learning (ML) are vast in current interactive systems development. However, comparatively little is known about how functionality for the system behind the interface is designed and how design methodologies such as user-centered design have influence. This research focuses on how interdisciplinary teams that include UX practitioners design real-world enterprise ML systems outside of big technology companies. We conducted a survey with product managers, and interviews with interdisciplinary teams", "keywords": {"keywords": ["UX practitioners", "design", "real-world", "ML system", "enterprise"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Interactive systems development"]}}, {"title": "Improving Trustworthiness of AI Solutions: A Qualitative Approach to Support Ethically-Grounded AI Desig", "author": "A Vianello"}, {"title": "Towards Explainable Artificial Intelligence and Explanation User Interfaces to Open the 'Black Box'of Automated ECG Interpretatio", "author": "C Knoery"}, {"title": "'We can't find fault with a friend': The Mediation of Accountability on Instant Loan Platforms in Indi", "author": "D Ramesh"}, {"title": "AI for the public. How public interest theory shifts the discourse on A", "author": "T Z\u00fcger"}, {"title": "TOWARDS USER-CENTRIC EXPLANATIONS FOR EXPLAINABLE MODELS: A REVIE", "author": "A Hassan"}, {"title": "Human-Centered Evaluation of Explanation", "author": "J Boyd-Graber"}, {"title": "ConceptExplainer: Understanding the Mental Model of Deep Learning Algorithms via Interactive Concept-based Explanation", "author": "J Huang"}, {"title": "Advances in XAI: Explanation Interfaces in Healthcar", "author": "C Manresa-Yee"}, {"title": "Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretabilit", "author": "JJ Benjamin"}, {"title": "How Does Netflix \u201cUnderstand\u201d Me?: Exploring End-user Needs for Human-centered Explanation", "author": "Y Choi", "abstract": "In this work, we explore the end-user needs of the explanations when using recommender systems and potential actions that the users could do with explanations. We conducted an online survey (N= 14) and a think-aloud study (N= 3) to investigate user needs for explanations in entertainment-domain recommender systems. The study revealed that users want to get 'self-referenced'explanations on how the system understands them. Furthermore, these insights confirmed that the users want to directly manipulate the algorithms either", "keywords": {"keywords": ["Netflix", "recommender systems", "explanations", "end-user needs", "user study"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Entertainment-domain recommender systems"]}}, {"title": "Deep Learning Visualization for Underspecification Analysis in Product Design Matching Model Developmen", "author": "N Chotisarn"}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Application", "author": "M Nourani"}, {"title": "Interpretable Machine Learning in Social Sciences: Use Cases and Limitation", "author": "A Suvorova"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Revie", "author": "A Bertrand"}, {"title": "User Study on the Effects Explainable AI Visualizations on Non-expert", "author": "S Schulze-Weddige"}, {"title": "Human-Centered Approaches to Designing Intelligent Agents' Manner for Supporting High-Level Thinkin", "author": "Z Peng"}, {"title": "Network-Based Dynamical System Models: Visual Properties to Model Propertie", "author": "HJ Yeon"}, {"title": "\u201cWhy Do I Care What's Similar?\u201d Probing Challenges in AI-Assisted Child Welfare Decision-Making through Worker-AI Interface Design Concept", "author": "A Kawakami"}, {"title": "Creative Uses of AI Systems and their Explanations: A Case Study from Insuranc", "author": "M Benk"}, {"title": "Towards a multi-stakeholder value-based assessment framework for algorithmic system", "author": "M Yurrita", "abstract": "In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical", "keywords": {"keywords": ["algorithmic systems", "value-based assessment framework", "machine learning", "ethics", "auditing processes"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["algorithmic systems"]}}, {"title": "Only limited material is available in the selected language. All content is available on the global site", "author": "MMV Brewer"}, {"title": "Adopting microservice architecture: A decision support model based on genetically evolved multi-layer FC", "author": "A Christoforou"}, {"title": "Opening the black-box of artificial intelligence predictions on clinical decision support system", "author": "MIL Neves"}, {"title": "Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirement", "author": "J Bogner"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XA", "author": "Q Wang"}, {"title": "Evaluation of practical accounting education in Jorda", "author": "A Morshed"}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Stud", "author": "L Chazette"}, {"title": "Multi-agent Social Choice for Dynamic Fairness-aware Recommendatio", "author": "R Burke"}, {"title": "Designing Conversational Agents to Encourage User Narrative and Self-Reflection in Mental Wellbein", "author": "\ubc15\uc18c\ud604"}, {"title": "Human-centered explainability for life sciences, healthcare, and medical informatic", "author": "S Dey", "abstract": "Rapid advances in artificial intelligence (AI) and availability of biological, medical, and healthcare data have enabled the development of a wide variety of models. Significant success has been achieved in a wide range of fields, such as genomics, protein folding, disease diagnosis, imaging, and clinical tasks. Although widely used, the inherent opacity of deep AI models has brought criticism from the research field and little adoption in clinical practice. Concurrently, there has been a significant amount of research focused on making", "keywords": [["ai", 0.4556], ["genomics", 0.37], ["diagnosis", 0.2725], ["imaging", 0.2509], ["intelligence", 0.2377], ["medical", 0.2293], ["deep", 0.2281], ["models", 0.2256], ["artificial", 0.2183], ["advances", 0.2051]]}, {"title": "What You See is What You Classify: Black Box Attribution", "author": "S Stalder"}, {"title": "Artificial intelligence pitfalls and how to avoid the", "author": "M Armstrong-Barnes"}, {"title": "Data Protection and Machine-Learning-Supported Decision-Making at the EU Border: ETIAS Profiling Under Scrutin", "author": "P Jo Pesch"}, {"title": "Explainability in Medical AI-Legal Regulation of Explainability in Clinical Decision Support System", "author": "SC Reinhold"}, {"title": "Agent-Based Models Using Artificial Intelligence: A Literature Revie", "author": "M Hauff"}, {"title": "Human-Centered Explainable AI (HCXAI): beyond opening the black-box of A", "author": "U Ehsan"}, {"title": "Achieving Trustable Explanations Through Multi-Task Learning Neural Network", "author": "J Kvamme"}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare System", "author": "M Balci"}, {"title": "Chatboter, tillit og brukeropplevelse i det offentlig", "author": "KYB Gjelle"}, {"title": "\uc9c8\ubcd1\uc758 \uc790\uac00 \uc9c4\ub2e8\uc744 \uc704\ud55c \uc54c\uace0\ub9ac\uc998 \uae30\ubc18 \uc99d\uc0c1 \ud655\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \uc0ac\uc6a9\uc790 \uacbd\ud5d8\uc5d0 \uad00\ud55c \ud0d0\uc0c9\uc801 \uc5f0", "author": "\uae40\ud0dc\uc644\uff0c \ud669\uc720\uc9c4\uff0c \uae40\uc900\ud55c\uff0c \uc774\uc900\ud658\uff0c \ud64d\ud654\uc815", "abstract": "\ube44\ub300\uba74 \uc9c4\ub2e8\uc758 \ud544\uc694\uc131\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uc778\uacf5\uc9c0\ub2a5 \uc54c\uace0\ub9ac\uc998 \uae30\ubc18 \uc99d\uc0c1 \ud655\uc778 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ud65c\uc6a9 \uac00\ub2a5\uc131 \uc5ed\uc2dc \ucee4\uc9c0\uace0 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc0ac\uc6a9\uc790\ub4e4\uc774 \uc99d\uc0c1 \uac80\uc0ac\uae30 (symptom checker) \ub97c \uc0ac\uc6a9\ud558\uba70 \uacbd\ud5d8\ud558\ub294 \uc54c\uace0\ub9ac\uc998 \uacbd\ud5d8 (Algorithmic Experience) \uc5d0 \uad00\ud574 \uc5f0\uad6c\ub41c \ubc14\ub294 \uc544\uc9c1 \uc801\ub2e4. \ubcf8 \uc5f0\uad6c\uc5d0\uc11c\ub294 \uc9c8\ubcd1\uc758 \uc790\uac00 \uc9c4\ub2e8\uc744 \uc704\ud55c \uc99d\uc0c1 \uac80\uc0ac\uae30\uc758 \uc54c\uace0\ub9ac\uc998 \uacbd\ud5d8\uc744 \uc774\ud574\ud558\uae30 \uc704\ud574 \uc5ec\uc12f \uba85\uc758 \uc131\uc778\uc744 \ub300\uc0c1\uc73c\ub85c \ubc1c\uc131 \uc0ac\uace0\ubc95 (think aloud) \uc744 \ud65c\uc6a9\ud55c \ud0d0\uc0c9\uc801 \uc5f0\uad6c\ub97c \uc9c4\ud589\ud558\uc600\ub2e4. \uadf8 \uacb0\uacfc \uc778\uacf5\uc9c0\ub2a5 \uc54c\uace0\ub9ac\uc998\uacfc \uc774\ub97c \ud1b5\ud574 \uc608\uce21\ub41c \uc9c4\ub2e8 \uacb0\uacfc\uc758 \uc2e0\ub8b0\ub3c4 \ubc0f \uc218\uc6a9\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc694\uc18c\ub4e4\uc744", "keywords": {"keywords": ["algorithmic experience", "symptom checker", "artificial intelligence", "diagnosis", "user experience"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Medical"]}}], "human interpretation of saliency-based explanation over text": [{"title": "Mediators: Conversational Agents Explaining NLP Model Behavior", "author": "N Feldhus", "abstract": "The human-centric explainable artificial intelligence (HCXAI) community has raised the need for framing the explanation process as a conversation between human and machine. In this position paper, we establish desiderata for Mediators, text-based conversational agents which are capable of explaining the behavior of neural models interactively using natural language. From the perspective of natural language processing (NLP) research, we engineer a blueprint of such a Mediator for the task of sentiment analysis and assess how far", "keywords": {"keywords": ["Mediators", "Conversational Agents", "Explaining", "NLP Model Behavior", "Sentiment Analysis"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-Centric Explainable Artificial Intelligence"]}}], "it's just not that simple: an empirical study of the accuracy-explainability trade-off in machine learning for public policy": [], "keep your friends close and your counterfactuals closer: improved learning from closest rather than plausible counterfactual explanations in an abstract setting": [{"title": "Let's Go to the Alien Zoo: Introducing an Experimental Framework to Study Usability of Counterfactual Explanations for Machine Learning", "author": "U Kuhl", "abstract": "To foster usefulness and accountability of machine learning (ML), it is essential to explain a model's decisions in addition to evaluating its performance. Accordingly, the field of explainable artificial intelligence (XAI) has resurfaced as a topic of active research, offering approaches to address the\" how\" and\" why\" of automated decision-making. Within this domain, counterfactual explanations (CFEs) have gained considerable traction as a psychologically grounded approach to generate post-hoc explanations. To do so, CFEs", "keywords": {"keywords": ["Explainable artificial intelligence", "Machine learning", "Counterfactual explanations", "Usability", "Experimental framework"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Human-computer interaction"]}}, {"title": "\" Why Here and Not There?\"--Diverse Contrasting Explanations of Dimensionality Reduction", "author": "A Artelt", "abstract": "Dimensionality reduction is a popular preprocessing and a widely used tool in data mining. Transparency, which is usually achieved by means of explanations, is nowadays a widely accepted and crucial requirement of machine learning based systems like classifiers and recommender systems. However, transparency of dimensionality reduction and other data mining tools have not been considered much yet, still it is crucial to understand their behavior--in particular practitioners might want to understand why a specific sample got", "keywords": {"keywords": ["Dimensionality reduction", "Transparency", "Explanations", "Data mining", "Machine learning"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Data preprocessing"]}}], "model agnostic multilevel explanations": [{"title": "A survey on neural network interpretability", "author": "Y Zhang", "abstract": "Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, eg, algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, eg, drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the", "keywords": {"keywords": ["neural network interpretability", "deep learning", "black-box nature", "algorithmic discrimination", "ethics"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "On the computational intelligibility of boolean classifiers", "author": "G Audemard", "abstract": "In this paper, we investigate the computational intelligibility of Boolean classifiers, characterized by their ability to answer XAI queries in polynomial time. The classifiers under consideration are decision trees, DNF formulae, decision lists, decision rules, tree ensembles, and Boolean neural nets. Using 9 XAI queries, including both explanation queries and verification queries, we show the existence of large intelligibility gap between the families of classifiers. On the one hand, all the 9 XAI queries are tractable for decision", "keywords": {"keywords": ["computational intelligibility", "boolean classifiers", "XAI queries", "intelligibility gap", "tractable"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Dax: Deep argumentative explanation for neural networks", "author": "E Albini", "abstract": "Despite the rapid growth in attention on eXplainable AI (XAI) of late, explanations in the literature provide little insight into the actual functioning of Neural Networks (NNs), signi cantly limiting their transparency. We propose a methodology for explaining NNs, providing transparency about their inner workings, by utilising computational argumentation (a form of symbolic AI o ering reasoning abstractions for a variety of settings where opinions matter) as the sca olding underpinning Deep Argumentative eXplanations (DAXs). We dene three DAX", "keywords": {"keywords": ["Deep Argumentative eXplanations", "Neural Networks", "Transparency", "Explainable AI", "Symbolic AI"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta", "abstract": "The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes, also raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties", "keywords": {"keywords": ["explainable artificial intelligence", "evaluating explanations", "machine learning models", "interpretability", "multi-faceted concept"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "CoFrNets: interpretable neural architecture inspired by continued fractions", "author": "I Puri", "abstract": "In recent years there has been a considerable amount of research on local post hoc explanations for neural networks. However, work on building interpretable neural architectures has been relatively sparse. In this paper, we present a novel neural architecture, CoFrNet, inspired by the form of continued fractions which are known to have many attractive properties in number theory, such as fast convergence of approximations to real numbers. We show that CoFrNets can be efficiently trained as well as interpreted", "keywords": {"keywords": ["interpretable neural architecture", "continued fractions", "neural networks", "fast convergence", "approximations"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning"}}, {"title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization", "author": "J Borowski", "abstract": "Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans to predict CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et", "keywords": {"keywords": ["exemplary natural images", "CNN activations", "feature visualization", "psychophysical paradigm", "informativeness"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image recognition"]}}, {"title": "Learning Groupwise Explanations for Black-Box Models.", "author": "J Gao", "abstract": "We study two user demands that are important during the exploitation of explanations in practice: 1) understanding the overall model behavior faithfully with limited cognitive load and 2) predicting the model behavior accurately on unseen instances. We illustrate that the two user demands correspond to two major sub-processes in the human cognitive process and propose a unified framework to fulfill them simultaneously. Given a local explanation method, our framework jointly 1) learns a limited number of groupwise explanations that", "keywords": {"keywords": ["Explanations", "Black-Box Models", "Cognitive Load", "Local Explanation", "Groupwise Explanations"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Extending LIME for business process automation", "author": "S Upadhyay", "abstract": "AI business process applications automate high-stakes business decisions where there is an increasing demand to justify or explain the rationale behind algorithmic decisions. Business process applications have ordering or constraints on tasks and feature values that cause lightweight, model-agnostic, existing explanation methods like LIME to fail. In response, we propose a local explanation framework extending LIME for explaining AI business process applications. Empirical evaluation of our extension underscores the", "keywords": {"keywords": ["extending LIME", "business process automation", "algorithmic decisions", "explanation methods", "AI business process applications"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Business Process Automation"]}}, {"title": "Counterfactual Explanation Trees: Transparent and Consistent Actionable Recourse with Decision Trees", "author": "K Kanamori", "abstract": "Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. An individual can interpret the perturbation as an\" action\" to obtain the desired decision results. Existing CE methods focus on providing an action, which is optimized for a given single instance. However, these CE methods do not address the case where we have to assign actions to multiple instances simultaneously. In such a case, we need a framework of CE that assigns actions to multiple", "keywords": {"keywords": ["Counterfactual Explanation", "Transparent", "Consistent", "Actionable Recourse", "Decision Trees"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Explainable deep learning in healthcare: A methodological survey from an attribution view", "author": "D Jin", "abstract": "The increasing availability of large collections of electronic health record (EHR) data and unprecedented technical advances in deep learning (DL) have sparked a surge of research interest in developing DL based clinical decision support systems for diagnosis, prognosis, and treatment. Despite the recognition of the value of deep learning in healthcare, impediments to further adoption in real healthcare settings remain due to the black\u2010box nature of DL. Therefore, there is an emerging need for interpretable DL, which allows end", "keywords": {"keywords": ["explainable deep learning", "healthcare", "methodological survey", "attribution", "electronic health record (EHR) data"], "research type": ["review"], "methodology": ["quantitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["clinical decision support systems"]}}, {"title": "Towards Explainable Recommendations of Resource Allocation Mechanisms in On-Demand Transport Fleets", "author": "A Daoud", "abstract": "Multi-agent systems can be considered a natural paradigm when modeling various transportation systems, whose management involves solving hard, dynamic, and distributed allocation problems. Such problems have been studied for decades, and various solutions have been proposed. However, even the most straightforward resource allocation mechanisms lead to debates on efficiency vs. fairness, business quality vs. passenger's user experience, or performance vs. robustness. We aim to design an analytical tool that", "keywords": {"keywords": ["multi-agent systems", "resource allocation", "on-demand transport", "explanability", "recommendations"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory", "explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["transportation"]}}, {"title": "Let the CAT out of the bag: Contrastive Attributed explanations for Text", "author": "S Chemmengath", "abstract": "Contrastive explanations for understanding the behavior of black box models has gained a lot of attention recently as they provide potential for recourse. In this paper, we propose a method Contrastive Attributed explanations for Text (CAT) which provides contrastive explanations for natural language text data with a novel twist as we build and exploit attribute classifiers leading to more semantically meaningful explanations. To ensure that our contrastive generated text has the fewest possible edits with respect to the original text", "keywords": {"keywords": ["contrastive explanations", "black box models", "natural language text data", "attribute classifiers", "semantically meaningful explanations"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Text Data Analysis"]}}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimization", "author": "\u91d1\u68ee\u61b2\u592a\u6717", "abstract": "This thesis studies new approaches to realizing the explainability of machine learning. For the last decade, due to the remarkable progress of machine learning technologies, complex machine learning models, such as deep neural networks, have been ubiquitous in society. They have performed well in various prediction tasks and assisted human users with their critical decision-making tasks in the real world, including medical diagnoses, loan approvals, and judicial decisions. In such critical decision-making tasks, decisions based on", "keywords": {"keywords": ["explainable machine learning", "integer linear optimization", "predictive models", "decision-making tasks", "critical decisions"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Efficient Construction and Explanation of Machine Learning Models through Database Techniques", "author": "S Hasani", "abstract": "Machine learning (ML) has been widely adopted in the last few years and it has had an undeniable impact on the ways many organizations make decisions. While great advances have been made in developing new ML algorithms and applications, there is a major need for scalable ML solutions in order to meet the demands of the Big data era. In this dissertation, we focus on improving the efficiency of two main machine learning solutions through database techniques: i) efficient construction of machine learning models", "keywords": {"keywords": ["efficient construction", "explanation", "machine learning models", "database techniques", "sc scalable ML solutions"], "research type": ["case study"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["big data"]}}, {"title": "KACE: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference", "author": "Q Chen", "abstract": "In order to better understand the reason behind model behaviors (ie, making predictions), most recent works have exploited generative models to provide complementary explanations. However, existing approaches in NLP mainly focus on \u201cWHY A\u201d rather than contrastive \u201cWHY A NOT B\u201d, which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields. In this paper, we focus on generating contrastive explanations with counterfactual examples in NLI and propose a", "keywords": {"keywords": ["knowledge aware", "contrastive explanations", "natural language inference", "generative models", "counterfactual examples"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Natural Language Processing"]}}, {"title": "Explainable Text Classification Model for COVID-19 Fake News Detection", "author": "M Ahmed", "abstract": "Artificial intelligence has achieved notable advances across many applications, and the field is recently concerned with developing novel methods to explain machine learning models. Deep neural networks deliver the best performance accuracy in different domains, such as text categorization, image classification, and speech recognition. Since the neural network models are black-box types, they lack transparency and explainability in predicting results. During the COVID-19 pandemic, Fake News Detection is a challenging research problem as", "keywords": {"keywords": ["Explainable Text Classification", "COVID-19 Fake News Detection", "Artificial intelligence", "Deep neural networks", "Transparency"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Fake News Detection"]}}, {"title": "\u201cJust\u201d Algorithms: Justification (Beyond Explanation) of Automated Decisions Under the General Data Protection Regulation", "author": "G Malgieri", "abstract": "This paper argues that if we want a sustainable environment of desirable AI systems, we should aim not only at transparent, explainable, fair, lawful, and accountable algorithms, but we also should seek for \u201cjust\u201d algorithms, that is, automated decision-making systems that include all the above-mentioned qualities (transparency, explainability, fairness, lawfulness, and accountability). This is possible through a practical \u201cjustification\u201d statement and process (eventually derived from algorithmic impact assessment) through which the data controller", "keywords": {"keywords": ["just algorithms", "automated decisions", "transparency", "explainability", "accountability"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["prescriptive"], "discipline": ["legal studies"], "content": ["methodological"], "application area": ["data protection"]}}, {"title": "Human-centered explainability for life sciences, healthcare, and medical informatics", "author": "S Dey", "abstract": "Rapid advances in artificial intelligence (AI) and availability of biological, medical, and healthcare data have enabled the development of a wide variety of models. Significant success has been achieved in a wide range of fields, such as genomics, protein folding, disease diagnosis, imaging, and clinical tasks. Although widely used, the inherent opacity of deep AI models has brought criticism from the research field and little adoption in clinical practice. Concurrently, there has been a significant amount of research focused on making", "keywords": {"keywords": ["Human-centered explainability", "life sciences", "healthcare", "medical informatics", "artificial intelligence"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["AI in healthcare"]}}, {"title": "Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners", "author": "KN Ramamurthy", "abstract": "Post-hoc explanations for black box models have been studied extensively in classification and regression settings. However, explanations for models that output similarity between two inputs have received comparatively lesser attention. In this paper, we provide model agnostic local explanations for similarity learners applicable to tabular and text data. We first propose a method that provides feature attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. We then propose analogies as a new", "keywords": {"keywords": ["model agnostic explanation", "similarity learners", "feature attributions", "analogies", "tabular and text data"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}], "explainable recommendation through attentive multi-view learning": [{"title": "Explainable recommendation: A survey and new perspectives", "author": "Y Zhang", "abstract": "Explainable recommendation attempts to develop models that generate not only high-quality recommendations but also intuitive explanations. The explanations may either be post-hoc or directly come from an explainable model (also called interpretable or transparent model in some contexts). Explainable recommendation tries to address the problem of why: by providing explanations to users or system designers, it helps humans to understand why certain items are recommended by the algorithm, where the human can either be users or", "keywords": {"keywords": ["Explainable recommendation", "Post-hoc explanations", "Intuitive explanations", "Interpretable models", "Transparent models"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Recommendation systems"]}}, {"title": "Fairness-aware explainable recommendation over knowledge graphs", "author": "Z Fu"}, {"title": "Survey on deep multi-modal data analytics: Collaboration, rivalry, and fusion", "author": "Y Wang"}, {"title": "Explainable fashion recommendation: A semantic attribute region guided approach", "author": "M Hou"}, {"title": "Generate neural template explanations for recommendation", "author": "L Li"}, {"title": "Graph learning based recommender systems: A review", "author": "S Wang"}, {"title": "Leveraging demonstrations for reinforcement recommendation reasoning over knowledge graphs", "author": "K Zhao"}, {"title": "The emergence of explainability of intelligent systems: Delivering explainable and personalized recommendations for energy efficiency", "author": "C Sardianos"}, {"title": "Towards explainable conversational recommendation", "author": "Z Chen"}, {"title": "A survey of recommender systems for energy efficiency in buildings: Principles, challenges and prospects", "author": "Y Himeur"}, {"title": "Personalized reason generation for explainable song recommendation", "author": "G Zhao", "abstract": "Personalized recommendation has received a lot of attention as a highly practical research topic. However, existing recommender systems provide the recommendations with a generic statement such as \u201cCustomers who bought this item also bought\u201d. Explainable recommendation, which makes a user aware of why such items are recommended, is in demand. The goal of our research is to make the users feel as if they are receiving recommendations from their friends. To this end, we formulate a new challenging problem", "keywords": {"keywords": ["personalized recommendation", "reason generation", "explainable recommendation", "recommender systems", "friends"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Song recommendation"]}}, {"title": "A survey on knowledge graph-based recommender systems", "author": "QIN Chuan"}, {"title": "Counterfactual explainable recommendation", "author": "J Tan"}, {"title": "Explainable recommendation via interpretable feature mapping and evaluation of explainability", "author": "D Pan"}, {"title": "Mlrda: A multi-task semi-supervised learning framework for drug-drug interaction prediction", "author": "X Chu"}, {"title": "Personalized employee training course recommendation with career development awareness", "author": "C Wang"}, {"title": "Explainable artificial intelligence for developing smart cities solutions", "author": "D Thakker"}, {"title": "Graph learning approaches to recommender systems: A review", "author": "S Wang"}, {"title": "Set-sequence-graph: A multi-view approach towards exploiting reviews for recommendation", "author": "J Gao"}, {"title": "Joint all domain command and control for modern warfare: an analytic framework for identifying and developing artificial intelligence applications", "author": "S Lingel"}, {"title": "Hierarchical attention based neural network for explainable recommendation", "author": "D Cong", "abstract": "In recent years, recommendation systems have attracted more and more attention due to the rapid development of e-commerce. Reviews information can offer help in modeling user's preference and item's performance. Some existing methods utilize reviews for the recommendation. However, few of those models consider the importance of reviews and words in corpus together. Therefore, we propose an approach for rating prediction using a hierarchical attention-based network named HANN, which can distinguish the importance of", "keywords": {"keywords": ["recommendation systems", "reviews", "hierarchical attention-based neural network", "explanability", "rating prediction"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["e-commerce"]}}, {"title": "News Graph: An Enhanced Knowledge Graph for News Recommendation.", "author": "D Liu"}, {"title": "Synthesizing aspect-driven recommendation explanations from reviews", "author": "TH Le"}, {"title": "CAESAR: context-aware explanation based on supervised attention for service recommendations", "author": "L Li"}, {"title": "STAR: Spatio-temporal taxonomy-aware tag recommendation for citizen complaints", "author": "J Gao"}, {"title": "Ex3: Explainable attribute-aware item-set recommendations", "author": "Y Xian"}, {"title": "Explainable recommendation with comparative constraints on product aspects", "author": "TH Le"}, {"title": "Fusing knowledge and aspect sentiment for explainable recommendation", "author": "P Bai"}, {"title": "Collaborative filtering with a deep adversarial and attention network for cross-domain recommendation", "author": "H Liu"}, {"title": "TAERT: triple-attentional explainable recommendation with temporal convolutional network", "author": "S Guo"}, {"title": "A long short-term memory deep learning framework for explainable recommendation", "author": "H Zarzour", "abstract": "Due to the growing quantity of information available on the Web, recommender systems have become crucial component for the success of online shopping stores. However, most of the existing recommender systems were only designed to improve the recommendation results and ignore the explainable recommendation aspect. Therefore, in this paper we propose a long short-term memory deep learning framework for explainable recommendation, that is able to generate an efficient explanation for any rating made by", "keywords": {"keywords": ["deep learning", "long short-term memory", "recommendation", "explainable recommendation", "recommender systems"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Online shopping"]}}, {"title": "Learning to deceive knowledge graph augmented models via targeted perturbation", "author": "M Raman"}, {"title": "ReXPlug: Explainable Recommendation using Plug-and-Play Language Model", "author": "DV Hada"}, {"title": "Explainable recommender systems via resolving learning representations", "author": "N Liu"}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta"}, {"title": "Unbalanced Incomplete Multi-view Clustering via the Scheme of View Evolution: Weak Views are Meat; Strong Views do Eat", "author": "X Fang"}, {"title": "Explainable Fairness in Recommendation", "author": "Y Ge"}, {"title": "Neural explainable recommender model based on attributes and reviews", "author": "YY Liu"}, {"title": "From Intrinsic to Counterfactual: On the Explainability of Contextualized Recommender Systems", "author": "Y Zhou"}, {"title": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u7814\u7a76\u8fdb\u5c55", "author": "\u5316\u76c8\u76c8\uff0c \u5f20\u5cb1\u5880\uff0c \u845b\u4ed5\u660e"}, {"title": "Adherence and constancy in lime-rs explanations for recommendation", "author": "VW Anelli", "abstract": "Explainable Recommendation has attracted a lot of attention due to a renewed interest in explainable artificial intelligence. In particular, post-hoc approaches have proved to be the most easily applicable ones to increasingly complex recommendation models, which are then treated as black-boxes. The most recent literature has shown that for post-hoc explanations based on local surrogate models, there are problems related to the robustness of the approach itself. This consideration becomes even more relevant in human-related", "keywords": {"keywords": ["Explainable Recommendation", "Artificial Intelligence", "Post-hoc approaches", "Local surrogate models", "Robustness"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Recommendation Systems"]}}, {"title": "Discover Micro-Influencers for Brands via Better Understanding", "author": "S Wang"}, {"title": "Learning domain semantics and cross-domain correlations for paper recommendation", "author": "Y Xie"}, {"title": "A survey on knowledge-aware news recommender systems", "author": "A Iana"}, {"title": "Graph neural network and multi-view learning based mobile application recommendation in heterogeneous graphs", "author": "F Xie"}, {"title": "Research Progress in the Interpretability of Deep Learning Models", "author": "HUA Yingying"}, {"title": "Enhancing Recommendation with Automated TagTaxonomy Construction in Hyperbolic Space", "author": "Y Tan"}, {"title": "Towards local post-hoc recommender systems explanations", "author": "A Chanson"}, {"title": "Explainable AI for Cultural Minds", "author": "H Kopeck\u00e1"}, {"title": "A convolutional neural network-based reviews classification method for explainable recommendations", "author": "H Zarzour"}, {"title": "Stream-based explainable recommendations via blockchain profiling", "author": "F Leal", "abstract": "Explainable recommendations enable users to understand why certain items are suggested and, ultimately, nurture system transparency, trustworthiness, and confidence. Large crowdsourcing recommendation systems ought to crucially promote authenticity and transparency of recommendations. To address such challenge, this paper proposes the use of stream-based explainable recommendations via blockchain profiling. Our contribution relies on chained historical data to improve the quality and transparency of online", "keywords": {"keywords": ["explainable recommendations", "blockchain profiling", "stream-based", "recommendation systems", "transparency"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["online recommendation systems"]}}, {"title": "Learning to Explain Recommendations", "author": "L Li"}, {"title": "Who wants to use an augmented reality shopping assistant application?", "author": "D Mora"}, {"title": "Preference-based and local post-hoc explanations for recommender systems", "author": "L Brunot"}, {"title": "AskMe: joint individual-level and community-level behavior interaction for question recommendation", "author": "N Li"}, {"title": "Explainable, trustworthy, and ethical machine learning for healthcare: A survey", "author": "K Rasheed"}, {"title": "Deep multi-graph neural networks with attention fusion for recommendation", "author": "Y Song"}, {"title": "Personalized and Explainable Employee Training Course Recommendations: A Bayesian Variational Approach", "author": "C Wang"}, {"title": "Reinforcement Learning over Sentiment-Augmented Knowledge Graphs towards Accurate and Explainable Recommendation", "author": "SJ Park"}, {"title": "A General Matrix Factorization Framework for Recommender Systems in Multi-access Edge Computing Network", "author": "G Liang"}, {"title": "Enhanced review-based rating prediction by exploiting aside information and user influence", "author": "S Wu", "abstract": "User-generated reviews greatly supplement the descriptions of items and thereby play an important role in decision making. Researchers have been exploiting these invaluable resources to discover the users' preferences, model the items' properties and further provide an explainable recommendation. Legacy strategies seek to quantify the reviews by directly processing the text. However, not all reviews are equally reliable or influential, as the reviews might be generated by different users under various conditions, purposes and", "keywords": {"keywords": ["review-based rating prediction", "aside information", "user influence", "user-generated reviews", "decision making"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Persuasive Technology"]}}, {"title": "Cross-Domain Explicit-Implicit-Mixed Collaborative Filtering Neural Network", "author": "CD Wang"}, {"title": "Exploring Multifaced User Modelling in Textual Data Streams", "author": "L Zhang"}, {"title": "Neural Explainable Recommender Model Based on Attributes and", "author": "YY Liu"}, {"title": "Enhancing explainability and scrutability of recommender systems", "author": "A Ghazimatin"}, {"title": "Personalized Chit-Chat Generation for Recommendation Using External Chat Corpora", "author": "C Chen"}, {"title": "The risks of opening the car bonnet: A semi-systematic review of explainability in AI systems", "author": "CE Serrano"}, {"title": "Recommender systems based on graph embedding techniques: A comprehensive review", "author": "Y Deng"}, {"title": "High-Quality Paths: Multi-View Attention Neural Network With High-Quality Paths for Top-N Recommendation", "author": "W Liu"}, {"title": "Using Deep Learning for Positive Reviews Prediction in Explainable Recommendation Systems", "author": "H Zarzour"}, {"title": "Multi-View Representation Learning: Approaches and Applications", "author": "P Tang", "abstract": "The performance of machine learning algorithms is heavily dependent on the quality of the data representation. Learning discriminative data representations becomes a critical step in the machine learning task pipeline. Data representations can be obtained from multiple views. For example, in a complex network model, the representation of each node can be viewed as a combination of the correlations between itself and its neighbors and the node attributes. A comprehensive representation can be obtained because data from different", "keywords": {"keywords": ["multi-view representation learning", "data representation", "machine learning algorithms", "complex network model", "correlations"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": [" methodological"], "application area": ["machine learning"]}}, {"title": "An interpretable mechanism for personalized recommendation based on cross feature", "author": "L Ye"}, {"title": "Explain and Conquer: Personalised Text-based Reviews to Achieve Transparency", "author": "ILR Botana"}, {"title": "A Systematic Review of Deep Knowledge Graph-Based Recommender Systems, with Focus on Explainable Embeddings", "author": "RF Doh"}, {"title": "Developing Future Human-Centered Smart Cities: Critical Analysis of Smart City Security, Interpretability, and Ethical Challenges", "author": "K Ahmada"}, {"title": "Enhancing Recommender System with Collaborative Filtering and User Experiences Filtering", "author": "SV Aciar"}, {"title": "Measuring\" Why\" in Recommender Systems: a Comprehensive Survey on the Evaluation of Explainable Recommendation", "author": "X Chen"}, {"title": "McHa: a multistage clustering-based hierarchical attention model for knowledge graph-aware recommendation", "author": "J Wang"}, {"title": "INTERPRETABLE MACHINE LEARNING AND APPLICATIONS", "author": "D PAN"}, {"title": "Recommender systems based on graph embedding techniques: A review", "author": "Y Deng"}, {"title": "Counterfactually Evaluating Explanations in Recommender Systems", "author": "Y Yao", "abstract": "Modern recommender systems face an increasing need to explain their recommendations. Despite considerable progress in this area, evaluating the quality of explanations remains a significant challenge for researchers and practitioners. Prior work mainly conducts human study to evaluate explanation quality, which is usually expensive, time-consuming, and prone to human bias. In this paper, we propose an offline evaluation method that can be computed without human involvement. To evaluate an explanation, our method quantifies its", "keywords": {"keywords": ["Recommender systems", "Explanations", "Evaluation", "Counterfactual", "Offline"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Recommendation Systems"]}}, {"title": "KR-GCN: Knowledge-aware Reasoning with Graph Convolution Network for Explainable Recommendation", "author": "T Ma"}, {"title": "Natural Language Explanation for Recommendations and Beyond", "author": "L Li"}, {"title": "Explainable AI: a narrative review at the crossroad of Knowledge Discovery, Knowledge Representation and Representation Learning", "author": "IC Kaadoud"}, {"title": "A Systematic Review of Deep Knowledge Graph-Based Recommender Systems, with Focus on Explainable Embeddings. Data 2022, 7, 94", "author": "RF Doh"}, {"title": "AmpSum: Adaptive Multiple-Product Summarization towards Improving Recommendation Captions", "author": "QT Truong"}, {"title": "Effects of Feature-Based Explanation and Its Output Modality on User Satisfaction With Service Recommender Systems", "author": "Z Zhang"}, {"title": "\u878d\u5408\u8bc4\u8bba\u6587\u672c\u7279\u5f81\u548c\u8bc4\u5206\u56fe\u5377\u79ef\u8868\u793a\u7684\u63a8\u8350\u6a21\u578b", "author": "\u51af\u6d77\u6797\uff0c \u5f20\u6f47\uff0c \u5218\u540c\u5b58"}, {"title": "\u63a8\u85a6\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u63a8\u85a6\u7406\u7531\u306e\u8aac\u660e\u53ef\u80fd\u6027\u306b\u95a2\u3059\u308b\u30b5\u30fc\u30d9\u30a4", "author": "\u677e\u5cf6\u3072\u308d\u3080\uff0c \u68ee\u6fa4\u7ae3\uff0c \u77f3\u5c71\u7422\u5df1\uff0c \u5c71\u540d\u65e9\u4eba"}], "exploring the role of local and global explanations in recommender systems": [{"title": "User Trust on an Explainable AI-based Medical Diagnosis Support Syste", "author": "Y Rong", "abstract": "Recent research has supported that system explainability improves user trust and willingness to use medical AI for diagnostic support. In this paper, we use chest disease diagnosis based on X-Ray images as a case study to investigate user trust and reliance. Building off explainability, we propose a support system where users (radiologists) can view causal explanations for final decisions. After observing these causal explanations, users provided their opinions of the model predictions and could correct explanations if they did", "keywords": {"keywords": ["user trust", "explainable AI", "medical diagnosis", "chest disease diagnosis", "X-ray images"], "research type": ["case study"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Healthcare"]}}], "manipulating and measuring model interpretability": [{"title": "Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)", "author": "A Adadi", "abstract": "At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI", "keywords": {"keywords": ["explainable artificial intelligence", "black-box", "transparency", "algorithms", "debate"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Rise: Randomized input sampling for explanation of black-box models", "author": "V Petsiuk"}, {"title": "Explainable machine learning in deployment", "author": "U Bhatt"}, {"title": "From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices", "author": "J Morley"}, {"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao"}, {"title": "Towards automatic concept-based explanations", "author": "A Ghorbani"}, {"title": "Interpreting interpretability: understanding data scientists' use of interpretability tools for machine learning", "author": "H Kaur"}, {"title": "Understanding the effect of accuracy on trust in machine learning models", "author": "M Yin"}, {"title": "What clinicians want: contextualizing explainable machine learning for clinical end use", "author": "S Tonekaboni"}, {"title": "\" Hello AI\": uncovering the onboarding needs of medical practitioners for human-AI collaborative decision-making", "author": "CJ Cai"}, {"title": "Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments", "author": "B Green", "abstract": "Despite vigorous debates about the technical characteristics of risk assessments being deployed in the US criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions---they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human", "keywords": {"keywords": ["risk assessments", "fairness", "decision-making processes", "algorithm-in-the-loop analysis", "criminal justice system"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Sociology", "content": "Application", "application area": "Criminal justice"}}, {"title": "How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation", "author": "M Narayanan"}, {"title": "Gamut: A design probe to understand how data scientists understand machine learning models", "author": "F Hohman"}, {"title": "Problems with Shapley-value-based explanations as feature importance measures", "author": "IE Kumar"}, {"title": "Toward trustworthy AI development: mechanisms for supporting verifiable claims", "author": "M Brundage"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "Beyond accuracy: The role of mental models in human-AI team performance", "author": "G Bansal"}, {"title": "Interpretable machine learning\u2013a brief history, state-of-the-art and challenges", "author": "C Molnar"}, {"title": "A survey on the explainability of supervised machine learning", "author": "N Burkart"}, {"title": "The principles and limits of algorithm-in-the-loop decision making", "author": "B Green"}, {"title": "On the interpretability of artificial intelligence in radiology: challenges and opportunities", "author": "M Reyes", "abstract": "As artificial intelligence (AI) systems begin to make their way into clinical radiology practice, it is crucial to assure that they function correctly and that they gain the trust of experts. Toward this goal, approaches to make AI \u201cinterpretable\u201d have gained attention to enhance the understanding of a machine learning algorithm, despite its complexity. This article aims to provide insights into the current state of the art of interpretability methods for radiology AI. This review discusses radiologists' opinions on the topic and suggests trends and", "keywords": {"keywords": ["interpretability", "artificial intelligence", "radiology", "challenges", "opportunities"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["medical science"], "content": ["methodological"], "application area": ["clinical radiology"]}}, {"title": "An evaluation of the human-interpretability of explanation", "author": "I Lage"}, {"title": "Interpretable to whom? A role-based model for analyzing interpretable machine learning systems", "author": "R Tomsett"}, {"title": "Explainable artificial intelligence: a systematic review", "author": "G Vilone"}, {"title": "Human-in-the-loop interpretability prior", "author": "I Lage"}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal"}, {"title": "Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research.", "author": "JW Vaughan"}, {"title": "AI in health: state of the art, challenges, and future directions", "author": "F Wang"}, {"title": "Should health care demand interpretable artificial intelligence or accept \u201cblack box\u201d medicine?", "author": "F Wang"}, {"title": "Interpretable machine learning: Fundamental principles and 10 grand challenges", "author": "C Rudin"}, {"title": "Towards accountable ai: Hybrid human-machine analyses for characterizing system failure", "author": "B Nushi", "abstract": "states of system components, and final output quality. We present Pandora, a set of hybrid  human-machine methods and tools for describing and explaining system failures. Pandora", "keywords": {"keywords": ["accountable AI", "hybrid human-machine analyses", "system failure", "system components", "output quality"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["Application"], "application area": ["explanation of system failures"]}}, {"title": "Evaluating saliency map explanations for convolutional neural networks: a user study", "author": "A Alqaraawi"}, {"title": "Co-designing a real-time classroom orchestration tool to support teacher\u2013AI complementarity", "author": "K Holstein"}, {"title": "Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems", "author": "Z Bu\u00e7inca"}, {"title": "Factors influencing perceived fairness in algorithmic decision-making: Algorithm outcomes, development procedures, and individual differences", "author": "R Wang"}, {"title": "Evaluating the quality of machine learning explanations: A survey on methods and metrics", "author": "J Zhou"}, {"title": "The false hope of current approaches to explainable artificial intelligence in health care", "author": "M Ghassemi"}, {"title": "Machine learning in mental health: A systematic review of the HCI literature to support the development of effective and implementable ML systems", "author": "A Thieme"}, {"title": "Explainability scenarios: towards scenario-based XAI design", "author": "CT Wolf"}, {"title": "NBDT: neural-backed decision trees", "author": "A Wan"}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan", "abstract": "As AI-powered systems increasingly mediate consequential decision-making, their explainability is critical for end-users to take informed and accountable actions. Explanations in human-human interactions are socially-situated. AI systems are often socio-organizationally embedded. However, Explainable AI (XAI) approaches have been predominantly algorithm-centered. We take a developmental step towards socially-situated XAI by introducing and exploring Social Transparency (ST), a sociotechnically informed", "keywords": {"keywords": ["explainability", "AI systems", "social transparency", "XAI", "sociotechnical"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI ethics"]}}, {"title": "A multi-objective approach for profit-driven feature selection in credit scoring", "author": "N Kozodoi"}, {"title": "Debugging tests for model explanations", "author": "J Adebayo"}, {"title": "Economics, fairness and algorithmic bias", "author": "B Cowgill"}, {"title": "Transparency and trust in artificial intelligence systems", "author": "P Schmidt"}, {"title": "Tree space prototypes: Another look at making tree ensembles interpretable", "author": "S Tan"}, {"title": "Personalized explanation in machine learning: A conceptualization", "author": "J Schneider"}, {"title": "Human evaluation of models built for interpretability", "author": "I Lage"}, {"title": "Evaluating and aggregating feature-based model explanations", "author": "U Bhatt"}, {"title": "Rapid trust calibration through interpretable and uncertainty-aware AI", "author": "R Tomsett"}, {"title": "\" Why is' Chicago'deceptive?\" Towards Building Model-Driven Tutorials for Humans", "author": "V Lai", "abstract": "To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a train-ing phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically", "keywords": {"keywords": ["machine learning", "model-driven tutorials", "human decision making", "explaining machine predictions", "science communication"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Building more accurate decision trees with the additive tree", "author": "JM Luna"}, {"title": "Benchmarking attribution methods with relative feature importance", "author": "M Yang"}, {"title": "Materials acceleration platforms: On the way to autonomous experimentation", "author": "MM Flores-Leonar"}, {"title": "ViCE: visual counterfactual explanations for machine learning models", "author": "O Gomez"}, {"title": "A human-grounded evaluation of shap for alert processing", "author": "HJP Weerts"}, {"title": "A review of possible effects of cognitive biases on interpretation of rule-based machine learning models", "author": "T Kliegr"}, {"title": "A human-grounded evaluation benchmark for local explanations of machine learning", "author": "S Mohseni"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "A human-centered agenda for intelligible machine learning", "author": "JW Vaughan"}, {"title": "Attitudes on autonomous vehicle adoption using interpretable gradient boosting machine", "author": "D Lee", "abstract": "This article applies machine learning (ML) to develop a choice model on three choice alternatives related to autonomous vehicles (AV): regular vehicle (REG), private AV (PAV), and shared AV (SAV). The learned model is used to examine users' preferences and behaviors on AV uptake by car commuters. Specifically, this study applies gradient boosting machine (GBM) to stated preference (SP) survey data (ie, panel data). GBM notably possesses more interpretable features than other ML methods as well as high predictive", "keywords": {"keywords": ["autonomous vehicles", "machine learning", "gradient boosting machine", "choice model", "stated preference"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Transportation"], "content": ["Methodological"], "application area": ["Autonomous vehicles adoption"]}}, {"title": "Data provenance: What next?", "author": "P Buneman"}, {"title": "Four principles of explainable artificial intelligence", "author": "PJ Phillips"}, {"title": "Measuring association between labels and free-text rationales", "author": "S Wiegreffe"}, {"title": "Machine learning explainability for external stakeholders", "author": "U Bhatt"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "A survey of explainable AI terminology", "author": "MA Clinciu"}, {"title": "Understanding client support strategies to improve clinical outcomes in an online mental health intervention", "author": "P Chikersal"}, {"title": "Notions of explainability and evaluation approaches for explainable artificial intelligence", "author": "G Vilone"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "Uncertainty in current and future health wearables", "author": "B Knowles", "abstract": "Here, we explore the implications of, and difficulties in designing for, uncertainties  regarding health wearables. We begin with the relatively minimal negative impact of uncertainty  in current consumer uses of these gadgets as a way to demonstrate the known-but-as-yet-unresolved  challenges in communicating health data to users. We next argue that seemingly innocuous  uncertainties emerging in the present use of wearables need attending to, as they are likely  to pro-", "keywords": {"keywords": ["health wearables", "uncertainty", "challenges", "communicating health data", "consumer uses"], "research type": ["theoretical"], "methodology": [" Mixed-methods"], "purposes": [" Exploratory"], "discipline": ["Others"], "content": [" Methodological"], "application area": ["Healthcare"]}}, {"title": "Quantifying model complexity via functional decomposition for better post-hoc interpretability", "author": "C Molnar"}, {"title": "Does explainable artificial intelligence improve human decision-making?", "author": "Y Alufaisan"}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "Manipulation-proof machine learning", "author": "D Bj\u00f6rkegren"}, {"title": "Interpretable question answering on knowledge bases and text", "author": "A Sydorova"}, {"title": "Assessing the local interpretability of machine learning models", "author": "D Slack"}, {"title": "Towards quantification of explainability in explainable artificial intelligence methods", "author": "SR Islam"}, {"title": "Unexplainability and incomprehensibility of artificial intelligence", "author": "RV Yampolskiy"}, {"title": "Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks", "author": "Z Lu"}, {"title": "Priu: A provenance-based approach for incrementally updating regression models", "author": "Y Wu", "abstract": "The ubiquitous use of machine learning algorithms brings new challenges to traditional database problems such as incremental view update. Much effort is being put in better understanding and debugging machine learning models, as well as in identifying and repairing errors in training datasets. Our focus is on how to assist these activities when they have to retrain the machine learning model after removing problematic training samples in cleaning or selecting different subsets of training data for interpretability. This paper presents", "keywords": {"keywords": ["machine learning", "incremental view update", "provenance", "regression models", "training datasets"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Incremental learning"]}}, {"title": "Evaluating and characterizing human rationales", "author": "S Carton"}, {"title": "Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle", "author": "S Dhanorkar"}, {"title": "Learning representations by humans, for humans", "author": "S Hilgard"}, {"title": "Contemporary symbolic regression methods and their relative performance", "author": "W La Cava"}, {"title": "Now you see me (CME): concept-based model extraction", "author": "D Kazhdan"}, {"title": "Explainable artificial intelligence approaches: A survey", "author": "SR Islam"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "Impact of algorithmic decision making on human behavior: Evidence from ultimatum bargaining", "author": "A Erlei"}, {"title": "Reliable post hoc explanations: Modeling uncertainty in explainability", "author": "D Slack"}, {"title": "Feature-Based Explanations Don't Help People Detect Misclassifications of Online Toxicity", "author": "S Carton", "abstract": "We present an experimental assessment of the impact of feature attribution-style explanations on human performance in predicting the consensus toxicity of social media posts with advice from an unreliable machine learning model. By doing so we add to a small but growing body of literature inspecting the utility of interpretable machine learning in terms of human outcomes. We also evaluate interpretable machine learning for the first time in the important domain of online toxicity, where fully-automated methods have faced criticism as", "keywords": {"keywords": ["online toxicity", "social media posts", "interpretable machine learning", "feature attribution", "human performance"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Online toxicity detection"]}}, {"title": "Explanations in autonomous driving: A survey", "author": "D Omeiza"}, {"title": "On parameter interpretability of phenomenological-based semiphysical models in biology", "author": "L Lema-Perez"}, {"title": "Misplaced trust: measuring the interference of machine learning in human decision-making", "author": "H Suresh"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Better metrics for evaluating explainable artificial intelligence", "author": "A Rosenfeld"}, {"title": "Constraints-based explanations of classifications", "author": "D Deutch"}, {"title": "Learning a formula of interpretability to learn interpretable formulas", "author": "M Virgolin"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "Crowdsourcing\u2013A step towards advanced machine learning", "author": "BS Abhigna"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu", "abstract": "Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in", "keywords": {"keywords": ["out-of-distribution examples", "interactive explanations", "human-ai decision making", "complementary performance", "prediction tasks"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory", "Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-AI Teams"]}}, {"title": "Unexplainability and Incomprehensibility of AI", "author": "RV Yampolskiy"}, {"title": "Counterfactual Explanations for Multivariate Time Series", "author": "E Ates"}, {"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer"}, {"title": "Crosscheck: Rapid, reproducible, and interpretable model evaluation", "author": "D Arendt"}, {"title": "Social Sensemaking with AI: Designing an Open-ended AI experience with a Blind Child", "author": "C Morrison"}, {"title": "A slow algorithm improves users' assessments of the algorithm's accuracy", "author": "JS Park"}, {"title": "Trust in public policy algorithms", "author": "RP Kennedy"}, {"title": "Evaluating the interpretability of generative models by interactive reconstruction", "author": "A Ross"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Psychological foundations of explainability and interpretability in artificial intelligence", "author": "DA Broniatowski", "abstract": "In this paper, we make the case that interpretability and explainability are distinct requirements for machine learning systems. To make this case, we provide an overview of the literature in experimental psychology pertaining to interpretation (especially of numerical stimuli) and comprehension. We find that interpretation refers to the ability to contextualize a model's output in a manner that relates it to the system's designed functional purpose, and the goals, values, and preferences of end users. In contrast, explanation refers to the ability", "keywords": {"keywords": ["interpretability", "explainability", "psychology", "numerical stimuli", "comprehension"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Epistemic values in feature importance methods: Lessons from feminist epistemology", "author": "L Hancox-Li"}, {"title": "Interpretable learning-to-rank with generalized additive models", "author": "H Zhuang"}, {"title": "A Survey of Human\u2010Centered Evaluations in Human\u2010Centered Machine Learning", "author": "F Sperrle"}, {"title": "You'd better stop! Understanding human reliance on machine learning models under covariate shift", "author": "CW Chiang"}, {"title": "Designing for human-AI complementarity in K-12 education", "author": "K Holstein"}, {"title": "Can you explain that? Lucid explanations help human-AI collaborative image retrieval", "author": "A Ray"}, {"title": "Belief polarization in a complex world: A learning theory perspective", "author": "N Haghtalab"}, {"title": "Leveraging latent features for local explanations", "author": "R Luss"}, {"title": "Deciding fast and slow: The role of cognitive biases in ai-assisted decision-making", "author": "C Rastogi", "abstract": "Several strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people's perception and understanding is often distorted by their cognitive biases, like confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive", "keywords": {"keywords": ["cognitive biases", "AI-assisted decision-making", "artificial intelligence", "cognitive science", "human decision-makers"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["decision-making"]}}, {"title": "Uncertainty handling in surrogate assisted optimisation of games", "author": "V Volz"}, {"title": "The flaws of policies requiring human oversight of government algorithms", "author": "B Green"}, {"title": "Deep learning in upper gastrointestinal disorders: status and future perspectives", "author": "CS Bang"}, {"title": "Investigating the intelligibility of a computer vision system for blind users", "author": "S Ahmed"}, {"title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations", "author": "S Arora"}, {"title": "SIDU: similarity difference and uniqueness method for explainable AI", "author": "SM Muddamsetty"}, {"title": "Biased TextRank: unsupervised graph-based content extraction", "author": "A Kazemi"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro"}, {"title": "Spatial relation learning for explainable image classification and annotation in critical applications", "author": "R Pierrard"}, {"title": "When does uncertainty matter?: Understanding the impact of predictive uncertainty in ML assisted decision making", "author": "S McGrath", "abstract": "As machine learning (ML) models are increasingly being employed to assist human decision makers, it becomes critical to provide these decision makers with relevant inputs which can help them decide if and how to incorporate model predictions into their decision making. For instance, communicating the uncertainty associated with model predictions could potentially be helpful in this regard. However, there is little to no research that systematically explores if and how conveying predictive uncertainty impacts decision", "keywords": {"keywords": ["uncertainty", "machine learning", "decision making", "predictive uncertainty", "ML assisted decision making"], "research type": [], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "The impact of algorithmic risk assessments on human predictions and its analysis via crowdsourcing studies", "author": "R Fogliato"}, {"title": "From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices", "author": "J Morley"}, {"title": "Challenging common interpretability assumptions in feature attribution explanations", "author": "J Dinu"}, {"title": "Interpretable ranking with generalized additive models", "author": "H Zhuang"}, {"title": "Model learning with personalized interpretability estimation (ml-pie)", "author": "M Virgolin"}, {"title": "Effects of agents' transparency on teamwork", "author": "S Tulli"}, {"title": "Breaking taboos in fair machine learning: An experimental study", "author": "J Nyarko"}, {"title": "A study on trust in black box models and post-hoc explanations", "author": "N El Bekri"}, {"title": "Hybrid deep learning type-2 fuzzy logic systems for explainable AI", "author": "R Chimatapu"}, {"title": "The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective", "author": "S Krishna", "abstract": "As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we", "keywords": {"keywords": ["high-stakes settings", "Explainable Machine Learning", "post hoc explanation methods", "disagreement problem", "complex models"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Explainable AI"]}}, {"title": "Topic modeling in software engineering research", "author": "CC Silva"}, {"title": "Interpreting a recurrent neural network's predictions of ICU mortality risk", "author": "LV Ho"}, {"title": "A multi-layered approach for tailored black-box explanations", "author": "C Henin"}, {"title": "I Don't Get IT, but IT seems Valid! The Connection between Explainability and Comprehensibility in (X) AI Research.", "author": "LV Herm"}, {"title": "Why did you do that?", "author": "G Pa\u00e7ac\u0131"}, {"title": "Learning dynamics of attention: Human prior for interpretable machine reasoning", "author": "W Kim"}, {"title": "The messiness of the menstruator: assessing personas and functionalities of menstrual tracking apps", "author": "A Pichon"}, {"title": "Do Input Gradients Highlight Discriminative Features?", "author": "H Shah"}, {"title": "The promises and perils of automated facial action coding in studying children's emotions.", "author": "AM Martinez"}, {"title": "Quantitative evaluation of machine learning explanations: A human-grounded benchmark", "author": "S Mohseni", "abstract": "Research in interpretable machine learning proposes different computational and human subject approaches to evaluate model saliency explanations. These approaches measure different qualities of explanations to achieve diverse goals in designing interpretable machine learning systems. In this paper, we propose a benchmark for image and text domains using multi-layer human attention masks aggregated from multiple human annotators. We then present an evaluation study to compare model saliency explanations", "keywords": {"keywords": ["interpretable machine learning", "model saliency explanations", "benchmark", "multi-layer human attention masks", "evaluation study"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Interpretable machine learning"]}}, {"title": "Explainable machine learning for public policy: Use cases, gaps, and research directions", "author": "K Amarasinghe"}, {"title": "Sequential explanations with mental model-based policies", "author": "A Yeung"}, {"title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspective", "author": "H Lakkaraju"}, {"title": "Managing the tension between opposing effects of explainability of artificial intelligence: a contingency theory perspective", "author": "B Abedin"}, {"title": "Teaching responsible data science: Charting new pedagogical territory", "author": "A Lewis"}, {"title": "Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approach", "author": "H Mucha"}, {"title": "Artificial intelligence sepsis prediction algorithm learns to say \u201cI don't know\u201d", "author": "SP Shashikumar"}, {"title": "The GIFT of scrutable learner models: why and how", "author": "J Kay"}, {"title": "Cut-n-reveal: Time series segmentations with explanations", "author": "N Muralidhar"}, {"title": "A United States fair lending perspective on machine learning", "author": "P Hall", "abstract": "The use of machine learning (ML) has become more widespread in many areas of consumer financial services, including credit underwriting and pricing of loans. ML's ability to automatically learn nonlinearities and interactions in training data is perceived to facilitate faster and more accurate credit decisions, and ML is now a viable challenger to traditional credit modeling methodologies. In this mini review, we further the discussion of ML in consumer finance by proposing uniform definitions of key ML and legal concepts related to", "keywords": {"keywords": ["machine learning", "fair lending", "consumer financial services", "credit underwriting", "loan pricing"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["economics"], "content": ["application"], "application area": ["finance"]}}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang"}, {"title": "Are you sure? Prediction revision in automated decision\u2010making", "author": "N Burkart"}, {"title": "Learning interpretable concept-based models with human feedback", "author": "I Lage"}, {"title": "Harnessing explanations to bridge ai and humans", "author": "V Lai"}, {"title": "Curating explanations of machine learning models for business stakeholders", "author": "I Golbin"}, {"title": "Explainability in Music Recommender Systems", "author": "D Afchar"}, {"title": "XAI for Operations in the Process Industry-Applications, Theses, and Research Directions.", "author": "A Kotriwala"}, {"title": "Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News", "author": "A Kazemi"}, {"title": "The psychology of Big Data: Developing a \u201ctheory of machine\u201d to examine perceptions of algorithms.", "author": "JM Logg"}, {"title": "On the interaction of belief bias and explanations", "author": "AV Gonzalez", "abstract": "A myriad of explainability methods have been proposed in recent years, but there is little consensus on how to evaluate them. While automatic metrics allow for quick benchmarking, it isn't clear how such metrics reflect human interaction with explanations. Human evaluation is of paramount importance, but previous protocols fail to account for belief biases affecting human performance, which may lead to misleading conclusions. We provide an overview of belief bias, its role in human evaluation, and ideas for NLP practitioners on how to account", "keywords": {"keywords": ["belief bias", "explanations", "evaluation", "NLP practitioners", "human performance"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Explainability methods"]}}, {"title": "Hierarchical classification of transposable elements with a weighted genetic algorithm", "author": "GT Pereira"}, {"title": "MetaUtil: Meta learning for utility maximization in regression", "author": "P Branco"}, {"title": "On the viscous dissipation caused by randomly rough indenters in smooth sliding motion", "author": "S Sukhomlinov"}, {"title": "Shapley values and meta-explanations for probabilistic graphical model inference", "author": "Y Liu"}, {"title": "Some critical and ethical perspectives on the empirical turn of AI interpretability", "author": "JM John-Mathews"}, {"title": "Machine Learning Explanations to Prevent Overtrust in Fake News Detection.", "author": "S Mohseni"}, {"title": "How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions", "author": "HF Cheng"}, {"title": "Enforcing constraints for machine learning systems via declarative feature selection: An experimental study", "author": "F Neutatz"}, {"title": "Machine learning in sports medicine: need for improvement", "author": "RK Martin"}, {"title": "Abstraction, validation, and generalization for explainable artificial intelligence", "author": "SCH Yang", "abstract": "Neural network architectures are achieving superhuman performance on an expanding range of tasks. To effectively and safely deploy these systems, their decision\u2010making must be understandable to a wide range of stakeholders. Methods to explain artificial intelligence (AI) have been proposed to answer this challenge, but a lack of theory impedes the development of systematic abstractions, which are necessary for cumulative knowledge gains. We propose Bayesian Teaching as a framework for unifying explainable AI (XAI) by", "keywords": {"keywords": ["neural network architectures", "explainable artificial intelligence", "abstraction", "validation", "generalization"], "research type": ["theoretical"], "methodology": [], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": []}}, {"title": "Partnering People with Deep Learning Systems: Human Cognitive Effects of Explanations", "author": "S Dougherty"}, {"title": "A model for the progressive visualization of multidimensional data structure", "author": "E Ventocilla"}, {"title": "Micro-entries: Encouraging deeper evaluation of mental models over time for interactive data systems", "author": "JE Block"}, {"title": "Assessing interpretation capacity in Machine Learning: A critical review", "author": "M Haddouchi"}, {"title": "Effect of ai explanations on human perceptions of patient-facing ai-powered healthcare systems", "author": "Z Zhang"}, {"title": "From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence", "author": "DA Melis"}, {"title": "Towards interpretable routing policy: A two stage multi-objective genetic programming approach with feature selection for uncertain capacitated arc routing problem", "author": "S Wang"}, {"title": "Learning optimal predictive checklists", "author": "H Zhang"}, {"title": "A multi-layered approach for interactive black-box explanations", "author": "C Henin"}, {"title": "Towards a model-and data-focused taxonomy of XAI systems", "author": "JP Kucklick", "abstract": "Explainable Artificial Intelligence (XAI) is currently an important topic for the application of Machine Learning (ML) in high-stakes decision scenarios. Related research focuses on evaluating ML algorithms in terms of interpretability. However, providing a human understandable explanation of an intelligent system does not only relate to the used ML algorithm. The data and features used also have a considerable impact on interpretability. In this paper, we develop a taxonomy for describing XAI systems based on aspects about the", "keywords": {"keywords": ["Explainable Artificial Intelligence", "XAI systems", "interpretability", "ML algorithm", "data and features"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["high-stakes decision scenarios"]}}, {"title": "Interpretable machine learning as decision support for processing fraud alerts", "author": "HJP Weerts"}, {"title": "Realizing AI in healthcare: challenges appearing in the wild", "author": "T Osman Andersen"}, {"title": "A Typology of AI Ethics Tools, Methods and Research to Translate Principles into Practices", "author": "J Morley"}, {"title": "Towards Semantically-Rich Spatial Network Representation Learning via Automated Feature Topic Pairing", "author": "D Wang"}, {"title": "When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models", "author": "A Rechkemmer"}, {"title": "Personal insights for altering decisions of tree-based ensembles over time", "author": "N Boer"}, {"title": "Investigations of Performance and Bias in Human-AI Teamwork in Hiring", "author": "A Peng"}, {"title": "Hive: evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "On Parameter Interpretability of Phenomenological-Based Semiphysical Models", "author": "L Lema-Perez"}, {"title": "Considerations when learning additive explanations for black-box models", "author": "S Tan", "abstract": "Many methods to explain black-box models, whether local or global, are additive. In this paper, we study global additive explanations for non-additive models, focusing on four explanation methods: partial dependence, Shapley explanations adapted to a global setting, distilled additive explanations, and gradient-based explanations. We show that different explanation methods characterize non-additive components in a black-box model's prediction function in different ways. We use the concepts of main and total effects to anchor", "keywords": {"keywords": ["additive explanations", "black-box models", "non-additive models", "explanation methods", "prediction function"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Supported decision-making by explainable predictions of ship trajectories", "author": "N Burkart"}, {"title": "Critical empirical study on black-box explanations in AI", "author": "JM John-Mathews"}, {"title": "Accelerating Text-as-Data Research in Computational Social Science", "author": "D Card"}, {"title": "\" A cold, technical decision-maker\": Can AI provide explainability, negotiability, and humanity?", "author": "A Woodruff"}, {"title": "Effects of data ambiguity and cognitive biases on the interpretability of machine learning models in humanitarian decision making", "author": "D Paulus"}, {"title": "REM: An integrative rule extraction methodology for explainable data analysis in healthcare", "author": "Z Shams"}, {"title": "How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI.", "author": "ZT Zhang"}, {"title": "Evaluating neural model robustness for machine comprehension", "author": "W Wu"}, {"title": "Biological Properties of Suture Materials", "author": "O Ba\u015f\u00e7\u0131"}, {"title": "Crowdsourcing and evaluating concept-driven explanations of machine learning models", "author": "S Mishra", "abstract": "An important challenge in building explainable artificially intelligent (AI) systems is designing interpretable explanations. AI models often use low-level data features which may be hard for humans to interpret. Recent research suggests that situating machine decisions in abstract, human understandable concepts can help. However, it is challenging to determine the right level of conceptual mapping. In this research, we explore granularity (of data features) and context (of data instances) as dimensions underpinning conceptual", "keywords": {"keywords": ["explainable AI", "concept-driven explanations", "machine learning models", "crowdsourcing", "interpretable explanations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Explainable AI"]}}, {"title": "Fairness, explainability, privacy, and robustness for trustworthy algorithmic decision making", "author": "S Majumdar"}, {"title": "Intuitively assessing ml model reliability through example-based explanations and editing model inputs", "author": "H Suresh"}, {"title": "Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Expl (AI) ned: The impact of explainable artificial intelligence on cognitive processes", "author": "K Bauer"}, {"title": "Amplifying domain expertise in clinical data pipelines", "author": "P Rahman"}, {"title": "\u4eba\u673a\u667a\u80fd\u534f\u540c\u7814\u7a76\u7efc\u8ff0", "author": "\u5b59\u6548\u534e\uff0c \u5f20\u4e49\u6587\uff0c \u79e6\u89c9\u6653\uff0c \u674e\u749f\u7490\uff0c \u738b\u8212\u8d85"}, {"title": "LEx: A Framework for Operationalising Layers of Machine Learning Explanations", "author": "R Singh"}, {"title": "Domain Knowledge-Aided Explainable Artificial Intelligence", "author": "SR Islam"}, {"title": "From human explanation to model interpretability: A framework based on weight of evidence", "author": "D Alvarez-Melis"}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization", "author": "M Fan", "abstract": "Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX", "keywords": {"keywords": ["Human-AI collaboration", "UX evaluation", "Explanation", "Synchronization", "Usability test videos"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["User Experience (UX) evaluation"]}}, {"title": "Design and Empirical Evaluation of Interactive and Interpretable Machine Learning", "author": "F Poursabzi-Sangdeh"}, {"title": "Explaining Vulnerability of Machine Learning to Adversarial Attacks", "author": "M Melis"}, {"title": "Learning how to approve updates to machine learning algorithms in non-stationary settings", "author": "J Feng"}, {"title": "Heterogeneity in Algorithm-Assisted Decision-Making: A Case Study in Child Abuse Hotline Screening", "author": "L Cheng"}, {"title": "Bringing Friends into the Loop of Recommender Systems: An Exploratory Study", "author": "J Piao"}, {"title": "Machine Learning Explainability for External Stakeholders", "author": "A Weller"}, {"title": "People underestimate the errors made by algorithms for credit scoring and recidivism prediction but accept even fewer errors", "author": "FG Rebitschek"}, {"title": "Designing for the human in the loop: Transparency and control in interactive machine learning", "author": "A Renner"}, {"title": "Risky Reforms: A Sociotechnical Analysis of Algorithms as Tools for Social Change", "author": "B Green"}, {"title": "Post hoc explanations may be ineffective for detecting unknown spurious correlation", "author": "J Adebayo", "abstract": "We investigate whether three types of post hoc model explanations\u2013feature attribution, concept activation, and training point ranking\u2013are effective for detecting a model's reliance on spurious signals in the training data. Specifically, we consider the scenario where the spurious signal to be detected is unknown, at test-time, to the user of the explanation method. We design an empirical methodology that uses semi-synthetic datasets along with pre-specified spurious artifacts to obtain models that verifiably rely on these spurious", "keywords": {"keywords": ["post hoc explanations", "spurious correlation", "feature attribution", "concept activation", "training point ranking"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "How Do People Rank Multiple Mutant Agents?", "author": "J Dodge"}, {"title": "User-Based Experiment Guidelines for Measuring Interpretability in Machine Learning", "author": "A Bibal"}, {"title": "On the Relationship Between Explanations, Fairness Perceptions, and Decisions", "author": "J Schoeffer"}, {"title": "Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness", "author": "K Donahue"}, {"title": "Measuring Machine Learning Model Interpretability", "author": "F Feldmann"}, {"title": "Human-Centered Explainable Artificial Intelligence for Marine Autonomous Surface Vehicles", "author": "E Veitch"}, {"title": "Correcting classification: A bayesian framework using explanation feedback to improve classification abilities", "author": "Y Bekkemoen"}, {"title": "A MEthOdOlOGy FOR IdENtiFyiNG PhENOMENOlOGicAl\u2010BASEd MOdElS USiNG A PARAMEtER HiERARchy", "author": "S Lopez\u2010Restrepo"}, {"title": "How good is good enough? Quantifying the impact of benefits, accuracy, and privacy on willingness to adopt COVID-19 decision aids", "author": "G Kaptchuk"}, {"title": "XAutoML: A Visual Analytics Tool for Establishing Trust in Automated Machine Learning", "author": "MA Z\u00f6ller", "abstract": "In the last ten years, various automated machine learning (AutoML) systems have been proposed to build end-to-end machine learning (ML) pipelines with minimal human interaction. Even though such automatically synthesized ML pipelines are able to achieve a competitive performance, recent studies have shown that users do not trust models constructed by AutoML due to missing transparency of AutoML systems and missing explanations for the constructed ML pipelines. In a requirements analysis study with 26", "keywords": {"keywords": ["automated machine learning", "trust", "visual analytics", "transparency", "explanations"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning"}}, {"title": "Explaining by Conversing: The Argument for Conversational Xai Systems", "author": "W Marrakchi"}, {"title": "Interpretable few-shot learning via linear distillation", "author": "A Asadulaev"}, {"title": "Let's Go to the Alien Zoo: Introducing an Experimental Framework to Study Usability of Counterfactual Explanations for Machine Learning", "author": "U Kuhl"}, {"title": "Generating and evaluating explanations of attended and error\u2010inducing input regions for VQA models", "author": "A Ray"}, {"title": "Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems", "author": "M Nourani"}, {"title": "Learning Global Additive Explanations of Black-Box Models", "author": "S Tan"}, {"title": "Evaluating Neural Machine Comprehension Model Robustness to Noisy Inputs and Adversarial Attacks", "author": "W Wu"}, {"title": "Towards trustworthy deception detection: Benchmarking model robustness across domains, modalities, and languages", "author": "M Glenski"}, {"title": "Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation", "author": "V Lai"}, {"title": "Transparency and Trust in AI", "author": "N Scharowski", "abstract": "), the relationship between trust and the transparency of an AI\u2019s  This is why the driving and  inhibiting factors of trust into AI  transparency can actually have a detrimental impact on trust", "keywords": {"keywords": ["transparency", "trust", "AI", "driving factors", "inhibiting factors"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI transparency"]}}, {"title": "Understanding User Sensemaking in Machine Learning Fairness Assessment Systems", "author": "Z Gu"}, {"title": "Standardized reporting of machine learning applications in urology: The STREAM-URO framework", "author": "JCC Kwong"}, {"title": "Polarization Through the Lens of Learning Theory", "author": "N HAGHTALAB"}, {"title": "Will You Accept the AI Recommendation? Predicting Human Behavior in AI-Assisted Decision Making", "author": "X Wang"}, {"title": "Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging", "author": "R Fogliato"}, {"title": "Manipulating SHAP via Adversarial Data Perturbations (Student Abstract)", "author": "H Baniecki"}, {"title": "Explainable Machine Learning Frameworks for Managing HPC Systems.", "author": "E Ates"}, {"title": "Re-examining the role of time in human-algorithm interaction", "author": "JS Park"}, {"title": "A feminist data ethics of care framework for machine learning: The what, why, who and how", "author": "J Gray"}, {"title": "AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation", "author": "O Gomez", "abstract": "Rapid improvements in the performance of machine learning models have pushed them to the forefront of data-driven decision-making. Meanwhile, the increased integration of these models into various application domains has further highlighted the need for greater interpretability and transparency. To identify problems such as bias, overfitting, and incorrect correlations, data scientists require tools that explain the mechanisms with which these model decisions are made. In this paper we introduce AdViCE, a visual analytics tool that", "keywords": {"keywords": ["machine learning models", "interpretability", "transparency", "visual analytics tool", "decision-making"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["data analysis"]}}, {"title": "Spotting When Algorithms Are Wrong", "author": "S Buijsman"}, {"title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations", "author": "A Balagopalan"}, {"title": "On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods", "author": "K Amarasinghe"}, {"title": "A psychological theory of explainability", "author": "SCH Yang"}, {"title": "Trust in automated decision making: how user's trust and perceived understanding is influenced by the quality of automatically generated explanations", "author": "A Papenmeier"}, {"title": "Homophily and Incentive Effects in Use of Algorithms", "author": "R Fogliato"}, {"title": "The Role of Human Knowledge in Explainable AI", "author": "A Tocchetti"}, {"title": "Amplifying Domain Expertise in Medical Data Pipelines", "author": "P Rahman"}, {"title": "Probabilistic modeling with human factors in machine learning", "author": "AT Nguyen"}, {"title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?", "author": "RS Zimmermann", "abstract": "A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated-an advantage over other alternatives like strongly activating dataset samples. If humans indeed gain causal insight from visualizations, this", "keywords": {"keywords": ["feature visualizations", "causal understanding", "CNN activations", "explanation artificial intelligence", "activation maximization"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial neural networks"]}}, {"title": "Algorithmic versus Human Advice: Does Presenting Prediction Performance Matter for Algorithm Appreciation?", "author": "S You"}, {"title": "Smart Contracts, Real-Virtual World Convergence and Economic Implications", "author": "W Lehr"}, {"title": "Towards Human-Centered Natural Language Processing", "author": "AV Gonzalez"}, {"title": "Preprint of the paper: A Multi-Objective Approach for Profit-Driven Feature Selection in Credit Scoring", "author": "N Kozodoi"}, {"title": "Overreliance on AI: Literature review", "author": "S Passi"}, {"title": "HINT: Integration Testing for AI-based features with Humans in the Loop", "author": "QZ Chen"}, {"title": "Building Trust in Interactive Machine Learning via User Contributed Interpretable Rules", "author": "L Guo"}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "OpenXAI: Towards a Transparent Evaluation of Model Explanations", "author": "C Agarwal"}, {"title": "Should I Follow this Model? The Effect of Uncertainty Visualization on the Acceptance of Time Series Forecasts", "author": "D Leffrang", "abstract": "Time series forecasts are ubiquitous, ranging from daily weather forecasts to projections of pandemics such as COVID-19. Communicating the uncertainty associated with such forecasts is important, because it may affect users' trust in a forecasting model and, in turn, the decisions made based on the model. Although there exists a growing body of research on visualizing uncertainty in general, the important case of visualizing prediction uncertainty in time series forecasting is under-researched. Against this background, we investigated", "keywords": {"keywords": ["time series forecasts", "uncertainty visualization", "acceptance", "forecasting model", "decision-making"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Statistics"], "content": ["Methodological"], "application area": ["Data visualization"]}}, {"title": "Interactive Scalable Interfaces for Machine Learning Interpretability", "author": "F Hohman"}, {"title": "Practical Algorithms for Resource Allocation and Decision Making", "author": "DC McElfresh"}, {"title": "Enhancing the Transparency of Personal Data Access through Semantic Web Technologies", "author": "RRG Hamed"}, {"title": "Use-Case-Grounded Simulations for Explanation Evaluation", "author": "V Chen"}, {"title": "RAIL", "author": "VP Spears"}, {"title": "Regulating the Safety of Health-Related Artificial Intelligence", "author": "M Da Silva"}, {"title": "Greek-Turkish arms race: an approach using neural networks", "author": "\u03a3 \u039d\u03c4\u03b5\u03bb\u03bb\u03ae\u03c2"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.0", "author": "JP Wanner"}, {"title": "Visual Exploration of Machine Learning Model Behavior with Hierarchical Surrogate Rule Sets", "author": "J Yuan"}, {"title": "Transportation Mode Choice Behavior in the Era of Autonomous Vehicles: The Application of Discrete Choice Modeling and Machine Learning", "author": "S Lee", "abstract": "New mobility technologies, such as shared mobility services (eg, car-sharing) and, more importantly, autonomous vehicles (AVs), continue to evolve. The supply-side advancement will likely disrupt and transform transportation mode choice behaviors, and create a new paradigm since they are emerging and becoming increasingly feasible alternatives to the existing modes of transportation. Accordingly, this dissertation employs discrete choice modeling (DCM) and machine learning (ML) using a US nationwide stated choice", "keywords": {"keywords": ["Transportation Mode Choice Behavior", "Autonomous Vehicles", "Discrete Choice Modeling", "Machine Learning", "Shared Mobility Services"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Transportation", "content": "Methodological", "application area": "Transportation Mode Choice"}}, {"title": "More Than Words: Towards Better Quality Interpretations of Text Classifiers", "author": "MB Zafar"}, {"title": "Scrutinizing XAI using linear ground-truth data with suppressor variables", "author": "R Wilming"}, {"title": "Beyond Importance Scores: Interpreting Tabular ML by Visualizing Feature Semantics", "author": "A Ghorbani"}, {"title": "Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure", "author": "BNEKE Horvitz"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Metal-free C-3 alkylation of imidazopyridines with xanthates and convenient access to alpidem and zolpidem", "author": "S Wang"}, {"title": "Machine Learning for Consumers and Markets", "author": "W Wang"}, {"title": "Interpretation Quality Score for Measuring the Quality of interpretability methods", "author": "Y Xie"}, {"title": "Building Trust by Supporting Situation Awareness: Exploring Pilots' Design Requirements for Decision Support Tools", "author": "C STORATH"}, {"title": "Governing Artificial Intelligence in Post-Pandemic Society", "author": "A Arunagiri", "abstract": "Pandemic escalated the need of adopting technology for human security and public service. Technological integration and digital transformation are of focus in the strategy to recover and reconstruct civic society post-pandemic across the globe, especially in the domains of healthcare, education, surveillance, and governance. Artificial intelligence (AI) is seen to benefit society through building and assisting critical socio-technical systems.", "keywords": {"keywords": ["Artificial intelligence", "Governance", "Post-pandemic society", "Technological integration", "Digital transformation"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Public services"]}}, {"title": "It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI", "author": "A Papenmeier"}, {"title": "Essays on the Human Propensity to Rely on Algorithms as Tasks Become More Difficult", "author": "E Bogert"}, {"title": "Backward recursive Class Activation Map refinement for high resolution saliency map", "author": "A Englebert"}, {"title": "Robo-Advising: Less AI and More XAI?", "author": "M Bianchi"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Unconventional Concerns for Human-Centered Artificial Intelligence", "author": "R Cheruvu"}, {"title": "Understanding and Exploring Datasets", "author": "F POURSABZI-SANGDEH"}, {"title": "Enabling type differentiated explainable queries across modalities for different fashion items", "author": "K Dusad", "abstract": "One of the biggest differences between shopping online and in person is the limited scope and expressibility of the queries that current systems allow and can handle. In person, users often employ a combination of linguistic and visual tools at their disposal to create complex queries. Handling such queries requires modeling relationships between products of the same type, products of different types, products and outfits, and products and their attributes. In this paper, we propose a system that models these relationships by:(i) building a robust", "keywords": {"keywords": ["online shopping", "queries", "modalities", "fashion items", "explainable queries"], "research type": ["theoretical"], "methodology": [], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["fashion"]}}, {"title": "Investigating Explanations that Target Training Data.", "author": "AI Anik"}, {"title": "Journal: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 2019", "author": "F Hohman"}, {"title": "Trust and Reliance in XAI--Distinguishing Between Attitudinal and Behavioral Measures", "author": "N Scharowski"}, {"title": "Explainable Neural Rule Learning", "author": "S Shi"}, {"title": "Machine Learning for Credit Risk Analytics", "author": "N Kozodoi"}, {"title": "How Accurate Does It Feel?\u2013Human Perception of Different Types of Classification Mistakes", "author": "A Papenmeier"}, {"title": "Analyzing and Evaluating Post hoc Explanation Methods for Black Box Machine Learning", "author": "J Pombra"}, {"title": "GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information", "author": "H Dang"}, {"title": "Tribe or Not? Critical Inspection of Group Differences Using TribalGram", "author": "Y Ahn"}, {"title": "Adapting to the algorithm: how accuracy comparisons promote the use of a decision aid", "author": "L Garston", "abstract": "In three experiments, we sought to understand when and why people use an algorithm decision aid. Distinct from recent approaches, we explicitly enumerate the algorithm's accuracy while also providing summary feedback and training that allowed participants to assess their own skills. Our results highlight that such direct performance comparisons between the algorithm and the individual encourages a strategy of selective reliance on the decision aid; individuals ignored the algorithm when the task was easier and relied on the", "keywords": {"keywords": ["algorithm decision aid", "accuracy comparisons", "selective reliance", "individuals", "performance comparisons"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Psychology"], "content": ["Methodological"], "application area": ["Decision making"]}}, {"title": "Similarity Difference And Uniqueness Method for Explainable AI", "author": "SM Muddamsetty"}, {"title": "Development of an Interpretable Maritime Accident Prediction System Using Machine Learning Techniques", "author": "G Kim"}, {"title": "Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations", "author": "J Dai"}, {"title": "The Need for Empirical Evaluation of Explanation Quality", "author": "N Halliwell"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimization", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "Explainable Classification and Annotation through Relation Learning and Reasoning", "author": "R Pierrard"}, {"title": "ORCID: 0000-0001-6311-3967", "author": "C Morrison"}, {"title": "The challenges of providing explanations of AI systems when they do not behave like users expect", "author": "M Riveiro"}, {"title": "Learning User-Interpretable Descriptions of Black-Box AI System Capabilities", "author": "P Verma"}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang", "abstract": "People consider recommendations from AI systems in diverse domains ranging from recognizing tumors in medical images to deciding which shoes look cute with an outfit. Implicit in the decision process is the perceived expertise of the AI system. In this paper, we investigate how people trust and rely on an AI assistant that performs with different levels of expertise relative to the person, ranging from completely overlapping expertise to perfectly complementary expertise. Through a series of controlled online lab studies where", "keywords": {"keywords": ["AI systems", "human-AI teams", "complementary expertise", "trust", "reliance"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-AI interaction"]}}, {"title": "Modeling Disclosive Transparency in NLP Application Descriptions", "author": "M Saxon"}, {"title": "Empirically Evaluating Meta Learning of Robot Explainability with Humans", "author": "A Silva"}, {"title": "Explainable and Network-Based Approaches for Decision-making in Emergency Management", "author": "A Tabassum"}, {"title": "Doktors der Naturwissenschaften", "author": "V Volz"}, {"title": "Exploring Ranked Local Selectors for Stable Explanations of ML Models", "author": "T Korhonen"}, {"title": "Concept-based Explanations for Out-Of-Distribution Detectors", "author": "J Choi"}, {"title": "Interpreting Deep Learning Models", "author": "X Liu"}, {"title": "The need for a numeric measure of explainability", "author": "W De Mulder"}, {"title": "Explanation Interfaces for Sales Forecasting", "author": "TB Fahse"}, {"title": "Explaining White-box Classifications to Data Scientists (technical report)", "author": "D Deutch", "abstract": "A main component of many Data Science applications is the invocation of Machine Learning (ML) classifiers. The typical complexity of these classification models makes it difficult to understand the reason for a result, and consequently to assess its trustworthiness and detect errors. We propose a simple generic approach for explaining classifications, by identifying relevant parts of the input whose perturbation would be significant in effecting the classification. Our solution requires white-box access to the model internals and a", "keywords": [["explanations", 0.5408], ["classification", 0.4793], ["classifications", 0.4201], ["ml", 0.3437], ["data", 0.2931], ["explaining", 0.2888], ["learning", 0.288], ["detect", 0.2634], ["complexity", 0.2632], ["machine", 0.2425]]}, {"title": "Machine Learning-Based Approach for Classifying the Source Code Using Programming Keywords", "author": "M Ifham"}, {"title": "Scrutinizing XAI using linear ground-truth data with suppressor variables", "author": "R Wilming"}, {"title": "Verifiability as a Complement to AI Explainability: A Conceptual Proposal", "author": "KR Patil"}, {"title": "Epistemic values in feature importance methods", "author": "L Hancox-Li"}, {"title": "Development and Validation of an Interpretable 3-day Intensive Care Unit Readmission Prediction Model Using Explainable Boosting Machines", "author": "S Hegselmann"}, {"title": "Interpretable Deep Learning: Beyond Feature-Importance with Concept-based Explanations", "author": "B Dimanov"}, {"title": "A Taxonomy of User-centered Explainable AI Studies", "author": "LV Herm"}, {"title": "Designing for Confidence: The Impact of Visualizing Artificial Intelligence Decisions", "author": "AJ Karran"}, {"title": "The Design and Evaluation of Neural Attention Mechanisms for Explaining Text Classifiers", "author": "S Carton"}, {"title": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "author": "H Kaur", "abstract": "Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact--an explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick's sensemaking theory, which focuses on who the", "keywords": {"keywords": ["AI interpretability", "Sensemaking theory", "Explainability", "ML-based systems", "Human understanding"], "research type": ["theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["AI interpretability"]}}, {"title": "Transparent Interaction Based Learning for Human-Robot Collaboration", "author": "E Bagheri"}, {"title": "Explainable Artificial Intelligence in Cybersecurity: A Brief Review", "author": "S Hariharan"}, {"title": "Measuring algorithmic interpretability: A human-learning-based framework and the corresponding cognitive complexity score", "author": "JP Lalor"}, {"title": "Machine Learning for Humans: Building Models that Adapt to Behavior", "author": "AS Hilgard"}, {"title": "Clinician Preimplementation Perspectives of a Decision-Support Tool for the Prediction of Cardiac Arrhythmia Based on Machine Learning: Near-Live \u2026", "author": "S Matthiesen"}, {"title": "Human-Interpretable Explanations for Black-Box Machine Learning Models: An Application to Fraud Detection", "author": "V Balayan"}, {"title": "Model Interpretation and Data Valuation for Machine Learning", "author": "A Ghorbani"}, {"title": "Investigating the Benefits of Free-Form Rationales", "author": "J Sun"}, {"title": "Automating telemetry-and trace-based analytics on large-scale distributed systems", "author": "E Ates"}, {"title": "Workshop on Trust and Reliance in AI-Human Teams (TRAIT)", "author": "G Bansal", "abstract": "As humans increasingly interact (and even collaborate) with AI systems during decision-making, creative exercises, and other tasks, appropriate trust and reliance are necessary to ensure proper usage and adoption of these systems. Specifically, people should understand when to trust or rely on an algorithm's outputs and when to override them. While significant research focus has aimed to measure and promote trust in human-AI interaction, the field lacks synthesized definitions and understanding of results across contexts. Indeed", "keywords": {"keywords": ["trust", "reliance", "AI-Human Teams", "decision-making", "human-AI interaction"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI ethics"]}}, {"title": "IOGOD: An interpretable outlier generation-based outlier detector for categorical databases", "author": "MA Zenkl-Galaz"}, {"title": "Adapting to the algorithm: how accuracy comparisons promote the use of a decision aid", "author": "G Liang"}, {"title": "Consumer Responses to Algorithmic Decisions", "author": "BI Demirdag"}, {"title": "TRUSTING MACHINE LEARNING ALGORITHMS IN PREDICTING MALICIOUS NODES ATTACKS", "author": "B Mahbooba"}, {"title": "Trust in AI: Interpretability is not necessary or sufficient, while black-box interaction is necessary and sufficient", "author": "MW Shen"}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications", "author": "M Nourani"}, {"title": "A feminist data ethics of care for machine learning: The what, why, who and how", "author": "J Gray"}, {"title": "Interactive Model Cards: A Human-Centered Approach to Model Documentation", "author": "A Crisan"}, {"title": "Trusting the Black Box: Factors Influencing Trust in Algorithmic Risk Assessment within Child Welfare Services", "author": "JJ Klomp"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand", "abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI", "keywords": {"keywords": ["Cognitive biases", "XAI-assisted decision-making", "Systematic review", "Explainability techniques", "Human explanation methods"], "research type": "Systematic review", "methodology": "Qualitative", "purposes": "Explanatory", "discipline": "Computer science", "content": "Methodological", "application area": "Artificial intelligence"}}, {"title": "Exploring the Effects of Machine Learning Literacy Interventions on Laypeople's Reliance on Machine Learning Models", "author": "CW Chiang"}, {"title": "Assessing interpretation capacity in Machine Learning", "author": "M Haddouchi"}, {"title": "Trust and AI in Clinical Decision Support", "author": "B Wilson"}, {"title": "Investigation of Local and Global Methods of Explainable Artificial Intelligence. Metrics for Evaluating Explanation Methods", "author": "NV Shevskaya"}, {"title": "\u201cRather Solve the Problem from Scratch\u201d: Gamesploring Human-Machine Collaboration for Optimizing the Debris Collection Problem", "author": "A Ulusan"}, {"title": "\u201cWhy Do I Care What's Similar?\u201d Probing Challenges in AI-Assisted Child Welfare Decision-Making through Worker-AI Interface Design Concepts", "author": "A Kawakami"}, {"title": "Poly-CAM: High resolution class activation map for convolutional neural networks", "author": "A Englebert"}, {"title": "Only limited material is available in the selected language. All content is available on the global site.", "author": "MMV Brewer"}, {"title": "Mind the gap between research and practice in operations management", "author": "X Chen"}, {"title": "Learning interpretable multi-class models by means of hierarchical decomposition: Threshold Control for Nested Dichotomies", "author": "JA Fdez-S\u00e1nchez", "abstract": "The success of Artificial Intelligence at solving real-world problems poses the need for interpretable models, especially in human-centered applications. The multi-class scenario is often present in these environments; however, the majority of research on interpretability has focused on binary classification. In this work, a novel method based on hierarchical decompositions to obtain interpretable multi-class models is introduced. The proposal, named Threshold Control for Nested Dichotomies (TC-ND) method, creates a", "keywords": {"keywords": ["interpretable models", "multi-class scenario", "hierarchical decompositions", "Threshold Control for Nested Dichotomies (TC-ND)", "artificial intelligence"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Interpretability"]}}, {"title": "Summarize with Caution: Comparing Global Feature Attributions.", "author": "A Okeson"}, {"title": "Human and artificial intelligence in decision systems for social development", "author": "A Noriega Campero"}, {"title": "UnIC: Towards Unmanned Intelligent Cluster and Its Integration into Society", "author": "F Zhang"}, {"title": "THE GIFTOFSCRUTABLE LEARNER MODELS: WHYAND HOW", "author": "J Kay"}, {"title": "An Explainable Artificial Intelligence Approach Based on Deep Type-2 Fuzzy Logic System", "author": "R Chimatapu"}, {"title": "A Systematic Literature Review of Digital Game-based Assessment Empirical Studies: Current Trends and Open Challenges", "author": "MJ Gomez"}, {"title": "Algorithmic explanations: to become a mockingbird", "author": "A Hare"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Interpretable Approaches to Opening Up Black-box Models", "author": "HFS Tan"}, {"title": "Metrics for evaluating the performance of complex engineering system health monitoring models", "author": "AD Lewis", "abstract": "Recent efforts to apply prognostics and health management (PHM) practices towards the risk management of complex engineering systems (CES) traditionally structured using probabilistic risk assessments (PRA) provide more opportunities for modeling and dynamically monitoring system-level heath. However, it is unclear what metrics should be used to assess and compare the performance of these models. PHM performance metrics center around prediction accuracy without considering operational realities of the system", "keywords": {"keywords": ["prognostics", "health management", "complex engineering systems", "performance metrics", "risk management"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["engineering"], "content": ["methodological"], "application area": ["system monitoring"]}}, {"title": "Strategies for Selecting and Adapting Machine Learning Systems to Support Different Types of Experts", "author": "A Okeson"}, {"title": "Visualizing Cluster Patterns at Scale: A Model and a Library", "author": "E Ventocilla"}, {"title": "Designing for human\u2013AI complementarity in K\u201012 education", "author": "K Holstein"}, {"title": "Data Protection and Machine-Learning-Supported Decision-Making at the EU Border: ETIAS Profiling Under Scrutiny", "author": "P Jo Pesch"}, {"title": "researchportal. unamur. be", "author": "A Bibal"}, {"title": "Artificial Intelligence in Pharmacovigilance: Do We Need Explainability?", "author": "M Hauben"}, {"title": "Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance", "author": "GBB Nushi"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}, {"title": "Parameters interpretability in phenomenological-based semiphysical models. A human glucose homeostasis model", "author": "L Lema Perez"}, {"title": "Expliquer et justifier les syst\u00e8mes de d\u00e9cisions algorithmiques", "author": "C Henin", "abstract": "Dans un contexte favorable \u00e0 la rationalisation des d\u00e9cisions par des objectifs mesurables et des m\u00e9thodes quantitatives, le d\u00e9veloppement r\u00e9cent des technologies num\u00e9riques a acc\u00e9l\u00e9r\u00e9 l'adoption des syst\u00e8mes de d\u00e9cisions fond\u00e9s sur un traitement algorithmique. De tels syst\u00e8mes sont d\u00e9j\u00e0 pr\u00e9sents dans de nombreux domaines et leur utilisation devrait encore s' accro\u00eetre. Toutefois, si aucune mesure sp\u00e9cifique n'est prise, l'utilisation de tels algorithmes est porteuse de risques pour les citoyens et pour la soci\u00e9t\u00e9. Alors que plusieurs", "keywords": {"keywords": ["systems de d\u00e9cisions algorithmiques", "rationalisation des d\u00e9cisions", "objectifs mesurables", "syst\u00e8mes de d\u00e9cisions fond\u00e9s sur un traitement algorithmique", "utilisation des algorithmes"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["decision-making systems"]}}, {"title": "\u041f\u0420\u041e\u0411\u041b\u0415\u041c\u0410 \u041e\u0411\u042a\u042f\u0421\u041d\u0418\u041c\u041e\u0421\u0422\u0418 \u0412 \u0410\u041b\u0413\u041e\u0420\u0418\u0422\u041c\u0410\u0425 \u041a\u041b\u0410\u0421\u0421\u0418\u0424\u0418\u041a\u0410\u0426\u0418\u0418", "author": "\u0422\u0410 \u041c\u043e\u0438\u0441\u0435\u0435\u0432\u0430"}], "contextualization and exploration of local feature importance explanations to improve understanding and satisfaction of non-expert users": [{"title": "Mediators: Conversational Agents Explaining NLP Model Behavio", "author": "N Feldhus", "abstract": "The human-centric explainable artificial intelligence (HCXAI) community has raised the need for framing the explanation process as a conversation between human and machine. In this position paper, we establish desiderata for Mediators, text-based conversational agents which are capable of explaining the behavior of neural models interactively using natural language. From the perspective of natural language processing (NLP) research, we engineer a blueprint of such a Mediator for the task of sentiment analysis and assess how far", "keywords": {"keywords": ["Mediators", "Conversational Agents", "NLP Model Behavior", "Human-Centric Explainable Artificial Intelligence", "Sentiment Analysis"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["Natural Language Processing"]}}, {"title": "A Case-study Led Investigation of Explainable AI (XAI) to Support Deployment of Prognostics in the industr", "author": "O Amin", "abstract": "Civil nuclear generation plant must maximise it's operational uptime in order to maintain it's viability. With aging plant and heavily regulated operating constraints, monitoring is commonplace, but identifying health indicators to pre-empt disruptive faults is challenging owing to the volumes of data involved. Machine learning (ML) models are increasingly deployed in prognostics and health management (PHM) systems in various industrial applications, however, many of these are black box models that provide good performance", "keywords": {"keywords": ["Explainable AI", "XAI", "Prognostics", "Industrial applications", "Machine learning"], "research type": ["Case study"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Civil nuclear generation plant"]}}, {"title": "Transparent but incomprehensible: Investigating the relation between transparency, explanations, and usability in automated decision-makin", "author": "J Dexe", "abstract": "Transparency is almost always seen as a desirable state of affairs. Governments should be more transparent towards their citizens, and corporations should be more transparent towards both public authorities and their customers. More transparency means more information which citizens can use to make decisions about their daily lives, and with increasing amounts of information in society, those citizens would be able to make more and more choices that align with their preferences. It is just that the story is slightly too good to be", "keywords": {"keywords": ["transparency", "explanations", "usability", "automated decision-making", "information"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["sociology"], "content": ["application"], "application area": ["decision-making"]}}], "investigating explainability of generative ai for code through scenario-based design": [{"title": "Creative Uses of AI Systems and their Explanations: A Case Study from Insuranc", "author": "M Benk", "abstract": "Recent works have recognized the need for human-centered perspectives when designing and evaluating human-AI interactions and explainable AI methods. Yet, current approaches fall short at intercepting and managing unexpected user behavior resulting from the interaction with AI systems and explainability methods of different stake-holder groups. In this work, we explore the use of AI and explainability methods in the insurance domain. In an qualitative case study with participants with different roles and professional backgrounds, we", "keywords": {"keywords": ["AI systems", "Explainability methods", "Human-AI interactions", "Insurance domain", "Case study"], "research type": ["Case study"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Insurance"]}}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Stud", "author": "L Chazette", "abstract": "Quality aspects such as ethics, fairness, and transparency have been proven to be essential for trustworthy software systems. Explainability has been identified not only as a means to achieve all these three aspects in systems, but also as a way to foster users' sentiments of trust. Despite this, research has only marginally focused on the activities and practices to develop explainable systems. To close this gap, we recommend six core activities and associated practices for the development of explainable systems based on the results of a", "keywords": {"keywords": ["explainable systems", "ethics", "fairness", "transparency", "trust"], "research type": ["literature review"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["trustworthy software systems"]}}], "explaining recommendations by means of aspect-based transparent memories": [{"title": "A review of recent deep learning approaches in human-centered machine learnin", "author": "T Kaluarachchi", "abstract": "After Deep Learning (DL) regained popularity recently, the Artificial Intelligence (AI) or Machine Learning (ML) field is undergoing rapid growth concerning research and real-world application development. Deep Learning has generated complexities in algorithms, and researchers and users have raised concerns regarding the usability and adoptability of Deep Learning systems. These concerns, coupled with the increasing human-AI interactions, have created the emerging field that is Human-Centered Machine Learning", "keywords": {"keywords": ["Deep Learning", "Artificial Intelligence", "Machine Learning", "Human-Centered Machine Learning", "complexities"], "research type": ["review"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "How to support users in understanding intelligent systems? Structuring the discussio", "author": "M Eiband", "abstract": "The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI", "keywords": {"keywords": ["intelligent systems", "transparency", "scrutability", "intelligibility", "explainability"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["human-computer interaction"]}}, {"title": "Effects of interactivity and presentation on review-based explanations for recommendation", "author": "DC Hernandez-Bocanegra", "abstract": "User reviews have become an important source for recommending and explaining products or services. Particularly, providing explanations based on user reviews may improve users' perception of a recommender system (RS). However, little is known about how review-based explanations can be effectively and efficiently presented to users of RS. We investigate the potential of interactive explanations in review-based RS in the domain of hotels, and propose an explanation scheme inspired by dialogue models and formal argument", "keywords": {"keywords": ["interactivity", "presentation", "review-based explanations", "recommender system", "hotels"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Recommendation systems"]}}, {"title": "Human-XAI interaction: a review and design principles for explanation user interface", "author": "M Chromik", "abstract": "The interdisciplinary field of explainable artificial intelligence (XAI) aims to foster human understanding of black-box machine learning models through explanation-generating methods. Although the social sciences suggest that explanation is a social and iterative process between an explainer and an explainee, explanation user interfaces and their user interactions have not been systematically explored in XAI research yet. Therefore, we review prior XAI research containing explanation user interfaces for ML-based intelligent systems", "keywords": {"keywords": ["explainable artificial intelligence", "human-XAI interaction", "explanation user interface", "black-box machine learning models", "explanation-generating methods"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explainability"]}}, {"title": "Leveraging arguments in user reviews for generating and explaining recommendation", "author": "T Donkers", "abstract": "Review texts constitute a valuable source for making system-generated recommendations both more accurate and more transparent. Reviews typically contain statements providing argumentative support for a given item rating that can be exploited to explain the recommended items in a personalized manner. We propose a novel method called Aspect-based Transparent Memories (ATM) to model user preferences with respect to relevant aspects and compare them to item properties to predict ratings, and, by the same", "keywords": {"keywords": ["user reviews", "recommendation", "transparent memories", "argumentative support", "aspect-based"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Personalized Recommendation"]}}, {"title": "Capturing the Trends, Applications, Issues, and Potential Strategies of Designing Transparent AI Agent", "author": "L Sun", "abstract": "With the increasing prevalence of Artificial Intelligence (AI) agents, the transparency of agents becomes vital in addressing the interaction issues (eg, explainability and trust). The existing body of research provides valuable theoretical and practical studies in this field. However, determining the transparency of AI agents requires the systematic consideration of the application categories and automation level, which is hardly considered by the prior literature. We thus apply the bibliometric analysis to gain insights from the published", "keywords": {"keywords": ["transparency", "Artificial Intelligence (AI)", "agents", "interaction issues", "explaiability", "trust"], "research type": ["Review"], "methodology": ["Bibliometric analysis"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["AI agent design"]}}, {"title": "Subjective Attributes in Conversational Recommendation Systems: Challenges and Opportunitie", "author": "F Radlinski", "abstract": "The ubiquity of recommender systems has increased the need for higher-bandwidth, natural and efficient communication with users. This need is increasingly filled by recommenders that support natural language interaction, often conversationally. Given the inherent semantic subjectivity present in natural language, we argue that modeling subjective attributes in recommenders is a critical, yet understudied, avenue of AI research. We propose a novel framework for understanding different forms of subjectivity, examine various", "keywords": {"keywords": ["subjective attributes", "conversational recommendation systems", "natural language interaction", "semantic subjectivity", "AI research"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommender systems"]}}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trus", "author": "J Ooge", "abstract": "In the scope of explainable artificial intelligence, explanation techniques are heavily studied to increase trust in recommender systems. However, studies on explaining recommendations typically target adults in e-commerce or media contexts; e-learning has received less research attention. To address these limits, we investigated how explanations affect adolescents' initial trust in an e-learning platform that recommends mathematics exercises with collaborative filtering. In a randomized controlled experiment with 37", "keywords": {"keywords": ["explainable artificial intelligence", "recommendations", "e-learning", "trust", "adolescents"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["education"], "content": ["application"], "application area": ["e-learning"]}}, {"title": "Development of an Instrument for Measuring Users' Perception of Transparency in Recommender System", "author": "M Hellmann", "abstract": "Transparency is increasingly seen as a critical requirement for achieving the goal of human-centered AI systems in general and also, specifically, recommender systems (RS). However, defining and operationalizing the concept is still difficult, due to its multi-faceted nature. Currently, there are hardly any measurement instruments to adequately assess the perceived transparency of RS in user studies. Thus, we present the development of a measurement instrument that aims at capturing perceived transparency as a", "keywords": {"keywords": ["transparency", "recommender system", "measurement instrument", "perception", "user study"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Human-centered AI systems"]}}, {"title": "Attention uncovers task-relevant semantics in emotional narrative understandin", "author": "TS Nguyen", "abstract": "Attention mechanisms in deep neural network models have helped them to achieve exceptional performance at complex natural language processing tasks. Previous attempts to investigate what these models have been \u201cpaying attention to\u201d suggest that these attention representations capture syntactic information, but there is less evidence for semantics. In this paper, we investigate the capability of an attention mechanism to \u201cattend to\u201d semantically meaningful words. Using a dataset of naturalistic emotional narratives, we", "keywords": {"keywords": ["Attention mechanisms", "natural language processing", "emotion", "narratives", "semantics"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Natural Language Understanding"]}}, {"title": "How to Support Users in Understanding Intelligent Systems? Structuring th", "author": "M EIBAND", "abstract": "The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI", "keywords": {"keywords": ["intelligent systems", "transparency", "scrutability", "intelligibility", "interpretability"], "research type": ["review"], "methodology": [], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["human-computer interaction"]}}, {"title": "Transparent-AI Blueprint: Developing a Conceptual Tool to Support the Design of Transparent AI Agent", "author": "Z Zhou", "abstract": "With the increasing prevalence of artificial intelligence (AI) agents, the transparency of agents has become vital in addressing interaction issues (eg, trust, usefulness, and understandability). However, determining the transparency of AI agents requires a systematic consideration of complex related factors, including stakeholders, algorithms, context, etc. Thus, in our study, we presented an overview of studies on the transparency of AI agents through multiple-stage bibliometric analysis, and identified an ontological", "keywords": {"keywords": ["transparency", "AI agents", "interaction issues", "complex factors", "ontology"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI agent design"]}}, {"title": "Generating Recommendations with Post-Hoc Explanations for Citizen Scienc", "author": "D Ben Zaken", "abstract": "Citizen science projects promise to increase scientific productivity while also connecting science with the general public. They create scientific value for researchers and provide pedagogical and social benefits to volunteers. Given the astounding number of available citizen science projects, volunteers find it difficult to find the projects that best fit their interests. This difficulty can be alleviated by providing personalized project recommendations to users. This paper studies whether combining project recommendations"}, {"title": "How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement and ", "author": "D Buschek", "abstract": "The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI", "keywords": {"keywords": ["intelligent systems", "user questions", "user mindsets", "involvement", "usability principles"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["human-computer interaction"], "content": ["application"], "application area": ["supporting users in understanding intelligent systems"]}}, {"title": "Interactive Methods for Model-based Collaborative Filtering Recommender System", "author": "B Loepp", "abstract": "Recommender systems have become very popular for reducing the information overload users are often confronted with in today's web. Collaborative filtering is the method of choice for generating personalized recommendations, supporting users in finding items that best match their preferences, from news articles and movies to all kinds of consumer goods and services. Model-based techniques have achieved great success in terms of recommendation accuracy and algorithmic performance. While there is a large body of", "keywords": {"keywords": ["recommender systems", "collaborative filtering", "model-based techniques", "interactive methods", "information overload"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["personalized recommendations"]}}], "automated rationale generation: a technique for explainable ai and its effects on human perceptions": [{"title": "Attention is not not explanation", "author": "S Wiegreffe", "abstract": "Existence does not Entail Exclusivity. On a more theoretical level, we hold that attention  scores are used as providing an explanation; not the explanation. The final layer of an LSTM", "keywords": {"keywords": ["attention scores", "explanation", "existence", "entail exclusivity", "LSTM"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "What is AI literacy? Competencies and design considerations", "author": "D Long"}, {"title": "Human\u2010centered artificial intelligence and machine learning", "author": "MO Riedl"}, {"title": "Learning to faithfully rationalize by construction", "author": "S Jain"}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "A survey of data-driven and knowledge-aware explainable ai", "author": "XH Li"}, {"title": "Human-centered explainable ai: Towards a reflective sociotechnical approach", "author": "U Ehsan"}, {"title": "No explainability without accountability: An empirical study of explanations and feedback in interactive ml", "author": "A Smith-Renner"}, {"title": "COGAM: measuring and moderating cognitive load in machine learning model explanations", "author": "A Abdul"}, {"title": "Operationalizing human-centered perspectives in explainable AI", "author": "U Ehsan"}, {"title": "Qed: A framework and dataset for explanations in question answering", "author": "M Lamm", "abstract": "A question answering system that in addition to providing an answer provides an explanation of the reasoning that leads to that answer has potential advantages in terms of debuggability, extensibility, and trust. To this end, we propose QED, a linguistically informed, extensible framework for explanations in question answering. A QED explanation specifies the relationship between a question and answer according to formal semantic notions such as referential equality, sentencehood, and entailment. We describe and publicly release an"}, {"title": "Ambiguity-aware ai assistants for medical data analysis", "author": "M Schaekermann"}, {"title": "What are people doing about XAI user experience? A survey on AI explainability research and practice", "author": "JJ Ferreira"}, {"title": "Effects of a social robot's self-explanations on how humans understand and evaluate its behavior", "author": "S Stange"}, {"title": "Unexplainability and incomprehensibility of artificial intelligence", "author": "RV Yampolskiy"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Levels of explainable artificial intelligence for human-aligned conversational explanations", "author": "R Dazeley"}, {"title": "Designing co-creative AI for public spaces", "author": "D Long"}, {"title": "Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle", "author": "S Dhanorkar"}, {"title": "Explainable automation: Personalized and adaptive uis to foster trust and understanding of driving automation systems", "author": "P Wintersberger"}, {"title": "Explainable ai for robot failures: Generating explanations that improve user assistance in fault recovery", "author": "D Das", "abstract": "With the growing capabilities of intelligent systems, the integration of robots in our everyday life is increasing. However, when interacting in such complex human environments, the occasional failure of robotic systems is inevitable. The field of explainable AI has sought to make complex-decision making systems more interpretable but most existing techniques target domain experts. On the contrary, in many failure cases, robots will require recovery assistance from non-expert users. In this work, we introduce a new type of explanation, \u03b5err", "keywords": {"keywords": ["explainable AI", "robot failures", "user assistance", "fault recovery", "non-expert users"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["robotics"]}}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable AI", "author": "M Chromik"}, {"title": "Explainable ai and reinforcement learning\u2014a systematic review of current approaches and trends", "author": "L Wells"}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checkers", "author": "CH Tsai"}, {"title": "Explainable goal-driven agents and robots-a comprehensive review and new framework", "author": "F Sado"}, {"title": "An interaction framework for studying co-creative ai", "author": "M Guzdial"}, {"title": "Leveraging rationales to improve human task performance", "author": "D Das"}, {"title": "Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs", "author": "A Marasovi\u0107"}, {"title": "Selfexplain: A self-explaining architecture for neural text classifiers", "author": "D Rajagopal"}, {"title": "Unexplainability and Incomprehensibility of AI", "author": "RV Yampolskiy", "abstract": "of AI research, for example: Unverifiability [71], Unpredictability1 [72] and limits on preference  deduction [73] or alignment [74]. In this section we introduce Unexplainability of AI and", "keywords": {"keywords": ["Unexplainability", "Incomprehensibility", "Artificial Intelligence", "Unverifiability", "Unpredictability", "Preference deduction", "Alignment"], "research type": ["Review"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "\u201cIt's Unwieldy and It Takes a Lot of Time\u201d\u2014Challenges and Opportunities for Creating Agents in Commercial Games", "author": "M Jacob"}, {"title": "Explainable reinforcement learning for broad-xai: A conceptual framework and survey", "author": "R Dazeley"}, {"title": "Keeping it\" organized and logical\" after-action review for AI (AAR/AI)", "author": "T Mai"}, {"title": "A secure, private, and explainable IoHT framework to support sustainable health monitoring in a smart city", "author": "MA Rahman"}, {"title": "How much is the black box? The value of explainability in machine learning models", "author": "J Wanner"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Exploring the design space of user-system communication for smart-home routine assistants", "author": "YS Chiang"}, {"title": "After-action review for AI (AAR/AI)", "author": "J Dodge"}, {"title": "Automatic detection of usability problem encounters in think-aloud sessions", "author": "M Fan"}, {"title": "Explainability pitfalls: Beyond dark patterns in explainable AI", "author": "U Ehsan", "abstract": "To make Explainable AI (XAI) systems trustworthy, understanding harmful effects is just as important as producing well-designed explanations. In this paper, we address an important yet unarticulated type of negative effect in XAI. We introduce explainability pitfalls (EPs), unanticipated negative downstream effects from AI explanations manifesting even when there is no intention to manipulate users. EPs are different from, yet related to, dark patterns, which are intentionally deceptive practices. We articulate the concept of EPs by demarcating", "keywords": {"keywords": ["Explainable AI", "trustworthy", "harmful effects", "explainability pitfalls", "dark patterns"], "research type": ["Theoretical"], "methodology": ["N/A"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Conceptual"], "application area": ["Explainable AI"]}}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro"}, {"title": "Teach me to explain: A review of datasets for explainable natural language processing", "author": "S Wiegreffe"}, {"title": "Influence tuning: Demoting spurious correlations via instance attribution and instance-driven updates", "author": "X Han"}, {"title": "Explanation as a social practice: Toward a conceptual framework for the social design of AI systems", "author": "KJ Rohlfing"}, {"title": "Wait, but why?: assessing behavior explanation strategies for real-time strategy games", "author": "J Robertson"}, {"title": "XAlgo: A design probe of explaining Algorithms' internal states via question-answering", "author": "J Rebanal"}, {"title": "Reframing human-ai collaboration for generating free-text explanations", "author": "S Wiegreffe"}, {"title": "Explainable autonomous robots: a survey and perspective", "author": "T Sakai"}, {"title": "Towards a robot explanation system: A survey and our approach to state summarization, storage and querying, and human interface", "author": "Z Han"}, {"title": "eXplainable AI (xai) An introduction to the XAI landscape with practical examples", "author": "R Hughes", "abstract": "So What is XAI? \u201cGiven an audience, an explainable Artificial Intelligence is one that  produces details or reasons to make its functioning clear or easy to understand.\u201d  Automated  rationale generation: a technique for explainable AI and its effects on human perceptions. In  Proceedings of the 24th International Conference on Intelligent User Interfaces (pp. 263-274).", "keywords": {"keywords": ["eXplainable AI", "XAI", "rationale generation", "explainable", "human perceptions"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Automated Let's Play Commentary", "author": "S Shah"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "On the diversity and limits of human explanations", "author": "C Tan"}, {"title": "Explainable Goal-Driven Agents and Robots--A Comprehensive Review", "author": "F Sado"}, {"title": "Effect of ai explanations on human perceptions of patient-facing ai-powered healthcare systems", "author": "Z Zhang"}, {"title": "The shoutcasters, the game enthusiasts, and the AI: Foraging for explanations of real-time strategy players", "author": "S Penney"}, {"title": "Understanding Mental Models of AI through Player-AI Interaction", "author": "J Villareale"}, {"title": "Post-hoc Explanations for Complex Model Recommendations using Simple Methods.", "author": "D Shmaryahu"}, {"title": "Understanding How People Reason about Aesthetic Evaluations of Artificial Intelligence", "author": "C Oh"}, {"title": "Abstraction hierarchy based explainable artificial intelligence", "author": "M Dikmen", "abstract": "This work explores the application of Cognitive Work Analysis (CWA) in the context of Explainable Artificial Intelligence (XAI). We built an AI system using a loan evaluation data set and applied an XAI technique to obtain data-driven explanations for predictions. Using an Abstraction Hierarchy (AH), we generated domain knowledge-based explanations to accompany data-driven explanations. An online experiment was conducted to test the usefulness of AH-based explanations. Participants read financial profiles of loan applicants", "keywords": {"keywords": ["Artificial Intelligence", "Explainable AI", "Cognitive Work Analysis", "Abstraction Hierarchy", "Loan Evaluation"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Financial Services"]}}, {"title": "Semantic-Based Explainable AI: Leveraging Semantic Scene Graphs and Pairwise Ranking to Explain Robot Failures", "author": "D Das"}, {"title": "AIST: An interpretable attention-based deep learning model for crime prediction", "author": "Y Rayhan"}, {"title": "Assessing Explainability in Reinforcement Learning", "author": "AE Zelvelder"}, {"title": "No explainability without accountability: an empirical study of explanations and feedback in interactive ML", "author": "A Smith"}, {"title": "Explainable Activity Recognition for Smart Home Systems", "author": "D Das"}, {"title": "Explainable AI for System Failures: Generating Explanations that Improve Human Assistance in Fault Recovery", "author": "D Das"}, {"title": "The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending", "author": "M Dikmen"}, {"title": "\u4eba\u2212 AI \u4ea4\u4e92: \u5b9e\u73b0 \u201c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3 AI\u201d \u7406\u5ff5\u7684\u8de8\u5b66\u79d1\u65b0\u9886\u57df", "author": "\u8bb8\u4e3a\uff0c \u845b\u5217\u4f17\uff0c \u9ad8\u5728\u5cf0"}, {"title": "LEx: A Framework for Operationalising Layers of Machine Learning Explanations", "author": "R Singh"}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization", "author": "M Fan", "abstract": "Analyzing usability test videos is arduous. Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI. Inspired by the concepts of agency and work context in human and AI collaboration literature, we studied two corresponding design factors for AI-assisted UX evaluation: explanations and synchronization. Explanations allow AI to further inform humans how it identifies UX", "keywords": {"keywords": ["Human-AI collaboration", "UX evaluation", "Explanation", "Synchronization", "Usability test videos"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["User Experience"]}}, {"title": "Explaining deep reinforcement learning agents in the Atari domain through a surrogate model", "author": "A Sieusahai"}, {"title": "Explainable Agency by Revealing Suboptimality in Child-Robot Learning Scenarios", "author": "S Tulli"}, {"title": "QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph", "author": "S Shekarpour"}, {"title": "An exploration of combinatorial testing-based approaches to fault localization for explainable AI", "author": "L Kampel"}, {"title": "Explainable search", "author": "H Baier"}, {"title": "Designing for the human in the loop: Transparency and control in interactive machine learning", "author": "A Renner"}, {"title": "Explaining before or after acting? How the timing of self-explanations affects user perception of robot behavior", "author": "S Stange"}, {"title": "Specifying and Interpreting Reinforcement Learning Policies through Simulatable Machine Learning", "author": "P Tambwekar"}, {"title": "Towards Explainable Recommendations of Resource Allocation Mechanisms in On-Demand Transport Fleets", "author": "A Daoud"}, {"title": "Descriptive and Prescriptive Visual Guidance to Improve Shared Situational Awareness in Human-Robot Teaming", "author": "A Tabrez", "abstract": "In collaborative tasks involving human and robotic teammates, live communication between agents has potential to substantially improve task efficiency and fluency. Effective communication provides essential situational awareness to adapt successfully during uncertain situations and encourage informed decision-making. In contrast, poor communication can lead to incongruous mental models resulting in mistrust and failures. In this work, we first introduce characterizations of and generative algorithms for two", "keywords": {"keywords": ["situational awareness", "human-robot teaming", "communication", "mental models", "decision-making"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["collaborative tasks"]}}, {"title": "How to explain AI systems to end users: a systematic literature review and research agenda", "author": "S Laato"}, {"title": "Acceptance of autonomous vehicles: An overview of user-specific, car-specific and contextual determinants", "author": "M Mara"}, {"title": "Mediators: Conversational Agents Explaining NLP Model Behavior", "author": "N Feldhus"}, {"title": "Explainable artificial intelligence for education and training", "author": "K Fiok"}, {"title": "Inherently Explainable Reinforcement Learning in Natural Language", "author": "X Peng"}, {"title": "Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions", "author": "A Daruna"}, {"title": "THE CAPCO INSTITUTE", "author": "M Ethelston"}, {"title": "Service Abstractions for Scalable Deep Learning Inference at the Edge", "author": "P Guo"}, {"title": "TEACHER PERFORMANCE ON STUDENT LEARNING MOTIVATION IN EFFORTS TO IMPROVE THE QUALITY OF EDUCATION at MTs DARUL HIKMAH \u2026", "author": "O Sinaga"}, {"title": "Human-AI interaction: An emerging interdisciplinary domain for enabling human-centered AI", "author": "W Xu", "abstract": "The new characteristics of AI technology have brought new challenges to the research and development of AI systems. AI technology has benefited humans, but if improperly developed, it will harm humans. At present, there is no systematic interdisciplinary approach to effectively deal with these new challenges. This paper analyzes the new challenges faced by AI systems and further elaborates the\" Human-Centered AI\"(HCAI) approach we proposed in 2019. In order to enable the implementation of the HCAI approach, we", "keywords": {"keywords": ["Human-AI interaction", "interdisciplinary domain", "human-centered AI", "challenges", "HCAI approach"], "research type": ["Theoretical"], "methodology": ["N/A"], "purposes": ["Exploratory"], "discipline": ["N/A"], "content": ["Methodological"], "application area": ["AI systems"]}}, {"title": "A Cognitive Work Analysis Approach to Explainable Artificial Intelligence in Non-Expert Financial Decision-Making", "author": "M Dikmen"}, {"title": "Ontogen: A Knowledge-Based Approach to Natural Language Generation", "author": "IE Leon"}, {"title": "Human performance modelling", "author": "M MacLean"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.0", "author": "JP Wanner"}, {"title": "The Medical Authority of AI: A Study of AI-enabled Consumer-Facing Health Technology", "author": "Y You"}, {"title": "Machine-annotated rationales: faithfully explaining machine learning models for text classification", "author": "E Herrewijnen"}, {"title": "Augmented Reality-Based Explainable AI Strategies for Establishing Appropriate Reliance and Trust in Human-Robot Teaming", "author": "MB Luebbers"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "The Value of Measuring Trust in AI-A Socio-Technical System Perspective", "author": "M Benk"}, {"title": "Understanding the Role of Explanation Modality in AI-assisted Decision-making", "author": "V Robbemond", "abstract": "Advances in artificial intelligence and machine learning have led to a steep rise in the adoption of AI to augment or support human decision-making across domains. There has been an increasing body of work addressing the benefits of model interpretability and explanations to help end-users or other stakeholders decipher the inner workings of the so-called\u201d black box AI systems\u201d. Yet, little is currently understood about the role of modalities through which explanations can be communicated (eg, text, visualizations, or audio) to", "keywords": {"keywords": ["Explanation Modality", "AI-assisted Decision-making", "Model interpretability", "Black Box AI systems", "Modalities of Explanation"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["AI"]}}, {"title": "Application of the General Data Protection Regulation on Household Social Robots", "author": "L Tr\u00f3cs\u00e1nyi"}, {"title": "Trust and Reliance in XAI--Distinguishing Between Attitudinal and Behavioral Measures", "author": "N Scharowski"}, {"title": "Sommelier: Curating DNN Models for the Masses", "author": "P Guo"}, {"title": "Making SHAP Rap: Bridging Local and Global Insights Through Interaction and Narratives", "author": "M Chromik"}, {"title": "Leveraging Subtle Verbalization and Speech Patterns to Help Evaluators Identify Usability Problem Encounters in Concurrent Think-aloud Sessions", "author": "M Fan"}, {"title": "Explain yourself! Effects of Explanations in Human-Robot Interaction", "author": "J Ambsdorf"}, {"title": "Exploring User Perception of Causality in Automated Data Insights", "author": "PM Law"}, {"title": "Influencing Reinforcement Learning through Natural Language Guidance", "author": "T Tasrin"}, {"title": "Metrics for Robot Proficiency Self-assessment and Communication of Proficiency in Human-robot Teams", "author": "A Norton"}, {"title": "Understanding User Perception of Explainable Algorithmic Decision-Making Systems: A Systematic Literature Review", "author": "S Bae", "abstract": "Improving the accuracy of algorithmic prediction has gained attention in Information Systems research in recent decades. Information systems which include algorithmic prediction have been seen to provide organisational value. However, as decisions based on these opaque algorithms become more ubiquitous, public demand for explanations for its output have naturally increased. This review evaluates research that examines the impact of providing explanations for the predictions made by algorithms, on how users respond to the", "keywords": {"keywords": ["algorithmic decision-making systems", "user perception", "explainable", "systematic literature review", "explanations"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["Information Systems"], "content": ["Application"], "application area": ["algorithmic prediction"]}}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang"}, {"title": "The Algorithmic Imprint", "author": "U Ehsan"}, {"title": "Positing a Sense of Agency-Aware Persuasive AI: Its Theoretical and Computational Frameworks", "author": "R Legaspi"}, {"title": "Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning", "author": "C Kim"}, {"title": "Machine-annotated Rationales: Faithfully Explaining Text Classification", "author": "E Herrewijnen"}, {"title": "DBias: Predicting attribute effectiveness using biased databases", "author": "A Dalvi"}, {"title": "Making AI Explainable in the Global South: A Systematic Review", "author": "CT Okolo"}, {"title": "Mistake Captioning: A Machine Learning Approach For Detecting Mistakes and Generating Instructive Feedback", "author": "A Vinogradov"}, {"title": "Model Distillation for Faithful Explanations of Medical Code Predictions", "author": "Z Wood-Doughty"}, {"title": "Assessing and Finding Faults in AI: Two Empirical Studies", "author": "R Khanna", "abstract": "With the advent of Artificial Intelligence (AI) in every sphere of life in today's day and age, it has become increasingly important for non-AI experts to be able to comprehend the underlying logic of how AI systems work, assess them and find faults in these systems, particularly when they are used in high risk scenarios such as in military strategies and medical applications. Recent developments to address the need to open the black boxes of these AI-powered systems have led to the emergence of AI explanations. There now exist", "keywords": {"keywords": ["Artificial Intelligence", "AI systems", "fault assessment", "AI explanations", "high-risk applications"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI fault assessment"]}}, {"title": "Hammers for Robots: Designing Tools for Reinforcement Learning Agents", "author": "MV Law"}, {"title": "Expect the Unexpected: Leveraging the Human-Robot Ecosystem to Handle Unexpected Robot Failures", "author": "S Honig"}, {"title": "Teacher Performance on Student Learning Motivation in Efforts to Improve the Quality of Education at Mts Darul Hikmah, Sumedang.", "author": "O Sinaga"}, {"title": "Metrics for Robot Proficiency Self-Assessment and Communication of Proficiency in Human-Robot Teams", "author": "S MATTHIAS"}, {"title": "Algorithmic explanations: to become a mockingbird", "author": "A Hare"}, {"title": "Uncertainty Communication by AI Assistants: The Effects on User Trust", "author": "LB Siegling"}, {"title": "From Humans and Back: a Survey on Using Machine Learning to both Socially Perceive Humans and Explain to Them Robot Behaviours", "author": "AM Panchea"}, {"title": "Acoustic Distance, Acoustic Absement, and the Lexicon", "author": "MC Kelley"}, {"title": "EFFECT OF WORK MOTIVATION AND WORK STRESS ON EMPLOYEE PERFORMANCE DURING PANDEMICS: https://doi. org/10.37178/ca-c. 23.2. 017", "author": "RADA Sujai"}, {"title": "Antecedents of Constructive Human-AI Collaboration: An Exploration of Human Actors' Key Competencies", "author": "T S\u00fc\u00dfe", "abstract": "Artificial intelligence (AI) has become an integral element of modern machines, devices and materials, and already transforms the way humans interact with technology in business and society. The traditionally more hierarchical interaction, where humans usually control machines, is constantly blurring as machines become more capable of bringing in their own (intelligent) initiatives to the interaction with humans. Thus, nowadays it is more appropriate to consider the interactive processes between humans and machines as a novel form of", "keywords": {"keywords": ["Constructive Human-AI Collaboration", "Human actors' Key Competencies", "Artificial Intelligence", "Interactive processes", "Human-Machine Interaction"], "research type": ["Theoretical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Interdisciplinary"], "content": ["Methodological"], "application area": ["Human-Machine Collaboration"]}}, {"title": "Human-Centered Explainable AI (HCXAI): beyond opening the black-box of AI", "author": "U Ehsan"}], "explaining recommendations in an interactive hybrid social recommender": [{"title": "Exploring mental models for transparent and controllable recommender systems: a qualitative stud", "author": "T Ngo", "abstract": "While online content is personalized to an increasing degree, eg. using recommender systems (RS), the rationale behind personalization and how users can adjust it typically remains opaque. This was often observed to have negative effects on the user experience and perceived quality of RS. As a result, research increasingly has taken user-centric aspects such as transparency and control of a RS into account, when assessing its quality. However, we argue that too little of this research has investigated the users' perception and", "keywords": {"keywords": ["personalization", "recommender systems", "transparency", "control", "user experience"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Online content"]}}, {"title": "Evaluating visual explanations for similarity-based recommendations: User perception and performanc", "author": "CH Tsai"}, {"title": "Exploring and promoting diagnostic transparency and explainability in online symptom checker", "author": "CH Tsai"}, {"title": "Exploring explainability: a definition, a model, and a knowledge catalogu", "author": "L Chazette"}, {"title": "How to support users in understanding intelligent systems? Structuring the discussio", "author": "M Eiband"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risk", "author": "M Naiseh"}, {"title": "The effects of controllability and explainability in a social recommender syste", "author": "CH Tsai"}, {"title": "Designing Explanation Interfaces for Transparency and Beyond", "author": "CH Tsai"}, {"title": "Generating and understanding personalized explanations in hybrid recommender system", "author": "P Kouki"}, {"title": "Newsviz: depicting and controlling preference profiles using interactive treemaps in news recommender system", "author": "J Kunkel"}, {"title": "Deep learning based semantic personalized recommendation syste", "author": "S Sharma", "abstract": "The past decade has seen significant development in the number of personalized recommendation applications on the World Wide Web. It aims to assist users to retrieve relevant items from a large repository of contents by providing items or services of likely interest based on examined evidence of the users' preferences and desires. However, this vision is complex due to the huge amount of information aka media-rich information available on the web. Most of the systems formulated so far use the metadata linked with the", "keywords": {"keywords": ["deep learning", "semantic", "personalized recommendation system", "metadata", "media-rich information"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommendation system"]}}, {"title": "Detailing explanations in the recommender system based on matching temporal knowledg", "author": "S Chalyi"}, {"title": "A systematic literature review on intelligent user interfaces: preliminary result", "author": "TG Gon\u00e7alves"}, {"title": "Controllability and explainability in a hybrid social recommender syste", "author": "CH Tsai"}, {"title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis", "author": "CH Tsai"}, {"title": "Diversity exposure in social recommender systems: a social capital theory perspectiv", "author": "CH Tsai"}, {"title": "Identifying Group-Specific Mental Models of Recommender Systems: A Novel Quantitative Approac", "author": "J Kunkel"}, {"title": "Exploring User-Controlled Hybrid Recommendation in a Conference Context", "author": "CH Tsai"}, {"title": "Recent studies of xai-revie", "author": "ZF Hu"}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trus", "author": "J Ooge"}, {"title": "LIMEADE: A General Framework for Explanation-Based Human Tuning of Opaque Machine Learner", "author": "BCG Lee", "abstract": "Research in human-centered AI has shown the benefits of systems that can explain their predictions. Methods that allow humans to tune a model in response to the explanations are similarly useful. While both capabilities are well-developed for transparent learning models (eg, linear models and GA2Ms), and recent techniques (eg, LIME and SHAP) can generate explanations for opaque models, no method for tuning opaque models in response to explanations has been user-tested to date. This paper introduces LIMEADE, a general", "keywords": {"keywords": ["Explainability", "Machine Learning", "Human-centered AI", "Model Tuning", "Opaque Models"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Explainable AI"]}}, {"title": "Development of an Instrument for Measuring Users' Perception of Transparency in Recommender System", "author": "M Hellmann"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender System", "author": "M Radensky"}, {"title": "Let Me Ask You This: How Can a Voice Assistant Elicit Explicit User Feedback", "author": "Z Xiao"}, {"title": "Social Recommender System", "author": "I Guy"}, {"title": "Inferring Local and Global Properties in Knowledge Graph", "author": "VR Embar"}, {"title": "How to Support Users in Understanding Intelligent Systems? Structuring th", "author": "M EIBAND"}, {"title": "Switching Strategy of Recommendation Algorithms in Online Dating Platfor", "author": "R Fang"}, {"title": "Erkl\u00e4rung der Selbstst\u00e4ndigkei", "author": "Z Wang"}, {"title": "An empirical investigation on user understanding of algorithm", "author": "PT Ngo"}, {"title": "The risks of opening the car bonnet: A semi-systematic review of explainability in AI system", "author": "CE Serrano", "abstract": "Executive summary Artificial intelligence (AI) systems are becoming ubiquitous nowadays, being seen in most of the fields that humans operate in. Their wide adoption is starting to profoundly disrupt society. Furthermore, some of the most advanced models have an inherent \u201cblack box opacity\u201d that prevents humans from sometimes discerning how an AI system comes up with a result or decision, presenting a novel challenge never seen before in history.", "keywords": {"keywords": ["AI systems", "explainability", "black box opacity", "disruption", "society"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Artificial intelligence"]}}, {"title": "A Diary Study of Social Explanations for Recommendations in Daily Lif", "author": "Z Zhang"}, {"title": "Making Filter Bubbles Understandabl", "author": "S Hirschmeier"}, {"title": "Managing Temporal Dynamics of Filter Bubble", "author": "S Hirschmeier"}, {"title": "How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement and ", "author": "D Buschek"}, {"title": "User Feedback in Controllable and Explainable Social Recommender Systems: a Linguistic Analysis. In: Proceedings of Joint Workshop on Interfaces and Human ", "author": "CH Tsai"}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Stud", "author": "L Chazette"}, {"title": "\u0406\u041d\u0424\u041e\u0420\u041c\u0410\u0426\u0406\u0419\u041d\u0410 \u0422\u0415\u0425\u041d\u041e\u041b\u041e\u0413\u0406\u042f \u041f\u041e\u0411\u0423\u0414\u041e\u0412\u0418 \u041f\u041e\u042f\u0421\u041d\u0415\u041d\u042c \u0417 \u0423\u0420\u0410\u0425\u0423\u0412\u0410\u041d\u041d\u042f\u041c \u0422\u0415\u041c\u041f\u041e\u0420\u0410\u041b\u042c\u041d\u0418\u0425 \u0417\u041c\u0406\u041d \u0423 \u0412\u0418\u041c\u041e\u0413\u0410\u0425 \u041a\u041e\u0420\u0418\u0421\u0422\u0423\u0412\u0410\u0427\u0406\u0412 \u0420\u0415\u041a\u041e\u041c\u0415\u041d\u0414\u0410\u0426\u0406\u0419\u041d\u041e\u0407 ", "author": "S Chalyi"}, {"title": "Sistema recomendador h\u00edbrido basado en modelos probabil\u00edstico", "author": "PM Valdiviezo Diaz"}, {"title": "\u0422\u0435\u043c\u043f\u043e\u0440\u0430\u043b\u044c\u043d\u0456 \u043f\u0430\u0442\u0435\u0440\u043d\u0438 \u0432\u043f\u043e\u0434\u043e\u0431\u0430\u043d\u044c \u043a\u043e\u0440\u0438\u0441\u0442\u0443\u0432\u0430\u0447\u0456\u0432 \u0432 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u0444\u043e\u0440\u043c\u0443\u0432\u0430\u043d\u043d\u044f \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u044c \u0432 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0456\u0439\u043d\u0456\u0439 \u0441\u0438\u0441\u0442\u0435\u043c", "author": "\u0421\u0424 \u0427\u0430\u043b\u0438\u0439"}, {"title": "\u0423\u0417\u0413\u041e\u0414\u0416\u0415\u041d\u041d\u042f \u041f\u041e\u042f\u0421\u041d\u0415\u041d\u041d\u042f \u0412 \u0406\u041d\u0422\u0415\u041b\u0415\u041a\u0422\u0423\u0410\u041b\u042c\u041d\u0406\u0419 \u0406\u041d\u0424\u041e\u0420\u041c\u0410\u0426\u0406\u0419\u041d\u0406\u0419 \u0421\u0418\u0421\u0422\u0415\u041c\u0406 \u0417 \u0411\u0410\u0417\u041e\u0412\u0418\u041c\u0418 \u0417\u041d\u0410\u041d\u041d\u042f\u041c\u0418 \u0429\u041e\u0414\u041e \u041f\u0420\u0415\u0414\u041c\u0415\u0422\u041d\u041e\u0407 \u041e\u0411\u041b\u0410\u0421\u0422", "author": "S Chalyi", "abstract": "\u0410\u043d\u043e\u0442\u0430\u0446\u0456\u044f \u041f\u0440\u0435\u0434\u043c\u0435\u0442\u043e\u043c \u0432\u0438\u0432\u0447\u0435\u043d\u043d\u044f \u0432 \u0441\u0442\u0430\u0442\u0442\u0456 \u0454 \u043f\u0440\u043e\u0446\u0435\u0441\u0438 \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0438 \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u044c \u0449\u043e\u0434\u043e \u0437\u0430\u043f\u0440\u043e\u043f\u043e\u043d\u043e\u0432\u0430\u043d\u0438\u0445 \u0456\u043d\u0442\u0435\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u044e \u0456\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0456\u0438\u0306\u043d\u043e\u044e \u0441\u0438\u0441\u0442\u0435\u043c\u043e\u044e \u0440\u0456\u0448\u0435\u043d\u044c. \u041c\u0435\u0442\u043e\u044e \u0454 \u0440\u043e\u0437\u0440\u043e\u0431\u043a\u0430 \u043c\u0435\u0442\u043e\u0434\u0443 \u0443\u0437\u0433\u043e\u0434\u0436\u0435\u043d\u043d\u044f \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u044c \u0432 \u0456\u043d\u0442\u0435\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u0456\u0438\u0306 \u0456\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0456\u0438\u0306\u043d\u0456\u0438\u0306 \u0441\u0438\u0441\u0442\u0435\u043c\u0456 \u0437 \u0443\u0440\u0430\u0445\u0443\u0432\u0430\u043d\u043d\u044f\u043c \u043e\u0431\u043c\u0435\u0436\u0435\u043d\u044c, \u044f\u043a\u0456 \u0432\u0438\u0437\u043d\u0430\u0447\u0430\u044e\u0442\u044c\u0441\u044f \u0431\u0430\u0437\u043e\u0432\u0438\u043c\u0438 \u0437\u043d\u0430\u043d\u043d\u044f\u043c\u0438 \u0449\u043e\u0434\u043e \u043e\u0431'\u0454\u043a\u0442\u0456\u0432 \u0442\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432 \u0443 \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u043e\u0456\u0308 \u043e\u0431\u043b\u0430\u0441\u0442\u0456. \u0417\u0430\u0432\u0434\u0430\u043d\u043d\u044f: \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0437\u0430\u0446\u0456\u044f \u043f\u0440\u043e\u0446\u0435\u0441\u0443 \u043f\u043e\u0431\u0443\u0434\u043e\u0432\u0438 \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u044c \u0437 \u0443\u0440\u0430\u0445\u0443\u0432\u0430\u043d\u043d\u044f\u043c \u043e\u0431\u043c\u0435\u0436\u0435\u043d\u044c, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0445 \u0437\u043d\u0430\u043d\u043d\u044f\u043c\u0438 \u0449\u043e\u0434\u043e \u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u043e\u0456\u0308 \u043e\u0431\u043b\u0430\u0441\u0442\u0456;", "keywords": [["\u0456\u043d\u0442\u0435\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u0456\u0438", 0.6773], ["\u0456\u043d\u0442\u0435\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u044e", 0.6025], ["\u0456\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0456\u0438", 0.5968], ["\u043f\u0440\u0435\u0434\u043c\u0435\u0442\u043d\u043e\u0456", 0.5708], ["\u043f\u0440\u043e\u0446\u0435\u0441\u0456\u0432", 0.5638], ["\u0437\u0430\u043f\u0440\u043e\u043f\u043e\u043d\u043e\u0432\u0430\u043d\u0438\u0445", 0.5472], ["\u043e\u0431\u043b\u0430\u0441\u0442\u0456", 0.5471], ["\u0430\u043d\u043e\u0442\u0430\u0446\u0456\u044f", 0.5442], ["\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0437\u0430\u0446\u0456\u044f", 0.5278], ["\u0441\u0438\u0441\u0442\u0435\u043c\u0456", 0.5244]]}], "bringing transparency design into practice": [{"title": "Designing theory-driven user-centric explainable A", "author": "D Wang", "abstract": "From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields", "keywords": {"keywords": ["explainable AI", "human decision making", "conceptual framework", "philosophy", "psychology"], "research type": ["theoretical"], "methodology": [], "purposes": ["exploratory"], "discipline": [], "content": [], "application area": []}}, {"title": "Questioning the AI: informing design practices for explainable AI user experience", "author": "QV Liao"}, {"title": "Explainability fact sheets: a framework for systematic assessment of explainable approache", "author": "K Sokol"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI system", "author": "S Mohseni"}, {"title": "Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concern", "author": "H Felzmann"}, {"title": "The effects of example-based explanations in a machine learning interfac", "author": "CJ Cai"}, {"title": "Machine learning in mental health: A systematic review of the HCI literature to support the development of effective and implementable ML system", "author": "A Thieme"}, {"title": "One explanation does not fit al", "author": "K Sokol"}, {"title": "Progressive disclosure: empirically motivated approaches to designing effective transparenc", "author": "A Springer"}, {"title": "Robots, healthcare, and the law: Regulating automation in personal car", "author": "E Fosch-Villaronga"}, {"title": "Explaining recommendations in an interactive hybrid social recommende", "author": "CH Tsai", "abstract": "Hybrid social recommender systems use social relevance from multiple sources to recommend relevant items or people to users. To make hybrid recommendations more transparent and controllable, several researchers have explored interactive hybrid recommender interfaces, which allow for a user-driven fusion of recommendation sources. In this field of work, the intelligent user interface has been investigated as an approach to increase transparency and improve the user experience. In this paper, we attempt to further", "keywords": {"keywords": ["hybrid social recommender systems", "interactive", "transparency", "user experience", "intelligent user interface"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["recommendations"]}}, {"title": "When people and algorithms meet: User-reported problems in intelligent everyday application", "author": "M Eiband"}, {"title": "Robots and transparency: The multiple dimensions of transparency in the context of robot technologie", "author": "H Felzmann"}, {"title": "I drive-you trust: Explaining driving behavior of autonomous car", "author": "G Wiegand"}, {"title": "Understanding client support strategies to improve clinical outcomes in an online mental health interventio", "author": "P Chikersal"}, {"title": "What is\" intelligent\" in intelligent user interfaces? a meta-analysis of 25 years of IU", "author": "ST V\u00f6lkel"}, {"title": "The effect of numerical and textual information on visual engagement and perceptions of AI-driven persona interface", "author": "J Salminen"}, {"title": "Exploring mental models for transparent and controllable recommender systems: a qualitative stud", "author": "T Ngo"}, {"title": "Evaluating visual explanations for similarity-based recommendations: User perception and performanc", "author": "CH Tsai"}, {"title": "Scenario-based requirements elicitation for user-centric explainable A", "author": "D Cirqueira"}, {"title": "Normative vs. Pragmatic: Two Perspectives on the Design of Explanations in Intelligent Systems", "author": "M Eiband", "abstract": "This paper compares two main perspectives on explanations in intelligent systems: 1) A normative view, based on recent legislation and ethical considerations, which motivates detailed and comprehensive explanations of algorithms in intelligent systems. 2) A pragmatic view, motivated by benefits for usability and efficient use, achieved through better understanding of the system. We introduce and discuss design dimensions for explanations in intelligent systems and their desired realizations as motivated by these two perspectives", "keywords": {"keywords": ["explanations", "intelligent systems", "normative view", "pragmatic view", "design dimensions"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Collaborative practices with structured data: Do tools support what users need", "author": "L Koesten"}, {"title": "Human-centered XAI: Developing design patterns for explanations of clinical decision support system", "author": "TAJ Schoonderwoerd"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparenc", "author": "AI Anik"}, {"title": "Exploring autonomy in the design of an intelligent health assistant for older adult", "author": "J Sanders"}, {"title": "Introduction to explainable A", "author": "QV Liao"}, {"title": "How to support users in understanding intelligent systems? Structuring the discussio", "author": "M Eiband"}, {"title": "Transparency by design in data-informed research: A collection of information design pattern", "author": "A Rossi"}, {"title": "The effects of controllability and explainability in a social recommender syste", "author": "CH Tsai"}, {"title": "Designing Explanation Interfaces for Transparency and Beyond", "author": "CH Tsai"}, {"title": "Exploring older adults' beliefs about the use of intelligent assistants for consumer health information management: A participatory design stud", "author": "A Martin-Hammond", "abstract": "Background: Intelligent assistants (IAs), also known as intelligent agents, use artificial intelligence to help users achieve a goal or complete a task. IAs represent a potential solution for providing older adults with individualized assistance at home, for example, to reduce social isolation, serve as memory aids, or help with disease management. However, to design IAs for health that are beneficial and accepted by older adults, it is important to understand their beliefs about IAs, how they would like to interact with IAs for consumer", "keywords": {"keywords": ["older adults", "intelligent assistants", "consumer health information management", "participatory design", "beliefs"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Designing interactions with intention-aware gaze-enabled artificial agent", "author": "J Newn"}, {"title": "\u201cI'd like an Explanation for That!\u201d Exploring Reactions to Unexpected Autonomous Drivin", "author": "G Wiegand"}, {"title": "A question of access: Exploring the perceived benefits and barriers of intelligent voice assistants for improving access to consumer health resources among low ", "author": "P Nallam"}, {"title": "The grammar of interactive explanatory model analysi", "author": "H Baniecki"}, {"title": "Opportunities and challenges of utilizing personality traits for personalization in HC", "author": "ST V\u00f6lkel"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experience", "author": "QV Liao"}, {"title": "Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognitio", "author": "M Nourani"}, {"title": "Investigating searchers' mental models to inform search explanation", "author": "P Thomas"}, {"title": "Progressive disclosure: When, why, and how do users want algorithmic transparency information", "author": "A Springer"}, {"title": "Older adults' perceptions of intelligent voice assistant privacy, transparency, and online privacy guideline", "author": "K Bonilla", "abstract": "The significant increase in the availability of intelligent virtual assistants (IVA) such as Amazon Alexa or Google Home provides a potential array of benefits to the daily life of older adult consumers; however, this increased consumption has also given rise to fears concerning privacy and security thereby limiting use by some groups. Our investigation explores older adults' experiences and understanding of intelligent voice assistants. In a pilot study, we conducted semi-structured interviews with seven older adults to identify their", "keywords": {"keywords": ["intelligent voice assistant", "privacy", "transparency", "online privacy guideline", "older adults"], "research type": ["empirical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["application"], "application area": ["technology and aging"]}}, {"title": "Making transparency clea", "author": "A Springer"}, {"title": "Combined Cognitive-Motivational Modules Delivered via an LMS Increase Undergraduate Biology Grades", "author": "JG Cromley"}, {"title": "Mediating community-AI interaction through situated explanation: the case of AI-Led moderatio", "author": "Y Kou"}, {"title": "Question-driven design process for explainable ai user experience", "author": "QV Liao"}, {"title": "What data should i protect? Recommender and planning support for data security analyst", "author": "T Li"}, {"title": "Towards a process model for co-creating AI experience", "author": "H Subramonyam"}, {"title": "Show or suppress? Managing input uncertainty in machine learning model explanation", "author": "D Wang"}, {"title": "Enabling effective transparency: towards user-centric intelligent system", "author": "A Springer"}, {"title": "ExSS: explainable smart systems 201", "author": "B Lim"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligenc", "author": "W Jin", "abstract": "The ability to explain decisions to its end-users is a necessity to deploy AI as critical decision support. Yet making AI explainable to end-users is a relatively ignored and challenging problem. To bridge the gap, we first identified twelve end-user-friendly explanatory forms that do not require technical knowledge to comprehend, including feature-, example-, and rule-based explanations. We then instantiated the explanatory forms as prototyping cards in four AI-assisted critical decision-making tasks, and conducted a user study to co-design low", "keywords": {"keywords": ["explanable Artificial Intelligence", "end-user-centered", "prototyping framework", "user study", "explanatory forms"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Prescriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Decision Support"]}}, {"title": "From flat file to interface: Synthesis of personas and analytics for enhanced user understandin", "author": "BJ Jansen"}, {"title": "Horses for courses: Making the case for persuasive engagement in smart system", "author": "S Stumpf"}, {"title": "Towards Relatable Explainable AI with the Perceptual Proces", "author": "W Zhang"}, {"title": "\" I had a solid theory before but it's falling apart\": Polarizing Effects of Algorithmic Transparenc", "author": "A Springer"}, {"title": "Real-time personalization in adaptive IDE", "author": "M Schmidmaier"}, {"title": "Educating Software and AI Stakeholders About Algorithmic Fairness, Accountability, Transparency and Ethic", "author": "V Bogina"}, {"title": "Controllability and explainability in a hybrid social recommender syste", "author": "CH Tsai"}, {"title": "Capturing the Trends, Applications, Issues, and Potential Strategies of Designing Transparent AI Agent", "author": "L Sun"}, {"title": "ExSS-ATEC: Explainable Smart Systems for Algorithmic Transparency in Emerging Technologies 202", "author": "A Smith-Renner"}, {"title": "Exploring Understandable Algorithms to Suggest Fitness Tracker Goals that Foster Commitmen", "author": "PW Wo\u017aniak", "abstract": "While fitness trackers are gaining popularity, they struggle to offer long-term health benefits, largely due to their inability to offer engaging goals. Understanding how trackers can suggest and update fitness goals can lead to building improved systems that support wellbeing. We investigate how to suggest fitness tracker goals to users and ways to help them commit to those goals. We compared algorithms for step goal setting in a pre-study. Next, we conducted two surveys (a vignette study and a survey using the users' Fitbit data)", "keywords": {"keywords": ["fitness trackers", "goals", "commitment", "algorithms", "health benefits"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Health and Wellness"]}}, {"title": "Tinkering: A Way Towards Designing Transparent Algorithmic User Interfaces", "author": "D Showkat"}, {"title": "Explainability for experts: A design framework for making algorithms supporting expert decisions more explainabl", "author": "A Simkute"}, {"title": "Design Methods for Artificial Intelligence Fairness and Transparenc", "author": "S Stumpf"}, {"title": "EUCA: the End-User-Centered Explainable AI Framewor", "author": "W Jin"}, {"title": "Accurate, Fair, and Explainable: Building Human-Centered A", "author": "A Springer"}, {"title": "Identifying Group-Specific Mental Models of Recommender Systems: A Novel Quantitative Approac", "author": "J Kunkel"}, {"title": "How to choose an explainability method? towards a methodical implementation of xai in practic", "author": "T Vermeire"}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trus", "author": "J Ooge"}, {"title": "Non-monotonic Explanation Function", "author": "L Amgoud"}, {"title": "Fairness, explainability and in-between: understanding the impact of different explanation methods on non-expert users' perceptions of fairness toward an algorithmic ", "author": "A Shulner-Tal", "abstract": "In light of the widespread use of algorithmic (intelligent) systems across numerous domains, there is an increasing awareness about the need to explain their underlying decision-making process and resulting outcomes. Since oftentimes these systems are being considered as black boxes, adding explanations to their outcomes may contribute to the perception of their transparency and, as a result, increase users' trust and fairness perception towards the system, regardless of its actual fairness, which can be measured", "keywords": {"keywords": ["fairness", "explainability", "impact", "explanation methods", "non-expert users"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Algorithmic systems"]}}, {"title": "Ai healthcare system interface: Explanation design for non-expert user trus", "author": "R Larasati"}, {"title": "Simultaneous pupillometry and functional Magnetic Resonance Imaging (fMRI) for the detection of stress-related endophenotype", "author": "M Schneider"}, {"title": "How to explain AI systems to end users: a systematic literature review and research agend", "author": "S Laato"}, {"title": "The Expert of Oz: A Two-sided Study Paradigm for Intelligent System", "author": "A Butz"}, {"title": "Mitigating Bias in Algorithmic Systems: A Fish-Eye View of Problems and Solutions Across Domain", "author": "K Orphanou"}, {"title": "A Method and Analysis to Elicit User-Reported Problems in Intelligent Everyday Application", "author": "M Eiband"}, {"title": "How to Support Users in Understanding Intelligent Systems? Structuring th", "author": "M EIBAND"}, {"title": "Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendation", "author": "R Shang"}, {"title": "Prediction for Retrospection: Integrating Algorithmic Stress Prediction into Personal Informatics Systems for College Students' Mental Healt", "author": "T Kim"}, {"title": "Nudging through Friction: an Approach for Calibrating Trust in Explainable A", "author": "M Naiseh", "abstract": "Explainability has become an essential requirement for safe and effective collaborative Human-AI environments., especially when generating recommendations through black-box modality. One goal of eXplainable AI (XAI) is to help humans calibrate their trust while working with intelligent systems., ie, avoid situations where human decision-makers over-trust the AI when it is incorrect., or under-trust the AI when it is correct. XAI., in this context., aims to help humans understand AI reasoning and decide whether to follow or reject its", "keywords": {"keywords": ["Explainability", "Trust calibration", "Human-AI collaboration", "eXplainable AI", "Recommendation systems"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Prescriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Trust in explainable AI"]}}, {"title": "Exploring folk theories of algorithmic news curation for explainable desig", "author": "T Ngo"}, {"title": "A Case Study Investigating a User-Centred and Expert Informed'Companion Guide'for a Complex Sensor-based Platfor", "author": "R Eardley"}, {"title": "Roles Matter! Understanding Differences in the Privacy Mental Models of Smart Home Visitors and Resident", "author": "K Marky"}, {"title": "PIP: Pictorial Interpretable Prototype Learning for Time Series Classificatio", "author": "A Ghods"}, {"title": "Design of an Explanation Interface for Recommender Systems in High Stakes Domain", "author": "BJ Ondruch"}, {"title": "Facets of algorithmic literacy: Information, experience, and individual factors predict attitudes toward algorithmic system", "author": "DE Silva"}, {"title": "Intelligent User Interfaces: Trends and application area", "author": "T Helldin"}, {"title": "Machine Learning Models Interpretations: User Demands Exploratio", "author": "A Smirnova"}, {"title": "Interaction Design for Explainable A", "author": "P Madumal"}, {"title": "An empirical investigation on user understanding of algorithm", "author": "PT Ngo"}, {"title": "Transparent-AI Blueprint: Developing a Conceptual Tool to Support the Design of Transparent AI Agent", "author": "Z Zhou"}, {"title": "Towards Involving End-users in Interactive Human-in-the-loop AI Fairnes", "author": "Y Nakao"}, {"title": "A Framework for Interactive Knowledge-Aided Machine Teachin", "author": "K Taneja"}, {"title": "Journal of Responsible Technolog", "author": "A Simkute"}, {"title": "Exploring design practices for explaining music recommendation", "author": "H Ryu"}, {"title": "It's Just a Recipe?\u2014Comparing Expert and Lay User Understanding of Algorithmic System", "author": "T Ngo"}, {"title": "Logic and Pragmatics in AI Explanatio", "author": "CH Tsai"}, {"title": "C. 4 Transparency use case: autonomous system behavior explanatio", "author": "M Houghtaling"}, {"title": "Mitigating Bias in Algorithmic Systems-A Fish-Eye Vie", "author": "K Orphanou"}, {"title": "Data-Driven Personas as Interfaces for Persona Analytics Syste", "author": "BJ Jansen", "abstract": "With increasing access to analytics data, personas are more commonly generated from online user analytics data, using algorithmic approaches and, less often, from assumptions. This integration of data and algorithms offers new opportunities to shift personas from flat files to interactive persona systems. We illustrate this transition with Automatic Persona Generation (APG), a persona system. In pushing advancements of both persona and analytics conceptualization, development, and use, APG presents a multi-layered, full-stack", "keywords": {"keywords": ["data-driven personas", "persona analytics", "interactive persona systems", "Automatic Persona Generation (APG)", "algorithmic approaches"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["user analytics"]}}, {"title": "Towards Scenario-Based and Question-Driven Explanations in Autonomous Vehicle", "author": "Y Zhang"}, {"title": "How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement and ", "author": "D Buschek"}, {"title": "Usability, User Comprehension, and Perceptions of Explanations for Complex Decision Support Systems in Finance: A Robo-Advisory Use Cas", "author": "S Deo"}, {"title": "Situated Case Studies for a Human-Centered Design of Explanation User Interface", "author": "C M\u00fcller-Birn"}, {"title": "Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Complianc", "author": "A Bell"}, {"title": "Improving Trustworthiness of AI Solutions: A Qualitative Approach to Support Ethically-Grounded AI Desig", "author": "A Vianello"}, {"title": "Explaining the Road Not Take", "author": "H Shen"}, {"title": "MoReXAI-A Model to Reason About the eXplanation Design in AI System", "author": "N de Oliveira Carvalho"}, {"title": "AI for the public. How public interest theory shifts the discourse on A", "author": "T Z\u00fcger"}, {"title": "Human-AI Interaction Design in Machine Teachin", "author": "K Taneja"}, {"title": "Designing Conversational Assistants to Support Older Adults' Personal Health Record Acces", "author": "P Karimi"}, {"title": "Designing for empowerment\u2013An investigation and critical reflectio", "author": "H Schneider"}, {"title": "Exploration of smart infrastructure for drivers of autonomous vehicles: design space, out-of-view visualization and explanation", "author": "G Wiegand"}, {"title": "What to explain when explaining is difficult? An interdisciplinary primer on XAI and meaningful information in automated decision-makin", "author": "H Asghari"}, {"title": "Can Requirements Engineering Support Explainable Artificial Intelligence? Towards a User-Centric Approach for Explainability Requirement", "author": "J Bogner"}, {"title": "Gaze-Based Intention Recognition for Human-Agent Collaboratio", "author": "J Newn"}, {"title": "Nonverbal Communication in Human-AI Interaction: Opportunities and Challenge", "author": "J Newn"}, {"title": "Towards Usable Explanations: Extending the Nested Model of Visualization Design for User-Centric XA", "author": "Q Wang"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI System", "author": "S Mohseni"}, {"title": "How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Stud", "author": "L Chazette", "abstract": "Quality aspects such as ethics, fairness, and transparency have been proven to be essential for trustworthy software systems. Explainability has been identified not only as a means to achieve all these three aspects in systems, but also as a way to foster users' sentiments of trust. Despite this, research has only marginally focused on the activities and practices to develop explainable systems. To close this gap, we recommend six core activities and associated practices for the development of explainable systems based on the results of a", "keywords": {"keywords": ["explainable systems", "trustworthy software systems", "ethics", "fairness", "transparency"], "research type": ["literature review", "interview study"], "methodology": ["qualitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": "software development"}}, {"title": "How does the User's Knowledge of the Recommender Influence their Behavior", "author": "MF Ghori"}, {"title": "Entwurf eines Requirements Engineering Workflows f\u00fcr erkl\u00e4rbare System", "author": "M Balci"}, {"title": "Everyday engagemen", "author": "J Niess"}, {"title": "Exploration of smart infrastructure for drivers of autonomous vehicle", "author": "G Wiegand"}, {"title": "Kommer jag fram?: En designstudie om transparens f\u00f6r digitalar\u00e4ckviddsm\u00e4tare i elbilar", "author": "O Lagervall"}], "why am i not seeing it? understanding users\u2019 needs for counterfactual explanations in everyday recommendations": [], "interpretable nlg for task-oriented dialogue systems with heterogeneous rendering machines": [{"title": "Neural sequence segmentation as determining the leftmost segments", "author": "Y Li", "abstract": "Prior methods to text segmentation are mostly at token level. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel framework that incrementally segments natural language sentences at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and recurrent neural networks (RNN) to model the", "keywords": {"keywords": ["neural sequence segmentation", "long-term dependencies", "phrase representations", "recurrent neural networks", "LSTM-minus"], "research type": ["applied"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["natural language processing"]}}, {"title": "Retrieval Enhanced Segment Generation Neural Network for Task-Oriented Dialogue Systems", "author": "M Chen", "abstract": "For task-oriented dialogue systems, Natural Language Generation (NLG) is the last and vital step which aims at generating an appropriate response according to the dialogue act (DA). While end-to-end neural networks have achieved promising performances on this task, the existing models still struggle to avoid slot mistakes. To address this challenge, we propose a novel segmented generation approach in this paper. The proposed method operates by progressively generating text for the span between two adjacent keywords (act type and", "keywords": {"keywords": ["Natural Language Generation", "dialogue act", "end-to-end neural networks", "slot mistakes", "segmented generation"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Task-oriented dialogue systems"]}}, {"title": "\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7814\u7a76\u8fdb\u5c55\u7efc\u8ff0", "author": "\u8983\u7acb\u6ce2\uff0c \u9ece\u5dde\u626c\uff0c \u5a04\u6770\u94ed\uff0c \u79b9\u68cb\u8d62\uff0c \u8f66\u4e07\u7fd4", "abstract": "\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6a21\u5757(ToDNLG) \u65e8\u5728\u5c06\u7cfb\u7edf\u7684\u5bf9\u8bdd\u52a8\u4f5c\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u56de\u590d, \u5176\u53d7\u5230\u7814\u7a76\u8005\u7684\u5e7f\u6cdb\u5173\u6ce8. \u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u5c55\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7206\u53d1, ToDNLG \u7684\u7814\u7a76\u5df2\u7ecf\u83b7\u5f97\u4e86\u91cd\u5927\u7a81\u7834. \u7136\u800c, \u76ee\u524d\u4ecd\u7136\u7f3a\u4e4f\u5bf9\u73b0\u6709\u65b9\u6cd5\u548c\u6700\u65b0\u8d8b\u52bf\u7684\u5168\u9762\u8c03\u7814. \u4e3a\u4e86\u586b\u8865\u8fd9\u4e2a\u7a7a\u767d, \u8be5\u6587\u5168\u9762\u8c03\u7814\u4e86ToDNLG \u7684\u6700\u65b0\u8fdb\u5c55\u548c\u524d\u6cbf\u9886\u57df, \u5305\u62ec:(1) \u7cfb\u7edf\u6027\u56de\u987e: \u56de\u987e\u548c\u603b\u7ed3\u4e86ToDNLG \u8fd110 \u5e74\u7684\u53d1\u5c55\u8109\u7edc\u548c\u65b9\u6cd5, \u5305\u62ec\u975e\u795e\u7ecf\u7f51\u7edc\u65f6\u4ee3\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684ToDNLG \u5de5\u4f5c;(2) \u524d\u6cbf\u4e0e\u6311\u6218: \u603b\u7ed3\u4e86\u590d\u6742ToDNLG \u7b49\u4e00\u4e9b\u65b0\u5174\u9886\u57df\u53ca\u5176\u76f8\u5e94\u7684\u6311\u6218;(3)", "keywords": {"keywords": ["\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf", "\u81ea\u7136\u8bed\u8a00\u751f\u6210", "\u7814\u7a76\u8fdb\u5c55", "ToDNLG", "\u56de\u987e"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["chatbot"]}}], "how can i explain this to you? an empirical study of deep neural network explanation methods": [{"title": "Benchmarking and survey of explanation methods for black box model", "author": "F Bodria", "abstract": "The widespread adoption of black-box models in Artificial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach specific decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with different explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison", "keywords": {"keywords": ["black box model", "explanation methods", "benchmarking", "survey", "visual comparison"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Prediction of stock price direction using a hybrid GA-XGBoost algorithm with a three-stage feature engineering proces", "author": "KK Yun"}, {"title": "Interactive label cleaning with example-based explanation", "author": "S Teso"}, {"title": "Dissect: Disentangled simultaneous explanations via concept traversal", "author": "A Ghandeharioun"}, {"title": "Probing gnn explainers: A rigorous theoretical and empirical analysis of gnn explanation method", "author": "C Agarwal"}, {"title": "Synthetic benchmarks for scientific research in explainable machine learnin", "author": "Y Liu"}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable a", "author": "M Nauta"}, {"title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualizatio", "author": "J Borowski"}, {"title": "Explaining Deep Learning using examples: Optimal feature weighting methods for twin systems using post-hoc, explanation-by-example in XA", "author": "EM Kenny"}, {"title": "User Scored Evaluation of Non-Unique Explanations for Relational Graph Convolutional Network Link Prediction on Knowledge Graph", "author": "N Halliwell"}, {"title": "Natural Images are More Informative for Interpreting CNN Activations than State-of-the-Art Synthetic Feature Visualization", "author": "J Borowski"}, {"title": "Toward Scalable and Unified Example-Based Explanation and Outlier Detectio", "author": "P Chong"}, {"title": "DIVINE: Diverse Influential Training Points for Data Visualization and Model Refinemen", "author": "U Bhatt"}, {"title": "Explainable Machine Learning for Breakdown Prediction in High Gradient RF Cavitie", "author": "C Obermair"}, {"title": "DeepSynthBody: the beginning of the end for data deficiency in medicin", "author": "V Thambawita"}, {"title": "Model selection and explainability in neural networks using a polytope interpolation framewor", "author": "S Shekkizhar"}, {"title": "Linked Data Ground Truth for Quantitative and Qualitative Evaluation of Explanations for Relational Graph Convolutional Network Link Prediction on Knowledge ", "author": "N Halliwell"}, {"title": "Cross-model consensus of explanations and beyond for image classification models: An empirical stud", "author": "X Li"}, {"title": "Bounded logit attention: Learning to explain image classifier", "author": "T Baumhauer"}, {"title": "How transparency modulates trust in artificial intelligenc", "author": "J Zerilli"}, {"title": "Automatic Concept Extraction for Concept Bottleneck-based Video Classificatio", "author": "JV Jeyakumar", "abstract": "Recent efforts in interpretable deep learning models have shown that concept-based explanation methods achieve competitive accuracy with standard end-to-end models and enable reasoning and intervention about extracted high-level visual concepts from images, eg, identifying the wing color and beak length for bird-species classification. However, these concept bottleneck models rely on a necessary and sufficient set of predefined concepts-which is intractable for complex tasks such as video classification. For complex tasks, the", "keywords": {"keywords": ["Automatic concept extraction", "Concept bottleneck", "Video classification", "Interpretable deep learning", "Concept-based explanation methods"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["video classification"]}}, {"title": "Crowdsourcing evaluation of saliency-based XAI method", "author": "X Lu"}, {"title": "From heatmaps to structured explanations of image classifier", "author": "L Fuxin"}, {"title": "Local explanation of dialogue response generatio", "author": "YL Tuan"}, {"title": "Towards a reliable evaluation of local interpretation method", "author": "J Li"}, {"title": "Towards Self-explainable Classifiers and Regressors in Neuroimaging with Normalizing Flow", "author": "M Wilms"}, {"title": "Discover Bacterial Interactions by Combining Clustering of Pairwise Univariate Time Series and Explainabilit", "author": "A de Linde Agerskov"}, {"title": "ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable A", "author": "S Hess"}, {"title": "Crowd-Sourced Evaluation of Explainable AI Techniques with Game", "author": "M Jain"}, {"title": "Using Machine Learning to Improve Operating Systems' I/O Subsystems A Dissertation Proposal presente", "author": "IU Akgun"}, {"title": "The Role of Human Knowledge in Explainable A", "author": "A Tocchetti"}, {"title": "Margin-distancing for safe model explanatio", "author": "T Yan"}, {"title": "KML: Using Machine Learning to Improve Storage System", "author": "IU Akgun"}, {"title": "Practical Algorithms for Resource Allocation and Decision Makin", "author": "DC McElfresh"}, {"title": "Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanation", "author": "S Hicks"}, {"title": "Open-source AI-ready data for prediction of coastal water and carbon budgets under a changing climat", "author": "B Arora"}, {"title": "Topological Representations of Local Explanation", "author": "P Xenopoulos"}, {"title": "Forecasting SEP Events based on Merged CME Catalogs using Machine Learnin", "author": "PW Tarsoly"}, {"title": "Unconventional Concerns for Human-Centered Artificial Intelligenc", "author": "R Cheruvu"}, {"title": "Towards Automating Model Explanations with Certified Robustness Guarantee", "author": "M Huai"}, {"title": "Adaptive Distributed Systems Spanning Cloud-Edge Network", "author": "JG Noor", "abstract": "The edge computing domain is notably diverse in its composition. Capabilities including sensing, actuation, and mobility are often constrained by limitations such as those in energy, storage, and hardware-specific implementations. Leveraging the diversity of cloud-edge systems when building applications pose fundamental challenges that constrain expressivity, portability, and reconfigurability. Given this complexity, system tooling is necessary to aid application development and deployment in order to provide a stable", "keywords": {"keywords": ["adaptive distributed systems", "cloud-edge network", "edge computing", "expressivity", "reconfigurability"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["technical"], "application area": ["distributed systems"]}}, {"title": "Time Series Model Attribution Visualizations as Explanation", "author": "U Schlegel"}, {"title": "Explanation using example", "author": "H Park"}, {"title": "On the amplification of security and privacy risks by post-hoc explanations in machine learning model", "author": "P Quan"}, {"title": "The Risks of Machine Learning System", "author": "S Tan"}, {"title": "arXiv: Explainable Machine Learning for Breakdown Prediction in High Gradient RF Cavitie", "author": "C Obermair"}, {"title": "Skeleton-Based Explainable Bodily Expressed Emotion Recognition Through Graph Convolutional Network", "author": "E Ghaleb"}, {"title": "Comparing the Machine Readability of Traffic Sign Pictograms in Austria and German", "author": "A Maletzky"}, {"title": "A Robust System to Detect and Explain Public Mask Wearing Behavio", "author": "A Gupta"}, {"title": "Adversarial Attacks Against DNNs Towards Real-World Threa", "author": "R Duan"}, {"title": "The Need for Empirical Evaluation of Explanation Qualit", "author": "N Halliwell", "abstract": "The proposed model was empirically tested using data collected from a survey of MC  was  used to evaluate the causal model and confirmatory factor analysis was performed to examine", "keywords": {"keywords": ["empirical evaluation", "explanation quality", "causal model", "confirmatory factor analysis", "survey data"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Social Science"], "content": ["Methodological"], "application area": ["None"]}}, {"title": "Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision tas", "author": "B Leichtmann"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimizatio", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "Robust and Interpretable Predictions for Multimodal Sensor System", "author": "JV Jeyakumar"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable A", "author": "QV Liao"}, {"title": "Task-aware Similarity Learning for Event-triggered Time Serie", "author": "S Dou"}, {"title": "Cycle-Consistent Counterfactuals by Latent Transformation", "author": "S Khorram"}, {"title": "Beyond Examples: Constructing Explanation Space for Explaining Prototype", "author": "H Joo"}, {"title": "Towards Human-Centered Optimality Criteri", "author": "A Ghandeharioun"}, {"title": "Explainable Artificial Intelligence for Person Identificatio", "author": "L Coroama"}, {"title": "Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clusterin", "author": "MO Attaoui", "abstract": "Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (eg, self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this paper, we propose SAFE, a black-box approach to", "keywords": {"keywords": ["deep neural networks", "safety analysis", "feature extraction", "clustering", "DNN retraining"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["safety-critical systems"]}}, {"title": "A Survey on Neural Networks for (Cyber-) Security and (Cyber-) Security of Neural Network", "author": "M Pawlicki"}, {"title": "Advancing Nearest Neighbor Explanation-by-Example with Critical Classification Region", "author": "EM Kenny"}, {"title": "Propositional Kernel", "author": "M Polato"}, {"title": "TorchPRISM: Principal Image Sections Mapping, a novel method for Convolutional Neural Network features visualizatio", "author": "T Szandala"}, {"title": "Explaining Deep Survival Analysis Models for Heterogenous Dat", "author": "M Wagner"}], "do users benefit from interpretable vision? a user study, baseline, and dataset": [], "explainable subgraph reasoning for forecasting on temporal knowledge graphs": [{"title": "TimeTraveler: Reinforcement learning for temporal knowledge graph forecastin", "author": "H Sun", "abstract": "Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult that faces two main challenges:(1) how to effectively model the time information to handle future timestamps?(2) how to make inductive inference to handle", "keywords": {"keywords": ["Temporal knowledge graph", "Reinforcement learning", "Forecasting", "Time information modeling", "Inductive inference"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "Probing gnn explainers: A rigorous theoretical and empirical analysis of gnn explanation method", "author": "C Agarwal"}, {"title": "Temporal knowledge graph completion: A surve", "author": "B Cai"}, {"title": "Understanding event predictions via contextualized multilevel feature learnin", "author": "S Deng"}, {"title": "Knowledge Graph Reasoning with Relational Directed Grap", "author": "Y Zhang"}, {"title": "TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graph", "author": "Y Liu"}, {"title": "Temporal knowledge graph representation learning with local and global evolution", "author": "J Zhang"}, {"title": "Hyperplane-based time-aware knowledge graph embedding for temporal knowledge graph completio", "author": "P He"}, {"title": "Adaptive Pseudo-Siamese Policy Network for Temporal Knowledge Predictio", "author": "P Shao"}, {"title": "A Survey of Trustworthy Graph Learning: Reliability, Explainability, and Privacy Protectio", "author": "B Wu"}, {"title": "Learning Meta Representations of One-shot Relations for Temporal Knowledge Graph Link Predictio", "author": "Z Ding"}, {"title": "Time-aware Entity Alignment using Temporal Relational Attentio", "author": "C Xu"}, {"title": "What is Event Knowledge Graph: A Surve", "author": "S Guan"}, {"title": "TempCaps: A Capsule Network-based Embedding Model for Temporal Knowledge Graph Completio", "author": "G Fu"}, {"title": "Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasonin", "author": "Z Li"}], "explain your move: understanding agent actions using specific and relevant feature attribution": [{"title": "Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond", "author": "X Li", "abstract": "for evaluating interpretation algorithms interpretability using \u201ctrustworthy\u201d interpretation  algorithms. Finally, we review and discuss the connections between deep models\u2019 interpretations", "keywords": {"keywords": ["Interpretable deep learning", "interpretation algorithms", "interpretability", "trustworthiness", "deep models' interpretations"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "Analysing deep reinforcement learning agents trained with domain randomisation", "author": "T Dai"}, {"title": "Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations", "author": "S Sreedharan"}, {"title": "Understanding rl vision", "author": "J Hilton"}, {"title": "Automatic discovery of interpretable planning strategies", "author": "J Skirzy\u0144ski"}, {"title": "Explanation augmented feedback in human-in-the-loop reinforcement learning", "author": "L Guan"}, {"title": "Perturbation-based methods for explaining deep neural networks: A survey", "author": "M Ivanovs"}, {"title": "Widening the pipeline in human-guided reinforcement learning with explanation and context-aware data augmentation", "author": "L Guan"}, {"title": "Counterfactual explanations in sequential decision making under uncertainty", "author": "S Tsirtsis"}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta"}, {"title": "Edge: Explaining deep reinforcement learning policies", "author": "W Guo"}, {"title": "Observation space matters: Benchmark and optimization algorithm", "author": "JT Kim"}, {"title": "Interactive visualization for debugging rl", "author": "S Deshpande"}, {"title": "Machine versus human attention in deep reinforcement learning tasks", "author": "SS Guo"}, {"title": "Explaining robot policies", "author": "O Watkins"}, {"title": "Temporal-Spatial Causal Interpretations for Vision-Based Reinforcement Learning", "author": "W Shi"}, {"title": "SOAC: The Soft Option Actor-Critic Architecture", "author": "C Li"}, {"title": "Explainable Deep Reinforcement Learning: State of the Art and Challenges", "author": "GA Vouros"}, {"title": "The Atari Data Scraper", "author": "BD Pierson"}, {"title": "Human-AI Shared Control via Frequency-based Policy Dissection", "author": "Q Li"}, {"title": "Lazy-MDPs: Towards Interpretable Reinforcement Learning by Learning When to Act", "author": "A Jacq", "abstract": "Traditionally, Reinforcement Learning (RL) aims at deciding how to act optimally for an artificial agent. We argue that deciding when to act is equally important. As humans, we drift from default, instinctive or memorized behaviors to focused, thought-out behaviors when required by the situation. To enhance RL agents with this aptitude, we propose to augment the standard Markov Decision Process and make a new mode of action available: being lazy, which defers decision-making to a default policy. In addition, we penalize non-lazy", "keywords": {"keywords": ["Reinforcement Learning", "Markov Decision Process", "artificial agent", "human behavior", "default policy"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purpose": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Interpretable Saliency Map for Deep Reinforcement Learning", "author": "H Zheng"}, {"title": "Interpretable Autonomous Flight Via Compact Visualizable Neural Circuit Policies", "author": "P Tylkin"}, {"title": "TOWARDS INTERPRETABLE REINFORCEMENT LEARNING INTERACTIVE VISUALIZATIONS TO INCREASE INSIGHT shuby v. deshpande", "author": "J Schneider"}, {"title": "Some Chess-Specific Improvements for Perturbation-Based Saliency Maps", "author": "J Fritz"}, {"title": "Explainable Deep Reinforcement Learning for Aircraft Separation Assurance", "author": "W Guo"}, {"title": "Contextualizing Artificial Intelligence: The History, Values, and Epistemology of Technology in the Philosophy of Science", "author": "C Grimsley"}, {"title": "Explaining Reinforcement Learning Policies through Counterfactual Trajectories", "author": "J Frost"}, {"title": "Benchmarking perturbation-based saliency maps for explaining Atari agents", "author": "T Huber"}, {"title": "ReCCoVER: Detecting Causal Confusion for Explainable Reinforcement Learning", "author": "J Gajcin"}, {"title": "Comparing human and machine attention in visuomotor tasks", "author": "S Guo"}, {"title": "A modular attention hypothesis for modeling visuomotor behaviors", "author": "R Zhang"}, {"title": "Interpreting a deep reinforcement learning model with conceptual embedding and performance analysis", "author": "Y Dai"}, {"title": "Lazy-MDPs: Towards Interpretable RL by Learning When to Act", "author": "A Jacq"}, {"title": "\uc2ec\uce35 \ubaa8\ubc29\ud559\uc2b5\uacfc Grad-CAM \uc744 \uc774\uc6a9\ud55c \ud2b9\uc9d5\uae30\ubc18 \uc758\uc0ac\uacb0\uc815 \ubaa8\ub378\uc758 \uc2dc\uac01\ud654\ubd84\uc11d", "author": "\ubc30\uccad\ubaa9\uff0c \uc8fc\ud638\ud0dd\uff0c \uae40\uacbd\uc911"}], "visual reasoning by progressive module networks": [{"title": "Visual entailment: A novel task for fine-grained image understandin", "author": "N Xie", "abstract": "Existing visual reasoning datasets such as Visual Question Answering (VQA), often suffer from biases conditioned on the question, image or answer distributions. The recently proposed CLEVR dataset addresses these limitations and requires fine-grained reasoning but the dataset is synthetic and consists of similar objects and sentence structures across the dataset. In this paper, we introduce a new inference task, Visual Entailment (VE)-consisting of image-sentence pairs whereby a premise is defined by an image, rather than a natural", "keywords": {"keywords": ["visual entailment", "fine-grained image understanding", "visual reasoning", "biases", "image-sentence pairs"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "Learning to caption images through a lifetime by asking question", "author": "T Shen"}, {"title": "Dynamic language binding in relational visual reasonin", "author": "TM Le"}, {"title": "KANDINSKYPatterns--An experimental exploration environment for Pattern Analysis and Machine Intelligenc", "author": "A Holzinger"}, {"title": "Learning from lexical perturbations for consistent visual question answerin", "author": "S Whitehead"}, {"title": "On the capability of neural networks to generalize to unseen category-pose combination", "author": "S Madan"}, {"title": "When and how do CNNs generalize to out-of-distribution category-viewpoint combinations", "author": "S Madan"}, {"title": "Compositetasking: Understanding images by spatial composition of task", "author": "N Popovic"}, {"title": "Lifelong learning for image captioning by asking natural language question", "author": "K Shen"}, {"title": "Visual question answering with modules and language modelin", "author": "V Pahuja"}, {"title": "An Information-theoretic Progressive Framework for Interpretatio", "author": "Z He"}], "cogam: measuring and moderating cognitive load in machine learning model explanations": [{"title": "Manipulating and measuring model interpretability", "author": "F Poursabzi-Sangdeh", "abstract": "As such, we argue that to understand interpretability, it is necessary to directly manipulate  diferent factors and measure their efects. What is or is not interpretable must be defned by", "keywords": {"keywords": ["interpretability", "manipulating", "measuring", "factors", "effects"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning Interpretability"}}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle", "author": "S Dhanorkar"}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "Evaluating the interpretability of generative models by interactive reconstruction", "author": "A Ross"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai", "abstract": "As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of", "keywords": {"keywords": ["human-ai decision making", "empirical studies", "AI systems", "augmented decision making", "safety, ethical, and legal concerns"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["decision making"]}}, {"title": "Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens", "author": "H Park"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence", "author": "W Jin"}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang"}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence", "author": "DA Melis"}, {"title": "Explainable AI for tailored electricity consumption feedback\u2013An experimental evaluation of visualizations", "author": "J Wastensteiner"}, {"title": "A Survey of Explainable Reinforcement Learning", "author": "S Milani"}, {"title": "Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia Research Agenda", "author": "C von der Weth"}, {"title": "EUCA: the End-User-Centered Explainable AI Framework", "author": "W Jin"}, {"title": "From human explanation to model interpretability: A framework based on weight of evidence", "author": "D Alvarez-Melis", "abstract": "We take inspiration from the study of human explanation to inform the design and evaluation of interpretability methods in machine learning. First, we survey the literature on human explanation in philosophy, cognitive science, and the social sciences, and propose a list of design principles for machinegenerated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for generating explanations that adhere to these principles. We show that this method can be", "keywords": {"keywords": ["human explanation", "model interpretability", "weight of evidence", "machine learning", "explanations"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["interpretability methods"]}}, {"title": "BreastScreening-AI: Evaluating medical intelligent agents for human-AI interactions", "author": "FM Calisto"}, {"title": "Time to Focus: A Comprehensive Benchmark Using Time Series Attribution Methods", "author": "D Mercier"}, {"title": "A Granular Computing Approach to Provide Transparency of Intelligent Systems for Criminal Investigations", "author": "S Hepenstal"}, {"title": "Why Am I Not Seeing It? Understanding Users' Needs for Counterfactual Explanations in Everyday Recommendations", "author": "R Shang"}, {"title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?", "author": "RS Zimmermann"}, {"title": "Right for the Right Reasons: Training Neural Networks to Be Interpretable, Robust, and Consistent with Expert Knowledge", "author": "AS Ross"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Designing Fair AI in Human Resource Management: Understanding Tensions Surrounding Algorithmic Evaluation and Envisioning Stakeholder-Centered Solutions", "author": "H Park"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "It's Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy", "author": "A Bell", "abstract": "To achieve high accuracy in machine learning (ML) systems, practitioners often use complex \u201cblack-box\u201d models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature\u2014and even the existence\u2014of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is", "keywords": {"keywords": ["accuracy", "explainability", "trade-off", "machine learning", "public policy"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Descriptive"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Public policy"]}}, {"title": "You Complete Me: Human-AI Teams and Complementary Expertise", "author": "Q Zhang"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Measuring algorithmic interpretability: A human-learning-based framework and the corresponding cognitive complexity score", "author": "JP Lalor"}, {"title": "Think About the Stakeholders First! Towards an Algorithmic Transparency Playbook for Regulatory Compliance", "author": "A Bell"}, {"title": "How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review", "author": "A Bertrand"}, {"title": "Summarize with Caution: Comparing Global Feature Attributions.", "author": "A Okeson"}, {"title": "Modeling Cognitive Load in Mobile Human Computer Interaction Using Eye Tracking Metrics", "author": "AW Joseph"}, {"title": "Helping Users Tackle Algorithmic Threats on Social Media: A Multimedia Research Agenda", "author": "CWA Abdul"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "Delegating Agency? The Effects of XAI, Personality Traits, and the Moral Significance of the Application on the Reliance on Autonomous Systems: A User Study", "author": "E Kallina", "abstract": "As autonomous systems are becoming more prevalent, it is essential that the users' willingness to delegate agency to them is calibrated to match the system's abilities, as well as the application context. The current study investigates the influence of three factors on such willingness: The availability of XAI, the moral significance of the application context, and the users' personality traits; in particular their dispositional trust and locus of control. In an online study, participants monitored an autonomous system that classified satellite", "keywords": {"keywords": ["Delegating agency", "XAI", "Personality traits", "Moral significance", "Reliance"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": "Autonomous systems"}}, {"title": "Strategies for Selecting and Adapting Machine Learning Systems to Support Different Types of Experts", "author": "A Okeson"}, {"title": "TimeREISE: Time Series Randomized Evolving Input Sample Explanation", "author": "D Mercier"}, {"title": "Development on Korean Visualization Literacy Assessment Test (K-VLAT) and Research Trend Analysis", "author": "HN Kim"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}, {"title": "\ud55c\uad6d\ud615 \ub370\uc774\ud130 \uc2dc\uac01\ud654 \ub9ac\ud130\ub7ec\uc2dc \ud3c9\uac00 \uac1c\ubc1c \ubc0f \uc5f0\uad6c \ub3d9\ud5a5 \ubd84\uc11d", "author": "\uae40\ud558\ub298\uff0c \uae40\uc131\ud76c"}], "explain, edit, and understand: rethinking user study design for evaluating model explanations": [{"title": "Mediators: Conversational Agents Explaining NLP Model Behavior", "author": "N Feldhus", "abstract": "The human-centric explainable artificial intelligence (HCXAI) community has raised the need for framing the explanation process as a conversation between human and machine. In this position paper, we establish desiderata for Mediators, text-based conversational agents which are capable of explaining the behavior of neural models interactively using natural language. From the perspective of natural language processing (NLP) research, we engineer a blueprint of such a Mediator for the task of sentiment analysis and assess how far", "keywords": {"keywords": ["Mediators", "conversational agents", "NLP model behavior", "human-centric explainable artificial intelligence", "sentiment analysis"], "research type": ["position paper"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["human-centric explainable AI"]}}, {"title": "Learning to Scaffold: Optimizing Model Explanations for Teaching", "author": "P Fernandes", "abstract": "Modern machine learning models are opaque, and as a result there is a burgeoning academic subfield on methods that explain these models' behavior. However, what is the precise goal of providing such explanations, and how can we demonstrate that explanations achieve this goal? Some research argues that explanations should help teach a student (either human or machine) to simulate the model being explained, and that the quality of explanations can be measured by the simulation accuracy of students on unexplained", "keywords": {"keywords": ["model explanations", "teaching", "simulation accuracy", "modern machine learning", "opaque models"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Towards Model Understanding", "author": "D Pruthi", "abstract": "In 2004 the FDA launched the process analytical technology initiative, aiming to guide  the industry towards advanced process monitoring and better understanding of how critical", "keywords": {"keywords": ["process analytical technology", "advanced process monitoring", "FDA", "model understanding", "critical understanding"], "research type": ["case study"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["others"], "content": ["Application"], "application area": ["process analytics"]}}, {"title": "Human Interpretation of Saliency-based Explanation Over Text", "author": "H Schuff", "abstract": "While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide", "keywords": {"keywords": ["saliency-based explanation", "text", "human interpretation", "feature-attribution explanations", "explanable AI"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Textual data analysis"]}}, {"title": "Robustness and Usefulness in AI Explanation Methods", "author": "E Galinkin", "abstract": "Explainability in machine learning has become incredibly important as machine learning-powered systems become ubiquitous and both regulation and public sentiment begin to demand an understanding of how these systems make decisions. As a result, a number of explanation methods have begun to receive widespread adoption. This work summarizes, compares, and contrasts three popular explanation methods: LIME, SmoothGrad, and SHAP. We evaluate these methods with respect to: robustness, in the sense of sample", "keywords": {"keywords": ["explanation methods", "explainability", "machine learning", "LIME", "SmoothGrad", "SHAP"], "research type": "review", "methodology": "quantitative", "purposes": "descriptive", "discipline": "computer science", "content": "technical", "application area": "AI explanation"}}, {"title": "MULTIVIZ: An Analysis Benchmark for Visualizing and Understanding Multimodal Models", "author": "PP Liang", "abstract": "The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our", "keywords": {"keywords": ["multimodal models", "visualizing", "understanding", "internal mechanics", "neural networks"], "research type": ["empirical"], "methodology": [], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Model Distillation for Faithful Explanations of Medical Code Predictions", "author": "Z Wood-Doughty", "abstract": "Machine learning models that offer excellent predictive performance often lack the interpretability necessary to support integrated human machine decision-making. In clinical medicine and other high-risk settings, domain experts may be unwilling to trust model predictions without explanations. Work in explainable AI must balance competing objectives along two different axes: 1) Models should ideally be both accurate and simple. 2) Explanations must balance faithfulness to the model's decision-making with their plausibility", "keywords": {"keywords": ["Model Distillation", "Faithful Explanations", "Medical Code Predictions", "Explainable AI", "Machine Learning"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Explanatory"], "discipline": ["Medicine"], "content": ["Methodological"], "application area": ["Clinical Decision Support"]}}, {"title": "Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language", "author": "R Sevastjanova", "abstract": "Language models learn and represent language differently than humans; they learn the form and not the meaning. Thus, to assess the success of language model explainability, we need to consider the impact of its divergence from a user's mental model of language. In this position paper, we argue that in order to avoid harmful rationalization and achieve truthful understanding of language models, explanation processes must satisfy three main conditions:(1) explanations have to truthfully represent the model behavior, ie, have a high", "keywords": {"keywords": ["language models", "explainability", "rationalization trap", "mental models", "truthful understanding"], "research type": ["Theoretical"], "methodology": ["Not applicable"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Natural language processing"]}}], "explaining a black-box by using a deep variational information bottleneck approach": [{"title": "Evaluating explainable AI: Which algorithmic explanations help users predict model behavior?", "author": "P Hase", "abstract": "Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods:(1) LIME,(2) Anchor,(3) Decision", "keywords": {"keywords": ["explainable AI", "algorithmic explanations", "model behavior prediction", "interpretability", "simulatability"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "An information bottleneck approach for controlling conciseness in rationale extraction", "author": "B Paranjape"}, {"title": "Generative causal explanations of black-box classifiers", "author": "M O'Shaughnessy"}, {"title": "Learning variational word masks to improve the interpretability of neural text classifiers", "author": "H Chen"}, {"title": "Adversarial infidelity learning for model interpretation", "author": "J Liang"}, {"title": "Deep-learning architecture in QSPR modeling for the prediction of energy conversion efficiency of solar cells", "author": "J Wu"}, {"title": "Py-CIU: a python library for explaining machine learning predictions using contextual importance and utility", "author": "S Anjomshoae"}, {"title": "The out-of-distribution problem in explainability and search methods for feature importance explanations", "author": "P Hase"}, {"title": "On the maximum mutual information capacity of neural architectures", "author": "B Foggo"}, {"title": "Why attentions may not be interpretable?", "author": "B Bai"}, {"title": "A framework to learn with interpretation", "author": "J Parekh", "abstract": "To tackle interpretability in deep learning, we present a novel framework to jointly learn a predictive model and its associated interpretation model. The interpreter provides both local and global interpretability about the predictive model in terms of human-understandable high level attribute functions, with minimal loss of accuracy. This is achieved by a dedicated architecture and well chosen regularization penalties. We seek for a small-size dictionary of high level attribute functions that take as inputs the outputs of selected hidden layers and", "keywords": {"keywords": ["interpretability", "deep learning", "framework", "predictive model", "interpretation model"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Learning to explain: Generating stable explanations fast", "author": "X Situ"}, {"title": "Information-Theoretic Methods in Deep Neural Networks: Recent Advances and Emerging Opportunities.", "author": "S Yu"}, {"title": "SnpReady for rice (SR4R) database", "author": "J Yan"}, {"title": "Neural image compression and explanation", "author": "X Li"}, {"title": "How does BERT rerank passages? an attribution analysis with information bottlenecks", "author": "Z Jiang"}, {"title": "Hierarchical Interpretation of Neural Text Classification", "author": "H Yan"}, {"title": "Towards interpreting deep neural networks via layer behavior understanding", "author": "J Cao"}, {"title": "Merlin-Arthur Classifiers: Formal Interpretability with Interactive Black Boxes", "author": "S W\u00e4ldchen"}, {"title": "Explanation-based Counterfactual Retraining (XCR): A Calibration Method for Black-box Models", "author": "L Zhendong"}, {"title": "An Additive Instance-Wise Approach to Multi-class Model Interpretation", "author": "V Vo", "abstract": "Interpretable machine learning offers insights into what factors drive a certain prediction of a black-box system and whether to trust it for high-stakes decisions or large-scale deployment. Existing methods mainly focus on selecting explanatory input features, which follow either locally additive or instance-wise approaches. Additive models use heuristically sampled perturbations to learn instance-specific explainers sequentially. The process is thus inefficient and susceptible to poorly-conditioned samples. Meanwhile, instance-wise", "keywords": {"keywords": ["Interpretable machine learning", "multi-class model interpretation", "additive instance-wise approach", "black-box system", "explanatory input features"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Explainable Machine Learning for Activity Modeling in GeoAi", "author": "L Zhang"}, {"title": "Structure and Causality in Understanding Complex Systems", "author": "MR O'Shaughnessy"}, {"title": "R2-Trans: Fine-Grained Visual Categorization with Redundancy Reduction", "author": "Y Wang"}, {"title": "Towards Interpreting Deep Neural Networks via Understanding Layer Behaviors", "author": "J Cao"}, {"title": "Why is Attention Not So Interpretable?", "author": "B Bai"}, {"title": "Knowledge Mining: A Cross-disciplinary Survey", "author": "Y Rui"}, {"title": "Py-CIU: A Python Library for Explaining Machine Learning Predictions Using Contextual Importance and Utility", "author": "SATKK Fr\u00e4mling"}, {"title": "Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF", "author": "J Parekh"}, {"title": "Reinforced Causal Explainer for Graph Neural Networks", "author": "X Wang"}, {"title": "Variational Distillation for Multi-View Learning", "author": "X Tian", "abstract": "Information Bottleneck (IB) based multi-view learning provides an information theoretic principle for seeking shared information contained in heterogeneous data descriptions. However, its great success is generally attributed to estimate the multivariate mutual information which is intractable when the network becomes complicated. Moreover, the representation learning tradeoff,{\\it ie}, prediction-compression and sufficiency-consistency tradeoff, makes the IB hard to satisfy both requirements simultaneously. In this paper, we", "keywords": {"keywords": ["Variational Distillation", "Multi-View Learning", "Information Bottleneck", "Representation Learning Tradeoff", "Mutual Information"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Data analysis"]}}, {"title": "Reinforcement Explanation Learning", "author": "S Agarwal"}, {"title": "Heterogeneous Feature Integration for Regression in Multimodal Healthcare Applications", "author": "MT Hosseinabadi"}, {"title": "Explainable Artificial Intelligence", "author": "N Posner"}, {"title": "Compressed Data Sharing Based On Information Bottleneck Model", "author": "B Razeghi"}, {"title": "Attending via both Fine-tuning and Compressing", "author": "J Zhou"}], "exemplary natural images explain cnn activations better than state-of-the-art feature visualization": [{"title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?", "author": "RS Zimmermann", "abstract": "A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated-an advantage over other alternatives like strongly activating dataset samples. If humans indeed gain causal insight from visualizations, this", "keywords": {"keywords": ["feature visualizations", "causal understanding", "CNN activations", "explainable artificial intelligence", "unit responses"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "From\" Where\" to\" What\": Towards Human-Understandable Explanations through Concept Relevance Propagation", "author": "R Achtibat", "abstract": "The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's", "keywords": {"keywords": ["eXplainable Artificial Intelligence", "deep learning models", "local XAI methods", "global explanation techniques", "concept relevance propagation"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Advancing Nearest Neighbor Explanation-by-Example with Critical Classification Regions", "author": "EM Kenny", "abstract": "There is an increasing body of evidence suggesting that post-hoc explanation-by-example with nearest neighbors is a promising solution for the eXplainable Artificial Intelligence (XAI) problem. However, despite being thoroughly researched for decades, such post-hoc methods have never seriously explored how to enhance these explanations by highlighting specific important\" parts\" in a classification. Here, we propose the notion of Critical Classification Regions (CCRs) to do this, and several possible methods are experimentally", "keywords": {"keywords": ["XAI", "nearest neighbors", "explanation-by-example", "critical classification regions", "post-hoc methods"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["XAI"]}}], "anchors: high-precision model-agnostic explanations": [{"title": "A survey of methods for explaining black box models", "author": "R Guidotti", "abstract": "In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific", "keywords": {"keywords": ["black box models", "explanation", "decision support systems", "interpretability", "accuracy"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["various"]}}, {"title": "Interpretable machine learning", "author": "C Molnar"}, {"title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI", "author": "AB Arrieta"}, {"title": "Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)", "author": "A Adadi"}, {"title": "From local explanations to global understanding with explainable AI for trees", "author": "SM Lundberg"}, {"title": "Techniques for interpretable machine learning", "author": "M Du"}, {"title": "Machine learning interpretability: A survey on methods and metrics", "author": "DV Carvalho"}, {"title": "Generating natural adversarial examples", "author": "Z Zhao"}, {"title": "A survey on data collection for machine learning: a big data-ai integration perspective", "author": "Y Roh"}, {"title": "Designing theory-driven user-centric explainable AI", "author": "D Wang"}, {"title": "Semantically equivalent adversarial rules for debugging NLP models", "author": "MT Ribeiro", "abstract": "Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs)\u2013semantic-preserving perturbations that induce changes in the model's predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs)\u2013simple, universal replacement rules that induce adversaries on many instances. We demonstrate the", "keywords": {"keywords": ["NLP", "adversarial rules", "debugging", "machine learning models", "semantically equivalent"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["NLP model debugging"]}}, {"title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives", "author": "A Dhurandhar"}, {"title": "Explainable ai: A review of machine learning interpretability methods", "author": "P Linardatos"}, {"title": "Fooling lime and shap: Adversarial attacks on post hoc explanation methods", "author": "D Slack"}, {"title": "Local rule-based explanations of black box decision systems", "author": "R Guidotti"}, {"title": "Actionable recourse in linear classification", "author": "B Ustun"}, {"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao"}, {"title": "One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques", "author": "V Arya"}, {"title": "Explaining deep neural networks and beyond: A review of methods and applications", "author": "W Samek"}, {"title": "Explainable AI for trees: From local explanations to global understanding", "author": "SM Lundberg"}, {"title": "What clinicians want: contextualizing explainable machine learning for clinical end use", "author": "S Tonekaboni", "abstract": "challenges for clinical ML. By highlighting clinicians\u2019 needs for explanation as specific  technical properties, we hope to channel explainable ML research intended for clinical end use to", "keywords": {"keywords": ["clinicians", "explainable machine learning", "clinical end use", "explanation", "challenges"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["healthcare"]}}, {"title": "Faithful and customizable explanations of black box models", "author": "H Lakkaraju"}, {"title": "Drug discovery with explainable artificial intelligence", "author": "J Jim\u00e9nez-Luna"}, {"title": "On human predictions with explanations and predictions of machine learning models: A case study on deception detection", "author": "V Lai"}, {"title": "The challenge of crafting intelligible intelligence", "author": "DS Weld"}, {"title": "Counterfactual explanations for machine learning: A review", "author": "S Verma"}, {"title": "A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability", "author": "X Huang"}, {"title": "Opportunities and challenges of deep learning methods for electrocardiogram data: A systematic review", "author": "S Hong"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "A survey on the explainability of supervised machine learning", "author": "N Burkart"}, {"title": "On the interpretability of machine learning-based model for predicting hypertension", "author": "R Elshawi", "abstract": "Although complex machine learning models are commonly outperforming the traditional simple interpretable models, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. The aim of this study to demonstrate the utility of various model-agnostic explanation techniques of machine learning models with a case study for analyzing the outcomes of the machine learning random forest model for predicting the individuals at risk of developing hypertension based", "keywords": {"keywords": ["machine learning", "interpretability", "hypertension prediction", "model-agnostic explanation techniques", "random forest"], "research type": ["case study"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["medicine"], "content": ["application"], "application area": ["healthcare"]}}, {"title": "Explainable artificial intelligence: a systematic review", "author": "G Vilone"}, {"title": "Model agnostic supervised local explanations", "author": "G Plumb"}, {"title": "A symbolic approach to explaining bayesian network classifiers", "author": "A Shih"}, {"title": "Evaluating weakly supervised object localization methods right", "author": "J Choe"}, {"title": "The effects of example-based explanations in a machine learning interface", "author": "CJ Cai"}, {"title": "explAIner: A visual analytics framework for interactive and explainable machine learning", "author": "T Spinner"}, {"title": "Evaluating explainable AI: Which algorithmic explanations help users predict model behavior?", "author": "P Hase"}, {"title": "Factual and counterfactual explanations for black box decision making", "author": "R Guidotti"}, {"title": "Structure-based protein function prediction using graph convolutional networks", "author": "V Gligorijevi\u0107"}, {"title": "\" How do I fool you?\" Manipulating User Trust via Misleading Black Box Explanations", "author": "H Lakkaraju", "abstract": "As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a human interpretable manner. There has been recent concern that a high-fidelity explanation of a black box ML model may not accurately reflect the biases in the black box. As a consequence, explanations have the potential to mislead human users into trusting a problematic black box. In this work, we", "keywords": {"keywords": ["machine learning black boxes", " explanation techniques", "misleading trust", "biases", "problematic black box"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["critical domains"]}}, {"title": "Meaningful explanations of black box AI decision systems", "author": "D Pedreschi"}, {"title": "BERTology meets biology: interpreting attention in protein language models", "author": "J Vig"}, {"title": "Abduction-based explanations for machine learning models", "author": "A Ignatiev"}, {"title": "Explanatory interactive machine learning", "author": "S Teso"}, {"title": "Principles and practice of explainable machine learning", "author": "V Belle"}, {"title": "Exploring interpretable lstm neural networks over multi-variable data", "author": "T Guo"}, {"title": "A survey on neural network interpretability", "author": "Y Zhang"}, {"title": "On the role of knowledge graphs in explainable AI", "author": "F Lecue"}, {"title": "Interpretability of machine learning\u2010based prediction models in healthcare", "author": "G Stiglic"}, {"title": "A survey of evaluation methods and measures for interpretable machine learning", "author": "S Mohseni", "abstract": "", "keywords": {"keywords": ["interpretable machine learning", "evaluation methods", "evaluation measures", "survey", "interpretable AI"], "research type": "Review", "methodology": "Mixed-methods", "purposes": "Descriptive", "discipline": "Computer science", "content": "Methodological", "application area": "Machine learning"}}, {"title": "The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and \u2026", "author": "AF Markus"}, {"title": "Learning global additive explanations for neural nets using model distillation", "author": "S Tan"}, {"title": "Quantifying interpretability and trust in machine learning systems", "author": "P Schmidt"}, {"title": "DLIME: A deterministic local interpretable model-agnostic explanations approach for computer-aided diagnosis systems", "author": "MR Zafar"}, {"title": "On interpretability of artificial neural networks: A survey", "author": "FL Fan"}, {"title": "Improving the interpretability of deep neural networks with knowledge distillation", "author": "X Liu"}, {"title": "Interpretable and steerable sequence learning via prototypes", "author": "Y Ming"}, {"title": "Doctor XAI: an ontology-based approach to black-box sequential data classification explanations", "author": "C Panigutti"}, {"title": "On the reasons behind decisions", "author": "A Darwiche"}, {"title": "An interpretable model with globally consistent explanations for credit risk", "author": "C Chen", "abstract": "We propose a possible solution to a public challenge posed by the Fair Isaac Corporation (FICO), which is to provide an explainable model for credit risk assessment. Rather than present a black box model and explain it afterwards, we provide a globally interpretable model that is as accurate as other neural networks. Our\" two-layer additive risk model\" is decomposable into subscales, where each node in the second layer represents a meaningful subscale, and all of the nonlinearities are transparent. We provide three types of", "keywords": {"keywords": ["interpretable model", "credit risk", "globally consistent explanations", "neural networks", "two-layer additive risk model"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Economics"], "content": ["Methodological"], "application area": ["Credit risk assessment"]}}, {"title": "Explainable AI in industry", "author": "K Gade"}, {"title": "Explainable deep one-class classification", "author": "P Liznerski"}, {"title": "On relating explanations and adversarial examples", "author": "A Ignatiev"}, {"title": "Human digital twin for fitness management", "author": "BR Barricelli"}, {"title": "A historical perspective of explainable Artificial Intelligence", "author": "R Confalonieri"}, {"title": "A survey of data-driven and knowledge-aware explainable ai", "author": "XH Li"}, {"title": "Measurable counterfactual local explanations for any classifier", "author": "A White"}, {"title": "Explicit bias discovery in visual question answering models", "author": "V Manjunatha"}, {"title": "How can i explain this to you? an empirical study of deep neural network explanation methods", "author": "JV Jeyakumar"}, {"title": "Good counterfactuals and where to find them: A case-based technique for generating counterfactuals for explainable ai (xai)", "author": "MT Keane", "abstract": "(see Sect. 3). To presage our results, to our surprise, we find that in most case-bases \u201cgood\u201d  counterfactuals  case-based technique for counterfactual generation in XAI (Sects. 4 and 5).", "keywords": {"keywords": ["counterfactuals", "explainable AI", "XAI", "case-based technique", "good"], "research type": ["empirical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": [], "content": [], "application area": []}}, {"title": "One explanation does not fit all", "author": "K Sokol"}, {"title": "Interpretability in healthcare: A comparative study of local machine learning interpretability techniques", "author": "R ElShawi"}, {"title": "Personalized explanation in machine learning: A conceptualization", "author": "J Schneider"}, {"title": "How model accuracy and explanation fidelity influence user trust", "author": "A Papenmeier"}, {"title": "ALIME: Autoencoder based approach for local interpretability", "author": "SM Shankaranarayana"}, {"title": "Dark Patterns of Explainability, Transparency, and User Control for Intelligent Systems.", "author": "M Chromik"}, {"title": "Exs: Explainable search using local model agnostic interpretability", "author": "J Singh"}, {"title": "Slice finder: Automated data slicing for model validation", "author": "Y Chung"}, {"title": "Compositional explanations of neurons", "author": "J Mu"}, {"title": "\" Why is' Chicago'deceptive?\" Towards Building Model-Driven Tutorials for Humans", "author": "V Lai", "abstract": "To support human decision making with machine learning models, we often need to elucidate patterns embedded in the models that are unsalient, unknown, or counterintuitive to humans. While existing approaches focus on explaining machine predictions with real-time assistance, we explore model-driven tutorials to help humans understand these patterns in a train-ing phase. We consider both tutorials with guidelines from scientific papers, analogous to current practices of science communication, and automatically", "keywords": {"keywords": ["Chicago", "deceptive", "model-driven tutorials", "machine learning models", "human decision making"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Machine learning"]}}, {"title": "Deep learning for insider threat detection: Review, challenges and opportunities", "author": "S Yuan"}, {"title": "Granger-causal attentive mixtures of experts: Learning important features with neural networks", "author": "P Schwab"}, {"title": "Interpreting neural network judgments via minimal, stable, and symbolic corrections", "author": "X Zhang"}, {"title": "Towards Trustable Explainable AI.", "author": "A Ignatiev"}, {"title": "Robustness verification of tree-based models", "author": "H Chen"}, {"title": "On interpretability of artificial neural networks", "author": "F Fan"}, {"title": "COGAM: measuring and moderating cognitive load in machine learning model explanations", "author": "A Abdul"}, {"title": "DACE: Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization.", "author": "K Kanamori"}, {"title": "Model agnostic contrastive explanations for structured data", "author": "A Dhurandhar"}, {"title": "Statistical stability indices for LIME: Obtaining reliable explanations for machine learning models", "author": "G Visani", "abstract": "Nowadays we are witnessing a transformation of the business processes towards a more computation driven approach. The ever increasing usage of Machine Learning techniques is the clearest example of such trend. This sort of revolution is often providing advantages, such as an increase in prediction accuracy and a reduced time to obtain the results. However, these methods present a major drawback: it is very difficult to understand on what grounds the algorithm took the decision. To address this issue we consider the LIME", "keywords": {"keywords": ["Machine Learning", "Explanations", "Statistical stability indices", "LIME", "Algorithm"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Interpretable Machine Learning"]}}, {"title": "Property inference for deep neural networks", "author": "D Gopinath"}, {"title": "Open the black box data-driven explanation of black box decision systems", "author": "D Pedreschi"}, {"title": "Answering questions about charts and generating visual explanations", "author": "DH Kim"}, {"title": "Intelligible artificial intelligence", "author": "DS Weld"}, {"title": "Robust and stable black box explanations", "author": "H Lakkaraju"}, {"title": "Assessing heuristic machine learning explanations with model counting", "author": "N Narodytska"}, {"title": "Explainability of vision-based autonomous driving systems: Review and challenges", "author": "\u00c9 Zablocki"}, {"title": "Benchmarking and survey of explanation methods for black box models", "author": "F Bodria"}, {"title": "Extracting relational explanations from deep neural networks: A survey from a neural-symbolic perspective", "author": "J Townsend"}, {"title": "Explain your move: Understanding agent actions using specific and relevant feature attribution", "author": "N Puri", "abstract": "As deep reinforcement learning (RL) is applied to more tasks, there is a need to visualize and understand the behavior of learned agents. Saliency maps explain agent behavior by highlighting the features of the input state that are most relevant for the agent in taking an action. Existing perturbation-based approaches to compute saliency often highlight regions of the input that are not relevant to the action taken by the agent. Our proposed approach, SARFA (Specific and Relevant Feature Attribution), generates more focused saliency maps", "keywords": {"keywords": ["deep reinforcement learning", "saliency maps", "agent behavior", "specific and relevant feature attribution", "visualization"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Artificial Intelligence"}}, {"title": "Three modern roles for logic in AI", "author": "A Darwiche"}, {"title": "A benchmark of machine learning approaches for credit score prediction", "author": "V Moscato"}, {"title": "On attribution of recurrent neural network predictions via additive decomposition", "author": "M Du"}, {"title": "Generating token-level explanations for natural language inference", "author": "J Thorne"}, {"title": "Self-explaining AI as an alternative to interpretable AI", "author": "DC Elton"}, {"title": "FAT Forensics: A Python toolbox for algorithmic fairness, accountability and transparency", "author": "K Sokol"}, {"title": "Model-agnostic interpretability with shapley values", "author": "A Messalas"}, {"title": "LIRME: locally interpretable ranking model explanation", "author": "M Verma"}, {"title": "Towards homochiral supramolecular entities from achiral molecules by vortex mixing-accompanied self-assembly", "author": "Y Sang"}, {"title": "Explaining the black-box model: A survey of local interpretation methods for deep neural networks", "author": "Y Liang", "abstract": "Recently, a significant amount of research has been investigated on interpretation of deep neural networks (DNNs) which are normally processed as black box models. Among the methods that have been developed, local interpretation methods stand out which have the features of clear expression in interpretation and low computation complexity. Different from existing surveys which cover a broad range of methods on interpretation of DNNs, this survey focuses on local interpretation methods with an in-depth analysis of the", "keywords": {"keywords": ["black-box model", "deep neural networks", "local interpretation methods", "clear expression", "computational complexity"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["deep learning"]}}, {"title": "Generative causal explanations of black-box classifiers", "author": "M O'Shaughnessy"}, {"title": "A study on the Interpretability of Neural Retrieval Models using DeepSHAP", "author": "ZT Fernando"}, {"title": "Declarative aspects in explicative data mining for computational sensemaking", "author": "M Atzmueller"}, {"title": "On tractable XAI queries based on compiled representations", "author": "G Audemard"}, {"title": "Relatif: Identifying explanatory training samples via relative influence", "author": "E Barshan"}, {"title": "Extractive adversarial networks: High-recall explanations for identifying personal attacks in social media posts", "author": "S Carton"}, {"title": "ProtoSteer: Steering deep sequence model with prototypes", "author": "Y Ming"}, {"title": "\" Why Should You Trust My Explanation?\" Understanding Uncertainty in LIME Explanations", "author": "Y Zhang"}, {"title": "Post-hoc explanation of black-box classifiers using confident itemsets", "author": "M Moradi"}, {"title": "On quantitative aspects of model interpretability", "author": "A Nguyen", "abstract": "Despite the growing body of work in interpretable machine learning, it remains unclear how to evaluate different explainability methods without resorting to qualitative assessment and user-studies. While interpretability is an inherently subjective matter, previous works in cognitive science and epistemology have shown that good explanations do possess aspects that can be objectively judged apart from fidelity), such assimplicity and broadness. In this paper we propose a set of metrics to programmatically evaluate interpretability methods", "keywords": {"keywords": ["interpretable machine learning", "quantitative evaluation", "model interpretability", "explanability methods", "evaluation metrics"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review", "author": "AM Antoniadi"}, {"title": "The explanation game: a formal framework for interpretable machine learning", "author": "DS Watson"}, {"title": "On counterfactual explanations under predictive multiplicity", "author": "M Pawelczyk"}, {"title": "On the privacy risks of model explanations", "author": "R Shokri"}, {"title": "On tractable representations of binary neural networks", "author": "W Shi"}, {"title": "Deep weighted averaging classifiers", "author": "D Card"}, {"title": "Feature interaction interpretability: A case for explaining ad-recommendation systems via neural interaction detection", "author": "M Tsang"}, {"title": "Why these explanations? Selecting intelligibility types for explanation goals.", "author": "BY Lim"}, {"title": "On the art and science of machine learning explanations", "author": "P Hall"}, {"title": "Can I trust the explainer? Verifying post-hoc explanatory methods", "author": "OM Camburu", "abstract": "We introduce a verification framework for explanatory methods under the feature-selection   We validate the efficacy of our evaluation by showing the failure modes of current explainers.", "keywords": {"keywords": ["verification", "explanatory methods", "feature-selection", "failure modes", "efficacy"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Notions of explainability and evaluation approaches for explainable artificial intelligence", "author": "G Vilone"}, {"title": "SurvLIME: A method for explaining machine learning survival models", "author": "MS Kovalev"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "Are we ready for learned cardinality estimation?", "author": "X Wang"}, {"title": "Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses", "author": "K Rawal"}, {"title": "Interpretable multiclass classification by MDL-based rule lists", "author": "HM Proen\u00e7a"}, {"title": "Optilime: Optimized lime explanations for diagnostic computer algorithms", "author": "G Visani"}, {"title": "Counterfactuals and causability in explainable artificial intelligence: Theory, algorithms, and applications", "author": "YL Chou"}, {"title": "Using ontologies to enhance human understandability of global post-hoc explanations of black-box models", "author": "R Confalonieri"}, {"title": "Glocalx-from local to global explanations of black box AI models", "author": "M Setzu", "abstract": "Artificial Intelligence (AI) has come to prominence as one of the major components of our society, with applications in most aspects of our lives. In this field, complex and highly nonlinear machine learning models such as ensemble models, deep neural networks, and Support Vector Machines have consistently shown remarkable accuracy in solving complex tasks. Although accurate, AI models often are \u201cblack boxes\u201d which we are not able to understand. Relying on these models has a multifaceted impact and raises significant", "keywords": {"keywords": ["Glocalx", "black box AI models", "ensemble models", "deep neural networks", "Support Vector Machines"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI model interpretation"]}}, {"title": "Generate your counterfactuals: Towards controlled counterfactual generation for text", "author": "N Madaan"}, {"title": "FIND: Human-in-the-loop debugging deep text classifiers", "author": "P Lertvittayakumjorn"}, {"title": "Enabling dynamic network access control with anomaly-based IDS and SDN", "author": "H Li"}, {"title": "Formal verification of Bayesian network classifiers", "author": "A Shih"}, {"title": "On validating, repairing and refining heuristic ML explanations", "author": "A Ignatiev"}, {"title": "Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond", "author": "X Li"}, {"title": "Explaining naive bayes and other linear classifiers with polynomial time and delay", "author": "J Marques-Silva"}, {"title": "TeleGam: Combining visualization and verbalization for interpretable machine learning", "author": "F Hohman"}, {"title": "Does explainable artificial intelligence improve human decision-making?", "author": "Y Alufaisan"}, {"title": "Machine learning explainability via microaggregation and shallow decision trees", "author": "A Blanco-Justicia", "abstract": "Artificial intelligence (AI) is being deployed in missions that are increasingly critical for human life. To build trust in AI and avoid an algorithm-based authoritarian society, automated decisions should be explainable. This is not only a right of citizens, enshrined for example in the European General Data Protection Regulation, but a desirable goal for engineers, who want to know whether the decision algorithms are capturing the relevant features. For explainability to be scalable, it should be possible to derive explanations in a", "keywords": {"keywords": ["machine learning", "explainability", "microaggregation", "shallow decision trees", "AI"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["technical"], "application area": ["artificial intelligence"]}}, {"title": "A user study on the effect of aggregating explanations for interpreting machine learning models", "author": "J Krause"}, {"title": "NormLime: A new feature importance metric for explaining deep neural networks", "author": "I Ahern"}, {"title": "Can we trust your explanations? Sanity checks for interpreters in Android malware analysis", "author": "M Fan"}, {"title": "{CADE}: Detecting and explaining concept drift samples for security applications", "author": "L Yang"}, {"title": "Explainable image classification with evidence counterfactual", "author": "T Vermeire"}, {"title": "Explainable ai meets healthcare: A study on heart disease dataset", "author": "D Dave"}, {"title": "Deep neural networks and tabular data: A survey", "author": "V Borisov"}, {"title": "Explainable artificial intelligence enhances the ecological interpretability of black\u2010box species distribution models", "author": "M Ryo"}, {"title": "Ask not what AI can do, but what AI should do: Towards a framework of task delegability", "author": "B Lubars"}, {"title": "Artificial cognition: How experimental psychology can help generate explainable artificial intelligence", "author": "JET Taylor", "abstract": "Artificial intelligence powered by deep neural networks has reached a level of complexity where it can be difficult or impossible to express how a model makes its decisions. This black-box problem is especially concerning when the model makes decisions with consequences for human well-being. In response, an emerging field called explainable artificial intelligence (XAI) aims to increase the interpretability, fairness, and transparency of machine learning. In this paper, we describe how cognitive psychologists can make", "keywords": {"keywords": ["Artificial cognition", "Experimental psychology", "Explainable artificial intelligence", "Deep neural networks", "Interpretability"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Machine learning"]}}, {"title": "Explaining black-box algorithms using probabilistic contrastive counterfactuals", "author": "S Galhotra"}, {"title": "Axiomatic interpretability for multiclass additive models", "author": "X Zhang"}, {"title": "A case-based ensemble learning system for explainable breast cancer recurrence prediction", "author": "D Gu"}, {"title": "Towards global explanations of convolutional neural networks with concept attribution", "author": "W Wu"}, {"title": "Relation-Based Counterfactual Explanations for Bayesian Network Classifiers.", "author": "E Albini"}, {"title": "Towards unifying feature attribution and counterfactual explanations: Different means to the same end", "author": "R Kommiya Mothilal"}, {"title": "Agnostic local explanation for time series classification", "author": "M Guillem\u00e9"}, {"title": "Concise fuzzy system modeling integrating soft subspace clustering and sparse learning", "author": "P Xu"}, {"title": "Trepan reloaded: A knowledge-driven approach to explaining artificial neural networks", "author": "R Confalonieri"}, {"title": "Interpretable machine learning with an ensemble of gradient boosting machines", "author": "AV Konstantinov", "abstract": "A method for the local and global interpretation of a black-box model on the basis of the well-known generalized additive models is proposed. It can be viewed as an extension or a modification of the algorithm using the neural additive model. The method is based on using an ensemble of gradient boosting machines (GBMs) such that each GBM is learned on a single feature and produces a shape function of the feature. The ensemble is composed as a weighted sum of separate GBMs resulting a weighted sum of shape functions which form", "keywords": {"keywords": ["interpretable machine learning", "gradient boosting machines", "ensemble", "interpretable models", "shape functions"], "research type": ["methodology"], "methodology": ["quantitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Approximating XGBoost with an interpretable decision tree", "author": "O Sagi"}, {"title": "Globally-consistent rule-based summary-explanations for machine learning models: application to credit-risk evaluation", "author": "C Rudin"}, {"title": "Evaluating local explanation methods on ground truth", "author": "R Guidotti"}, {"title": "Explaining black box models by means of local rules", "author": "E Pastor"}, {"title": "Evaluations and methods for explanation through robustness analysis", "author": "CY Hsieh"}, {"title": "Equalizing recourse across groups", "author": "V Gupta"}, {"title": "Explainable deep learning: A field guide for the uninitiated", "author": "G Ras"}, {"title": "Model agnostic interpretability of rankers via intent modelling", "author": "J Singh"}, {"title": "Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language?", "author": "P Hase"}, {"title": "CHIRPS: Explaining random forest classification", "author": "J Hatwell", "abstract": "Modern machine learning methods typically produce \u201cblack box\u201d models that are opaque to interpretation. Yet, their demand has been increasing in the Human-in-the-Loop processes, that is, those processes that require a human agent to verify, approve or reason about the automated decisions before they can be applied. To facilitate this interpretation, we propose Collection of High Importance Random Path Snippets (CHIRPS); a novel algorithm for explaining random forest classification per data instance. CHIRPS extracts a decision path", "keywords": {"keywords": ["random forest classification", "interpretability", "machine learning", "model explanation", "human-in-the-loop"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Interpretability in machine learning"]}}, {"title": "Do not trust additive explanations", "author": "A Gosiewska"}, {"title": "Explainer: entity resolution explanations", "author": "A Ebaid"}, {"title": "Automated data slicing for model validation: A big data-ai integration approach", "author": "Y Chung"}, {"title": "Competency problems: On finding and removing artifacts in language data", "author": "M Gardner"}, {"title": "Gradient-based analysis of NLP models is manipulable", "author": "J Wang"}, {"title": "What is interpretability?", "author": "A Erasmus"}, {"title": "The effect of heterogeneous data for Alzheimer's disease detection from speech", "author": "A Balagopalan"}, {"title": "Scenario-based requirements elicitation for user-centric explainable AI", "author": "D Cirqueira"}, {"title": "Relex: A model-agnostic relational model explainer", "author": "Y Zhang"}, {"title": "Data-empowered argumentation for dialectically explainable predictions", "author": "O Cocarascu", "abstract": "Today's AI landscape is permeated by plentiful data and dominated by powerful data-centric methods with the potential to impact a wide range of human sectors. Yet, in some settings this potential is hindered by these data-centric AI methods being mostly opaque. Considerable efforts are currently being devoted to defining methods for explaining black-box techniques in some settings, while the use of transparent methods is being advocated in others, especially when high-stake decisions are involved, as in healthcare and the practice", "keywords": {"keywords": ["data-empowered argumentation", "dialectically explainable predictions", "black-box techniques", "transparent methods", "high-stake decisions"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explainability"]}}, {"title": "Actionable recourse in linear classification", "author": "A Spangher"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "Compiling neural networks into tractable Boolean circuits", "author": "A Choi"}, {"title": "Reliable post hoc explanations: Modeling uncertainty in explainability", "author": "D Slack"}, {"title": "Coordinating Human and Machine Learning for Effective Organizational Learning.", "author": "T Sturm"}, {"title": "Practitioners' perceptions of the goals and visual explanations of defect prediction models", "author": "J Jiarpakdee"}, {"title": "A programmatic and semantic approach to explaining and debugging neural network based object detectors", "author": "E Kim"}, {"title": "FairLens: Auditing black-box clinical decision support systems", "author": "C Panigutti"}, {"title": "Explain your move: Understanding agent actions using focused feature saliency", "author": "P Gupta"}, {"title": "The computational complexity of understanding binary classifier decisions", "author": "S Waeldchen", "abstract": "For a d-ary Boolean function \u03a6:{0, 1} d\u2192{0, 1} and an assignment to its variables x=(x 1, x 2,..., xd) we consider the problem of finding those subsets of the variables that are sufficient to determine the function value with a given probability \u03b4. This is motivated by the task of interpreting predictions of binary classifiers described as Boolean circuits, which can be seen as special cases of neural networks. We show that the problem of deciding whether such subsets of relevant variables of limited size k\u2264 d exist is complete for the complexity", "keywords": {"keywords": ["Binary classifier", "Boolean function", "Computational complexity", "Interpretation", "Subset"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "Hybrid Predictive Models: When an Interpretable Model Collaborates with a Black-box Model.", "author": "T Wang"}, {"title": "Evaluating explainable Artifical intelligence\u2013What users really appreciate", "author": "M F\u00f6rster"}, {"title": "Fastif: Scalable influence functions for efficient model interpretation and debugging", "author": "H Guo"}, {"title": "Complaint-driven training data debugging for query 2.0", "author": "W Wu"}, {"title": "An information-theoretic approach to personalized explainable machine learning", "author": "A Jung"}, {"title": "A survey on understanding, visualizations, and explanation of deep neural networks", "author": "A Shahroudnejad"}, {"title": "Testing and verification of neural-network-based safety-critical control software: A systematic literature review", "author": "J Zhang"}, {"title": "Towards global explanations for credit risk scoring", "author": "I Unceta"}, {"title": "Improving the quality of explanations with local embedding perturbations", "author": "Y Jia"}, {"title": "Nationwide prediction of type 2 diabetes comorbidities", "author": "P Dworzynski", "abstract": "Identification of individuals at risk of developing disease comorbidities represents an important task in tackling the growing personal and societal burdens associated with chronic diseases. We employed machine learning techniques to investigate to what extent data from longitudinal, nationwide Danish health registers can be used to predict individuals at high risk of developing type 2 diabetes (T2D) comorbidities. Leveraging logistic regression-, random forest-and gradient boosting models and register data spanning hospitalizations", "keywords": {"keywords": ["type 2 diabetes", "comorbidities", "machine learning", "logistic regression", "nationwide Danish health registers"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Descriptive", "discipline": "Medical Sciences", "content": "Application", "application area": "Healthcare"}}, {"title": "On symbolically encoding the behavior of random forests", "author": "A Choi"}, {"title": "Computing optimal decision sets with SAT", "author": "J Yu"}, {"title": "A performance-explainability framework to benchmark machine learning methods: application to multivariate time series classifiers", "author": "K Fauvel"}, {"title": "Post-hoc interpretability for neural nlp: A survey", "author": "A Madsen"}, {"title": "Counterfactual state explanations for reinforcement learning agents via generative deep learning", "author": "ML Olson"}, {"title": "Review study of interpretation methods for future interpretable machine learning", "author": "JX Mi"}, {"title": "Carla: a python library to benchmark algorithmic recourse and counterfactual explanation algorithms", "author": "M Pawelczyk"}, {"title": "Regularizing black-box models for improved interpretability", "author": "G Plumb"}, {"title": "A challenge set and methods for noun-verb ambiguity", "author": "A Elkahky"}, {"title": "On explaining random forests with SAT", "author": "Y Izza", "abstract": "Random Forest (RFs) are among the most widely used Machine Learning (ML) classifiers. Even though RFs are not interpretable, there are no dedicated non-heuristic approaches for computing explanations of RFs. Moreover, there is recent work on polynomial algorithms for explaining ML models, including naive Bayes classifiers. Hence, one question is whether finding explanations of RFs can be solved in polynomial time. This paper answers this question negatively, by proving that computing one PI-explanation of an RF is D^ P", "keywords": {"keywords": ["random forests", "explanations", "SAT", "machine learning classifiers", "polynomial time"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Looking deeper into tabular LIME", "author": "D Garreau"}, {"title": "Modality specific U-Net variants for biomedical image segmentation: a survey", "author": "NS Punn"}, {"title": "Copying machine learning classifiers", "author": "I Unceta"}, {"title": "Human-aware planning revisited: A tale of three models", "author": "T Chakraborti"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "A machine learning damage prediction model for the 2017 Puebla-Morelos, Mexico, earthquake", "author": "S Roeslin"}, {"title": "Software vulnerability analysis and discovery using deep learning techniques: A survey", "author": "P Zeng"}, {"title": "Embedding deep networks into visual explanations", "author": "Z Qi"}, {"title": "Polyjuice: Automated, general-purpose counterfactual generation", "author": "T Wu"}, {"title": "Machine learning explainability through comprehensible decision trees", "author": "A Blanco-Justicia", "abstract": "The role of decisions made by machine learning algorithms in our lives is ever increasing. In reaction to this phenomenon, the European General Data Protection Regulation establishes that citizens have the right to receive an explanation on automated decisions affecting them. For explainability to be scalable, it should be possible to derive explanations in an automated way. A common approach is to use simpler, more intuitive decision algorithms to build a surrogate model of the black-box model (for example a deep learning algorithm)", "keywords": {"keywords": ["machine learning", "explainability", "decision trees", "automated explanations", "surrogate model"], "research type": ["theoretical"], "methodology": ["quantitative"], "purpose": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["information technology"]}}, {"title": "Step-wise multi-grained augmented gradient boosting decision trees for credit scoring", "author": "W Liu"}, {"title": "How can I choose an explainer? An application-grounded evaluation of post-hoc explanations", "author": "S Jesus"}, {"title": "From contrastive to abductive explanations and back again", "author": "A Ignatiev"}, {"title": "A robust algorithm for explaining unreliable machine learning survival models using the Kolmogorov\u2013Smirnov bounds", "author": "MS Kovalev"}, {"title": "Entropy-based logic explanations of neural networks", "author": "P Barbiero"}, {"title": "Active anomaly detection via ensembles: Insights, algorithms, and interpretability", "author": "S Das"}, {"title": "Explaining sentiment classification with synthetic exemplars and counter-exemplars", "author": "O Lampridis"}, {"title": "Explainable ai for interpretable credit scoring", "author": "LM Demajo"}, {"title": "Measure utility, gain trust: practical advice for XAI researchers", "author": "B Davis"}, {"title": "LoRMIkA: Local rule-based model interpretability with K-optimal associations", "author": "D Rajapaksha", "abstract": "As we rely more and more on machine learning models for real-life decision-making, being able to understand and trust the predictions becomes ever more important. Local explainer models have recently been introduced to explain the predictions of complex machine learning models at the instance level. In this paper, we propose Local Rule-based Model Interpretability with k-optimal Associations (LoRMIkA), a novel model-agnostic approach that obtains k-optimal association rules from a neighbourhood of the instance to be explained", "keywords": {"keywords": ["local explainer models", "machine learning models", "model interpretability", "association rules", "k-optimal associations"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Harnessing machine intelligence in automatic echocardiogram analysis: Current status, limitations, and future directions", "author": "G Zamzmi"}, {"title": "Astraea: Grammar-based fairness testing", "author": "E Soremekun"}, {"title": "A model-agnostic approach to quantifying the informativeness of explanation methods for time series classification", "author": "TT Nguyen"}, {"title": "Mononet: towards interpretable models by learning monotonic features", "author": "A Nguyen"}, {"title": "White, Grey, Black: Effects of XAI Augmentation on the Confidence in AI-based Decision Support Systems.", "author": "J Wanner"}, {"title": "LINDA-BN: An interpretable probabilistic approach for demystifying black-box predictive models", "author": "C Moreira"}, {"title": "Explanations for Monotonic Classifiers.", "author": "J Marques-Silva"}, {"title": "SQAPlanner: Generating data-informed software quality improvement plans", "author": "D Rajapaksha"}, {"title": "The natural language explanation algorithms for the lung cancer computer-aided diagnosis system", "author": "A Meldo"}, {"title": "Xcm: An explainable convolutional neural network for multivariate time series classification", "author": "K Fauvel", "abstract": "Multivariate Time Series (MTS) classification has gained importance over the past decade with the increase in the number of temporal datasets in multiple domains. The current state-of-the-art MTS classifier is a heavyweight deep learning approach, which outperforms the second-best MTS classifier only on large datasets. Moreover, this deep learning approach cannot provide faithful explanations as it relies on post hoc model-agnostic explainability methods, which could prevent its use in numerous applications. In this paper, we present", "keywords": [["explainability", 0.4446], ["explanations", 0.2809], ["classifier", 0.2614], ["classification", 0.2534], ["datasets", 0.243], ["multivariate", 0.2231], ["temporal", 0.2006], ["learning", 0.1858], ["deep", 0.1802], ["time", 0.1747]]}, {"title": "The best way to select features? comparing mda, lime, and shap", "author": "X Man"}, {"title": "Explaining and improving model behavior with k nearest neighbor representations", "author": "NF Rajani"}, {"title": "Middle-level features for the explanation of classification systems by sparse dictionary methods", "author": "A Apicella"}, {"title": "Trustworthy ai", "author": "R Chatila"}, {"title": "Learning interpretable shapelets for time series classification through adversarial regularization", "author": "Y Wang"}, {"title": "Deep learning for android malware defenses: a systematic literature review", "author": "Y Liu"}, {"title": "Interpretable machine learning for covid-19: An empirical study on severity prediction task", "author": "H Wu"}, {"title": "Knowledge graphs as tools for explainable machine learning: A survey", "author": "I Tiddi"}, {"title": "Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline", "author": "S Biswas"}, {"title": "Learning propagation rules for attribution map generation", "author": "Y Yang", "abstract": "Prior gradient-based attribution-map methods rely on hand-crafted propagation rules for the non-linear/activation layers during the backward pass, so as to produce gradients of the input and then the attribution map. Despite the promising results achieved, such methods are sensitive to the non-informative high-frequency components and lack adaptability for various models and samples. In this paper, we propose a dedicated method to generate attribution maps that allow us to learn the propagation rules automatically, overcoming the", "keywords": {"keywords": ["attribution maps", "propagation rules", "gradient-based", "non-linear layers", "adaptability"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer science", "content": "Methodological", "application area": "Machine learning"}}, {"title": "Explainable AI in industry: Practical challenges and lessons learned", "author": "K Gade"}, {"title": "A comparative evaluation of unsupervised deep architectures for intrusion detection in sequential data streams", "author": "D Sovilj"}, {"title": "Lionets: Local interpretation of neural networks through penultimate layer decoding", "author": "I Mollas"}, {"title": "Interpreting predictions of NLP models", "author": "E Wallace"}, {"title": "Softmax-based classification is k-means clustering: Formal proof, consequences for adversarial attacks, and improvement through centroid based tailoring", "author": "S Hess"}, {"title": "xgail: Explainable generative adversarial imitation learning for explainable human decision analysis", "author": "M Pan"}, {"title": "Ada-WHIPS: explaining AdaBoost classification with applications in the health sciences", "author": "J Hatwell"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition", "author": "M Nourani"}, {"title": "The struggles of feature-based explanations: Shapley values vs. minimal sufficient subsets", "author": "OM Camburu", "abstract": "For neural models to garner widespread public trust and ensure fairness, we must have human-intelligible explanations for their predictions. Recently, an increasing number of works focus on explaining the predictions of neural models in terms of the relevance of the input features. In this work, we show that feature-based explanations pose problems even for explaining trivial models. We show that, in certain cases, there exist at least two ground-truth feature-based explanations, and that, sometimes, neither of them is enough to provide", "keywords": {"keywords": ["explanations", "neural models", "feature-based", "Shapley values", "minimal sufficient subsets"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["model interpretability"]}}, {"title": "Psychological foundations of explainability and interpretability in artificial intelligence", "author": "DA Broniatowski"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Vine: visualizing statistical interactions in black box models", "author": "M Britton"}, {"title": "ILDC for CJPE: Indian legal documents corpus for court judgment prediction and explanation", "author": "V Malik"}, {"title": "Peeking inside the black-box: Explainable machine learning applied to household transportation energy consumption", "author": "SS Amiri"}, {"title": "An ontology-based approach to explaining artificial neural networks", "author": "R Confalonieri"}, {"title": "Where and when to look? Spatial-temporal attention for action recognition in videos", "author": "L Meng"}, {"title": "Explaining Entity Resolution Predictions: Where are we and What needs to be done?", "author": "S Thirumuruganathan"}, {"title": "Exathlon: A benchmark for explainable anomaly detection over time series", "author": "V Jacob"}, {"title": "Facilitator training program: The universit\u00e9 laval interprofessional initiative", "author": "\u00c9 Milot", "abstract": "A facilitator training program (FTP) for interprofessional learning (IPL) facilitators has been developed at Universit\u00e9 Laval. This article describes the impacts of this program as perceived by the 22 IPL facilitators involved and outlines recommendations. Two qualitative data collection strategies were used to document the facilitators' pedagogical needs and views of the program's impacts. Results suggest that the FTP's pedagogical approach was effective. The IPL facilitators became more aware of their challenges and", "keywords": {"keywords": ["Facilitator training program", "Interprofessional learning", "IPL facilitators", "Pedagogical approach", "Challenges"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": [], "content": ["Application"], "application area": ["Education"]}}, {"title": "Commonsense justification for action explanation", "author": "S Yang"}, {"title": "What makes a good explanation? Cognitive dimensions of explaining intelligent machines.", "author": "R Confalonieri"}, {"title": "Regularizing black-box models for improved interpretability", "author": "G Plumb"}, {"title": "Why should i trust this item? explaining the recommendations of any model", "author": "C Lonjarret"}, {"title": "Connecting interpretability and robustness in decision trees through separation", "author": "M Moshkovitz"}, {"title": "On efficiently explaining graph-based classifiers", "author": "X Huang"}, {"title": "Toward personalized XAI: A case study in intelligent tutoring systems", "author": "C Conati"}, {"title": "EXplainable Neural-Symbolic Learning (X-NeSyL) methodology to fuse deep learning representations with expert knowledge graphs: The MonuMAI cultural \u2026", "author": "N D\u00edaz-Rodr\u00edguez"}, {"title": "Interpretable companions for black-box models", "author": "D Pan"}, {"title": "Deep learning interpretation: Flip points and homotopy methods", "author": "R Yousefzadeh", "abstract": "Deep learning models are complicated mathematical functions, and their interpretation remains a challenging research question. We formulate and solve optimization problems to answer questions about the models and their outputs. Specifically, we develop methods to study the decision boundaries of classification models using {\\em flip points}. A flip point is any point that lies on the boundary between two output classes: eg for a neural network with a binary yes/no output, a flip point is any input that generates equal scores for \u201cyes\u201d and \u201cno\u201d", "keywords": {"keywords": ["deep learning interpretation", "flip points", "homotopy methods", "decision boundaries", "classification models"], "research type": ["theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Leveraging latent features for local explanations", "author": "R Luss"}, {"title": "Deterministic local interpretable model-agnostic explanations for stable explainability", "author": "MR Zafar"}, {"title": "SAT-based rigorous explanations for decision lists", "author": "A Ignatiev"}, {"title": "Supporting deep neural network safety analysis and retraining through heatmap-based unsupervised learning", "author": "H Fahmy"}, {"title": "From context mediation to declarative values and explainability", "author": "GJ Nalepa"}, {"title": "Modeling paths for explainable knowledge base completion", "author": "J Stadelmaier"}, {"title": "Model explanations with differential privacy", "author": "N Patel"}, {"title": "Recent advances in trustworthy explainable artificial intelligence: Status, challenges and perspectives", "author": "A Rawal"}, {"title": "Evaluation for weakly supervised object localization: Protocol, metrics, and datasets", "author": "J Choe"}, {"title": "Looking for trouble: Analyzing classifier behavior via pattern divergence", "author": "E Pastor", "abstract": "Machine learning models may perform differently on different data subgroups, which we represent as itemsets (ie, conjunctions of simple predicates). The identification of these critical data subgroups plays an important role in many applications, for example model validation and testing, or evaluation of model fairness. Typically, domain expert help is required to identify relevant (or sensitive) subgroups.", "keywords": {"keywords": ["classifier behavior", "pattern divergence", "data subgroups", "itemsets", "model validation"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Machine learning"]}}, {"title": "On Relating'Why?'and'Why Not?'Explanations", "author": "A Ignatiev"}, {"title": "Monitoring and explainability of models in production", "author": "J Klaise"}, {"title": "Question-driven design process for explainable ai user experiences", "author": "QV Liao"}, {"title": "Counterfactual explanation of machine learning survival models", "author": "M Kovalev"}, {"title": "ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision", "author": "R ElShawi"}, {"title": "Interpretable machine learning for genomics", "author": "DS Watson"}, {"title": "Mining learning management system data using interpretable neural networks", "author": "M Matetic"}, {"title": "Interpreting neural ranking models using grad-cam", "author": "J Choi"}, {"title": "Explainable AI based interventions for pre-season decision making in fashion retail", "author": "S Sajja"}, {"title": "Dissecting span identification tasks with performance prediction", "author": "S Papay", "abstract": "Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks' properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are:(a) we identify key properties", "keywords": {"keywords": ["span identification", "performance prediction", "neural architectures", "NLP", "model families"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["Natural Language Processing"]}}, {"title": "Model distillation for revenue optimization: Interpretable personalized pricing", "author": "M Biggs"}, {"title": "On guaranteed optimal robust explanations for NLP models", "author": "E La Malfa"}, {"title": "LimeOut: an ensemble approach to improve process fairness", "author": "V Bhargava"}, {"title": "Exploiting patterns to explain individual predictions", "author": "Y Jia"}, {"title": "Explaining groups of points in low-dimensional representations", "author": "G Plumb"}, {"title": "Rule extraction in unsupervised anomaly detection for model explainability: Application to OneClass SVM", "author": "A Barbado"}, {"title": "CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models", "author": "AR Akula"}, {"title": "VAE-LIME: deep generative model based approach for local data-driven model interpretability applied to the ironmaking industry", "author": "C Schockaert"}, {"title": "Provably efficient, succinct, and precise explanations", "author": "G Blanc"}, {"title": "ki67 nuclei detection and ki67-index estimation: a novel automatic approach based on human vision modeling", "author": "BR Barricelli", "abstract": "The protein ki67 (pki67) is a marker of tumor aggressiveness, and its expression has been proven to be useful in the prognostic and predictive evaluation of several types of tumors. To numerically quantify the pki67 presence in cancerous tissue areas, pathologists generally analyze histochemical images to count the number of tumor nuclei marked for pki67. This allows estimating the ki67-index, that is the percentage of tumor nuclei positive for pki67 over all the tumor nuclei. Given the high image resolution and dimensions, its estimation by", "keywords": {"keywords": ["ki67", "nuclei detection", "ki67-index estimation", "human vision modeling", "tumor aggressiveness"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Biology", "content": "Methodological", "application area": "Biomedical research"}}, {"title": "Local explanations via necessity and sufficiency: Unifying theory and practice", "author": "DS Watson"}, {"title": "Understanding learned models by identifying important features at the right resolution", "author": "K Lee"}, {"title": "Taxonomy and survey of interpretable machine learning method", "author": "S Das"}, {"title": "Bridging ai developers and end users: an end-user-centred explainable ai taxonomy and visual vocabularies", "author": "W Jin"}, {"title": "Using MaxSAT for efficient explanations of tree ensembles", "author": "A Ignatiev"}, {"title": "Ordered counterfactual explanation by mixed-integer linear optimization", "author": "K Kanamori"}, {"title": "LionForests: local interpretation of random forests", "author": "I Mollas"}, {"title": "Calibrating human-ai collaboration: Impact of risk, ambiguity and transparency on algorithmic bias", "author": "P Schmidt"}, {"title": "Machine Learning: The Basics", "author": "A Jung"}, {"title": "The out-of-distribution problem in explainability and search methods for feature importance explanations", "author": "P Hase", "abstract": "Feature importance (FI) estimates are a popular form of explanation, and they are commonly created and evaluated by computing the change in model confidence caused by removing certain input features at test time. For example, in the standard Sufficiency metric, only the top-k most important tokens are kept. In this paper, we study several under-explored dimensions of FI explanations, providing conceptual and empirical improvements for this form of explanation. First, we advance a new argument for why it can be problematic to", "keywords": {"keywords": ["feature importance", "explanation", "out-of-distribution problem", "search methods", "FI explanations"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["explainability methods"]}}, {"title": "Interpretable machine learning tools: a survey", "author": "N Agarwal"}, {"title": "Cross-validation is insufficient for model validation", "author": "TL Keevers"}, {"title": "Sparse robust regression for explaining classifiers", "author": "A Bj\u00f6rklund"}, {"title": "A survey of visual analytics for Explainable Artificial Intelligence methods", "author": "G Alicioglu"}, {"title": "Zorro: Valid, sparse, and stable explanations in graph neural networks", "author": "T Funke"}, {"title": "A holistic approach to interpretability in financial lending: Models, visualizations, and summary-explanations", "author": "C Chen"}, {"title": "Defect reduction planning (using timeLIME)", "author": "K Peng"}, {"title": "Undersensitivity in neural reading comprehension", "author": "J Welbl"}, {"title": "Foundations of explanations as model reconciliation", "author": "S Sreedharan"}, {"title": "Pydl8. 5: a library for learning optimal decision trees", "author": "G Aglin", "abstract": "Decision Trees (DTs) are widely used Machine Learning (ML) models with a broad range of applications. The interest in these models has increased even further in the context of Explainable AI (XAI), as decision trees of limited depth are very interpretable models. However, traditional algorithms for learning DTs are heuristic in nature; they may produce trees that are of suboptimal quality under depth constraints. We introduce PyDL8. 5, a Python library to infer depth-constrained Optimal Decision Trees (ODTs). PyDL8. 5 provides", "keywords": {"keywords": ["Machine Learning", "Decision Trees", "Explainable AI", "Optimal Decision Trees", "Python library"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "M2Lens: visualizing and explaining multimodal models for sentiment analysis", "author": "X Wang"}, {"title": "The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective", "author": "S Krishna"}, {"title": "Adversarial attacks on text classification models using layer\u2010wise relevance propagation", "author": "J Xu"}, {"title": "RankViz: A visualization framework to assist interpretation of Learning to Rank algorithms", "author": "MM Pereira"}, {"title": "Logic explained networks", "author": "G Ciravegna"}, {"title": "Explainability methods for identifying root-cause of sla violation prediction in 5G network", "author": "A Terra"}, {"title": "Link prediction using graph neural networks for master data management", "author": "B Ganesan"}, {"title": "Explanations within conversational recommendation systems: improving coverage through knowledge graph embedding", "author": "GP Polleti"}, {"title": "Sparse oblique decision trees: A tool to understand and manipulate neural net features", "author": "SS Hada"}, {"title": "Directive explanations for actionable explainability in machine learning applications", "author": "R Singh", "abstract": "actionable explanations in the form of directive explanations. We define directive explanations  as explanations that give individuals directives for recourse of machine learning decisions", "keywords": {"keywords": ["actionable explainability", "directive explanations", "machine learning applications", "recourse", "machine learning decisions"], "research type": ["theoretical"], "methodology": ["N/A"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Inferring disease subtypes from clusters in explanation space", "author": "MA Schulz"}, {"title": "Understanding the interpretability of search result summaries", "author": "S Mi"}, {"title": "How to (better) find a perpetrator in a haystack", "author": "Y Neuman"}, {"title": "A survey of methods and tools used for interpreting random forest", "author": "M HADDOUCHI"}, {"title": "Deepview: Visualizing classification boundaries of deep neural networks as scatter plots using discriminative dimensionality reduction", "author": "A Schulz"}, {"title": "Tractable explanations for d-DNNF classifiers", "author": "X Huang"}, {"title": "A multi-layered approach for tailored black-box explanations", "author": "C Henin"}, {"title": "Probabilistic sufficient explanations", "author": "E Wang"}, {"title": "HeTROPY: Explainable learning diagnostics via heterogeneous maximum-entropy and multi-spatial knowledge representation", "author": "Y Huo"}, {"title": "Environmental adaptation and differential replication in machine learning", "author": "I Unceta", "abstract": "When deployed in the wild, machine learning models are usually confronted with an environment that imposes severe constraints. As this environment evolves, so do these constraints. As a result, the feasible set of solutions for the considered need is prone to change in time. We refer to this problem as that of environmental adaptation. In this paper, we formalize environmental adaptation and discuss how it differs from other problems in the literature. We propose solutions based on differential replication, a technique where the", "keywords": {"keywords": ["environmental adaptation", "differential replication", "machine learning", "constraints", "feasible set"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": "machine learning"}}, {"title": "Towards Interpretable Neural Networks: An Exact Transformation to Multi-Class Multivariate Decision Trees", "author": "DT Nguyen"}, {"title": "Interpretable machine learning: Moving from mythos to diagnostics", "author": "V Chen"}, {"title": "Towards the integration of a post-hoc interpretation step into the machine learning workflow for IoT botnet detection", "author": "A Guerra-Manzanares"}, {"title": "CBR-LIME: a case-based reasoning approach to provide specific local interpretable model-agnostic explanations", "author": "JA Recio-Garc\u00eda"}, {"title": "Evaluating the correctness of explainable AI algorithms for classification", "author": "O Yalcin"}, {"title": "XAlgo: A design probe of explaining Algorithms' internal states via question-answering", "author": "J Rebanal"}, {"title": "Towards explainable CNNs for Android malware detection", "author": "M Kinkead"}, {"title": "Keep calm and improve visual feature attribution", "author": "JM Kim"}, {"title": "From shallow to deep interactions between knowledge representation, reasoning and machine learning", "author": "KR Amel"}, {"title": "Interpreting neural networks using flip points", "author": "R Yousefzadeh", "abstract": "Neural networks have been criticized for their lack of easy interpretation, which undermines confidence in their use for important applications. Here, we introduce a novel technique, interpreting a trained neural network by investigating its flip points. A flip point is any point that lies on the boundary between two output classes: eg for a neural network with a binary yes/no output, a flip point is any input that generates equal scores for\" yes\" and\" no\". The flip point closest to a given input is of particular importance, and this point is the solution to a", "keywords": {"keywords": ["neural networks", "interpretation", "flip points", "boundary", "output classes"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Assessing and comparing interpretability techniques for artificial neural networks breast cancer classification", "author": "H Hakkoum"}, {"title": "Explaining clinical decision support systems in medical imaging using cycle-consistent activation maximization", "author": "A Katzmann"}, {"title": "Lightly-supervised representation learning with global interpretability", "author": "A Zupon"}, {"title": "Towards a generic framework for black-box explanations of algorithmic decision systems", "author": "C Henin"}, {"title": "Opportunities for Explainable Artificial Intelligence in Aerospace Predictive Maintenance", "author": "B Shukla"}, {"title": "Is my model using the right evidence? systematic probes for examining evidence-based tabular reasoning", "author": "V Gupta"}, {"title": "Local interpretations for explainable natural language processing: A survey", "author": "S Luo"}, {"title": "Experimental study on generating multi-modal explanations of black-box classifiers in terms of gray-box classifiers", "author": "JM Alonso"}, {"title": "Explainable deep learning for personalized age prediction with brain morphology", "author": "A Lombardi"}, {"title": "Case-based reasoning for assisting domain experts in processing fraud alerts of black-box machine learning models", "author": "HJP Weerts", "abstract": "In many contexts, it can be useful for domain experts to understand to what extent predictions made by a machine learning model can be trusted. In particular, estimates of trustworthiness can be useful for fraud analysts who process machine learning-generated alerts of fraudulent transactions. In this work, we present a case-based reasoning (CBR) approach that provides evidence on the trustworthiness of a prediction in the form of a visualization of similar previous instances. Different from previous works, we consider", "keywords": {"keywords": ["case-based reasoning", "trustworthiness", "fraud alerts", "black-box machine learning models", "visualization"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Fraud detection"]}}, {"title": "Explaining deep learning-based driver models", "author": "MPS Lorente"}, {"title": "Towards Socio-Technical Design of Explicative Systems: Transparent, Interpretable and Explainable Analytics and Its Perspectives in Social Interaction \u2026", "author": "M Atzmueller"}, {"title": "Explainable machine learning for public policy: Use cases, gaps, and research directions", "author": "K Amarasinghe"}, {"title": "EMAP: Explanation by minimal adversarial perturbation", "author": "M Chapman-Rounds"}, {"title": "Layerwise knowledge extraction from deep convolutional networks", "author": "S Odense"}, {"title": "Interpreting machine learning models and application of homotopy methods", "author": "R Yousefzadeh"}, {"title": "Near-Optimal Explainable k-Means for All Dimensions", "author": "M Charikar"}, {"title": "Sequential explanations with mental model-based policies", "author": "A Yeung"}, {"title": "FIMAP: feature importance by minimal adversarial perturbation", "author": "M Chapman-Rounds"}, {"title": "Grasp: A library for extracting and exploring human-interpretable textual patterns", "author": "P Lertvittayakumjorn", "abstract": "Data exploration is an important step of every data science and machine learning project, including those involving textual data. We provide a Python library for GrASP, an existing algorithm for drawing patterns from textual data. The library is equipped with a web-based interface empowering human users to conveniently explore the data and the extracted patterns. We also demonstrate the use of the library in two settings (spam detection and argument mining) and discuss future deployments of the library, eg, beyond textual data", "keywords": {"keywords": ["textual data", "data exploration", "GrASP", "Python library", "human-interpretable textual patterns"], "research type": ["case study"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["text mining"]}}, {"title": "Surrogate Decision Tree Visualization.", "author": "F Di Castro"}, {"title": "From shallow to deep interactions between knowledge representation, reasoning and machine learning (Kay R. Amel group)", "author": "Z Bouraoui"}, {"title": "Data augmentation for fairness in personal knowledge base population", "author": "LS Vannur"}, {"title": "EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence", "author": "W Jin"}, {"title": "Regularizing conjunctive features for classification", "author": "P Barcel\u00f3"}, {"title": "Delivering Trustworthy AI through formal XAI", "author": "J Marques-Silva"}, {"title": "Ontology-based interpretable machine learning for textual data", "author": "P Lai"}, {"title": "Contextual local explanation for black box classifiers", "author": "Z Zhang"}, {"title": "SurvLIME-Inf: A simplified modification of SurvLIME for explanation of machine learning survival models", "author": "LV Utkin"}, {"title": "A framework to learn with interpretation", "author": "J Parekh", "abstract": "To tackle interpretability in deep learning, we present a novel framework to jointly learn a predictive model and its associated interpretation model. The interpreter provides both local and global interpretability about the predictive model in terms of human-understandable high level attribute functions, with minimal loss of accuracy. This is achieved by a dedicated architecture and well chosen regularization penalties. We seek for a small-size dictionary of high level attribute functions that take as inputs the outputs of selected hidden layers and", "keywords": {"keywords": ["interpretability", "deep learning", "predictive model", "interpretation model", "high level attribute functions"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Explainable artificial intelligence for tabular data: A survey", "author": "M Sahakyan"}, {"title": "Toward learning human-aligned cross-domain robust models by countering misaligned features", "author": "H Wang"}, {"title": "Efficient explanations for knowledge compilation languages", "author": "X Huang"}, {"title": "An investigation of covid-19 spreading factors with explainable ai techniques", "author": "X Fan"}, {"title": "Evaluation of cnn performance in semantically relevant latent spaces", "author": "J Doorenmalen"}, {"title": "A game-theoretic taxonomy of visual concepts in dnns", "author": "X Cheng"}, {"title": "Artificial Intelligence in economic decision making: how to assure a trust?", "author": "S Bejger"}, {"title": "Altruist: argumentative explanations through local interpretations of predictive models", "author": "I Mollas"}, {"title": "Data science: a game changer for science and innovation", "author": "V Grossi"}, {"title": "Certification of embedded systems based on Machine Learning: A survey", "author": "G Vidot", "abstract": "Advances in machine learning (ML) open the way to innovating functions in the avionic domain, such as navigation/surveillance assistance (eg vision-based navigation, obstacle sensing, virtual sensing), speechto-text applications, autonomous flight, predictive maintenance or cockpit assistance. Current certification standards and practices, which were defined and refined decades over decades with classical programming in mind, do not however support this new development paradigm. This article provides an overview of the", "keywords": {"keywords": ["Certification", "Embedded systems", "Machine Learning", "Avionic domain", "Certification standards"], "research type": ["Review"], "methodology": [], "purposes": ["Descriptive"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Aviation"]}}, {"title": "EXPLAN: explaining black-box classifiers using adaptive neighborhood generation", "author": "P Rasouli"}, {"title": "Decoupled feature-temporal CNN: Explaining deep learning-based machine health monitoring", "author": "C Zhu"}, {"title": "A comparison of explanations given by explainable artificial intelligence methods on analysing electronic health records", "author": "J Duell"}, {"title": "Predicting severe clinical events by learning about life-saving actions and outcomes using distant supervision", "author": "DH Lee"}, {"title": "Fair and adequate explanations", "author": "N Asher"}, {"title": "Text backdoor detection using an interpretable rnn abstract model", "author": "M Fan"}, {"title": "A United States fair lending perspective on machine learning", "author": "P Hall"}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang"}, {"title": "A modified LIME and its application to explain service supply chain forecasting", "author": "H Li"}, {"title": "Model-based counterfactual synthesizer for interpretation", "author": "F Yang", "abstract": "Counterfactuals, serving as one of the emerging type of model interpretations, have recently received attention from both researchers and practitioners. Counterfactual explanations formalize the exploration of\" what-if''scenarios, and are an instance of example-based reasoning using a set of hypothetical data samples. Counterfactuals essentially show how the model decision alters with input perturbations. Existing methods for generating counterfactuals are mainly algorithm-based, which are time-inefficient and assume the same", "keywords": {"keywords": ["counterfactuals", "model-based", "interpretation", "algorithm-based", "input perturbations"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Interpretability"]}}, {"title": "Efficiency of hybrid algorithms for estimating the shear strength of deep reinforced concrete beams", "author": "MS Barkhordari"}, {"title": "Synthetic benchmarks for scientific research in explainable machine learning", "author": "Y Liu"}, {"title": "Efficient explanations with relevant sets", "author": "Y Izza"}, {"title": "Interpreting neural network judgments via minimal, stable, and symbolic corrections", "author": "A Solar Lezama"}, {"title": "Lionforests: Local interpretation of random forests through path selection", "author": "I Mollas"}, {"title": "Exact and consistent interpretation of piecewise linear models hidden behind APIs: A closed form solution", "author": "Z Cong"}, {"title": "Improving the interpretability of neural sentiment classifiers via data augmentation", "author": "H Chen"}, {"title": "Learning to explain: Generating stable explanations fast", "author": "X Situ"}, {"title": "gbt-hips: Explaining the classifications of gradient boosted tree ensembles", "author": "J Hatwell"}, {"title": "Towards big industrial data mining through explainable automated machine learning", "author": "M Garouani", "abstract": "Industrial systems resources are capable of producing large amount of data. These data are often in heterogeneous formats and distributed, yet they provide means to mine the information which can allow the deployment of intelligent management tools for production activities. For this purpose, it is necessary to be able to implement knowledge extraction and prediction processes using Artificial Intelligence (AI) models, but the selection and configuration of intended AI models tend to be increasingly complex for a non-expert user. In", "keywords": {"keywords": ["industrial data mining", "explainable automated machine learning", "heterogeneous data", "knowledge extraction", "Artificial Intelligence"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["production management"]}}, {"title": "Practical machine learning safety: A survey and primer", "author": "S Mohseni"}, {"title": "Gaining Justified Human Trust by Improving Explainability in Vision and Language Reasoning Models", "author": "AR Akula"}, {"title": "On the logical design of a prototypical data lake system for biological resources", "author": "H Che"}, {"title": "Anomaly detection and explanation discovery on event streams", "author": "F Song"}, {"title": "CASTLE: Cluster-aided space transformation for local explanations", "author": "V La Gatta"}, {"title": "A generic framework for black-box explanations", "author": "C Henin"}, {"title": "Utilizing a transparency-driven environment toward trusted automatic genre classification: A case study in journalism history", "author": "A Bilgin"}, {"title": "RL-LIM: Reinforcement learning-based Locally interpretable modeling", "author": "J Yoon"}, {"title": "Conceptual challenges for interpretable machine learning", "author": "DS Watson"}, {"title": "ASTERYX: A model-Agnostic SaT-basEd appRoach for sYmbolic and score-based eXplanations", "author": "R Boumazouza", "abstract": "The ever increasing complexity of machine learning techniques used more and more in practice, gives rise to the need to explain the outcomes of these models, often used as black-boxes. Explainable AI approaches are either numerical feature-based aiming to quantify the contribution of each feature in a prediction or symbolic providing certain forms of symbolic explanations such ascounterfactuals. This paper proposes a generic agnostic approach named ASTERYX allowing to generate both symbolic explanations and score-based ones", "keywords": {"keywords": ["Explainable AI", "Machine Learning", "Symbolic explanations", "Score-based explanations", "ASTERYX"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning Interpretability"]}}, {"title": "Preliminary study on interpreting stock price forecasting based on tree regularization of GRU", "author": "W Wu"}, {"title": "Generalizability of an automatic explanation method for machine learning prediction results on asthma-related hospital visits in patients with asthma \u2026", "author": "G Luo"}, {"title": "Supporting decision making in engineering design using parallel coordinates in virtual reality", "author": "SK Tadeja"}, {"title": "Classification of explainable artificial intelligence methods through their output formats", "author": "G Vilone"}, {"title": "Interpreting deep learning models in natural language processing: A review", "author": "X Sun"}, {"title": "Xdeep: An interpretation tool for deep neural networks", "author": "F Yang"}, {"title": "CaSE: Explaining text classifications by fusion of local surrogate explanation models with contextual and semantic knowledge", "author": "S Kiefer"}, {"title": "XAI for Operations in the Process Industry-Applications, Theses, and Research Directions.", "author": "A Kotriwala"}, {"title": "A general approach to compute the relevance of middle-level input features", "author": "A Apicella"}, {"title": "User Driven Model Adjustment via Boolean Rule Explanations", "author": "EM Daly", "abstract": "AI solutions are heavily dependant on the quality and accuracy of the input training data, however the training data may not always fully reflect the most up-to-date policy landscape or may be missing business logic. The advances in explainability have opened the possibility of allowing users to interact with interpretable explanations of ML predictions in order to inject modifications or constraints that more accurately reflect current realities of the system. In this paper, we present a solution which leverages the predictive power of ML", "keywords": {"keywords": ["User Driven Model Adjustment", "Boolean Rule Explanations", "AI solutions", "Training data", "Explainability"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Machine learning"]}}, {"title": "Auditing and debugging deep learning models via decision boundaries: Individual-level and group-level analysis", "author": "R Yousefzadeh"}, {"title": "Continuous support for rehabilitation using machine learning", "author": "P Philipp"}, {"title": "Explainable anomaly detection on high-dimensional time series data", "author": "B Rad"}, {"title": "An investigation of interpretability techniques for deep learning in predictive process analytics", "author": "C Moreira"}, {"title": "A case study of efficacy and challenges in practical human-in-loop evaluation of nlp systems using checklist", "author": "S Bhatt"}, {"title": "Introducing uncertainty into explainable ai methods", "author": "S Bobek"}, {"title": "RAPID: Rapid and precise interpretable decision sets", "author": "S Dhamnani"}, {"title": "Interpretability with accurate small models", "author": "A Ghose"}, {"title": "Towards a rigorous theoretical analysis and evaluation of gnn explanations", "author": "C Agarwal"}, {"title": "MonoForest framework for tree ensemble analysis", "author": "I Kuralenok", "abstract": "In this work, we introduce a new decision tree ensemble representation framework: instead of using a graph model we transform each tree into a well-known polynomial form. We apply the new representation to three tasks: theoretical analysis, model reduction, and interpretation. The polynomial form of a tree ensemble allows a straightforward interpretation of the original model. In our experiments, it shows comparable results with state-of-the-art interpretation techniques. Another application of the framework is the ensemble-wise", "keywords": {"keywords": ["decision tree ensemble", "representation framework", "polynomial form", "theoretical analysis", "interpretation"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Explainable inference on sequential data via memory-tracking", "author": "B La Rosa"}, {"title": "What do Deep Networks Like to Read?", "author": "J Pfeiffer"}, {"title": "Xai-driven explainable multi-view game cheating detection", "author": "J Tao"}, {"title": "Individual explanations in machine learning models: A survey for practitioners", "author": "A Carrillo"}, {"title": "Neural generators of sparse local linear models for achieving both accuracy and interpretability", "author": "Y Yoshikawa"}, {"title": "Using blackbox ML techniques to diagnose QoE problems for an IPTV service", "author": "J Ahmed"}, {"title": "Transparency promotion with model-agnostic linear competitors", "author": "H Rafique"}, {"title": "Trust xai: Model-agnostic explanations for ai with a case study on iiot security", "author": "M Zolanvari"}, {"title": "Studying and exploiting the relationship between model accuracy and explanation quality", "author": "Y Jia"}, {"title": "Explanatory and actionable debugging for machine learning: A tableqa demonstration", "author": "M Cho", "abstract": "Question answering from tables (TableQA) extracting answers from tables from the question given in natural language, has been actively studied. Existing models have been trained and evaluated mostly with respect to answer accuracy using public benchmark datasets such as WikiSQL. The goal of this demonstration is to show a debugging tool for such models, explaining answers to humans, known as explanatory debugging. Our key distinction is making it\" actionable\" to allow users to directly correct models upon", "keywords": {"keywords": ["question answering", "tables", "explanatory debugging", "actionable debugging", "machine learning"], "research type": "Empirical", "methodology": "Qualitative", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Artificial Intelligence"}}, {"title": "Event mining for explanatory modeling", "author": "L Jalali"}, {"title": "Ensembles of random SHAPs", "author": "LV Utkin"}, {"title": "SurvNAM: The machine learning survival model explanation", "author": "LV Utkin"}, {"title": "Understanding uncertainty in bayesian neural networks", "author": "JA Cabiscol"}, {"title": "A framework of explanation generation toward reliable autonomous robots", "author": "T Sakai"}, {"title": "Interpretability in the medical field: A systematic mapping and review study", "author": "H Hakkoum"}, {"title": "Predicting illness for a sustainable dairy agriculture: predicting and explaining the onset of mastitis in dairy cows", "author": "C Ryan"}, {"title": "DA-DGCEx: Ensuring validity of deep guided counterfactual explanations with distribution-aware autoencoder loss", "author": "J Labaien"}, {"title": "Extracting optimal explanations for ensemble trees via logical reasoning", "author": "G Zhang"}, {"title": "Pace: Posthoc architecture-agnostic concept extractor for explaining cnns", "author": "V Kamakshi", "abstract": "Deep CNNs, though have achieved the state of the art performance in image classification tasks, remain a black-box to a human using them. There is a growing interest in explaining the working of these deep models to improve their trustworthiness. In this paper, we introduce a Posthoc Architecture-agnostic Concept Extractor (PACE) that automatically extracts smaller sub-regions of the image called concepts relevant to the black-box prediction. PACE tightly integrates the faithfulness of the explanatory framework to the black", "keywords": {"keywords": ["deep CNNs", "explanation", "trustworthiness", "concept extraction"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image classification"]}}, {"title": "Connecting attributions and QA model behavior on realistic counterfactuals", "author": "X Ye"}, {"title": "Explainable AI methods-a brief overview", "author": "A Holzinger"}, {"title": "Interpretability in sequence tagging models for Named Entity Recognition", "author": "SH Villarroya"}, {"title": "Interpretable spatiotemporal deep learning model for traffic flow prediction based on potential energy fields", "author": "J Ji"}, {"title": "Robust transparency against model inversion attacks", "author": "Y Alufaisan"}, {"title": "Deep learning for industrial image: challenges, methods for enriching the sample space and restricting the hypothesis space, and possible issue", "author": "T Liu"}, {"title": "Trends in explanations: Understanding and debugging data-driven systems", "author": "B Glavic"}, {"title": "Interpretability is harder in the multiclass setting: axiomatic interpretability for multiclass additive models", "author": "X Zhang"}, {"title": "Towards probabilistic sufficient explanations", "author": "E Wang"}, {"title": "XEM: An explainable-by-design ensemble method for multivariate time series classification", "author": "K Fauvel", "abstract": "We present XEM, an eXplainable-by-design Ensemble method for Multivariate time series classification. XEM relies on a new hybrid ensemble method that combines an explicit boosting-bagging approach to handle the bias-variance trade-off faced by machine learning models and an implicit divide-and-conquer approach to individualize classifier errors on different parts of the training data. Our evaluation shows that XEM outperforms the state-of-the-art MTS classifiers on the public UEA datasets. Furthermore, XEM provides faithful", "keywords": {"keywords": ["XEM", "ensemble method", "multivariate time series classification", "explainable-by-design", "UEA datasets"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Time series classification"]}}, {"title": "Synthesizing pareto-optimal interpretations for black-box models", "author": "H Torfah"}, {"title": "Explainable ai for inspecting adversarial attacks on deep neural networks", "author": "Z Klawikowska"}, {"title": "Global explanations with local scoring", "author": "M Setzu"}, {"title": "Explainable link prediction for privacy-preserving contact tracing", "author": "B Ganesan"}, {"title": "Analyzing learning-based networked systems with formal verification", "author": "A Dethise"}, {"title": "Interpretable run-time monitoring and replanning for safe autonomous systems operations", "author": "C Di Franco"}, {"title": "Towards an explainable mortality prediction model", "author": "JR Epifano"}, {"title": "An ai-augmented lesion detection framework for liver metastases with model interpretability", "author": "XJ Hunt"}, {"title": "Declarative approaches to counterfactual explanations for classification", "author": "L Bertossi"}, {"title": "PyExplainer: Explaining the Predictions of Just-In-Time Defect Models", "author": "C Pornprasit", "abstract": "Just-In-Time (JIT) defect prediction (ie, an AI/ML model to predict defect-introducing commits) is proposed to help developers prioritize their limited Software Quality Assurance (SQA) resources on the most risky commits. However, the explainability of JIT defect models remains largely unexplored (ie, practitioners still do not know why a commit is predicted as defect-introducing). Recently, LIME has been used to generate explanations for any AI/ML models. However, the random perturbation approach used by LIME to generate synthetic", "keywords": {"keywords": ["Just-In-Time defect prediction", "explainability", "AI/ML models", "LIME", "defect-introducing commits"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Software Quality Assurance"]}}, {"title": "Explainable AI and Adoption of Financial Algorithmic Advisors: An Experimental Study", "author": "D Ben David"}, {"title": "Cargo wagon structural health estimation using computer vision", "author": "AF Posada Moreno"}, {"title": "Constraint-Driven Explanations of Black-Box ML Models", "author": "AA Shrotri"}, {"title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization", "author": "J Borowski"}, {"title": "Hindsight logging for model training", "author": "R Garcia"}, {"title": "Explaining bad forecasts in global time series models", "author": "J Ro\u017eanec"}, {"title": "The role of explainability in assuring safety of machine learning in healthcare", "author": "Y Jia"}, {"title": "A systematic review of explainable artificial intelligence in terms of different application domains and tasks", "author": "MR Islam"}, {"title": "Approximating a deep reinforcement learning docking agent using linear model trees", "author": "VB Gj\u00e6rum"}, {"title": "On the quality of classification models for inferring ABAC policies from access logs", "author": "L Cappelletti", "abstract": "The attribute-based access control (ABAC) model has been gaining popularity in recent years because of its advantages in granularity, flexibility, and usability. Few approaches based on association rules mining have been proposed for the automatic generation of ABAC policies from access logs. Their aim is the identification of policies that do not overfit over training data, are not too general and thus does to disclose sensitive resources to everyone, and are interpretable by humans. The large ABAC privilege space along with the", "keywords": {"keywords": ["attribute-based access control", "ABAC policies", "access logs", "classification models", "association rules mining"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["access control policy inference"]}}, {"title": "On Requirements and Design Criteria for Explainability in Legal AI", "author": "M van Otterlo"}, {"title": "A multi-layered approach for interactive black-box explanations", "author": "C Henin"}, {"title": "Explain your move: understanding agent actions using specific and relevant feature attribution", "author": "P Gupta"}, {"title": "Explanation-guided diagnosis of machine learning evasion attacks", "author": "A Amich"}, {"title": "Making ML models fairer through explanations: the case of LimeOut", "author": "G Alves"}, {"title": "Explainable anomaly detection for Hot-rolling industrial process", "author": "J Jakubowski"}, {"title": "A psychophysics approach for quantitative comparison of interpretable computer vision models", "author": "F Biessmann"}, {"title": "What does it mean to explain? A user-centered study on AI explainability", "author": "L Yang"}, {"title": "New explainability method for BERT-based model in fake news detection", "author": "M Szczepa\u0144ski"}, {"title": "Exacta: Explainable column annotation", "author": "Y Xian", "abstract": "In this section, we propose a new method called EXACTA, which casts explainable column  annotation as a KG-based IRL problem. As illustrated in Fig. 2, it iteratively learns the reward", "keywords": {"keywords": ["explainable column annotation", "KG-based IRL", "iterative learning", "reward", "exacta"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Knowledge Graph Annotation"]}}, {"title": "I Beg to Differ: A study of constructive disagreement in online conversations", "author": "C De Kock"}, {"title": "Knowledge graph-based rich and confidentiality preserving Explainable Artificial Intelligence (XAI)", "author": "JM Ro\u017eanec"}, {"title": "The fidelity of global surrogates in interpretable Machine Learning", "author": "C Schwartzenberg"}, {"title": "An exploration and validation of visual factors in understanding classification rule sets", "author": "J Yuan"}, {"title": "Principles of explainable artificial intelligence", "author": "R Guidotti"}, {"title": "Learning accurate and interpretable decision rule sets from neural networks", "author": "L Qiao"}, {"title": "Towards design principles for user-centric explainable AI in fraud detection", "author": "D Cirqueira"}, {"title": "Trustworthy human-AI partnerships", "author": "SD Ramchurn"}, {"title": "Interpretable machine learning as decision support for processing fraud alerts", "author": "HJP Weerts"}, {"title": "Moral decision making in human-agent teams: Human control and the role of explanations", "author": "J van der Waa", "abstract": "With the progress of Artificial Intelligence, intelligent agents are increasingly being deployed in tasks for which ethical guidelines and moral values apply. As artificial agents do not have a legal position, humans should be held accountable if actions do not comply, implying humans need to exercise control. This is often labeled as Meaningful Human Control (MHC). In this paper, achieving MHC is addressed as a design problem, defining the collaboration between humans and agents. We propose three possible team designs (Team Design", "keywords": {"keywords": ["Moral decision making", "Human-agent teams", "Human control", "Explanations", "Meaningful Human Control"], "research type": ["Theoretical"], "methodology": ["Not applicable"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Ethics and Artificial Intelligence"]}}, {"title": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5, \u5e94\u7528\u4e0e\u5b89\u5168\u7814\u7a76\u7efc\u8ff0", "author": "\u7eaa\u5b88\u9886\uff0c \u674e\u8fdb\u950b\uff0c \u675c\u5929\u5b87\uff0c \u674e\u535a"}, {"title": "\u6570\u636e\u667a\u80fd: \u8d8b\u52bf\u4e0e\u6311\u6218", "author": "\u5434\u4fca\u6770\uff0c \u5218\u51a0\u7537\uff0c \u738b\u9759\u8fdc\uff0c \u5de6\u6e90\uff0c \u90e8\u6167\uff0c \u6797\u6d69"}, {"title": "Legal perspective on possible fairness measures\u2013A legal discussion using the example of hiring decisions", "author": "MP Hauer"}, {"title": "Anomaly attribution with likelihood compensation", "author": "T Id\u00e9"}, {"title": "Towards the Modeling of the Hot Rolling Industrial Process. Preliminary Results", "author": "M Szel\u0105\u017cek"}, {"title": "Collaborative explanation of deep models with limited interaction for trade secret and privacy preservation", "author": "J Domingo-Ferrer"}, {"title": "Understanding machine learning classifier decisions in automated radiotherapy quality assurance", "author": "Y Chen"}, {"title": "DataExposer: Exposing Disconnect between Data and Systems", "author": "S Galhotra"}, {"title": "Fine-grained explanations using markov logic", "author": "KM Al Farabi"}, {"title": "How good is your explanation? algorithmic stability measures to assess the quality of explanations for deep neural networks", "author": "T Fel", "abstract": "A plethora of methods have been proposed to explain how deep neural networks reach their decisions but comparatively, little effort has been made to ensure that the explanations produced by these methods are objectively relevant. While several desirable properties for trustworthy explanations have been formulated, objective measures have been harder to derive. Here, we propose two new measures to evaluate explanations borrowed from the field of algorithmic stability: mean generalizability MeGe and relative consistency ReCo. We", "keywords": {"keywords": ["explanation", "algorithmic stability", "deep neural networks", "mean generalizability", "relative consistency"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": ["computer science"], "content": ["methodological"], "application area": []}}, {"title": "Explainable identification of dementia from transcripts using transformer networks", "author": "L Ilias"}, {"title": "Natural Images are More Informative for Interpreting CNN Activations than State-of-the-Art Synthetic Feature Visualizations", "author": "J Borowski"}, {"title": "Foundations of Symbolic Languages for Model Interpretability", "author": "M Arenas"}, {"title": "Attention mechanism for multivariate time series recurrent model interpretability applied to the ironmaking industry", "author": "C Schockaert"}, {"title": "NLS: an accurate and yet easy-to-interpret regression method", "author": "V Coscrato"}, {"title": "How to improve ai tools (by adding in se knowledge): Experiments with the timelime defect reduction tool", "author": "K Peng"}, {"title": "ChatrEx: Designing explainable chatbot interfaces for enhancing usefulness, transparency, and trust", "author": "A Khurana"}, {"title": "Global explanations with decision rules: a co-learning approach", "author": "G Nanfack"}, {"title": "Generative counterfactuals for neural networks via attribute-informed perturbation", "author": "F Yang"}, {"title": "The explainability paradox: Challenges for xAI in digital pathology", "author": "T Evans", "abstract": "The increasing prevalence of digitised workflows in diagnostic pathology opens the door to life-saving applications of artificial intelligence (AI). Explainability is identified as a critical component for the safety, approval and acceptance of AI systems for clinical use. Despite the cross-disciplinary challenge of building explainable AI (xAI), very few application-and user-centric studies in this domain have been carried out. We conducted the first mixed-methods study of user interaction with samples of state-of-the-art AI explainability techniques for", "keywords": {"keywords": ["digital pathology", "explainability", "xAI", "artificial intelligence", "user interaction"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Clinical pathology"]}}, {"title": "Finding representative interpretations on convolutional neural networks", "author": "PCH Lam"}, {"title": "Explainability is in the mind of the beholder: Establishing the foundations of explainable artificial intelligence", "author": "K Sokol"}, {"title": "Sufficient reasons for classifier decisions in the presence of constraints", "author": "N Gorji"}, {"title": "Counterfactual Explanations for Models of Code", "author": "J Cito"}, {"title": "Using Visual Analytics to Explain Black-Box Machine Learning", "author": "JWH Krause"}, {"title": "Differentiated explanation of deep neural networks with skewed distributions", "author": "W Fu"}, {"title": "Trading complexity for sparsity in random forest explanations", "author": "G Audemard"}, {"title": "Trader: Trace divergence analysis and embedding regulation for debugging recurrent neural networks", "author": "G Tao"}, {"title": "Opening the black box: Machine learning interpretability and inference tools with an application to economic forecasting", "author": "M Buckmann"}, {"title": "Trepan Reloaded: A Knowledge-driven Approach to Explaining Artificial Neural Networks", "author": "R Confalonieri", "abstract": "knowledge to drive the explanation process (eg, [39,  neural-symbolic learning approach  based on TREPAN [8], an algorithm devised in order to explain trained artificial neural networks", "keywords": {"keywords": ["knowledge-driven approach", "artificial neural networks", "explanation process", "neural-symbolic learning", "TREPAN"], "research type": ["empirical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": "artificial intelligence"}}, {"title": "Detection and Explanation of Distributed Denial of Service (DDoS) Attack Through Interpretable Machine Learning", "author": "S Das"}, {"title": "Clusters in Explanation Space: Inferring disease subtypes from model explanations", "author": "MA Schulz"}, {"title": "On the Importance of Domain-specific Explanations in AI-based Cybersecurity Systems (Technical Report)", "author": "JN Paredes"}, {"title": "Ganchors: Realistic image perturbation distributions for anchors using generative models", "author": "KE David"}, {"title": "Towards Knowledge-driven Distillation and Explanation of Black-box Models", "author": "R Confalonieri"}, {"title": "On Evaluating Correctness of Explainable AI Algorithms: an Empirical Study on Local Explanations for Classification", "author": "MO Yalcin"}, {"title": "Towards verified robustness under text deletion interventions", "author": "J Welbl"}, {"title": "Interpretable neural networks with frank-wolfe: Sparse relevance maps and relevance orderings", "author": "J Macdonald"}, {"title": "Aequevox: Automated fairness testing of speech recognition systems", "author": "SS Rajan"}, {"title": "Semantic-Based Explainable AI: Leveraging Semantic Scene Graphs and Pairwise Ranking to Explain Robot Failures", "author": "D Das", "abstract": "When interacting in unstructured human environments, occasional robot failures are inevitable. When such failures occur, everyday people, rather than trained technicians, will be the first to respond. Existing natural language explanations hand-annotate contextual information from an environment to help everyday people understand robot failures. However, this methodology lacks generalizability and scalability. In our work, we introduce a more generalizable semantic explanation framework. Our framework autonomously", "keywords": {"keywords": ["Semantic-Based", "Explainable AI", "Semantic Scene Graphs", "Pairwise Ranking", "Robot Failures"], "research type": ["Empirical"], "methodology": [], "purposes": [], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Human-Robot Interaction"]}}, {"title": "Help Me Learn! Architecture and Strategies to Combine Recommendations and Active Learning in Manufacturing", "author": "P Zajec"}, {"title": "When stakes are high: balancing accuracy and transparency with Model-Agnostic Interpretable Data-driven suRRogates", "author": "R Henckaerts"}, {"title": "Understanding safety based on urban perception", "author": "F Moreno-Vera"}, {"title": "Face Mis-ID: An Interactive Pedagogical Tool Demonstrating Disparate Accuracy Rates in Facial Recognition", "author": "D Raz"}, {"title": "Explainable artificial intelligence in forensics: Realistic explanations for number of contributor predictions of DNA profiles", "author": "MS Veldhuis"}, {"title": "Explaining predictions by approximating the local decision boundary", "author": "G Vlassopoulos"}, {"title": "ERIC: extracting relations inferred from convolutions", "author": "J Townsend"}, {"title": "Explainable artificial intelligence for pharmacovigilance: What features are important when predicting adverse outcomes?", "author": "IR Ward"}, {"title": "Post-hoc Explanation using a Mimic Rule for Numerical Data.", "author": "K Asano"}, {"title": "FixOut: an ensemble approach to fairer models", "author": "G Alves", "abstract": "FixOut instantiated by SHAP5 [13], an explanation method that is based on coalitional  game theory, to assess model  Also, instead of a simple average as aggregation rule, FixOut", "keywords": {"keywords": ["FixOut", "ensemble approach", "fairer models", "SHAP5", "explanation method"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "One explanation is not enough: structured attention graphs for image classification", "author": "V Shitole"}, {"title": "Model exploration using conditional visualization", "author": "CB Hurley"}, {"title": "Adversarial regularization for explainable-by-design time series classification", "author": "Y Wang"}, {"title": "Auditing and debugging deep learning models via flip points: Individual-level and group-level analysis", "author": "R Yousefzadeh"}, {"title": "A review of interpretable ml in healthcare: Taxonomy, applications, challenges, and future directions", "author": "TAA Abdullah"}, {"title": "Model-agnostic explanations for decisions using minimal patterns", "author": "K Asano"}, {"title": "Sufficient Reasons for Classifier Decisions in the Presence of Domain Constraints", "author": "N Gorji"}, {"title": "Learning Groupwise Explanations for Black-Box Models.", "author": "J Gao"}, {"title": "Model-agnostic explanations using minimal forcing subsets", "author": "X Han"}, {"title": "VARF: Verifying and Analyzing Robustness of Random Forests", "author": "C Nie", "abstract": "With the large-scale application of machine learning in various fields, the security of models has attracted great attention. Recent studies have shown that tree-based models are vulnerable to adversarial examples. This problem may cause serious security risks. It is important to verify the safety of models. In this paper, we study the robustness verification problem of Random Forests (RF) which is a fundamental machine learning technique. We reduce the verification problem of an RF model into a constraint solving problem solved by", "keywords": {"keywords": ["Random Forests", "robustness verification", "adversarial examples", "security risks", "constraint solving"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Ranking rule-based automatic explanations for machine learning predictions on asthma hospital encounters in patients with asthma: retrospective cohort \u2026", "author": "X Zhang"}, {"title": "A robust low data solution: dimension prediction of semiconductor nanorods", "author": "X Liu"}, {"title": "Intelligent Decision Assistance Versus Automated Decision-Making: Enhancing Knowledge Work Through Explainable Artificial Intelligence", "author": "M Schemmer"}, {"title": "EUCA: the End-User-Centered Explainable AI Framework", "author": "W Jin"}, {"title": "Counterfactual Evaluation for Explainable AI", "author": "Y Ge"}, {"title": "Urban Perception: Can we understand why a street is safe?", "author": "F Moreno-Vera"}, {"title": "On nonlinear incidence rate of Covid-19", "author": "SK Paul"}, {"title": "Guided-LORE: Improving LORE with a Focused Search of Neighbours", "author": "N Maaroof"}, {"title": "Explainable Activity Recognition for Smart Home Systems", "author": "D Das"}, {"title": "Improving anchor-based explanations", "author": "J Delaunay", "abstract": "faithful explanations, yet anchor-based explanations are not  notion of pertinent negatives  to explanations on textual data.  such methods on the quality of anchor-based explanations.", "keywords": {"keywords": ["anchor-based explanations", "faithful explanations", "notion of pertinent negatives", "textual data", "quality"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Explainable AI"]}}, {"title": "Improving the explainability of neural sentiment classifiers via data augmentation", "author": "H Chen"}, {"title": "Counterfactual Explanations as Interventions in Latent Space", "author": "R Crupi"}, {"title": "A Categorisation of Post-hoc Explanations for Predictive Models", "author": "J Mitros"}, {"title": "Accurate, Fair, and Explainable: Building Human-Centered AI", "author": "A Springer"}, {"title": "Generating realistic natural language counterfactuals", "author": "M Robeer"}, {"title": "Opening the black box\u2013Quantile neural networks for loss given default prediction", "author": "R Kellner"}, {"title": "MAIRE-A Model-Agnostic Interpretable Rule Extraction Procedure for Explaining Classifiers", "author": "R Sharma"}, {"title": "Morphology-based identification of Bemisia tabaci cryptic species puparia via embedded group-contrast convolution neural network analysis", "author": "N MacLeod"}, {"title": "Semantic data mining in ubiquitous sensing: A survey", "author": "GJ Nalepa"}, {"title": "REDS: rule extraction for discovering scenarios", "author": "V Arzamasov", "abstract": "Scenario discovery is the process of finding areas of interest, known as scenarios, in data spaces resulting from simulations. For instance, one might search for conditions, ie, inputs of the simulation model, where the system is unstable. Subgroup discovery methods are commonly used for scenario discovery. They find scenarios in the form of hyperboxes, which are easy to comprehend. Given a computational budget, results tend to get worse as the number of inputs of the simulation model and the cost of simulations increase. We propose a", "keywords": {"keywords": ["scenario discovery", "subgroup discovery methods", "hyperboxes", "computational budget", "simulation model"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Data analysis"]}}, {"title": "A comparative approach to explainable artificial intelligence methods in application to high-dimensional electronic health records: Examining the usability of xai", "author": "JA Duell"}, {"title": "How level of explanation detail affects human performance in interpretable intelligent systems: A study on explainable fact checking", "author": "R Linder"}, {"title": "c-Eval: A unified metric to evaluate feature-based explanations via perturbation", "author": "MN Vu"}, {"title": "An explanation method for black-box machine learning survival models using the Chebyshev distance", "author": "LV Utkin"}, {"title": "Explaining mispredictions of machine learning models using rule induction", "author": "J Cito"}, {"title": "Interpretable data-based explanations for fairness debugging", "author": "R Pradhan"}, {"title": "A survey on artificial intelligence (ai) and explainable ai in air traffic management: Current trends and development with future research trajectory", "author": "A Degas"}, {"title": "Querying in the Age of Graph Databases and Knowledge Graphs", "author": "M Arenas"}, {"title": "Extracting per Query Valid Explanations for Blackbox Learning-to-Rank Models", "author": "J Singh"}, {"title": "AI Explainability. A Bridge Between Machine Vision and Natural Language Processing", "author": "M Oussalah", "abstract": "This paper attempts to present an appraisal review of explainable Artificial Intelligence research, with a focus on building a bridge between image processing community and natural language processing (NLP) community. The paper highlights the implicit link between the two disciplines as exemplified from the emergence of automatic image annotation systems, visual question-answer systems. Text-To-Image generation and multimedia analytics. Next, the paper identified a set of natural language processing fields", "keywords": {"keywords": ["AI Explainability", "Machine Vision", "Natural Language Processing", "Automatic Image Annotation", "Visual Question-Answer Systems"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Classifica\u00e7\u00e3o de pe\u00e7as processuais jur\u00eddicas: Intelig\u00eancia Artificial no Direito", "author": "MHP Ferreira"}, {"title": "On the Explainability of Natural Language Processing Deep Models", "author": "JE Zini"}, {"title": "Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes", "author": "K Xie"}, {"title": "Training characteristic functions with reinforcement learning: Xai-methods play connect four", "author": "S W\u00e4ldchen"}, {"title": "DuReader_robust: a Chinese dataset towards evaluating robustness and generalization of machine reading comprehension in real-world applications", "author": "H Tang"}, {"title": "You can do better! if you elaborate the reason when making prediction", "author": "D Li"}, {"title": "Trends in Explanations", "author": "B Glavic"}, {"title": "From Risk Prediction Models to Risk Assessment Service: A Formulation of Development Paradigm", "author": "E Xia"}, {"title": "Explaining Vulnerability of Machine Learning to Adversarial Attacks", "author": "M Melis"}, {"title": "Quest: A Query-driven Explanation Framework for Black-Box Classifiers on Tabular Data.", "author": "N Geisler", "abstract": "Explainability efforts are well established in the ML and AI communities by now, with local, model-agnostic approaches currently being the tool of choice in information retrieval [2] and search [3] as well as many other areas. One major challenge in the field is the lack of sophisticated approaches for tabular/relational data, as opposed to text or images. Generic approaches, eg feature importance, limit expressiveness and readability. LIME [4] still remains the basis of many approaches for local, model-agnostic explanations. It was", "keywords": {"keywords": ["explainability", "black-box classifiers", "tabular data", "local, model-agnostic approaches", "LIME"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Non-monotonic Explanation Functions", "author": "L Amgoud"}, {"title": "Rule discovery in labeled sequential data: Application to game analytics", "author": "R Mathonat"}, {"title": "Interpretable stock anomaly detection based on spatio-temporal relation networks with genetic algorithm", "author": "MS Cheong"}, {"title": "Development and validation of a deep learning-based algorithm for colonoscopy quality assessment", "author": "YY Chang"}, {"title": "An active approach for model interpretation", "author": "J Lu"}, {"title": "Improved Feature Importance Computations for Tree Models: Shapley vs. Banzhaf", "author": "A Karczmarz"}, {"title": "Factors Impacting K-12 Teachers in Understanding Explanations of Machine Learning Model on Students' Performance", "author": "H Guo"}, {"title": "Teaching the Machine to Explain Itself using Domain Knowledge", "author": "V Balayan"}, {"title": "OpenXAI: Towards a Transparent Evaluation of Model Explanations", "author": "C Agarwal"}, {"title": "A Progressive and Cross-Domain Deep Transfer Learning Framework for Wrist Fracture Detection", "author": "C Karam", "abstract": "There has been an amplified focus on and benefit from the adoption of artificial intelligence (AI) in medical imaging applications. However, deep learning approaches involve training with massive amounts of annotated data in order to guarantee generalization and achieve high accuracies. Gathering and annotating large sets of training images require expertise which is both expensive and time-consuming, especially in the medical field. Furthermore, in health care systems where mistakes can have catastrophic consequences, there is a", "keywords": {"keywords": ["deep transfer learning", "wrist fracture detection", "artificial intelligence", "medical imaging", "annotated data"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Medical imaging"]}}, {"title": "Counterfactual Explanation Trees: Transparent and Consistent Actionable Recourse with Decision Trees", "author": "K Kanamori"}, {"title": "Individual Explanations in Machine Learning Models: A Case Study on Poverty Estimation", "author": "A Carrillo"}, {"title": "Explaining Black Box Models Through Twin Systems", "author": "FM Cau"}, {"title": "Explainable, interactive c ontent\u2010based image retrieval", "author": "B Vasu"}, {"title": "Research on Explainable Artificial Intelligence Techniques: An User Perspective", "author": "F Daudt"}, {"title": "HESIP: A Hybrid System for Explaining Sub-symbolic Predictions", "author": "A Salam"}, {"title": "A Quantitative Perspective on Values of Domain Knowledge for Machine Learning", "author": "J Yang"}, {"title": "Knowledge representation for explainable artificial intelligence", "author": "J Borrego-D\u00edaz"}, {"title": "Argument Schemes and Dialogue Protocols: Doug Walton's legacy in artificial intelligence", "author": "P McBurney"}, {"title": "Distilling neural networks into skipgram-level decision lists", "author": "M Sushil", "abstract": "Several previous studies on explanation for recurrent neural networks focus on approaches that find the most important input segments for a network as its explanations. In that case, the manner in which these input segments combine with each other to form an explanatory pattern remains unknown. To overcome this, some previous work tries to find patterns (called rules) in the data that explain neural outputs. However, their explanations are often insensitive to model parameters, which limits the scalability of text explanations. To", "keywords": {"keywords": ["neural networks", "decision lists", "explanation", "input segments", "scalability"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Text analysis"}}, {"title": "Integrating Intrinsic and Extrinsic Explainability: The Relevance of Understanding Neural Networks for Human-Robot Interaction", "author": "T Weber"}, {"title": "Towards a Formalization of Explanations for Robots' Actions and Beliefs.", "author": "F Lindner"}, {"title": "Can Explanations Be Useful for Calibrating Black Box Models?", "author": "X Ye"}, {"title": "Designing counterfactual generators using deep model inversion", "author": "J Thiagarajan"}, {"title": "Intelligent Data Engineering and Automated Learning\u2013IDEAL 2020: 21st International Conference, Guimaraes, Portugal, November 4\u20136, 2020, Proceedings \u2026", "author": "C Analide"}, {"title": "Feature Attribution and Recourse via Probabilistic Contrastive Counterfactuals", "author": "S Galhotra"}, {"title": "XPROAX-Local explanations for text classification with progressive neighborhood approximation", "author": "Y Cai"}, {"title": "Explainable deep learning in healthcare: A methodological survey from an attribution view", "author": "D Jin"}, {"title": "Visually analyzing and steering zero shot learning", "author": "S Sahoo"}, {"title": "Anchors vs attention: Comparing XAI on a real-life use case", "author": "G Jouis", "abstract": "Recent advances in eXplainable Artificial Intelligence (XAI) led to many different methods in order to improve explainability of deep learning algorithms. With many options at hand, and maybe the need to adapt existing ones to new problems, one may find in a struggle to choose the right method to generate explanations. This paper presents an objective approach to compare two different existing XAI methods. These methods are applied to a use case from literature and to a real use case of a French administration.", "keywords": {"keywords": ["XAI", "deep learning algorithms", "explanations", "real-life use case", "comparing"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Artificial intelligence"]}}, {"title": "Large-scale prediction of patient-level antibiotic resistance: towards clinical decision support for improved antimicrobial stewardship", "author": "H Zhou"}, {"title": "Towards Explainable Evaluation Metrics for Natural Language Generation", "author": "C Leiter"}, {"title": "Back to the Feature: A Neural-Symbolic Perspective on Explainable AI", "author": "A Campagner"}, {"title": "What to Blame? On the Granularity of Fault Localization for Deep Neural Networks", "author": "M Duran"}, {"title": "Preference-based and local post-hoc explanations for recommender systems", "author": "L Brunot"}, {"title": "LioNets: a neural-specific local interpretation technique exploiting penultimate layer information", "author": "I Mollas"}, {"title": "ExSum: From Local Explanations to Model Understanding", "author": "Y Zhou"}, {"title": "Active Assessment of Prediction Services as Accuracy Surface Over Attribute Combinations", "author": "V Piratla"}, {"title": "RES: A Robust Framework for Guiding Visual Explanation", "author": "Y Gao"}, {"title": "Combining an autoencoder and a variational autoencoder for explaining the machine learning model predictions", "author": "L Utkin", "abstract": "A method for explaining a deep learning model prediction is proposed. It uses a combination of the standard autoencoder and the variational autoencoder. The standard autoencoder is exploited to reconstruct original images and to produce hidden representation vectors. The variational autoencoder is trained to transform the deep learning model outputs (embedding vectors) into the hidden representation vectors of the standard autoencoder. In explaining or testing phase, the variational autoencoder produces a set of vectors based on the explained", "keywords": {"keywords": ["autoencoder", "variational autoencoder", "deep learning model", "explanation", "hidden representation"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["interpretability"]}}, {"title": "Contextual explanation rules for neural clinical classifiers", "author": "M Sushil"}, {"title": "Local explanations of global rankings: insights for competitive rankings", "author": "H Anahideh"}, {"title": "Unboxing the Black Box Using Case-Based Argumentation", "author": "RJ Ratsma"}, {"title": "A Case-Based Approach for the Selection of Explanation Algorithms in Image Classification", "author": "JA Recio-Garc\u00eda"}, {"title": "Machine learning for fraud detection in e-Commerce: A research agenda", "author": "N Tax"}, {"title": "Disinformation Detection with Model Explanations", "author": "I Konstantinidis"}, {"title": "Using Machine Learning to Advance Early Warning Systems: Promise and Pitfalls", "author": "J Soland"}, {"title": "Local Interpretable Explanations for GBDT", "author": "X Qiu"}, {"title": "Model-Agnostic Linear Competitors--When Interpretable Models Compete and Collaborate with Black-Box Models", "author": "H Rafique"}, {"title": "Model-agnostic local explanations with genetic algorithms for text classification", "author": "Q Du", "abstract": "The interpretability of black-box text classification models has been receiving widespread attention in recent years accompanying the growing popularity of artificial intelligence. To garner user trust on the model's decision-making process, it is imperative to provide faithful instance-wise justifications and rationalize the prediction in a human-readable way. In this paper, we address this challenge by introducing Locally Universal Rules (LURs) as model-agnostic local explanations. LURs are a subset of input words sufficient for the model to", "keywords": {"keywords": ["model-agnostic", "local explanations", "genetic algorithms", "text classification", "interpretability"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "Aid: Active distillation machine to leverage pre-trained black-box models in private data settings", "author": "TN Hoang"}, {"title": "ModelSpeX: Model specification using explainable artificial intelligence methods", "author": "U Schlegel"}, {"title": "Why should we teach machines to read charts made for humans?", "author": "JP Ono"}, {"title": "Evaluating Feature Attribution Methods in the Image Domain", "author": "A Gevaert"}, {"title": "Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional networks", "author": "W Zhang"}, {"title": "Random Forest for Histogram Data: An application in data-driven prognostic models for heavy-duty trucks", "author": "RB Gurung"}, {"title": "Explainable NLP for Human-AI Collaboration", "author": "P Lertvittayakumjorn"}, {"title": "Model-Centric Verification of Artificial Intelligence", "author": "N Gisolfi"}, {"title": "More Accurate Learning of k-DNF Reference Classes", "author": "B Juba"}, {"title": "Machine Learning in Precision Agriculture: A Survey on Trends, Applications and Evaluations Over Two Decades", "author": "S Condran", "abstract": "Precision agriculture represents the new age of conventional agriculture. This is made possible by the advancement of various modern technologies such as the internet of things. The unparalleled potential for data collection and analytics has resulted in an increase in multi-disciplinary research within machine learning and agriculture. However, the application of machine learning techniques to agriculture seems to be out of step with core machine learning research. This gap is further exacerbated by the inherent challenges", "keywords": {"keywords": ["Precision agriculture", "Machine learning", "Trends", "Applications", "Evaluations"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Agriculture"], "content": ["Application"], "application area": ["Precision agriculture"]}}, {"title": "Explainability in supply chain operational risk management: A systematic literature review", "author": "SF Nimmy"}, {"title": "Influence-Driven Explanations for Bayesian Network Classifiers", "author": "E Albini"}, {"title": "Sissos: intervention of tabular data and its applications", "author": "Y Liu"}, {"title": "Predicting the evolution of pain relief: Ensemble learning by diversifying model explanations", "author": "ABD Costa"}, {"title": "Efficient Decompositional Rule Extraction for Deep Neural Networks", "author": "ME Zarlenga"}, {"title": "Understanding Prediction Discrepancies in Machine Learning Classifiers", "author": "X Renard"}, {"title": "Static Analysis of Source Code Vulnerability Using Machine Learning Techniques: A Survey", "author": "J Wang"}, {"title": "Adequate and fair explanations", "author": "N Asher"}, {"title": "Explaining Machine Learning Models of Emotion Using the BIRAFFE Dataset", "author": "S Bobek"}, {"title": "Challenge AI mind: A crowd system for proactive AI testing", "author": "S Fu", "abstract": "Artificial Intelligence (AI) has burrowed into our lives in various aspects; however, without appropriate testing, deployed AI systems are often being criticized to fail in critical and embarrassing cases. Existing testing approaches mainly depend on fixed and pre-defined datasets, providing a limited testing coverage. In this paper, we propose the concept of proactive testing to dynamically generate testing data and evaluate the performance of AI systems. We further introduce Challenge. AI, a new crowd system that features the", "keywords": {"keywords": ["AI testing", "proactive testing", "crowd system", "artificial intelligence", "performance evaluation"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["AI system evaluation"]}}, {"title": "Feature Attribution As Feature Selection", "author": "S Hara"}, {"title": "Explaining a Deep Reinforcement Learning Docking Agent Using Linear Model Trees with User Adapted Visualization", "author": "VB Gj\u00e6rum"}, {"title": "Expanded Basis Sets for the Manipulation of Random Forests", "author": "TL Keevers"}, {"title": "Deep Learning in Neuroimaging: Overcoming Challenges With Emerging Approaches", "author": "J Smucny"}, {"title": "MIRAI: A Modifiable, Interpretable, and Rational AI Decision Support System", "author": "N Mallia"}, {"title": "Rethinking Stability for Attribution-based Explanations", "author": "C Agarwal"}, {"title": "Causal Explanations and XAI", "author": "S Beckers"}, {"title": "An Explanation Framework for Interpretable Credit Scoring", "author": "LM Demajo"}, {"title": "Addressing Pragmatic Challenges in Utilizing AI for Security of Industrial IoT", "author": "M Zolanvari"}, {"title": "Integration of explainable AI and blockchain for secure storage of human readable justifications for credit risk assessment", "author": "R Walambe", "abstract": "Automated credit risk assessment is one of the most important applications of artificial intelligence. However, the traditional black-box AI models are no longer suitable due to the regulations imposed all over the world to ensure security, privacy and interpretability of the data. A notable example is the General Data Protection Regulation (GDPR). The problem is twofold; firstly, human-understandable explanations are required and secondly, these explanations must be stored securely to ensure the privacy of the user and should be", "keywords": {"keywords": ["explainable AI", "blockchain", "secure storage", "human readable justifications", "credit risk assessment"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["financial services"]}}, {"title": "Interpretable Artificial Intelligence through the Lens of Feature Interaction", "author": "M Tsang"}, {"title": "Explaining Classi ers", "author": "B Shih"}, {"title": "Simulated User Studies for Explanation Evaluation", "author": "V Chen"}, {"title": "Evaluation of Local Model-Agnostic Explanations Using Ground Truth", "author": "AHA Rahnama"}, {"title": "Explaining link prediction systems based on knowledge graph embeddings", "author": "A Rossi"}, {"title": "Review on Interpretable Machine Learning in Smart Grid", "author": "C Xu"}, {"title": "InterpretableSAD: Interpretable Anomaly Detection in Sequential Log Data", "author": "X Han"}, {"title": "Virtual reality-based parallel coordinates plots enhanced with explainable ai and data-science analytics for decision-making processes", "author": "S Bobek"}, {"title": "Semantic XAI for contextualized demand forecasting explanations", "author": "JM Ro\u017eanec"}, {"title": "Occlusion method to obtain saliency maps for CNN", "author": "T Eriksson", "abstract": "This Bachelor project will study convolutional neural networks created for image classification. Furthermore, it will specifically use an explanatory model for how the network decided a certain classification output. This is to increase the interpretability of the network. However, the completeness of the explanatory model needs to be high for it to be useful. A saliency map of how valuable each image pixel is for the classification will be created, by occluding parts of the image. The MNIST dataset was used, which contains handwritten", "keywords": {"keywords": ["convolutional neural networks", "image classification", "explanatory model", "saliency map", "occlusion method"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["image analysis"]}}, {"title": "Producing Decisions and Explanations: A Joint Approach Towards Explainable CNNs", "author": "ICRT de Oliveira"}, {"title": "Analysis of the user experience with a multiperspective tool for explainable machine learning in light of interactive principles", "author": "BGCO Lopes"}, {"title": "Comparative Analysis for Real-Estate Price Index Prediction Models using Machine Learning Algorithms: LIME's Interpretability Evaluation", "author": "BG Jo"}, {"title": "Consistent Sufficient Explanations and Minimal Local Rules for explaining regression and classification models", "author": "SI Amoukou"}, {"title": "Robust rules for prediction and description", "author": "HM Proen\u00e7a"}, {"title": "High-precision linearized interpretation for fully connected neural network", "author": "X Lei"}, {"title": "Cause and effect: Concept-based explanation of neural networks", "author": "M Nokhbeh Zaeem"}, {"title": "PASTLE: Pivot-aided space transformation for local explanations", "author": "V La Gatta"}, {"title": "Towards a generic framework for black-box explanation methods (Extended version)", "author": "C Henin"}, {"title": "Contextualized LORE for Fuzzy Attributes.", "author": "N Maaroof", "abstract": "-LORE-F, a new method to explain the decisions of fuzzy-based systems, that uses the  information about the fuzzy sets  values of the fuzzy attributes. It also considers the character of the", "keywords": {"keywords": ["fuzzy attributes", "explanation", "LORE-F", "fuzzy-based systems", "contextualization"], "research type": ["methodological"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["fuzzy systems"]}}, {"title": "Augmenting automatic clustering with expert knowledge and explanations", "author": "S Bobek"}, {"title": "Explaining automated data cleaning with cleanex", "author": "L Berti-\u00c9quille"}, {"title": "Confident Interpretations of Black Box Classifiers", "author": "N Radulovic"}, {"title": "Learning Global Additive Explanations of Black-Box Models", "author": "S Tan"}, {"title": "Human-Centered Concept Explanations for Neural Networks", "author": "P Hitzler"}, {"title": "FROTE: Feedback Rule-Driven Oversampling for Editing Models", "author": "O Alkan"}, {"title": "Explainable Machine Learning (XML) to predict external wind pressure of a low-rise building in urban-like settings", "author": "DPP Meddage"}, {"title": "Explainable artificial intelligence (XAI) in auditing: A framework and research needs", "author": "CA Zhang"}, {"title": "Cluster Discovery from Sensor Data Incorporating Expert Knowledge.", "author": "S Bobek"}, {"title": "Cluster Discovery from Sensor Data Incorporating Expert Knowledge.", "author": "S Bobek", "abstract": "Analysis of sensor data in the industrial setting is commonly performed with the use of data mining methods based on the machine learning algorithms. However, we argue that a proper understanding of this data requires incorporation of expert knowledge. In fact, it is often the case that such an explicit knowledge is available and can be used to enhance the learning process. In this paper we discuss how expert knowledge can be used to validate a machine learning model. More importantly, we demonstrate how a machine learning model", "keywords": {"keywords": ["sensor data", "cluster discovery", "expert knowledge", "machine learning model", "validation"], "research type": ["empirical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["industrial"]}}, {"title": "Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization", "author": "K Kanamori"}, {"title": "LoMEF: A framework to produce local explanations for global model time series forecasts", "author": "D Rajapaksha"}, {"title": "Probabilistic robustness analysis for DNNs based on PAC learning", "author": "R Li"}, {"title": "Covered Information Disentanglement: Model Transparency via Unbiased Permutation Importance", "author": "JPB Pereira"}, {"title": "RSM: An explainable predictive sales route selector", "author": "B Chen"}, {"title": "Learning analytics dashboard: a tool for providing actionable insights to learners", "author": "T Susnjak"}, {"title": "Explainable AI: Foundations, Applications, Opportunities for Data Management Research", "author": "R Pradhan"}, {"title": "Human-understandable and machine-processable explanations for sub-symbolic predictions", "author": "A Salam"}, {"title": "Explainable AI for Cheating Detection and Churn Prediction in Online Games", "author": "J Tao"}, {"title": "Evaluating Information Leakage by Quantitative and Interpretable Measurements", "author": "Z Zhou", "abstract": "Noninterference, a strong security property for a computation process, informally says that the process output is insensitive to the value of its secret inputs\u2013the secret inputs do not\" interfere\" with those outputs. This is too strong, however; a degree of interference is necessary in almost all real systems. In this dissertation, we propose a measure of noninterference that is more practical. Based on a model of computations with three types of input (secret, attacker-controlled, and others) and an attacker-observable output, we define a", "keywords": {"keywords": ["information leakage", "quantitative measurements", "interpretable measurements", "noninterference", "computations"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": ["computer science"], "content": ["methodological"], "application area": []}}, {"title": "ADOPS: Aspect Discovery OPinion Summarisation Methodology based on deep learning and subgroup discovery for generating explainable opinion summaries", "author": "M L\u00f3pez"}, {"title": "Mitigating Bias in Algorithmic Systems: A Fish-Eye View of Problems and Solutions Across Domains", "author": "K Orphanou"}, {"title": "Tour d'horizon autour de l'explicabilit\u00e9 des mod\u00e8les profonds", "author": "G Jouis"}, {"title": "Gouvernance des algorithmes d'intelligence artificielle dans le secteur financier", "author": "L Dupont"}, {"title": "Supporting users in understanding intelligent everyday systems", "author": "M Eiband"}, {"title": "Adapting by copying. Towards a sustainable machine learning", "author": "I Unceta"}, {"title": "Information-theoretic Evolution of Model Agnostic Global Explanations", "author": "S Verma"}, {"title": "Explainable Deep Reinforcement Learning: State of the Art and Challenges", "author": "GA Vouros"}, {"title": "Transformers for Machine Learning: A Deep Dive", "author": "U Kamath"}, {"title": "Construction and Elicitation of a Black Box Model in the Game of Bridge", "author": "V Ventos", "abstract": "We address the problem of building a decision model for a specific bidding situation in the game of Bridge. We propose the following multi-step methodology i) Build a set of examples for the decision problem and use simulations to associate a decision to each example ii) Use supervised relational learning to build an accurate and readable model iii) Perform a joint analysis between domain experts and data scientists to improve the learning language, including the production by experts of a handmade model iv) Build a better, more readable", "keywords": {"keywords": ["bridge", "black box model", "bidding situation", "supervised relational learning", "simulation"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["game of Bridge"]}}, {"title": "ProtoShotXAI: Using Prototypical Few-Shot Architecture for Explainable AI", "author": "S Hess"}, {"title": "Data Summaries for Scalable Visual Analysis", "author": "GYY Chan"}, {"title": "Automated identification of higher-order behaviors in a machine-learning network security system", "author": "TD Keanini"}, {"title": "Leveraging Explanations in Interactive Machine Learning: An Overview", "author": "S Teso"}, {"title": "Explaining Black-box Predictions by Generating Local Meaningful Perturbations", "author": "T Verma"}, {"title": "Getting the Best of Both Worlds? Combining Local and Global Methods to Make AI Explainable", "author": "D Lenders"}, {"title": "The Use of Responsible Artificial Intelligence Techniques in the Context of Loan Approval Processes", "author": "E Purificato"}, {"title": "Understanding peace through the world news", "author": "V Voukelatou"}, {"title": "Towards expert\u2013machine collaborations for technology valuation: An interpretable machine learning approach", "author": "J Kim"}, {"title": "Interpretable Student Performance Prediction", "author": "T Ledeboer", "abstract": "Students' performance prediction can have many uses in the education sector. It helps to take measures to support struggling students and to improve course delivery. However, having meaningful explanations along each prediction is essential for the reliability of the predictions and hence is desirable. In this work, we propose a method for predicting student performance while generating explanations of the predictions made. An Explainable Boosting Machine is implemented to suit multi-class classification to achieve the mentioned", "keywords": {"keywords": ["student performance", "prediction", "explanation", "interpretable", "boosting machine"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["education"], "content": ["methodological"], "application area": ["education"]}}, {"title": "Explaining Explanation Methods.", "author": "R Guidotti"}, {"title": "Behind The Mask: Masquerading The Reason for Prediction", "author": "T Koide"}, {"title": "Evaluation of Visualization Concepts for Explainable Machine Learning Methods in the Context of Manufacturing.", "author": "A Gerling"}, {"title": "An Applied Uncertainty Analysis on the Techno-Economic Valuation of Engine Wash Procedures", "author": "B Asselman"}, {"title": "DRIB: Interpreting DNN with Dynamic Reasoning and Information Bottleneck", "author": "Y Si"}, {"title": "Finding Local Explanations Through Masking Models", "author": "F Angiulli"}, {"title": "From Correlation to Causation: Formalizing Interpretable Machine Learning as a Statistical Process", "author": "L Klein"}, {"title": "AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation", "author": "O Gomez"}, {"title": "A Case Study of Interpretable Counterfactual Explanations for the Task of Predicting Student Academic Performance", "author": "M Tsiakmaki"}, {"title": "Explainable Artificial Intelligence (XAI) in auditing", "author": "CA Zhang", "abstract": "Artificial Intelligence (AI) and Machine Learning (ML) are gaining increasing attention regarding their potential applications in auditing. One major challenge of their adoption in auditing is the lack of explainability of their results. As AI/ML matures, so do techniques that can enhance the interpretability of AI, aka, Explainable Artificial Intelligence (XAI). This paper introduces XAI techniques to auditing practitioners and researchers. We discuss how different XAI techniques can be used to meet the requirements of audit", "keywords": {"keywords": ["Explainable Artificial Intelligence", "AI", "Machine Learning", "auditing", "XAI"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Accounting"], "content": ["Application"], "application area": ["Auditing"]}}, {"title": "Achieving Transparency Report Privacy in Linear Time", "author": "CL Chen"}, {"title": "A Systematic Review on Model-agnostic XAI Libraries.", "author": "JM Darias"}, {"title": "A Roadmap for Automating Lineage Tracing to Aid Automatically Explaining Machine Learning Predictions for Clinical Decision Support", "author": "G Luo"}, {"title": "Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models", "author": "E Balkir"}, {"title": "The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations", "author": "A Balagopalan"}, {"title": "Building Trust in Artificial Conversational Agents.", "author": "C Bove"}, {"title": "On the Computation of Necessary and Sufficient Explanations", "author": "A Darwiche"}, {"title": "Explainable Predictive Process Monitoring: A User Evaluation", "author": "W Rizzi"}, {"title": "Explainability of Deep Learning-Based System in Health Care", "author": "S Kinger"}, {"title": "Piecewise Simplification Approach for Accurate and Understandable Model", "author": "H Namba", "abstract": "The scope of AI application is expanding to mission-critical domains such as safety, healthcare and finance. To introduce AI in such domains, understandability of the prediction logic is required. However typical accurate models such as deep neural networks are too complicated to understand. Although there exist methods for generating globally understandable models, their accuracy and applicability are often insufficient. To achieve high accuracy by understandable models for classification and regression tasks, we propose"}, {"title": "Generating argumentation frameworks from text", "author": "O Cocarascu"}, {"title": "Interpretable Machine Learning Model Selection for Breast Cancer Diagnosis Based on K-means Clustering", "author": "DN Ouedraogo"}, {"title": "On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods", "author": "K Amarasinghe"}, {"title": "A Human-centric Explainable Approach for Fake News Spreading Detection", "author": "O Lampridis"}, {"title": "On Computing Relevant Features for Explaining NBCs", "author": "Y Izza"}, {"title": "Trust in automated decision making: how user's trust and perceived understanding is influenced by the quality of automatically generated explanations", "author": "A Papenmeier"}, {"title": "Explaining black box algorithms: epistemological challenges and machine learning solutions", "author": "D Watson"}, {"title": "A Simple Framework for XAI Comparisons with a Case Study", "author": "GFA Yeo"}, {"title": "Exploring the Chemical Space of Urease Inhibitors to Extract Meaningful Trends and Drivers of Activity", "author": "N Aniceto"}, {"title": "Exploring the Chemical Space of Urease Inhibitors to Extract Meaningful Trends and Drivers of Activity", "author": "N Aniceto", "abstract": "Blocking the catalytic activity of urease has been shown to have a key role in different diseases as well as in different agricultural applications. A vast array of molecules have been tested against ureases of different species, but the clinical translation of these compounds has been limited due to challenges of potency, chemical and metabolic stability as well as promiscuity against other proteins. The design and development of new compounds greatly benefit from insights from previously tested compounds; however, no", "keywords": {"keywords": ["urease inhibitors", "meaningful trends", "drivers of activity", "diseases", "agricultural applications"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["biology"], "content": ["application"], "application area": ["medicine"]}}, {"title": "A Conceptual View on the Design and Properties of Explainable AI Systems for Legal Settings", "author": "M Otterlo"}, {"title": "Explaining Causal Influence of External Factors on Incidence Rate of Covid-19", "author": "SK Paul"}, {"title": "Landmark Explanation: An Explainer for Entity Matching Models", "author": "A Baraldi"}, {"title": "A classification and review of tools for developing and interacting with machine learning systems", "author": "E Mosqueira-Rey"}, {"title": "A Novel Plug-and-Play Approach for Adversarially Robust Generalization", "author": "D Maurya"}, {"title": "Justifying Short-Term Load Forecasts Obtained with the Use of Neural Models", "author": "TA Grzeszczyk"}, {"title": "C-LIME: A Consistency-Oriented LIME for Time-Series Health-Risk Predictions", "author": "T Ito"}, {"title": "Diagnostic Curves for Black Box Models", "author": "DI Inouye"}, {"title": "Human-Centered Efficient Explanation on Intrusion Detection Prediction", "author": "Y Lee"}, {"title": "Towards Better Model Understanding with Path-Sufficient Explanations", "author": "R Luss", "abstract": "Feature based local attribution methods are amongst the most prevalent in explainable artificial intelligence (XAI) literature. Going beyond standard correlation, recently, methods have been proposed that highlight what should be minimally sufficient to justify the classification of an input (viz. pertinent positives). While minimal sufficiency is an attractive property, the resulting explanations are often too sparse for a human to understand and evaluate the local behavior of the model, thus making it difficult to judge its overall quality. To", "keywords": {"keywords": ["explainable artificial intelligence", "feature-based local attribution methods", "path-sufficient explanations", "pertinent positives", "model understanding"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Explainable AI"]}}, {"title": "Use of Predictive Analytics within Learning Analytics Dashboards: A Review of Case Studies", "author": "G Ramaswami"}, {"title": "Provable methods for non-convex min-max optimization and models for interpretable machine learning", "author": "H Rafique"}, {"title": "Effective Explanations for Entity Resolution Models", "author": "T Teofili"}, {"title": "Towards Model-Agnostic Ensemble Explanations", "author": "S Bobek"}, {"title": "How to Reduce the Time Necessary for Evaluation of Tree-Based Models", "author": "V Anderkov\u00e1"}, {"title": "Postoperative MPA-AUC Prediction for Kidney Transplant Recipients Based on Model Interpretability Technique", "author": "Q Pan"}, {"title": "Interpretation of Black Box NLP Models: A Survey", "author": "S Choudhary"}, {"title": "Interpretability of Machine Learning Models", "author": "U Czerwinska"}, {"title": "Building Trust in Interactive Machine Learning via User Contributed Interpretable Rules", "author": "L Guo"}, {"title": "Comparing Feature Importance and Rule Extraction for Interpretability on Text Data", "author": "G Lopardo", "abstract": "Complex machine learning algorithms are used more and more often in critical tasks involving text data, leading to the development of interpretability methods. Among local methods, two families have emerged: those computing importance scores for each feature and those extracting simple logical rules. In this paper we show that using different methods can lead to unexpectedly different explanations, even when applied to simple models for which we would expect qualitative coincidence. To quantify this effect, we propose a new", "keywords": {"keywords": ["feature importance", "rule extraction", "interpretability", "text data", "explanations"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["text classification"]}}, {"title": "Th ese de doctorat de l'Universit e de Lyon", "author": "L L'INSA"}, {"title": "On the Effect of Information Asymmetry in Human-AI Teams", "author": "P Hemmer"}, {"title": "Explanation-Driven Learning-Based Models for Visual Recognition Tasks", "author": "ZA Daniels"}, {"title": "Machine Learning Approaches for Hospital Acquired Pressure Injuries: A Retrospective Study of Electronic Medical Records", "author": "JJ Levy"}, {"title": "Towards transfer learning in e-discovery: finding the optimal classifier and evaluating domain adaptation methods", "author": "JL Pebesma"}, {"title": "Automatic entity labeling through explanation techniques", "author": "S Castano"}, {"title": "Pattern Aided Explainable Machine Learning", "author": "Y Jia"}, {"title": "Rigorous Explanations for Machine Learning Models", "author": "J Marques-Silva"}, {"title": "Interactive Scalable Interfaces for Machine Learning Interpretability", "author": "F Hohman"}, {"title": "Searching for explanations of black-box classifiers in the space of semantic queries", "author": "J Liartis", "abstract": "Deep learning models have achieved impressive performance in various tasks, but they are usually opaque with regards to their inner complex operation, obfuscating the reasons for which they make decisions. This opacity raises ethical and legal concerns regarding the real-life use of such models, especially in critical domains such as in medicine, and has led to the emergence of the eXplainable Artificial Intelligence (XAI) field of research, which aims to make the operation of opaque AI systems more comprehensible to humans. The problem of", "keywords": {"keywords": ["black-box classifiers", "explanations", "semantic queries", "deep learning models", "XAI"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI transparency"]}}, {"title": "Towards explainable artificial intelligence", "author": "S W\u00e4ldchen"}, {"title": "Provably Precise, Succinct and Efficient Explanations for Decision Trees", "author": "Y Izza"}, {"title": "Interpretable machine learning for CLAS12 data analysis", "author": "N Cherrier"}, {"title": "Uncertainty Interpretation of the Machine Learning Survival Model Predictions", "author": "LV Utkin"}, {"title": "Transcending XAI Algorithm Boundaries through End-User-Inspired Design", "author": "W Jin"}, {"title": "User Trust on an Explainable AI-based Medical Diagnosis Support System", "author": "Y Rong"}, {"title": "Developing a Fidelity Evaluation Approach for Interpretable Machine Learning", "author": "M Velmurugan"}, {"title": "A Rationale-Centric Framework for Human-in-the-loop Machine Learning", "author": "J Lu"}, {"title": "Explainable machine learning predictions to support personalized cardiology strategies", "author": "DR Loh"}, {"title": "Deep Neural Networks Explainability: Algorithms and Applications", "author": "M Du", "abstract": "Deep neural networks (DNNs) are progressing at an astounding rate, and these models have a wide range of real-world applications, such as movie recommendations of Netflix, neural machine translation of Google, speech recognition of Amazon Alexa. Despite the successes, DNNs have their own limitations and drawbacks. The most significant one is the lack of transparency behind their behaviors, which leaves users with little understanding of how particular decisions are made by these models. Consider, for instance, an advanced", "keywords": {"keywords": ["Deep Neural Networks", "Explainability", "Algorithms", "Applications", "Transparency"], "research type": ["Review"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Machine Learning"]}}, {"title": "TSInterpret: A unified framework for time series interpretability", "author": "J H\u00f6llig"}, {"title": "Use-Case-Grounded Simulations for Explanation Evaluation", "author": "V Chen"}, {"title": "Statistical and machine learning for credit and market risk management", "author": "M Nagl"}, {"title": "SMACE: A New Method for the Interpretability of Composite Decision Systems", "author": "G Lopardo"}, {"title": "Explaining black-box classification models with arguments", "author": "L Amgoud"}, {"title": "A Preliminary Study of Interpreting CNNs Using Soft Decision Trees", "author": "Q Zhao"}, {"title": "Explanatory machine learning for sequential human teaching", "author": "L Ai"}, {"title": "Explainable clustering with multidimensional bounding boxes", "author": "M Kuk"}, {"title": "The Past, Present, and Prospective Future of XAI: A Comprehensive Review", "author": "MU Islam"}, {"title": "Large Scale Electronic Health Record Data and Echocardiography Video Analysis for Mortality Risk Prediction", "author": "AE Ulloa Cerna", "abstract": "Electronic health records contain the clinical history of patients. The enormous potential for discovery in such a rich dataset is hampered by their complexity. We hypothesize that machine learning models trained on EHR data can predict future clinical events significantly better than current models. We analyze an EHR database of 594,862 Echocardiography studies from 272,280 unique patients with both unsupervised and supervised machine learning techniques.", "keywords": {"keywords": ["Electronic health records", "Echocardiography", "Machine learning", "Mortality risk prediction", "Large scale data"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Medical"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Developing and Experimenting on Approaches to Explainability in AI Systems.", "author": "Y Zhang"}, {"title": "Designing an Interpretability-Based Model to Explain the Artificial Intelligence Algorithms in Healthcare", "author": "M Ennab"}, {"title": "Understanding Machine learning models through Shapley values and Shapley residuals", "author": "J Ngo"}, {"title": "Decrypting the Black Boxing of Artificial Intelligence Using Explainable Artificial Intelligence in Smart Healthcare", "author": "TA Shaikh"}, {"title": "Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating", "author": "B Smyth"}, {"title": "Making Sense of Human-generated Spatial-temporal Data from Urban Environment", "author": "M Pan"}, {"title": "Design and validation of a comparison methodology for Explainable AI techniques", "author": "F VANNI"}, {"title": "Visual Exploration of Machine Learning Model Behavior with Hierarchical Surrogate Rule Sets", "author": "J Yuan"}, {"title": "Techniques for trustworthy artificial intelligence systems in the context of a loan approval process", "author": "E Baralis"}, {"title": "Visual Exploration of Machine Learning Model Behavior with Hierarchical Surrogate Rule Sets", "author": "J Yuan", "abstract": "One of the potential solutions for model interpretation is to train a surrogate model: a more transparent model that approximates the behavior of the model to be explained. Typically, classification rules or decision trees are used due to the intelligibility of their logic-based expressions. However, decision trees can grow too deep and rule sets can become too large to approximate a complex model. Unlike paths on a decision tree that must share ancestor nodes (conditions), rules are more flexible. However, the unstructured visual", "keywords": {"keywords": ["Machine Learning", "Model Interpretation", "Surrogate Model", "Hierarchical Rule Sets", "Visual Exploration"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["Data Analysis"]}}, {"title": "Techniques for trustworthy artificial intelligence systems in the context of a loan approval process", "author": "E Baralis"}, {"title": "Introducing explainable supervised machine learning into interactive feedback loops for statistical production system", "author": "C Mougan"}, {"title": "Deep Learning for Android Malware Defenses: a Systematic", "author": "YUE LIU"}, {"title": "More Than Words: Towards Better Quality Interpretations of Text Classifiers", "author": "MB Zafar"}, {"title": "Structure and Causality in Understanding Complex Systems", "author": "MR O'Shaughnessy"}, {"title": "Training Deep Models to be Explained with Fewer Examples", "author": "T Iwata"}, {"title": "An Explainable Predictive Model for the Geolocation of English Tweets", "author": "S Hasni"}, {"title": "Vision and Natural Language for Creative Applications, and Their Analysis", "author": "V Manjunatha"}, {"title": "Aditya A. Shrotri", "author": "M Vardi"}, {"title": "Certifying Top-Down Decision-DNNF Compilers", "author": "F Capelli", "abstract": "Certifying the output of tools solving complex problems so as to ensure the correctness of the results they provide is of tremendous importance. Despite being widespread for SAT-solvers, this level of exigence has not yet percolated for tools solving more complex tasks, such as model counting or knowledge compilation. In this paper, the focus is laid on a general family of top-down Decision-DNNF compilers. We explain how those compilers can be tweaked so as to output certifiable Decision-DNNF circuits, which are mainly standard", "keywords": {"keywords": ["Certifying", "Top-Down Decision-DNNF Compilers", "Complex problems", "Model counting", "Knowledge compilation"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Algorithms and computational models"]}}, {"title": "TalkToModel: Understanding Machine Learning Models With Open Ended Dialogues", "author": "D Slack"}, {"title": "Semantic Explanation for Deep Neural Networks Using Feature Interactions", "author": "B Xia"}, {"title": "Scrutinizing XAI using linear ground-truth data with suppressor variables", "author": "R Wilming"}, {"title": "Towards Automated Evaluation of Explanations in Graph Neural Networks", "author": "V BK"}, {"title": "Beyond Importance Scores: Interpreting Tabular ML by Visualizing Feature Semantics", "author": "A Ghorbani"}, {"title": "Factual and counterfactual explanations in fuzzy classification trees", "author": "G Fernandez"}, {"title": "Explaining Anomalies in Industrial Multivariate Time-series Data with the help of eXplainable AI", "author": "SM Tripathy"}, {"title": "Comparing Explanations for Black-box Intrusion Detection Systems", "author": "P Michalopoulos"}, {"title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation", "author": "H Guo"}, {"title": "Interpreting Text Classification with Human-Understandable Counterfactual Instances", "author": "T Li", "abstract": "As the omnipresent machine learning models play increasingly important roles in our society, powerful interpretation tools to uncover their black boxes are needed. On the other hand, proven by psychological study, we humans are more likely to learn new concepts presented with contrastive instances. Therefore, interpreting ML models using the contrast between the original data instance and its counterfactuals has become a popular problem. Traditional counterfactual interpretation approaches tend to generate counterfactuals faithful", "keywords": {"keywords": ["text classification", "interpretation", "human-understandable", "counterfactual instances", "machine learning models"], "research type": "Theoretical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Computer science", "content": "Methodological", "application area": "Artificial intelligence"}}, {"title": "Designing Counterfactual Generators On-the-Fly", "author": "JJ Thiagarajan"}, {"title": "Dealing with imbalanced data for interpretable defect prediction", "author": "Y Gao"}, {"title": "Statistics and Deep Learning-based Hybrid Model for Interpretable Anomaly Detection", "author": "T Mathonsi"}, {"title": "Learning Optimal Decision Sets and Lists with SAT", "author": "J Yu"}, {"title": "Principles and Interactive Tools for Evaluating and Improving the Behavior of Natural Language Processing models", "author": "T Wu"}, {"title": "Explainable and secure artificial intelligence: taxonomy, cases of study, learned lessons, challenges and future directions", "author": "KA Eldrandaly"}, {"title": "Mining Experienced Developers in Open-source Projects", "author": "Q Perez"}, {"title": "Distilling ensemble of explanations for weakly-supervised pre-training of image segmentation models", "author": "X Li"}, {"title": "On the Evaluation of the Plausibility and Faithfulness of Sentiment Analysis Explanations", "author": "J El Zini"}, {"title": "Performance evaluation of Explainable Machine Learning on Non-Communicable Diseases", "author": "C Ho", "abstract": "The advancements in machine learning and artificial intelligence can significantly benefit the diagnosis of Non-Communicable Diseases (NCDs). However, the inherent complexity of black-box models hinders the interpretability of the model. Potential regulatory issues arise, and the lack of trust within the medical community is apparent due to the lack of understanding of how and why a model made a prediction. In this study, we demonstrate how model-agnostic methods of eXplainable AI (XAI) can help provide explanations to", "keywords": {"keywords": ["Explainable Machine Learning", "Non-Communicable Diseases", "interpretability", "model-agnostic methods", "regulatory issues"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Medical Science"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Improving Analysis of Explainable Ai Using Human Gait", "author": "H Yang"}, {"title": "Department of Electrical and Computer Engineering Concordia University", "author": "R Bahrevar"}, {"title": "Visualizing Global Explanations of Point Cloud DNNs", "author": "H Tan"}, {"title": "Leveraging explainable machine learning to raise awareness among preadolescents about gender bias in supervised learning", "author": "GI Melsi\u00f3n P\u00e9rez"}, {"title": "Ensemble learning by diversifying explanations: predicting the evolution of pain relief", "author": "AB da Costa"}, {"title": "Pitfalls of Explainable ML: An Industry Perspective", "author": "S Verma"}, {"title": "Semantic Data Analytics Engine with Domain-specific Implementation: a Case Study in Diabetes", "author": "TN Tran"}, {"title": "Material Classification with a Transfer Learning based Deep Model on an imbalanced Dataset using an epochal Deming-Cycle-Methodology", "author": "M Klaiber"}, {"title": "Automating the design and development of gradient descent trained expert system networks", "author": "J Straub"}, {"title": "Contributions to Time Series Classification: Meta-Learning and Explainability", "author": "A Abanda", "abstract": "This thesis includes 3 contributions of different types to the area of supervised time series classification, a growing field of research due to the amount of time series collected daily in a wide variety of domains. In this context, the number of methods available for classifying time series is increasing, and the classifiers are becoming more and more competitive and varied. Thus, the first contribution of the thesis consists of proposing a taxonomy of distance-based time series classifiers, where an exhaustive review of the existing methods and their", "keywords": {"keywords": ["time series classification", "meta-learning", "explainability", "distance-based time series classifiers"], "research type": ["review"], "methodology": ["quantitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["supervised time series classification"]}}, {"title": "Counterfactual Models for Fair and Adequate Explanations", "author": "N Asher"}, {"title": "IPL HyAIAI: Hybrid Approaches for Interpretable AI", "author": "E Fromont"}, {"title": "Towards Interpretable and Reliable Deep Neural Networks for Visual Intelligence", "author": "N Xie"}, {"title": "The magazine archive includes every article published in Communications of the ACM for over the past 50 years.", "author": "O Fawzi"}, {"title": "A Short Survey on Machine Learning Explainability: An Application to Periocular Recognition", "author": "J Brito"}, {"title": "Locally Interpretable Predictions of Parkinson's Disease Progression", "author": "Q Li"}, {"title": "Beyond Average Performance--exploring regions of deviating performance for black box classification models", "author": "L Torgo"}, {"title": "Additive Feature Attribution Explainable Methods to Craft Adversarial Attacks for Text Classification and Text Regression", "author": "Y Chai"}, {"title": "It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI", "author": "A Papenmeier"}, {"title": "Interpretability versus Explainability: Classification for Understanding Deep Learning Systems and Models", "author": "I Namat\u0113vs", "abstract": "The techniques of explainability and interpretability are not alternatives for many realworld problems, as recent studies often suggest. Interpretable machine learning is not a subset of explainable artificial intelligence or vice versa. While the former aims to build glass-box predictive models, the latter seeks to understand a black box using an explanatory model, a surrogate model, an attribution approach, relevance importance, or other statistics. There is concern that definitions, approaches, and methods do not match, leading to the inconsistent", "keywords": {"keywords": ["interpretability", "explainability", "classification", "deep learning systems", "models"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Robust machine learning models and their applications", "author": "H Chen"}, {"title": "Explainable AI and Adoption of Algorithmic Advisors: an Experimental Study", "author": "DB Davida"}, {"title": "Training Datasets for Machine Reading Comprehension and Their Limitations", "author": "J Welbl"}, {"title": "Integrating Explainable Machine Learning and Predictive Process Monitoring.", "author": "W Rizzi"}, {"title": "On Tackling Explanation Redundancy in Decision Trees", "author": "Y Izza"}, {"title": "Explainable Link Prediction for Privacy-Preserving Contact Tracing", "author": "D Dev"}, {"title": "A Comparative Study of Two Rule-Based Explanation Methods for Diabetic Retinopathy Risk Assessment", "author": "N Maaroof"}, {"title": "Visualizing Rule Sets: Exploration and Validation of a Design Space", "author": "J Yuan"}, {"title": "Improving Algorithmic Decision\u2013Making in the Presence of Untrustworthy Training Data", "author": "W Qi"}, {"title": "A Comparative Study of Two Rule-Based Explanation Methods for Diabetic Retinopathy Risk Assessment", "author": "N Maaroof", "abstract": "Understanding the reasons behind the decisions of complex intelligent systems is crucial in many domains, especially in healthcare. Local explanation models analyse a decision on a single instance, by using the responses of the system to the points in its neighbourhood to build a surrogate model. This work makes a comparative analysis of the local explanations provided by two rule-based explanation methods on RETIPROGRAM, a system based on a fuzzy random forest that analyses the health record of a diabetic person to assess his/her", "keywords": {"keywords": ["diabetic retinopathy", "rule-based explanation methods", "healthcare", "local explanation models", "fuzzy random forest"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "Visualizing Rule Sets: Exploration and Validation of a Design Space", "author": "J Yuan"}, {"title": "Improving Algorithmic Decision\u2013Making in the Presence of Untrustworthy Training Data", "author": "W Qi"}, {"title": "Artificial Intelligence Algorithms for Treatment of Diabetes", "author": "MM Rashid"}, {"title": "Explaining Decisions of Black-box Models using Association Rules", "author": "MH Motallebi Shabestari"}, {"title": "Exploiting Rationale Data for Explainable NLP Models", "author": "M Reimer"}, {"title": "Using Direct Error Predictors to Improve Model Safety and Interpretability", "author": "V Narayanaswamy"}, {"title": "MFPP: Morphological Fragmental Perturbation Pyramid for Black-Box Model Explanations", "author": "Q Yang"}, {"title": "A survey on artificial intelligence in histopathology image analysis", "author": "MM Abdelsamea"}, {"title": "Explaining Decision-Tree Predictions by Addressing Potential Conflicts between Predictions and Plausible Expectations", "author": "S Maruf"}, {"title": "Towards Explainable Software Defect Prediction Models to Support SQA Planning", "author": "J Jiarpakdee", "abstract": "Software defects are expensive, but hard to detect and prevent. Thus, Software Quality Assurance (SQA) activities (eg, code review, software testing, and SQA planning) are applied to ensure the highest quality of software systems by detection and prevention. However, such SQA activities are time-consuming and demanding. Defect prediction models have been proposed to help developers prioritise their limited SQA effort on the most risky files. Yet, the adoption of defect prediction models is still limited due to the following reasons", "keywords": {"keywords": ["software defects", "Software Quality Assurance (SQA)", "defect prediction models", "SQA planning"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Software engineering"]}}, {"title": "Transparent and Explainable ML", "author": "A Jung"}, {"title": "Towards Human-Centred Explainable AI", "author": "MSM ten Koppel"}, {"title": "Muhammad Usama Islam, Md. Mozaharul Mottalib, Mehedi Hassan", "author": "ZI Alam"}, {"title": "MUC-driven Feature Importance Measurement and Adversarial Analysis for Random Forest", "author": "S Ma"}, {"title": "Accurate and Intuitive Contextual Explanations using Linear Model Trees", "author": "A Lahiri"}, {"title": "Machine Learning with Provable Robustness Guarantees", "author": "H Zhang"}, {"title": "Adversarial Examples for Neural Automatic Essay Scoring Systems", "author": "K Kanopka"}, {"title": "FCE: Feedback Based Counterfactual Explanations for Explainable AI", "author": "M Suffian"}, {"title": "Large Scale Electronic Health Record Data and Echocardiography Video Analysis for Mortality Risk Prediction", "author": "AEU Cerna"}, {"title": "Perspectives on the challenges of generalizability, transparency and ethics in predictive learning analytics", "author": "A Mathrani", "abstract": "Educational institutions need to formulate a well-established data-driven plan to get long-term value from their learning analytics (LA) strategy. By tracking learners' digital traces and measuring learners' performance, institutions can discern consequential learning trends via use of predictive models to enhance their instructional services. However, questions remain on how the proposed LA system is suitable, meaningful, and justifiable. In this concept paper, we examine generalizability and transparency of the internals of predictive models", "keywords": {"keywords": ["learning analytics", "predictive models", "generalizability", "transparency", "ethics"], "research type": ["concept paper"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "Adversarially Robust Machine Learning with Guarantees", "author": "A Raghunathan"}, {"title": "OnML: an ontology-based approach for interpretable machine learning", "author": "P Ayranci"}, {"title": "Shahin: Faster Algorithms for Generating Explanations for Multiple Predictions", "author": "S Hasani"}, {"title": "Survey on Fairness Notions and Related Tensions", "author": "G Alves"}, {"title": "ExMed: An AI Tool for Experimenting Explainable AI Techniques on Medical Data Analytics", "author": "M Kapcia"}, {"title": "Assessing Explanation Quality by Venn Prediction", "author": "A Alkhatib"}, {"title": "Training calibration-based counterfactual explainers for deep learning models in medical image analysis", "author": "JJ Thiagarajan"}, {"title": "Towards Automated Error Analysis: Learning to Characterize Errors", "author": "T Gao"}, {"title": "Data farming output analysis using explainable AI", "author": "N Feldkamp"}, {"title": "Explainable AI for Classification Using Probabilistic Logic Inference", "author": "X Fan", "abstract": "The overarching goal of Explainable AI is to develop systems that not only exhibit intelligent behaviours, but also are able to explain their rationale and reveal insights. In explainable machine learning, methods that produce a high level of prediction accuracy as well as transparent explanations are valuable. In this work, we present an explainable classification method, which works by first constructing a symbolic Knowledge Base from the training data, and then performing probabilistic inferences on such Knowledge Base with linear", "keywords": {"keywords": ["Explainable AI", "Classification", "Probabilistic Logic Inference", "Transparent Explanations", "Knowledge Base"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Generating and Modifying Natural Language Explanations", "author": "A Salam"}, {"title": "Improving and Understanding Deep Models for Natural Language Comprehension", "author": "R Ghaeini"}, {"title": "Knowledge extraction from SME data for the implementation of PHM process", "author": "N Omri"}, {"title": "Random Forest Similarity Maps: A Scalable Visual Representation for Global and Local Interpretation", "author": "D Mazumdar"}, {"title": "A Survey of Interpretability of Machine Learning in Accelerator-based High Energy Physics", "author": "D Turvill"}, {"title": "Ray (Sungsoo) Hong ViDA, New York University New York City, United States rayhong@ nyu. edu", "author": "J Freire"}, {"title": "SAGE Intrusion Detection System: Sensitivity Analysis Guided Explainability for Machine Learning.", "author": "M Smith"}, {"title": "Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning", "author": "V Gupta"}, {"title": "in Linear Explanations", "author": "V Bhargava"}, {"title": "UniformLIME: A Uniformly Perturbed Local Interpretable Model-Agnostic Explanations Approach for Aerodynamics", "author": "E Jiang", "abstract": "Machine learning and deep learning are widely used in the field of aerodynamics. But most models are often seen as black boxes due to lack of interpretability. Local Interpretable Model-agnostic Explanations (LIME) is a popular method that uses a local surrogate model to explain a single instance of machine learning. Its main disadvantages are the instability of the explanations and low local fidelity. In this paper, we propose an original modification to LIME by employing a new perturbed sample generation method for"}, {"title": "Regularized Deep Network Learning for Multi-Label Visual Recognition", "author": "H Guo"}, {"title": "Quantifying the Knowledge in a DNN to Explain Knowledge Distillation for Classification", "author": "Q Zhang"}, {"title": "Three Essays on Information Retrieval", "author": "D Zhu"}, {"title": "Assessing the communication gap between AI models and healthcare professionals: explainability, utility and trust in AI-driven clinical decision-making", "author": "O Wysocki"}, {"title": "Counterfactual explanations and how to find them: literature review and benchmarking", "author": "R Guidotti"}, {"title": "IISM-IS-I-Teaching-Thesis & Practical Seminar Projects-Counterfactual explanations for a tensor flow neural network using Microsoft DICE library: An Experimental \u2026", "author": "A M\u00e4dche"}, {"title": "Black box algorithms and the rights of individuals: No easy solution to the\" explainability\" problem", "author": "J Gryz"}, {"title": "Counterfactual Multi-Token Fairness in Text Classification", "author": "P Lohia"}, {"title": "Explaining Image Enhancement Black-Box Methods through a Path Planning Based Algorithm", "author": "M Cotogni"}, {"title": "Explanation-driven model stacking", "author": "S Bobek", "abstract": ", recall in order to deliver better AI models. In this paper we present a method that allows  for weighted ML model stacking and demonstrates its practical use in an illustrative example.", "keywords": {"keywords": ["explanation-driven", "model stacking", "weighted", "ML models", "practical use"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Artificial intelligence"]}}, {"title": "Exploring the potential for use of AI to help researchers improve their research funding relevance and performance", "author": "O Spyroglou"}, {"title": "CNN LEGO: Disassembling and Assembling Convolutional Neural Network", "author": "J Hu"}, {"title": "ExMo: plainable AI del Using Inverse Frequency Decision Rules", "author": "P Mainali"}, {"title": "Auditing Algorithms: Determining Ethical Parameters of Algorithmic Decision-Making Systems in Healthcare", "author": "A Aldrees"}, {"title": "Robust Botnet DGA Detection: Blending XAI and OSINT for Cyber Threat Intelligence Sharing", "author": "H Suryotrisongko"}, {"title": "A Robust System to Detect and Explain Public Mask Wearing Behavior", "author": "A Gupta"}, {"title": "Towards Explainable AI: Feature Significance and Importance for Machine Learning Models", "author": "E Horel"}, {"title": "A Systems Approach to Rule-Based Data Cleaning", "author": "AH Ebaid"}, {"title": "Sparse Activation Maps for Interpreting 3D Object Detection", "author": "Q Chen"}, {"title": "Quantifying the Performance of Explainability Algorithms", "author": "ZQ Lin", "abstract": "Given the complexity of the deep neural network (DNN), DNN has long been criticized for its lack of interpretability in its decision-making process. This' black box'nature has been preventing the adaption of DNN in life-critical tasks. In recent years, there has been a surge of interest around the concept of artificial intelligence explainability/interpretability (XAI), where the goal is to produce an interpretation for a decision made by a DNN algorithm. While many explainability algorithms have been proposed for peaking into the decision", "keywords": {"keywords": ["DNN", "interpretability", "explainability", "XAI", "performance"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": "Artificial Intelligence"}}, {"title": "Neural Network-Augmented Locally Adaptive Linear Regression Model for Tabular Data", "author": "L Munkhdalai"}, {"title": "Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations", "author": "J Dai"}, {"title": "Explanations in AI as Claims of Tacit Knowledge", "author": "N Lam"}, {"title": "An Imprecise SHAP as a Tool for Explaining the Class Probability Distributions under Limited Training Data", "author": "LV Utkin"}, {"title": "Building Trustworthy Machine Learning Models", "author": "X Liu"}, {"title": "Ancres vs Attention: comparaison de m\u00e9thodes d'explicabilit\u00e9 des r\u00e9seaux profonds sur un cas d'usage r\u00e9el", "author": "G Jouis"}, {"title": "Untersuchung der Anf\u00e4lligkeit perturbationsbasierter Erkl\u00e4rbarkeitsmethoden f\u00fcr Adversarial Attacks", "author": "RL Wilking"}, {"title": "\u80cc\u666f\u5165\u66ff\u3048\u30c7\u30fc\u30bf\u96c6\u8a08\u306b\u3088\u308b XAI \u7d50\u679c\u8a55\u4fa1\u65b9\u5f0f\u306e\u63d0\u6848", "author": "\u5b89\u4e95\u96c5\u5f66\uff0c \u6d5c\u76f4\u53f2\uff0c \u68ee\u9756\u82f1\uff0c \u548c\u4e45\u4e95\u4e00\u5247"}, {"title": "Utvidelse av modell-induksjonsmetoder ved bruk av Case-Based Reasoning for forklaring av dype nevrale nett", "author": "S Engen"}, {"title": "LIME ein vielseitiges Erkl\u00e4rermodell", "author": "S Hoffmann", "abstract": "Die Erkl\u00e4rung durch LIME soll dabei f\u00fcr eine erweiterte Gruppe von fachfremden  praktischen  Selbstversuchs gepr\u00fcft, ob sich LIME als Erkl\u00e4rermodell f\u00fcr Laien eignet und somit das", "keywords": {"keywords": ["LIME", "Erkl\u00e4rermodell", "vielseitig", "fachfremde", "Selbstversuchs"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Application"], "application area": ["Explanatory model for Laien"]}}, {"title": "Abordagens adversariais para explica\u00e7\u00e3o de imagens em redes neurais profundas", "author": "AJS Junior"}, {"title": "Extraction de connaissances \u00e0 partir des donn\u00e9es d'une PME en vue de l'impl\u00e9mentation du PHM", "author": "N Omri"}, {"title": "JULHO DE 2020", "author": "FLD Morais"}, {"title": "THESIS/TH\u00c8SE", "author": "D Corbugy"}, {"title": "Expliquer et justifier les syst\u00e8mes de d\u00e9cisions algorithmiques", "author": "C Henin"}, {"title": "\u4e0d\u5747\u8861\u30c7\u30fc\u30bf\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u7528\u3044\u305f\u6a5f\u68b0\u5b66\u7fd2\u306e\u5224\u65ad\u6839\u62e0\u8aac\u660e\u306b\u304a\u3051\u308b\u5b89\u5b9a\u5316\u624b\u6cd5\u306e\u63d0\u6848", "author": "\u4e2d\u91ce\u7fd4\uff0c \u5185\u7530\u771f\u4eba"}, {"title": "An\u00e1lise comparativa das t\u00e9cnicas de Explainable AI e um novo m\u00e9todo para gera\u00e7\u00e3o de explica\u00e7\u00f5es textuais", "author": "MV GAMA NETO"}, {"title": "Apprentissage profond pour la segmentation et la d\u00e9tection automatique en imagerie multi-modale: application \u00e0 l'oncologie h\u00e9patique", "author": "V Couteaux"}, {"title": "\u57fa\u4e8e\u6700\u5c0f\u4e0d\u6ee1\u8db3\u6838\u7684\u968f\u673a\u68ee\u6797\u5c40\u90e8\u89e3\u91ca\u6027\u5206\u6790", "author": "\u9a6c\u8212\u5c91\uff0c \u53f2\u5efa\u7426\uff0c \u9ec4\u6edf\u9e3f\uff0c \u79e6\u80dc\u6f6e\uff0c \u4faf\u54f2"}], "let's go to the alien zoo: introducing an experimental framework to study usability of counterfactual explanations for machine learning": [{"title": "Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract \u2026", "author": "U Kuhl", "abstract": "Counterfactual explanations (CFEs) highlight what changes to a model's input would have changed its prediction in a particular way. CFEs have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of computational plausibility for automatically generated CFEs, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of such a constraint on user experience and behavior is yet unclear. In this", "keywords": {"keywords": ["counterfactual explanations", "plausibility", "explainable artificial intelligence", "user experience", "behavior"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Prescriptive", "discipline": "Computer Science", "content": "Application", "application area": "XAI"}}, {"title": "\" Why Here and Not There?\"--Diverse Contrasting Explanations of Dimensionality Reduction", "author": "A Artelt", "abstract": "Dimensionality reduction is a popular preprocessing and a widely used tool in data mining. Transparency, which is usually achieved by means of explanations, is nowadays a widely accepted and crucial requirement of machine learning based systems like classifiers and recommender systems. However, transparency of dimensionality reduction and other data mining tools have not been considered much yet, still it is crucial to understand their behavior--in particular practitioners might want to understand why a specific sample got", "keywords": {"keywords": ["dimensionality reduction", "transparency", "explanations", "data mining", "machine learning"], "research type": "Theoretical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Methodological", "application area": "Data analysis"}}], "does explainable artificial intelligence improve human decision-making?": [{"title": "Expl (AI) n it to me\u2013explainable AI and information systems research", "author": "K Bauer", "abstract": "The field of Artificial Intelligence has seen dramatic progress over the last 15 years. Using machine learning methods, software systems that automatically learn and improve relationships using digitized experience, researchers and practitioners alike have developed practical applications that are indispensable and strongly facilitate people's everyday life (Jordan and Mitchell 2015). Pervasive examples include object recognition (eg, Facebook's Moments and Intel Security's True Key), natural language processing (eg", "keywords": {"keywords": ["explainable AI", "information systems research", "machine learning", "object recognition", "natural language processing"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["AI"]}}, {"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer"}, {"title": "Rationale-inspired natural language explanations with commonsense", "author": "BP Majumder"}, {"title": "XAI for Operations in the Process Industry-Applications, Theses, and Research Directions.", "author": "A Kotriwala"}, {"title": "Towards a Trust Reliance Paradox? Exploring the Gap Between Perceived Trust in and Reliance on Algorithmic Advice", "author": "A Schmitt"}, {"title": "On the risk of confusing interpretability with explicability", "author": "C Herzog"}, {"title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization", "author": "J Borowski"}, {"title": "What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods", "author": "T Fel"}, {"title": "Natural Images are More Informative for Interpreting CNN Activations than State-of-the-Art Synthetic Feature Visualizations", "author": "J Borowski"}, {"title": "Attitudes towards trusting artificial intelligence insights and factors to prevent the passive adherence of GPs: a pilot study", "author": "M Micocci"}, {"title": "How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI.", "author": "ZT Zhang", "abstract": "Given the opaqueness and complexity of modern AI algorithms, there is currently a strong focus on developing transparent and explainable AI, especially in high-stakes domains. We claim that opaqueness and complexity are not the core issues for end users when interacting with AI. Instead, we propose that the output uncertainty inherent to AI systems is the actual problem, with opaqueness and complexity as contributing factors. Transparency and explainability should therefore not be the end goals, as such a focus tends to place the", "keywords": {"keywords": ["AI algorithms", "output uncertainty", "interactions", "transparency", "explainability"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["AI technology"]}}, {"title": "Do GPs trust artificial intelligence insights and what could this mean for patient care? A case study on GPs skin cancer diagnosis in the UK", "author": "M Micocci"}, {"title": "Rationale production to support clinical decision-making", "author": "N Taylor"}, {"title": "On the Relationship Between Explanations, Fairness Perceptions, and Decisions", "author": "J Schoeffer"}, {"title": "A Human-Centric Assessment Framework for AI", "author": "S Saralajew"}, {"title": "On the Effect of Information Asymmetry in Human-AI Teams", "author": "P Hemmer"}, {"title": "Captioning model based on meta-learning using prior-convergence knowledge for explainable images", "author": "JW Baek"}, {"title": "Information Model to Advance Explainable AI-Based Decision Support Systems in Manufacturing System Design", "author": "DS Cochran"}, {"title": "Pitfalls of Explainable ML: An Industry Perspective", "author": "S Verma"}, {"title": "Adapt and overcome: Perceptions of adaptive autonomous agents for human-AI teaming", "author": "AI Hauptman"}, {"title": "Assessing the communication gap between AI models and healthcare professionals: explainability, utility and trust in AI-driven clinical decision-making", "author": "O Wysocki", "abstract": "This paper contributes with a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support. The study revealed a more nuanced role for ML explanation models, when these are pragmatically embedded in the clinical context. Despite the general positive attitude of healthcare professionals (HCPs) towards explanations as a safety and trust mechanism, for a significant set of participants there were negative effects associated with confirmation bias, accentuating model over-reliance and", "keywords": {"keywords": ["communication gap", "explainability", "utility", "trust", "AI-driven clinical decision-making"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Medicine"], "content": ["Application"], "application area": ["Healthcare"]}}, {"title": "An Empirical Evaluation of Estimated Outcomes as Explanations in Human-AI Decision-Making", "author": "J Jakubik"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimization", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "Explanation Interfaces for Sales Forecasting", "author": "TB Fahse"}, {"title": "Predicting and Understanding Human Action Decisions during Skillful Joint-Action via Machine Learning and Explainable-AI", "author": "F Auletta"}], "human perceptions of fairness in algorithmic decision making: a case study of criminal risk prediction": [{"title": "Classification with fairness constraints: A meta-algorithm with provable guarantees", "author": "LE Celis", "abstract": "Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with", "keywords": {"keywords": ["fairness constraints", "classification algorithms", "meta-algorithm", "provable guarantees", "sensitive attributes"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["fairness in classification"]}}, {"title": "Explaining models: an empirical study of how explanations impact fairness judgment", "author": "J Dodge"}, {"title": "Fairness in machine learning: A survey", "author": "S Caton"}, {"title": "How do fairness definitions fare? Examining public attitudes towards algorithmic definitions of fairness", "author": "NA Saxena"}, {"title": "Toward algorithmic accountability in public services: A qualitative study of affected community perspectives on algorithmic decision-making in child welfare services", "author": "A Brown"}, {"title": "Mathematical notions vs. human perception of fairness: A descriptive approach to fairness for machine learning", "author": "M Srivastava"}, {"title": "Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation", "author": "MK Lee"}, {"title": "The ethics of algorithms: key problems and solutions", "author": "A Tsamados"}, {"title": "Factors influencing perceived fairness in algorithmic decision-making: Algorithm outcomes, development procedures, and individual differences", "author": "R Wang"}, {"title": "Discovering fair representations in the data domain", "author": "N Quadrianto"}, {"title": "Algorithmic fairness", "author": "D Pessach", "abstract": "Concerns that algorithms may discriminate against certain groups have led to numerous  efforts to \u2018blind\u2019 the algorithm to race. We argue that this intuitive perspective is misleading and", "keywords": {"keywords": ["algorithmic fairness", "discrimination", "race", "algorithm", "misleading"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "One-network adversarial fairness", "author": "T Adel"}, {"title": "Implications of AI (un-) fairness in higher education admissions: the effects of perceived AI (un-) fairness on exit, voice and organizational reputation", "author": "F Marcinkowski"}, {"title": "Learning model-agnostic counterfactual explanations for tabular data", "author": "M Pawelczyk"}, {"title": "Ethics emerging: the story of privacy and security perceptions in virtual reality", "author": "D Adams"}, {"title": "How good is good enough for COVID19 apps? The influence of benefits, accuracy, and privacy on willingness to adopt", "author": "G Kaptchuk"}, {"title": "Human decision making with machine assistance: An experiment on bailing and jailing", "author": "N Grgi\u0107-Hla\u010da"}, {"title": "An empirical study on the perceived fairness of realistic, imperfect machine learning models", "author": "G Harrison"}, {"title": "Crowdsourcing perceptions of fair predictors for machine learning: A recidivism case study", "author": "N Van Berkel"}, {"title": "Iteratively questioning and answering for interpretable legal judgment prediction", "author": "H Zhong"}, {"title": "The future of artificial intelligence at work: A review on effects of decision automation and augmentation on workers targeted by algorithms and third-party observers", "author": "M Langer", "abstract": "The main purpose of this paper is to review the future of artificial intelligence at work especially how it will affect decision automation and augmentation on workers. Automation and augmentation of decision-making processes in the workplace are becoming more common because of advancements in artificial intelligence. In addition to transforming our homes, smart technologies are also making inroads into a wide range of businesses and causing havoc in the workplace [1]. Although AI can boost productivity, efficiency, and", "keywords": {"keywords": ["artificial intelligence", "decision automation", "augmentation", "workers", "algorithms"], "research type": "review", "methodology": "mixed-methods", "purposes": "explanatory", "discipline": "sociology", "content": "Application", "application area": "workplace"}}, {"title": "Fair generative modeling via weak supervision", "author": "K Choi"}, {"title": "Investigating human+ machine complementarity for recidivism predictions", "author": "S Tan"}, {"title": "Measuring non-expert comprehension of machine learning fairness metrics", "author": "D Saha"}, {"title": "\u201cI just want to feel safe\u201d: A Diary Study of Safety Perceptions on Social Media", "author": "EM Redmiles"}, {"title": "AI and gender: Four proposals for future research", "author": "S Dillon"}, {"title": "Fairness in rankings and recommendations: an overview", "author": "E Pitoura"}, {"title": "Algorithmic fairness in mortgage lending: from absolute conditions to relational trade-offs", "author": "MSA Lee"}, {"title": "\" Should I Worry?\" A Cross-Cultural Examination of Account Security Incident Response", "author": "EM Redmiles"}, {"title": "Demographics and discussion influence views on algorithmic fairness", "author": "E Pierson"}, {"title": "Bridging machine learning and mechanism design towards algorithmic fairness", "author": "J Finocchiaro", "abstract": "Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (eg, school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or", "keywords": {"keywords": ["Machine learning", "Mechanism design", "Algorithmic fairness", "Decision-making systems", "Discrimination"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Societal fairness"]}}, {"title": "Human-centered approaches to fair and responsible AI", "author": "MK Lee"}, {"title": "How do fairness definitions fare? Testing public attitudes towards three algorithmic definitions of fairness in loan allocations", "author": "NA Saxena"}, {"title": "Algorithmic bias: review, synthesis, and future research directions", "author": "N Kordzadeh"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Effect of information presentation on fairness perceptions of machine learning predictors", "author": "N Van Berkel"}, {"title": "Designing alternative representations of confusion matrices to support non-expert public understanding of algorithm performance", "author": "H Shen"}, {"title": "Soliciting stakeholders' fairness notions in child maltreatment predictive systems", "author": "HF Cheng"}, {"title": "Lift: A scalable framework for measuring fairness in ml applications", "author": "S Vasudevan"}, {"title": "A systematic assessment of national artificial intelligence policies: Perspectives from the Nordics and beyond", "author": "N van Berkel"}, {"title": "Fairness perceptions of algorithmic decision-making: A systematic review of the empirical literature", "author": "C Starke", "abstract": "Algorithmic decision-making (ADM) increasingly shapes people's daily lives. Given that such autonomous systems can cause severe harm to individuals and social groups, fairness concerns have arisen. A human-centric approach demanded by scholars and policymakers requires taking people's fairness perceptions into account when designing and implementing ADM. We provide a comprehensive, systematic literature review synthesizing the existing empirical insights on perceptions of algorithmic fairness from 39 empirical", "keywords": {"keywords": ["fairness perceptions", "algorithmic decision-making", "empirical literature", "systematic review", "autonomous systems"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Descriptive"], "discipline": ["Social sciences"], "content": ["Methodological"], "application area": ["Ethics, Technology"]}}, {"title": "I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions", "author": "M Kasinidou"}, {"title": "Human comprehension of fairness in machine learning", "author": "D Saha"}, {"title": "What is fair? exploring pareto-efficiency for fairness constrained classifiers", "author": "A Balashankar"}, {"title": "Dimensions of diversity in human perceptions of algorithmic fairness", "author": "N Grgi\u0107-Hla\u010da"}, {"title": "Fairkit, Fairkit, on the Wall, Who's the Fairest of Them All? Supporting Data Scientists in Training Fair Models", "author": "B Johnson"}, {"title": "Conceptualising contestability: Perspectives on contesting algorithmic decisions", "author": "H Lyons"}, {"title": "Fairness and missing values", "author": "F Mart\u00ednez-Plumed"}, {"title": "Human perceptions on moral responsibility of AI: A case study in AI-assisted bail decision-making", "author": "G Lima"}, {"title": "Artificial intelligence ethics by design. Evaluating public perception on the importance of ethical design principles of artificial intelligence", "author": "K Kieslich"}, {"title": "An algorithmic framework for fairness elicitation", "author": "C Jung", "abstract": "We consider settings in which the right notion of fairness is not captured by simple mathematical definitions (such as equality of error rates across groups), but might be more complex and nuanced and thus require elicitation from individual or collective stakeholders. We introduce a framework in which pairs of individuals can be identified as requiring (approximately) equal treatment under a learned model, or requiring ordered treatment such as\" applicant Alice should be at least as likely to receive a loan as applicant Bob\". We", "keywords": {"keywords": ["fairness elicitation", "algorithmic framework", "equal treatment", "learned model", "ordered treatment"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["fairness in machine learning"]}}, {"title": "LimeOut: an ensemble approach to improve process fairness", "author": "V Bhargava"}, {"title": "Missing the missing values: The ugly duckling of fairness in machine learning", "author": "MP Fernando"}, {"title": "Datasheets for Datasets help ML Engineers Notice and Understand Ethical Issues in Training Data", "author": "KL Boyd"}, {"title": "Equality before the law: legal judgment consistency analysis for fairness", "author": "Y Wang"}, {"title": "Why we are failing to understand the societal impact of artificial intelligence", "author": "L Jaume-Palasi"}, {"title": "Risk identification questionnaire for detecting unintended bias in the machine learning development lifecycle", "author": "MSA Lee"}, {"title": "Confronting barriers to human-robot cooperation: balancing efficiency and risk in machine behavior", "author": "T Whiting"}, {"title": "Took a pic and got declined, vexed and perplexed: facial recognition in algorithmic management", "author": "EA Watkins"}, {"title": "Discrimination in Algorithmic Decision Making: From Principles to Measures and Mechanisms", "author": "MB Zafar"}, {"title": "Integrating behavioral, economic, and technical insights to address algorithmic bias: Challenges and opportunities for is research", "author": "G Adomavicius", "abstract": "Many important decisions are increasingly being made with the help of information systems that use artificial intelligence and machine learning models. These computational models are designed to discover useful patterns from large amounts of data, which augment human capabilities to make decisions in various application domains. However, there are growing concerns regarding the ethics challenges that these augmented decision making (ADM) models are facing, most notably on the issue of \u201calgorithmic bias\u201d, where the models", "keywords": {"keywords": ["algorithmic bias", "information systems", "artificial intelligence", "machine learning models", "ethics challenges"], "research type": ["Theoretical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Augmented decision making"]}}, {"title": "Fair inputs and fair outputs: The incompatibility of fairness in privacy and accuracy", "author": "B Rastegarpanah"}, {"title": "Perceptions of fairness", "author": "NA Saxena"}, {"title": "Project report: Perceptions of AI in hiring", "author": "R Gelles"}, {"title": "Wann ist K\u00fcnstliche Intelligenz (un-) fair?", "author": "F Marcinkowski"}, {"title": "Fairness in rankings and recommenders: Models, methods and research directions", "author": "E Pitoura"}, {"title": "Appropriate fairness perceptions? On the effectiveness of explanations in enabling people to assess the fairness of automated decision systems", "author": "J Schoeffer"}, {"title": "The Role of Accuracy in Algorithmic Process Fairness Across Multiple Domains", "author": "M Albach"}, {"title": "\" Look! It's a Computer Program! It's an Algorithm! It's AI!\": Does Terminology Affect Human Perceptions and Evaluations of Intelligent Systems?", "author": "M Langer"}, {"title": "Using automated decision-making (ADM) to allocate Covid-19 vaccinations? Exploring the roles of trust and social group preference on the legitimacy of ADM vs \u2026", "author": "M L\u00fcnich"}, {"title": "Charge prediction modeling with interpretation enhancement driven by double-layer criminal system", "author": "L Li", "abstract": "With the rapid development of artificial intelligence and the increasing demand for legal intelligence, using AI methods to predict legal judgments has become a hot spot in recent years. Charge prediction is one of the core tasks of Legal Judgment Prediction (LJP). It aims to predict charge from complicated legal facts, so as to help the court make judgments or provide legal professional guidance to non-professionals. In the field of legalAI, interpretability is crucial compared to others. Reasonable interpretability can eliminate", "keywords": {"keywords": ["charge prediction modeling", "interpretation enhancement", "double-layer criminal system", "legal intelligence", "AI methods"], "research type": ["empirical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["legal judgment prediction"]}}, {"title": "Making ML models fairer through explanations: the case of LimeOut", "author": "G Alves"}, {"title": "Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation", "author": "J Hannan"}, {"title": "Cognitive and Emotional Response to Fairness in AI\u2013A Systematic Review", "author": "J Baleis"}, {"title": "Swinging doors: An autoethnographic look at the challenges faced by previously incarcerated people in the USA and Australia", "author": "L Carey"}, {"title": "Mallard: Turn the Web into a Contextualized Prototyping Environment for Machine Learning", "author": "X Zhang"}, {"title": "A Review on Fairness in Machine Learning", "author": "D Pessach"}, {"title": "Perception of fairness in algorithmic decisions: Future developers' perspective", "author": "S Kleanthous"}, {"title": "A human-in-the-loop framework to construct context-aware mathematical notions of outcome fairness", "author": "M Yaghini"}, {"title": "Investigating Human+ Machine Complementarity: A Case Study on Recidivism", "author": "S Tan"}, {"title": "Users' Perception of Search-Engine Biases and Satisfaction", "author": "B Han", "abstract": "Search engines could consistently favor certain values over the others, which is considered as biased due to the built-in infrastructures. Many studies have been dedicated to detect, control, and mitigate the impacts of those biases from the perspectives of search engines themselves. In our study, we pitched the perspective from end-users to analyze their perceptions of search engine biases and their satisfaction when the biases are regulated. In the study, we paired a real page from search engine Bing and a synthesized page with more", "keywords": {"keywords": ["search engine biases", "user perception", "satisfaction", "regulation", "end-users"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": "Information retrieval"}}, {"title": "A Survey on the Fairness of Recommender Systems", "author": "Y Wang"}, {"title": "Risk identification questionnaire for unintended bias in machine learning development lifecycle", "author": "MSA Lee"}, {"title": "\" How I Know For Sure\": People's Perspectives on Solely Automated {Decision-Making}({{{{{SADM}}}}})", "author": "S Kaushik"}, {"title": "HOLMES: a platform for detecting malicious inputs in secure collaborative computation", "author": "W Chen"}, {"title": "Descriptive AI Ethics: Collecting and Understanding the Public Opinion", "author": "G Lima"}, {"title": "Marrying fairness and explainability in supervised learning", "author": "PA Grabowicz"}, {"title": "Mitigating algorithmic bias in Artificial Intelligence systems", "author": "J Fyrvald"}, {"title": "The Tension Between Information Justice and Security: Perceptions of Facial Recognition Targeting.", "author": "EA Watkins"}, {"title": "On the Relationship Between Explanations, Fairness Perceptions, and Decisions", "author": "J Schoeffer"}, {"title": "The Algorithmic Imprint", "author": "U Ehsan", "abstract": "Put differently, algorithmic consequences extend well beyond the algorithm's \u201clifetime\u201d (  of algorithmic imprint broadens the boundaries of what is typically considered for algorithmic", "keywords": {"keywords": ["algorithmic imprint", "consequences", "lifetime", "boundaries", "algorithmic"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "How good is good enough? Quantifying the impact of benefits, accuracy, and privacy on willingness to adopt COVID-19 decision aids", "author": "G Kaptchuk"}, {"title": "Blindspots in Python and Java APIs Result in Vulnerable Code", "author": "Y Brun"}, {"title": "The effects of situational and individual factors on algorithm acceptance in covid-19-related decision-making: A preregistered online experiment", "author": "S Utz"}, {"title": "AI-Fairness Towards Activity Recognition of Older Adults", "author": "MAU Alam"}, {"title": "Biases as Values: Evaluating Algorithms in Context", "author": "M D\u00edaz"}, {"title": "Mitigating Bias in Algorithmic Systems: A Fish-Eye View of Problems and Solutions Across Domains", "author": "K Orphanou"}, {"title": "Fairness perceptions of artificial intelligence decision-making", "author": "L JIANG"}, {"title": "Fairness of Machine Learning Algorithms in Demography", "author": "IC Emmanuel"}, {"title": "Exploring the roles of trust and social group preference on the legitimacy of algorithmic decision-making vs. human decision-making for allocating COVID-19 \u2026", "author": "M L\u00fcnich"}, {"title": "On Learning and Enforcing Latent Assessment Models using Binary Feedback from Human Auditors Regarding Black-Box Classifiers", "author": "M Telukunta", "abstract": "Algorithmic fairness literature presents numerous mathematical notions and metrics, and also points to a tradeoff between them while satisficing some or all of them simultaneously. Furthermore, the contextual nature of fairness notions makes it difficult to automate bias evaluation in diverse algorithmic systems. Therefore, in this paper, we propose a novel model called latent assessment model (LAM) to characterize binary feedback provided by human auditors, by assuming that the auditor compares the classifier's output to his or her", "keywords": {"keywords": ["algorithmic fairness", "latent assessment model", "black-box classifiers", "binary feedback", "human auditors"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Practical Algorithms for Resource Allocation and Decision Making", "author": "DC McElfresh"}, {"title": "Advancing Explainability and Fairness in AI with Human-Algorithm Collaborations", "author": "HF Cheng"}, {"title": "Machine Learning in Artificial Intelligence", "author": "A Chakraborty"}, {"title": "Enhancing Fairness Perception\u2013Towards Human-Centred AI and Personalized Explanations Understanding the Factors Influencing Laypeople's Fairness Perceptions \u2026", "author": "A Shulner-Tal"}, {"title": "Fair Governance with Humans and Machines", "author": "Y Hermstr\u00fcwer"}, {"title": "The effects of situational and individual factors on algorithm preference in COVID-19 related decision-making: a preregistered online experiment", "author": "S Utz"}, {"title": "Do Americans Think the Digital Economy is Fair? Using Supervised Learning to Explore Evaluations of Predictive Automation", "author": "E Lehoucq"}, {"title": "Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives", "author": "A Peng"}, {"title": "Tools for responsible decision-making in machine learning", "author": "B Rastegarpanah"}, {"title": "Columbia University Doctoral program in Communications New York, NY 10024 EAW2198@ Columbia. edu", "author": "EA Watkins", "abstract": "The rise of biometric security changes how users make decisions about their privacy. As passwords give way to faces and fingerprints, the algorithmic nature of these processes creates new cognitive labor for users. When biometrics are used in spaces of algorithmic management, workers must negotiate tradeoffs between security, privacy, fairness, and their livelihood. A mixed-methods, human-centered research design paired with theory frameworks from algorithmic management, usable security, and algorithmic", "keywords": {"keywords": ["biometric security", "privacy", "algorithmic nature", "cognitive labor", "algorithmic management"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Communications"], "content": ["Application"], "application area": ["Usable security"]}}, {"title": "AI and Gender: Four Proposals for", "author": "C Collett"}, {"title": "Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation", "author": "A Bringas Colmenarejo"}, {"title": "Governing Artificial Intelligence in Post-Pandemic Society", "author": "A Arunagiri"}, {"title": "Investigating Perceptions of AI-Based Decision Making in Student Success Prediction", "author": "F Afrin"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "Towards Supporting and Documenting Algorithmic Fairness in the Data Science Workflow", "author": "G Harrison"}, {"title": "Investigating Explanations that Target Training Data.", "author": "AI Anik"}, {"title": "Social impacts of algorithmic decision-making: A research agenda for the social sciences", "author": "F Gerdon"}, {"title": "Decision-Making with Artificial Intelligence in the Social Context Responsibility, Accountability and Public Perception with Examples from the Banking Industry", "author": "J Saffenreuther"}, {"title": "How Accurate Does It Feel?\u2013Human Perception of Different Types of Classification Mistakes", "author": "A Papenmeier", "abstract": "Supervised machine learning utilizes large datasets, often with ground truth labels annotated by humans. While some data points are easy to classify, others are hard to classify, which reduces the inter-annotator agreement. This causes noise for the classifier and might affect the user's perception of the classifier's performance. In our research, we investigated whether the classification difficulty of a data point influences how strongly a prediction mistake reduces the \u201cperceived accuracy\u201d. In an experimental online study, 225"}, {"title": "Criteria for algorithmic fairness metric selection under different supervised classification scenarios", "author": "C Breger"}, {"title": "Exploring Diversity and Fairness in Machine Learning", "author": "C Schumann"}, {"title": "\u201cIt's not wrong, but I'm quite disappointed\u201d: Toward an Inclusive Algorithmic Experience for Content Creators with Disabilities", "author": "D Choi"}, {"title": "Integrating Behavioral, Economic, and Technical Insights to Understand and Address Algorithmic Bias: A Human-Centric Perspective", "author": "G Adomavicius"}, {"title": "Mitigating Bias in Algorithmic Systems-A Fish-Eye View", "author": "K Orphanou"}, {"title": "The role of explainable AI in the research field of AI ethics: systematic mapping study", "author": "H Vainio-Pekka"}, {"title": "Image fairness in deep learning: problems, models, and challenges", "author": "H Tian"}, {"title": "Examination of Fairness in Scheduling Tasks with Heterogeneous Resources", "author": "S Erd\u0151s"}, {"title": "Non-Comparative Fairness for Human-Auditing and Its Relation to Traditional Fairness Notions", "author": "M Telukunta"}, {"title": "Understanding Group Dynamics to Identify Sources of Bias", "author": "R Lakshmanan", "abstract": "As algorithms are increasingly used to make decisions that can have a significant impact on human lives and society at large in a variety of ways, concerns about fairness of algorithmic decision making systems have been raised. While most of the current research has been focussed towards fairness definitions and formulating fairness metrics, this paper aims to identify sources of bias by showing how humans' subconscious tendency to categorise information leads to polarisation, in turn altering judgement and leading to biased outcomes", "keywords": {"keywords": ["bias", "group dynamics", "algorithmic decision making", "fairness", "polarisation"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["sociology"], "content": ["application"], "application area": ["algorithmic decision making"]}}, {"title": "On the Explanation of AI-Based Student Success Prediction", "author": "F Afrin"}, {"title": "\u201cI'm not sure what difference is between their content and mine, other than the person itself\u201d: A Study of Fairness Perception of Content Moderation on YouTube", "author": "R MA"}, {"title": "Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits", "author": "WH Deng"}, {"title": "What's the Appeal? Perceptions of Review Processes for Algorithmic Decisions", "author": "H Lyons"}, {"title": "On the Identification of Fair Auditors to Evaluate Recommender Systems based on a Novel Non-Comparative Fairness Notion", "author": "M Telukunta"}, {"title": "Intersectional Experiences of Unfair Treatment Caused by Automated Computational Systems", "author": "T van Nuenen"}, {"title": "The Landscape of Artificial Intelligence Ethics: Analysis of Developments, Challenges, and Comparison of Different Markets", "author": "S Natrup"}, {"title": "Educating Computer Science Students about Algorithmic Fairness, Accountability, Transparency and Ethics", "author": "M Kasinidou"}, {"title": "Predicting Angiographic Disease Status: Drawing the line between demographically decoupled and jointly trained models", "author": "A Balashankar"}, {"title": "Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation", "author": "AB Colmenarejo", "abstract": "With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI", "keywords": {"keywords": ["fairness", "discrimination", "AI regulation", "European Values", "interdisciplinary perspective"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["Application"], "application area": ["AI regulation"]}}, {"title": "Involve Humans in Algorithmic Fairness Issue: A Systematic Review", "author": "D Wu"}, {"title": "How Does Predictive Information Affect Human Ethical Preferences?", "author": "S Narayanan"}, {"title": "\u201cLook! It'sa Computer Program! It's an Algorithm! It's AI!\u201d: Does Terminology Affect Human Perceptions and Evaluations of Algorithmic Decision-Making Systems?", "author": "M Langer"}, {"title": "A Framework for Fairer Machine Learning in Organizations", "author": "L Morse"}, {"title": "Unfairness towards subjective opinions in Machine Learning", "author": "A Balayn"}, {"title": "Towards Explainability for AI Fairness", "author": "J Zhou"}, {"title": "Contextualized exploratory programming using webpages as substrates", "author": "X Zhang"}, {"title": "How Is Socially Responsible Academic Performance Prediction Possible?: Insights From a Concept of Perceived AI Fairness", "author": "B Keller"}, {"title": "Eliminating Disparate Impact in MCDM: The case of TOPSIS", "author": "S Radovanovi\u0107"}, {"title": "Modeling Human Behavior Part II--Cognitive approaches and Uncertainty", "author": "A Fuchs", "abstract": "As we discussed in Part I of this topic, there is a clear desire to model and comprehend human behavior. Given the popular presupposition of human reasoning as the standard for learning and decision-making, there have been significant efforts and a growing trend in research to replicate these innate human abilities in artificial systems. In Part I, we discussed learning methods which generate a model of behavior from exploration of the system and feedback based on the exhibited behavior as well as topics relating to the use of or", "keywords": {"keywords": ["modeling", "human behavior", "cognitive approaches", "uncertainty"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["artificial intelligence"]}}, {"title": "How Different Groups Prioritize Ethical Values for Responsible AI", "author": "M Jakesch"}, {"title": "Interpretable Approaches to Opening Up Black-box Models", "author": "HFS Tan"}, {"title": "\" There Is Not Enough Information\": On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making", "author": "J Schoeffer"}, {"title": "\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u7684\u516c\u5e73\u611f\u77e5", "author": "\u848b\u8def\u8fdc\uff0c \u66f9\u674e\u6885\uff0c \u79e6\u6615\uff0c \u8c2d\u73b2\uff0c \u9648\u6668\uff0c \u5f6d\u5c0f\u6590"}, {"title": "\uc778\uacf5\uc9c0\ub2a5 \uc54c\uace0\ub9ac\uc998 \uae30\ubc18 \uc758\uc0ac\uacb0\uc815\uc758 \uacf5\uc815\uc131 \uc9c0\uac01", "author": "\uc190\uc601\uc2e0"}], "evaluating explainable ai: which algorithmic explanations help users predict model behavior?": [{"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal", "abstract": "Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with", "keywords": {"keywords": ["explainable AI", "team performance", "complementary performance", "decision-making tasks", "user studies"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer Science", "content": "Application", "application area": "Human-AI team performance on decision-making tasks"}}, {"title": "How can i explain this to you? an empirical study of deep neural network explanation methods", "author": "JV Jeyakumar"}, {"title": "Evaluating Explanations: How much do explanations from the teacher aid students?", "author": "D Pruthi"}, {"title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models", "author": "T Wu"}, {"title": "Aligning faithful interpretations with their social attribution", "author": "A Jacovi"}, {"title": "Benchmarking and survey of explanation methods for black box models", "author": "F Bodria"}, {"title": "Four principles of explainable artificial intelligence", "author": "PJ Phillips"}, {"title": "Measuring association between labels and free-text rationales", "author": "S Wiegreffe"}, {"title": "Deep learning in electron microscopy", "author": "JM Ede"}, {"title": "An update on drug-induced pigmentation", "author": "AF Nahhas"}, {"title": "Artificial intelligence and systemic risk", "author": "J Danielsson"}, {"title": "Explainable deep learning: A field guide for the uninitiated", "author": "G Ras"}, {"title": "Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language?", "author": "P Hase"}, {"title": "Coloring molecules with explainable artificial intelligence for preclinical relevance assessment", "author": "J Jim\u00e9nez-Luna"}, {"title": "An engineer's guide to eXplainable Artificial Intelligence and Interpretable Machine Learning: Navigating causality, forced goodness, and the false perception of \u2026", "author": "MZ Naser"}, {"title": "Fastif: Scalable influence functions for efficient model interpretation and debugging", "author": "H Guo"}, {"title": "Selfexplain: A self-explaining architecture for neural text classifiers", "author": "D Rajagopal"}, {"title": "Polyjuice: Automated, general-purpose counterfactual generation", "author": "T Wu"}, {"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer"}, {"title": "To what extent do human explanations of model behavior align with actual model behavior?", "author": "G Prasad"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations", "author": "S Arora"}, {"title": "Learning to rationalize for nonmonotonic reasoning with distant supervision", "author": "F Brahman"}, {"title": "Human evaluation of spoken vs. visual explanations for open-domain qa", "author": "AV Gonzalez"}, {"title": "Explainable artificial intelligence for human decision support system in the medical domain", "author": "S Knapi\u010d"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen"}, {"title": "The out-of-distribution problem in explainability and search methods for feature importance explanations", "author": "P Hase"}, {"title": "Explainable machine learning with prior knowledge: an overview", "author": "K Beckh"}, {"title": "On the faithfulness measurements for model interpretations", "author": "F Yin"}, {"title": "Sequential explanations with mental model-based policies", "author": "A Yeung"}, {"title": "On the lack of robust interpretability of neural text classifiers", "author": "MB Zafar"}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta"}, {"title": "Probing multimodal embeddings for linguistic properties: the visual-semantic case", "author": "AD Lindstr\u00f6m"}, {"title": "On the interaction of belief bias and explanations", "author": "AV Gonzalez"}, {"title": "Explainable multi-class classification of the camh covid-19 mental health data", "author": "YZ Hu"}, {"title": "Quantus: an explainable AI toolkit for responsible evaluation of neural network explanations", "author": "A Hedstr\u00f6m"}, {"title": "Connecting attributions and QA model behavior on realistic counterfactuals", "author": "X Ye"}, {"title": "Explainable Goal-Driven Agents and Robots--A Comprehensive Review", "author": "F Sado"}, {"title": "Quality metrics for transparent machine learning with and without humans in the loop are not correlated", "author": "F Biessmann"}, {"title": "Unirex: A unified learning framework for language model rationale extraction", "author": "A Chan"}, {"title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization", "author": "J Borowski"}, {"title": "Explainable sentiment analysis application for social media crisis management in retail", "author": "D Cirqueira"}, {"title": "What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods", "author": "T Fel"}, {"title": "Do explanations help users detect errors in open-domain QA? an evaluation of spoken vs. visual explanations", "author": "AV Gonz\u00e1lez"}, {"title": "Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools", "author": "N Feldhus"}, {"title": "Natural Images are More Informative for Interpreting CNN Activations than State-of-the-Art Synthetic Feature Visualizations", "author": "J Borowski"}, {"title": "Benchmarking Post-Hoc Interpretability Approaches for Transformer-based Misogyny Detection", "author": "G Attanasio"}, {"title": "A review of explainable and interpretable AI with applications in COVID\u201019 imaging", "author": "JD Fuhrman"}, {"title": "Hive: evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "Towards a rigorous evaluation of explainability for multivariate time series", "author": "R Saluja", "abstract": "Machine learning-based systems are rapidly gaining popularity and in-line with that there has been a huge research surge in the field of explainability to ensure that machine learning models are reliable, fair, and can be held liable for their decision-making process. Explainable Artificial Intelligence (XAI) methods are typically deployed to debug black-box machine learning models but in comparison to tabular, text, and image data, explainability in time series is still relatively unexplored. The aim of this study was to achieve and evaluate", "keywords": {"keywords": ["explanability", "multivariate time series", "machine learning", "explainable artificial intelligence", "reliability"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}, {"title": "DIVINE: Diverse Influential Training Points for Data Visualization and Model Refinement", "author": "U Bhatt"}, {"title": "Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?", "author": "W Jin"}, {"title": "Counterfactual Evaluation for Explainable AI", "author": "Y Ge"}, {"title": "Teaching humans when to defer to a classifier via exemplars", "author": "H Mozannar"}, {"title": "Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes", "author": "K Xie"}, {"title": "Guidelines and evaluation for clinical explainable AI on medical image analysis", "author": "W Jin"}, {"title": "Can Explanations Be Useful for Calibrating Black Box Models?", "author": "X Ye"}, {"title": "Bounded logit attention: Learning to explain image classifiers", "author": "T Baumhauer"}, {"title": "Improving explainable recommendations by deep review-based explanations", "author": "S Ouyang"}, {"title": "ExSum: From Local Explanations to Model Understanding", "author": "Y Zhou"}, {"title": "Explainable NLP for Human-AI Collaboration", "author": "P Lertvittayakumjorn"}, {"title": "Natural Language Deduction through Search over Statement Compositions", "author": "K Bostrom"}, {"title": "Exploring the Role of Local and Global Explanations in Recommender Systems", "author": "M Radensky"}, {"title": "Adapting language complexity for ai-based assistance", "author": "M Zhao"}, {"title": "Evaluating feature attribution: An information-theoretic perspective", "author": "Y Rong"}, {"title": "Simulated User Studies for Explanation Evaluation", "author": "V Chen"}, {"title": "\" Ndsm Risk\" Primer of the Canadian Financial Institutions and Insurance Sector", "author": "M Afreen"}, {"title": "Mediators: Conversational Agents Explaining NLP Model Behavior", "author": "N Feldhus"}, {"title": "ADOPS: Aspect Discovery OPinion Summarisation Methodology based on deep learning and subgroup discovery for generating explainable opinion summaries", "author": "M L\u00f3pez"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Increasing the Trustworthiness ofAI-based In-Vehicle IDS usingeXplainable AI", "author": "H Lundberg"}, {"title": "Explorative Study of Explainable Artificial Intelligence Techniques for Sentiment Analysis Applied for English Language", "author": "RK Rathore"}, {"title": "When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration", "author": "H VASCONCELOS"}, {"title": "Explainable Predictive Process Monitoring: A User Evaluation", "author": "W Rizzi"}, {"title": "Learning to Scaffold: Optimizing Model Explanations for Teaching", "author": "P Fernandes"}, {"title": "On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods", "author": "K Amarasinghe"}, {"title": "A Human-Centric Assessment Framework for AI", "author": "S Saralajew"}, {"title": "A psychological theory of explainability", "author": "SCH Yang"}, {"title": "The Role of Human Knowledge in Explainable AI", "author": "A Tocchetti"}, {"title": "How Well do Feature Visualizations Support Causal Understanding of CNN Activations?", "author": "RS Zimmermann"}, {"title": "Justifying Short-Term Load Forecasts Obtained with the Use of Neural Models", "author": "TA Grzeszczyk"}, {"title": "Towards Human-Centered Natural Language Processing", "author": "AV Gonzalez"}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "Use-Case-Grounded Simulations for Explanation Evaluation", "author": "V Chen"}, {"title": "Evolution of Machine Learning in Tuberculosis Diagnosis: A Review of Deep Learning-Based Medical Applications", "author": "M Singh"}, {"title": "More Than Words: Towards Better Quality Interpretations of Text Classifiers", "author": "MB Zafar"}, {"title": "On the Sensitivity and Stability of Model Interpretations in NLP", "author": "F Yin"}, {"title": "Principles and Interactive Tools for Evaluating and Improving the Behavior of Natural Language Processing models", "author": "T Wu"}, {"title": "Perception Visualization: Seeing Through the Eyes of a DNN", "author": "L Giulivi"}, {"title": "Human-in-the-loop Approach towards Dual Process AI Decisions", "author": "H Uchida"}, {"title": "A survey on artificial intelligence in histopathology image analysis", "author": "MM Abdelsamea"}, {"title": "Concept-level Debugging of Part-Prototype Networks", "author": "A Bontempelli"}, {"title": "CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior", "author": "ED Abraham"}, {"title": "Methods for the inspection of learning machines", "author": "R Schwarzenberg"}, {"title": "HAEM: Obtaining Higher-Quality Classification Task Results with AI Workers", "author": "Y Yamashita"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimization", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "Do not explain without context: addressing the blind spot of model explanations", "author": "K Wo\u017anica"}, {"title": "Robust and Interpretable Predictions for Multimodal Sensor Systems", "author": "JV Jeyakumar"}, {"title": "Explainable Artificial Intelligence: Evaluating the Objective and Subjective Impacts of xAI on Human-Agent Interaction", "author": "A Silva", "abstract": "Intelligent agents must be able to communicate intentions and explain their decision-making processes to build trust, foster confidence, and improve human-agent team dynamics. Recognizing this need, academia and industry are rapidly proposing new ideas, methods, and frameworks to aid in the design of more explainable AI. Yet, there remains no standardized metric or experimental protocol for benchmarking new methods, leaving researchers to rely on their own intuition or ad hoc methods for assessing new concepts. In", "keywords": {"keywords": ["Explainable Artificial Intelligence", "Human-Agent Interaction", "Trust", "Decision-making processes", "Human-Agent team dynamics"], "research type": ["Review"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Artificial Intelligence"]}}, {"title": "ProtoTEx: Explaining Model Decisions with Prototype Tensors", "author": "A Das"}, {"title": "Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI", "author": "QV Liao"}, {"title": "Explainability in Automated Training and Feedback Systems", "author": "PSB Rao"}, {"title": "Evaluating human understanding in XAI systems", "author": "D Gentile"}, {"title": "Generating Instructive Questions from Multiple Articles to Guide Reading in E-Bibliotherapy", "author": "Y Xin"}, {"title": "Model Distillation for Faithful Explanations of Medical Code Predictions", "author": "Z Wood-Doughty"}, {"title": "Deep Learning in Electron Microscopy", "author": "M Learning"}, {"title": "Explainable Artificial Intelligence in Cybersecurity: A Brief Review", "author": "S Hariharan"}, {"title": "Minun: evaluating counterfactual explanations for entity matching", "author": "J Wang"}, {"title": "Explaining the Road Not Taken", "author": "H Shen"}, {"title": "Towards ML Methods for Biodiversity: A Novel Wild Bee Dataset and Evaluations of XAI Methods for ML-Assisted Rare Species Annotations", "author": "T Chiaburu"}, {"title": "Lifelong Explainer for Lifelong Learners", "author": "X Situ"}, {"title": "Investigating the Benefits of Free-Form Rationales", "author": "J Sun"}, {"title": "Explainable AI foundations to support human-robot teaching and learning", "author": "SL Booth"}, {"title": "Syntax and prejudice: ethically-charged biases of a syntax-based hate speech recognizer unveiled", "author": "M Mastromattei"}, {"title": "Building Human Values into Recommender Systems: An Interdisciplinary Synthesis", "author": "J Stray"}, {"title": "ferret: a Framework for Benchmarking Explainers on Transformers", "author": "G Attanasio"}, {"title": "Common Pitfalls When Explaining AI and Why Mechanistic Explanation Is a Hard Problem", "author": "DC Elton"}, {"title": "Metrics and Evaluations of Time Series Explanations: An Application in Affect Computing", "author": "N Fouladgar"}, {"title": "Evaluating Explanation Correctness in Legal Decision Making", "author": "CF Luo"}, {"title": "Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language", "author": "R Sevastjanova"}, {"title": "FRAME: Evaluating Simulatability Metrics for Free-Text Rationales", "author": "A Chan"}, {"title": "Interpreting Multivariate Time Series for an Organization Health Platform", "author": "R Saluja"}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}], "on completeness-aware concept-based explanations in deep neural networks": [{"title": "Opportunities and challenges in explainable artificial intelligence (xai): A survey", "author": "A Das", "abstract": "Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human", "keywords": {"keywords": ["explainable artificial intelligence", "xai", "deep neural networks", "mission critical systems", "ethical and judicial concerns"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Concept whitening for interpretable image recognition", "author": "Z Chen"}, {"title": "Benchmarking and survey of explanation methods for black box models", "author": "F Bodria"}, {"title": "Now you see me (CME): concept-based model extraction", "author": "D Kazhdan"}, {"title": "Selfexplain: A self-explaining architecture for neural text classifiers", "author": "D Rajagopal"}, {"title": "Leveraging sparse linear layers for debuggable deep networks", "author": "E Wong"}, {"title": "Entropy-based logic explanations of neural networks", "author": "P Barbiero"}, {"title": "Illuminating the black box: interpreting deep neural network models for psychiatric research", "author": "Y Sheu"}, {"title": "3db: A framework for debugging computer vision models", "author": "G Leclerc"}, {"title": "Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification", "author": "D Rymarczyk"}, {"title": "XAI method properties: A (meta-) study", "author": "G Schwalbe", "abstract": "state-of-research. In a structured literature analysis and meta-study we identified and reviewed  more than 50 of the most cited and current surveys on XAI methods, metrics, and method", "keywords": {"keywords": ["XAI methods", "meta-study", "literature analysis", "surveys", "metrics"], "research type": ["meta-analysis"], "methodology": ["quantitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["explainable artificial intelligence"]}}, {"title": "Debiasing concept-based explanations with causal analysis", "author": "MT Bahadori"}, {"title": "Protopshare: Prototype sharing for interpretable image classification and similarity discovery", "author": "D Rymarczyk"}, {"title": "Is disentanglement all you need? comparing concept-based & disentanglement approaches", "author": "D Kazhdan"}, {"title": "Cause and effect: Concept-based explanation of neural networks", "author": "MN Zaeem"}, {"title": "A framework to learn with interpretation", "author": "J Parekh"}, {"title": "SoK: Machine learning governance", "author": "V Chandrasekaran"}, {"title": "Model-based counterfactual synthesizer for interpretation", "author": "F Yang"}, {"title": "Practical machine learning safety: A survey and primer", "author": "S Mohseni"}, {"title": "Adversarial TCAV--Robust and Effective Interpretation of Intermediate Layers in Neural Networks", "author": "R Soni"}, {"title": "Abstracting deep neural networks into concept graphs for concept level interpretability", "author": "A Kori", "abstract": "The black-box nature of deep learning models prevents them from being completely trusted in domains like biomedicine. Most explainability techniques do not capture the concept-based reasoning that human beings follow. In this work, we attempt to understand the behavior of trained models that perform image processing tasks in the medical domain by building a graphical representation of the concepts they learn. Extracting such a graphical representation of the model's behavior on an abstract, higher conceptual level would", "keywords": {"keywords": ["deep neural networks", "concept graphs", "interpretability", "biomedicine", "image processing"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Medical image processing"]}}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta"}, {"title": "Concept-based model explanations for electronic health records", "author": "D Mincu"}, {"title": "Pace: Posthoc architecture-agnostic concept extractor for explaining cnns", "author": "V Kamakshi"}, {"title": "Connecting attributions and QA model behavior on realistic counterfactuals", "author": "X Ye"}, {"title": "Explaining deep neural networks using unsupervised clustering", "author": "Y Liu"}, {"title": "Achievements and challenges in explaining deep learning based computer-aided diagnosis systems", "author": "A Lucieri"}, {"title": "Comprehensible convolutional neural networks via guided concept learning", "author": "S Wickramanayake"}, {"title": "Domino: Discovering systematic errors with cross-modal embeddings", "author": "S Eyuboglu"}, {"title": "Neuro-symbolic artificial intelligence", "author": "MK Sarker"}, {"title": "Interpreting Deep Neural Networks through Prototype Factorization", "author": "S Das", "abstract": "Typical deep neural networks (DNNs) are complex black-box models and their decision making process can be difficult to comprehend even for experienced machine learning practitioners. Therefore their use could be limited in mission-critical scenarios despite state-of-the-art performance on many challenging ML tasks. Through this work, we empower users to interpret DNNs with a post-hoc analysis protocol. We propose ProtoFac, an explainable matrix factorization technique that decomposes the latent representations at any", "keywords": {"keywords": ["deep neural networks", "interpretation", "prototype factorization", "explainable AI", "post-hoc analysis"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "Machine Learning"}}, {"title": "The Definitions of Interpretability and Learning of Interpretable Models", "author": "W Pan"}, {"title": "Corpus-level and concept-based explanations for interpretable document classification", "author": "T Shi"}, {"title": "Mace: Model agnostic concept extractor for explaining image classification networks", "author": "A Kumar"}, {"title": "ProtoMIL: Multiple instance learning with prototypical parts for fine-grained interpretability", "author": "D Rymarczyk"}, {"title": "Explainable Machine Learning for Breakdown Prediction in High Gradient RF Cavities", "author": "C Obermair"}, {"title": "Verification of size invariance in DNN activations using concept embeddings", "author": "G Schwalbe"}, {"title": "Interpreting deep neural networks for medical imaging using concept graphs", "author": "A Kori"}, {"title": "Toward a Unified Framework for Debugging Gray-box Models", "author": "A Bontempelli"}, {"title": "Instance-wise or Class-wise? A Tale of Neighbor Shapley for Concept-based Explanation", "author": "J Li"}, {"title": "AR-BERT: Aspect-relation enhanced Aspect-level Sentiment Classification with Multi-modal Explanations", "author": "SM Islam", "abstract": "Aspect level sentiment classification (ALSC) is a difficult problem with state-of-the-art models showing less than 80% macro-F1 score on benchmark datasets. Existing models do not incorporate information on aspect-aspect relations in knowledge graphs (KGs), eg DBpedia. Two main challenges stem from inaccurate disambiguation of aspects to KG entities, and the inability to learn aspect representations from the large KGs in joint training with ALSC models. We propose AR-BERT, a novel two-level global-local entity embedding scheme that", "keywords": {"keywords": ["Aspect-level sentiment classification", "Aspect-aspect relations", "Knowledge graphs", "ALSC models", "Entity embedding"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Natural Language Processing"]}}, {"title": "Auditing ai models for verified deployment under semantic specifications", "author": "H Bharadhwaj"}, {"title": "Sparse Subspace Clustering for Concept Discovery (SSCCD)", "author": "J Vielhaben"}, {"title": "Leveraging PDDL to Make Inscrutable Agents Interpretable: A Case for Post Hoc Symbolic Explanations for Sequential-Decision Making Problems", "author": "S Sreedharan"}, {"title": "Algorithmic Concept-Based Explainable Reasoning", "author": "D Georgiev"}, {"title": "Post hoc explanations may be ineffective for detecting unknown spurious correlation", "author": "J Adebayo"}, {"title": "Automatic Concept Extraction for Concept Bottleneck-based Video Classification", "author": "JV Jeyakumar"}, {"title": "Cause and effect: Concept-based explanation of neural networks", "author": "M Nokhbeh Zaeem"}, {"title": "Threading the Needle of On and Off-Manifold Value Functions for Shapley Explanations", "author": "CK Yeh"}, {"title": "From heatmaps to structured explanations of image classifiers", "author": "L Fuxin"}, {"title": "Human-Centered Concept Explanations for Neural Networks", "author": "P Hitzler", "abstract": "Deep neural networks have been instrumental in many modern Artificial Intelligence (AI) successes, with super-human performance across application areas such as vision recognition, speech recognition, and language understanding [1, 2, 3]. As the performance and complexity of deep neural networks improve, understanding how they operate has also become increasingly difficult. This has led to the burgeoning research area of explainable AI (XAI), which provides tools than enable us to better understand an AI model. The initial set of", "keywords": {"keywords": ["neural networks", "explainable AI", "human-centered", "concept explanations", "deep learning"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Interpretable image classification with differentiable prototypes assignment", "author": "D Rymarczyk"}, {"title": "Semantic Bottlenecks: Quantifying and Improving Inspectability of Deep Representations", "author": "M Losch"}, {"title": "Multiview Concept Bottleneck Models Applied to Diagnosing Pediatric Appendicitis", "author": "U Klimiene"}, {"title": "Unsupervised Causal Binary Concepts Discovery with VAE for Black-box Model Explanation", "author": "TQ Tran"}, {"title": "CACTUS: Detecting and Resolving Conflicts in Objective Functions", "author": "S Das"}, {"title": "What Do Navigation Agents Learn About Their Environment?", "author": "K Dwivedi"}, {"title": "Attention-based Interpretability with Concept Transformers", "author": "M Rigotti"}, {"title": "Learnable Visual Words for Interpretable Image Recognition", "author": "W Xiao"}, {"title": "Measuring the interpretability of unsupervised representations via quantized reversed probing", "author": "I Laina"}, {"title": "Disentangling Embedding Spaces with Minimal Distributional Assumptions", "author": "T Leemann", "abstract": "Interest in understanding and factorizing learned embedding spaces is growing. For instance, recent concept-based explanation techniques analyze a machine learning model in terms of interpretable latent components. Such components have to be discovered in the model's embedding space, eg, through independent component analysis (ICA) or modern disentanglement learning techniques. While these unsupervised approaches offer a sound formal framework, they either require access to a data generating function or impose rigid", "keywords": {"keywords": ["embedding spaces", "interpretable latent components", "disentanglement learning techniques", "unsupervised approaches", "concept-based explanation techniques"], "research type": ["theoretical"], "methodology": ["quantitative"], "purpose": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["machine learning"]}}, {"title": "Looking into Black Boxes: Techniques for Interpreting Neural Networks", "author": "X Jin"}, {"title": "Provable concept learning for interpretable predictions using variational inference", "author": "A Taeb"}, {"title": "Empowering users to communicate their preferences to machine learning models in Visual Analytics", "author": "S Das"}, {"title": "Encoding Concepts in Graph Neural Networks", "author": "LC Magister"}, {"title": "Ontology-Driven Semantic Alignment of Artificial Neurons and Visual Concepts", "author": "R Massidda"}, {"title": "ConceptEvo: Interpreting Concept Evolution in Deep Learning Training", "author": "H Park"}, {"title": "CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior", "author": "ED Abraham"}, {"title": "On the Two-fold Role of Logic Constraints in Deep Learning", "author": "G Ciravegna"}, {"title": "Towards Automating Model Explanations with Certified Robustness Guarantees", "author": "M Huai"}, {"title": "Xplique: A Deep Learning Explainability Toolbox", "author": "T Fel", "abstract": "Today's most advanced machine-learning models are hardly scrutable. The key challenge for explainability methods is to help assisting researchers in opening up these black boxes, by revealing the strategy that led to a given decision, by characterizing their internal states or by studying the underlying data representation. To address this challenge, we have developed Xplique: a software library for explainability which includes representative explainability methods as well as associated evaluation metrics. It interfaces with one of the", "keywords": {"keywords": ["explainability", "deep learning", "toolbox", "scrutability", "machine learning"], "research type": ["review"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning explainability"]}}, {"title": "Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations", "author": "W Stammer"}, {"title": "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis", "author": "S Omidshafiei"}, {"title": "A Framework for Learning Ante-hoc Explainable Models via Concepts", "author": "A Sarkar"}, {"title": "Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF", "author": "J Parekh"}, {"title": "Robust and Interpretable Predictions for Multimodal Sensor Systems", "author": "JV Jeyakumar"}, {"title": "Faith-Shap: The Faithful Shapley Shapley Interaction Index", "author": "CP Tsai"}, {"title": "Post-hoc Concept Bottleneck Models", "author": "M Yuksekgonul"}, {"title": "Validating Automatic Concept-Based Explanations for AI-Based Digital Histopathology", "author": "D Sauter"}, {"title": "Explainable Supervised Domain Adaptation", "author": "V Kamakshi"}, {"title": "Towards Concept-based Interpretability of Pre-miRNA Detection using Convolutional Neural Networks", "author": "I van den Brandt", "abstract": "Precursor microRNA (pre-miRNA) sequences are the precursors of microRNAs (miRNAs), which are non-coding RNA sequences regulating gene expression in organisms. Deregulated miRNAs in humans are linked to various diseases, such as cancer and Alzheimer's disease. Given their important role in organisms, domain experts focus on discovering new sequences and researching their functioning. Interestingly, precursor microRNAs can aid the discovery of microRNAs. Also, computational methods have proven", "keywords": {"keywords": ["pre-miRNA", "miRNA", "Convolutional Neural Networks", "interpretability", "detection"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Biology"], "content": ["Technical"], "application area": ["Genetics"]}}, {"title": "Concept-based Explanations for Out-Of-Distribution Detectors", "author": "J Choi"}, {"title": "Interpretable by Design: Learning Predictors by Composing Interpretable Queries", "author": "A Chattopadhyay"}, {"title": "Learning Unsupervised Hierarchies of Audio Concepts", "author": "D Afchar"}, {"title": "Interpretable Deep Learning: Beyond Feature-Importance with Concept-based Explanations", "author": "B Dimanov"}, {"title": "Representation Learning for Treatment Effect Estimation", "author": "L Yao"}, {"title": "The Manifold Hypothesis for Gradient-Based Explanations", "author": "S Bordt"}, {"title": "Concept Embedding Analysis: A Review", "author": "G Schwalbe"}, {"title": "Defining Explanation and Explanatory Depth in XAI", "author": "S Buijsman"}, {"title": "ConceptExplainer: Understanding the Mental Model of Deep Learning Algorithms via Interactive Concept-based Explanations", "author": "J Huang"}, {"title": "From\" Where\" to\" What\": Towards Human-Understandable Explanations through Concept Relevance Propagation", "author": "R Achtibat", "abstract": "The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's", "keywords": {"keywords": ["explainable artificial intelligence", "transparent deep learning models", "attribution maps", "global explanation techniques", "concept relevance propagation"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Predicting biochemical recurrence of prostate cancer with artificial intelligence", "author": "H Pinckaers"}, {"title": "Towards Enabling Explanation in Safety-Critical Artificial Intelligence Systems", "author": "A Michel"}, {"title": "Using Expert Interpretation and Reasoning to Guide Model Selection in Machine Learning", "author": "J Wang"}, {"title": "Semantic Bottlenecks: Quantifying and Improving Inspectability of Deep Representations", "author": "Z Akata"}, {"title": "On The Quality Assurance Of Concept-Based Representations", "author": "ME Zarlenga"}, {"title": "Concept-based Abstraction-Aggregation Network for Interpretable Document Classification", "author": "T Shi"}, {"title": "Distilling Model Failures as Directions in Latent Space", "author": "S Jain"}, {"title": "Novel Algorithms for Understanding Online Reviews", "author": "T Shi"}, {"title": "Proto2Proto: Can you recognize the car, the way I do?", "author": "M Keswani"}, {"title": "TOWARDS LEARNING TO EXPLAIN WITH CONCEPT BOTTLENECK MODELS: MITIGATING INFORMATION LEAKAGE", "author": "J Lockhart", "abstract": "Concept bottleneck models perform classification by first predicting which of a list of human provided concepts are true about a datapoint. Then a downstream model uses these predicted concept labels to predict the target label. The predicted concepts act as a rationale for the target prediction. Model trust issues emerge in this paradigm when soft concept labels are used: it has previously been observed that extra information about the data distribution leaks into the concept predictions. In this work we show how Monte-Carlo", "keywords": {"keywords": ["concept bottleneck models", "information leakage", "learning to explain", "Monte-Carlo", "model trust"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Classification"]}}, {"title": "Studying CNN representations through activation dimensionality reduction and visualization", "author": "NS Dey"}, {"title": "Towards learning trustworthily, automatically, and with guarantees on graphs: An overview", "author": "L Oneto"}, {"title": "La# felicidad en Twitter:\u00bf qu\u00e9 representa realmente?", "author": "GB Enguix"}], "towards automatic concept-based explanations": [{"title": "A survey on explainable artificial intelligence (xai): Toward medical xai", "author": "E Tjoa", "abstract": "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This", "keywords": {"keywords": ["artificial intelligence", "explainable artificial intelligence", "medical", "machine learning", "deep learning"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Medical AI"]}}, {"title": "Opportunities and challenges in explainable artificial intelligence (xai): A survey", "author": "A Das"}, {"title": "On the (in) fidelity and sensitivity of explanations", "author": "CK Yeh"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "Concept bottleneck models", "author": "PW Koh"}, {"title": "A survey on neural network interpretability", "author": "Y Zhang"}, {"title": "Concept whitening for interpretable image recognition", "author": "Z Chen"}, {"title": "Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai", "author": "A Jacovi"}, {"title": "On completeness-aware concept-based explanations in deep neural networks", "author": "CK Yeh"}, {"title": "Neuron shapley: Discovering the responsible neurons", "author": "A Ghorbani"}, {"title": "Cocox: Generating conceptual and counterfactual explanations via fault-lines", "author": "A Akula", "abstract": "We present CoCoX (short for Conceptual and Counterfactual Explanations), a model for explaining decisions made by a deep convolutional neural network (CNN). In Cognitive Psychology, the factors (or semantic-level features) that humans zoom in on when they imagine an alternative to a model prediction are often referred to as fault-lines. Motivated by this, our CoCoX model explains decisions made by a CNN using fault-lines. Specifically, given an input image I for which a CNN classification model M predicts class c pred, our fault", "keywords": {"keywords": ["conceptual explanations", "counterfactual explanations", "fault-lines", "convolutional neural network (CNN)", "decision explanations"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image classification"]}}, {"title": "Explainable artificial intelligence: an analytical review", "author": "PP Angelov"}, {"title": "Benchmarking and survey of explanation methods for black box models", "author": "F Bodria"}, {"title": "Explaining the black-box model: A survey of local interpretation methods for deep neural networks", "author": "Y Liang"}, {"title": "Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies", "author": "EM Kenny"}, {"title": "Trustworthy AI in the age of pervasive computing and big data", "author": "A Kumar"}, {"title": "Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations", "author": "W Stammer"}, {"title": "Explaining in style: Training a gan to explain a classifier in stylespace", "author": "O Lang"}, {"title": "On concept-based explanations in deep neural networks", "author": "CK Yeh"}, {"title": "Artificial cognition: How experimental psychology can help generate explainable artificial intelligence", "author": "JET Taylor"}, {"title": "Towards global explanations of convolutional neural networks with concept attribution", "author": "W Wu", "abstract": "With the growing prevalence of convolutional neural networks (CNNs), there is an urgent demand to explain their behaviors. Global explanations contribute to understanding model predictions on a whole category of samples, and thus have attracted increasing interest recently. However, existing methods overwhelmingly conduct separate input attribution or rely on local approximations of models, making them fail to offer faithful global explanations of CNNs. To overcome such drawbacks, we propose a novel two-stage framework, Attacking", "keywords": {"keywords": ["convolutional neural networks", "explanations", "global", "concept attribution", "two-stage framework"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Concept learners for few-shot learning", "author": "K Cao"}, {"title": "On interpretability of deep learning based skin lesion classifiers using concept activation vectors", "author": "A Lucieri"}, {"title": "Causality-based explanation of classification outcomes", "author": "L Bertossi"}, {"title": "A review of recent deep learning approaches in human-centered machine learning", "author": "T Kaluarachchi"}, {"title": "Now you see me (CME): concept-based model extraction", "author": "D Kazhdan"}, {"title": "Interpretability-guided content-based medical image retrieval", "author": "W Silva"}, {"title": "Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations", "author": "S Sreedharan"}, {"title": "Post-hoc interpretability for neural nlp: A survey", "author": "A Madsen"}, {"title": "Leveraging sparse linear layers for debuggable deep networks", "author": "E Wong"}, {"title": "Entropy-based logic explanations of neural networks", "author": "P Barbiero", "abstract": "Explainable artificial intelligence has rapidly emerged since lawmakers have started requiring interpretable models for safety-critical domains. Concept-based neural networks have arisen as explainable-by-design methods as they leverage human-understandable symbols (ie concepts) to predict class memberships. However, most of these approaches focus on the identification of the most relevant concepts but do not provide concise, formal explanations of how such concepts are leveraged by the classifier to make predictions. In", "keywords": {"keywords": ["explainable artificial intelligence", "concept-based neural networks", "entropy-based logic", "formal explanations", "classifier predictions"], "research type": ["case study"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["safety-critical domains"]}}, {"title": "Illuminating the black box: interpreting deep neural network models for psychiatric research", "author": "Y Sheu"}, {"title": "Bayes-trex: a bayesian sampling approach to model transparency by example", "author": "S Booth"}, {"title": "Overinterpretation reveals image classification model pathologies", "author": "B Carter"}, {"title": "Facilitating knowledge sharing from domain experts to data scientists for building nlp models", "author": "S Park"}, {"title": "Mimic-if: Interpretability and fairness evaluation of deep learning models on mimic-iv dataset", "author": "C Meng"}, {"title": "Robust and interpretable convolutional neural networks to detect glaucoma in optical coherence tomography images", "author": "KA Thakoor"}, {"title": "Explaining self-supervised image representations with visual probing", "author": "D Basaj"}, {"title": "Spatial relation learning for explainable image classification and annotation in critical applications", "author": "R Pierrard"}, {"title": "Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification", "author": "D Rymarczyk"}, {"title": "Expressive explanations of DNNs by combining concept analysis with ILP", "author": "J Rabold", "abstract": "Explainable AI has emerged to be a key component for black-box machine learning approaches in domains with a high demand for reliability or transparency. Examples are medical assistant systems, and applications concerned with the General Data Protection Regulation of the European Union, which features transparency as a cornerstone. Such demands require the ability to audit the rationale behind a classifier's decision. While visualizations are the de facto standard of explanations, they come short in terms of", "keywords": {"keywords": ["Explainable AI", "DNNs", "concept analysis", "ILP", "transparency"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["medical assistant systems, General Data Protection Regulation"]}}, {"title": "Promises and pitfalls of black-box concept learning models", "author": "A Mahinpei"}, {"title": "XAI method properties: A (meta-) study", "author": "G Schwalbe"}, {"title": "Explaining ai-based decision support systems using concept localization maps", "author": "A Lucieri"}, {"title": "Transparency of deep neural networks for medical image analysis: A review of interpretability methods", "author": "Z Salahuddin"}, {"title": "IB-M: a flexible framework to align an interpretable model and a black-box model", "author": "M Li"}, {"title": "Rationalization through concepts", "author": "D Antognini"}, {"title": "Symbols as a lingua franca for bridging human-ai chasm for explainable and advisable ai systems", "author": "S Kambhampati"}, {"title": "Acquisition of chess knowledge in alphazero", "author": "T McGrath"}, {"title": "Logic explained networks", "author": "G Ciravegna"}, {"title": "Attack to fool and explain deep networks", "author": "N Akhtar", "abstract": "Deep visual models are susceptible to adversarial perturbations to inputs. Although these signals are carefully crafted, they still appear noise-like patterns to humans. This observation has led to the argument that deep visual representation is misaligned with human perception. We counter-argue by providing evidence of human-meaningful patterns in adversarial perturbations. We first propose an attack that fools a network to confuse a whole category of objects (source class) with a target label. Our attack also limits the unintended", "keywords": {"keywords": ["adversarial perturbations", "deep networks", "human perception", "fooling attack", "target label"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "Deep Learning"}}, {"title": "Towards visual explainable active learning for zero-shot classification", "author": "S Jia"}, {"title": "Debiasing concept-based explanations with causal analysis", "author": "MT Bahadori"}, {"title": "Concept-based explanation for fine-grained images and its application in infectious keratitis classification", "author": "Z Fang"}, {"title": "Trustworthy artificial intelligence: a review", "author": "D Kaur"}, {"title": "Protopshare: Prototype sharing for interpretable image classification and similarity discovery", "author": "D Rymarczyk"}, {"title": "A peek into the reasoning of neural networks: Interpreting with structural visual concepts", "author": "Y Ge"}, {"title": "Cause and effect: Concept-based explanation of neural networks", "author": "MN Zaeem"}, {"title": "Interpretable image recognition by constructing transparent embedding space", "author": "J Wang"}, {"title": "Adding seemingly uninformative labels helps in low data regimes", "author": "C Matsoukas"}, {"title": "Robust semantic interpretability: Revisiting concept activation vectors", "author": "J Pfau", "abstract": "Interpretability methods for image classification assess model trustworthiness by attempting to expose whether the model is systematically biased or attending to the same cues as a human would. Saliency methods for feature attribution dominate the interpretability literature, but these methods do not address semantic concepts such as the textures, colors, or genders of objects within an image. Our proposed Robust Concept Activation Vectors (RCAV) quantifies the effects of semantic concepts on individual model predictions and on", "keywords": {"keywords": ["interpretability", "image classification", "concept activation vectors", "semantic concepts", "saliency methods"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "Explaining neural networks by decoding layer activations", "author": "J Schneider"}, {"title": "A framework to learn with interpretation", "author": "J Parekh"}, {"title": "Invertible concept-based explanations for cnn models with non-negative concept activation vectors", "author": "R Zhang"}, {"title": "Dissect: Disentangled simultaneous explanations via concept traversals", "author": "A Ghandeharioun"}, {"title": "An interpretable deep architecture for similarity learning built upon hierarchical concepts", "author": "X Gao"}, {"title": "Sketching an ai marketplace: Tech, economic, and regulatory aspects", "author": "A Kumar"}, {"title": "What can crowd computing do for the next generation of AI systems?", "author": "U Gadiraju"}, {"title": "Towards Relatable Explainable AI with the Perceptual Process", "author": "W Zhang"}, {"title": "Model-based counterfactual synthesizer for interpretation", "author": "F Yang"}, {"title": "Learning to predict with supporting evidence: Applications to clinical risk prediction", "author": "A Raghu", "abstract": "The impact of machine learning models on healthcare will depend on the degree of trust that healthcare professionals place in the predictions made by these models. In this paper, we present a method to provide individuals with clinical expertise with domain-relevant evidence about why a prediction should be trusted. We first design a probabilistic model that relates meaningful latent concepts to prediction targets and observed data. Inference of latent variables in this model corresponds to both making a prediction and providing", "keywords": {"keywords": ["machine learning", "clinical risk prediction", "evidence", "probabilistic model", "latent variables"], "research type": ["Empirical", "theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory", "descriptive"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Healthcare"]}}, {"title": "Practical machine learning safety: A survey and primer", "author": "S Mohseni"}, {"title": "Gaining Justified Human Trust by Improving Explainability in Vision and Language Reasoning Models", "author": "AR Akula"}, {"title": "Classification of explainable artificial intelligence methods through their output formats", "author": "G Vilone"}, {"title": "Why? Why not? When? Visual Explanations of Agent Behaviour in Reinforcement Learning", "author": "A Mishra"}, {"title": "Abstracting deep neural networks into concept graphs for concept level interpretability", "author": "A Kori"}, {"title": "Interpretability of a deep learning model in the application of cardiac MRI segmentation with an ACDC challenge dataset", "author": "A Janik"}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta"}, {"title": "Debugging the internals of convolutional networks", "author": "B Alsallakh"}, {"title": "Adversarial attacks and defenses: An interpretation perspective", "author": "N Liu"}, {"title": "Human-in-the-loop extraction of interpretable concepts in deep learning models", "author": "Z Zhao", "abstract": "The interpretation of deep neural networks (DNNs) has become a key topic as more and more people apply them to solve various problems and making critical decisions. Concept-based explanations have recently become a popular approach for post-hoc interpretation of DNNs. However, identifying human-understandable visual concepts that affect model decisions is a challenging task that is not easily addressed with automatic approaches. We present a novel human-in-the-Ioop approach to generate user-defined concepts for model", "keywords": {"keywords": ["deep learning", "interpretability", "DNNs", "concept-based explanations", "human-in-the-loop"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Computer science", "content": "Application", "application area": "Machine learning"}}, {"title": "Macro discourse relation recognition via discourse argument pair graph", "author": "Z Sun"}, {"title": "Pace: Posthoc architecture-agnostic concept extractor for explaining cnns", "author": "V Kamakshi"}, {"title": "DANCE: Enhancing saliency maps using decoys", "author": "YY Lu"}, {"title": "Explaining deep neural networks using unsupervised clustering", "author": "Y Liu"}, {"title": "Weakly supervised multi-task learning for concept-based explainability", "author": "C Bel\u00e9m"}, {"title": "Achievements and challenges in explaining deep learning based computer-aided diagnosis systems", "author": "A Lucieri"}, {"title": "An exploratory study of interpretability for face presentation attack detection", "author": "AF Sequeira"}, {"title": "Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization", "author": "J Borowski"}, {"title": "Neurocartography: Scalable automatic visual summarization of concepts in deep neural networks", "author": "H Park"}, {"title": "Comprehensible convolutional neural networks via guided concept learning", "author": "S Wickramanayake", "abstract": "Learning concepts that are consistent with human perception is important for Deep Neural Networks to win end-user trust. Post-hoc interpretation methods lack transparency in the feature representations learned by the models. This work proposes a guided learning approach with an additional concept layer in a CNN-based architecture to learn the associations between visual features and word phrases. We design an objective function that optimizes both prediction accuracy and semantics of the learned feature", "keywords": {"keywords": ["Comprehensible convolutional neural networks", "guided concept learning", "perception", "interpretation methods", "CNN-based architecture"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Image classification"]}}, {"title": "MEME: generating RNN model explanations via model extraction", "author": "D Kazhdan"}, {"title": "Finding representative interpretations on convolutional neural networks", "author": "PCH Lam"}, {"title": "NeuroView: Explainable Deep Network Decision Making", "author": "CJ Barberan"}, {"title": "Interpreting Deep Neural Networks through Prototype Factorization", "author": "S Das"}, {"title": "Deep Learning Based Decision Support for Medicine--A Case Study on Skin Cancer Diagnosis", "author": "A Lucieri"}, {"title": "BAYES-PROBE: Distribution-Guided Sampling for Prediction Level Sets", "author": "S Booth"}, {"title": "The Definitions of Interpretability and Learning of Interpretable Models", "author": "W Pan"}, {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "author": "L Sixt"}, {"title": "Corpus-level and concept-based explanations for interpretable document classification", "author": "T Shi"}, {"title": "Mace: Model agnostic concept extractor for explaining image classification networks", "author": "A Kumar", "abstract": "Deep convolutional networks have been quite successful at various image classification tasks. The current methods to explain the predictions of a pretrained model rely on gradient information, often resulting in saliency maps that focus on the foreground object as a whole. However, humans typically reason by dissecting an image and pointing out the presence of smaller concepts. The final output is often an aggregation of the presence or absence of these smaller concepts. In this work, we propose MACE: a model agnostic concept extractor", "keywords": {"keywords": ["deep convolutional networks", "image classification", "MACE", "concept extractor", "explanation"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image analysis"]}}, {"title": "ProtoMIL: Multiple instance learning with prototypical parts for fine-grained interpretability", "author": "D Rymarczyk"}, {"title": "Inverse-Based Approach to Explaining and Visualizing Convolutional Neural Networks", "author": "HJ Kwon"}, {"title": "When and How to Fool Explainable Models (and Humans) with Adversarial Examples", "author": "J Vadillo"}, {"title": "Explainable Machine Learning for Breakdown Prediction in High Gradient RF Cavities", "author": "C Obermair"}, {"title": "Explanation-driven characterization of android ransomware", "author": "M Scalas"}, {"title": "Verification of size invariance in DNN activations using concept embeddings", "author": "G Schwalbe"}, {"title": "Why we should build robots that both teach and learn", "author": "T Adamson"}, {"title": "Explaining classifiers by constructing familiar concepts", "author": "J Schneider"}, {"title": "Feature Extraction on Synthetic Black Hole Images", "author": "JYY Lin"}, {"title": "Explanation-based Data Augmentation for Image Classification", "author": "S Wickramanayake", "abstract": "Existing works have generated explanations for deep neural network decisions to provide insights into model behavior. We observe that these explanations can also be used to identify concepts that caused misclassifications. This allows us to understand the possible limitations of the dataset used to train the model, particularly the under-represented regions in the dataset. This work proposes a framework that utilizes concept-based explanations to automatically augment the dataset with new images that can cover these under-represented", "keywords": {"keywords": ["Explanation-based data augmentation", "Image classification", "Deep neural networks", "Concept-based explanations", "Under-represented regions"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Computer science", "content": "Application", "application area": "Artificial intelligence"}}, {"title": "Pytorch, explain! a python library for logic explained networks", "author": "P Barbiero"}, {"title": "Artificial intelligence for colonoscopy: Past, present, and future", "author": "W Tavanapong"}, {"title": "Interpreting deep neural networks for medical imaging using concept graphs", "author": "A Kori"}, {"title": "Teaching the Machine to Explain Itself using Domain Knowledge", "author": "V Balayan"}, {"title": "Instance-wise or Class-wise? A Tale of Neighbor Shapley for Concept-based Explanation", "author": "J Li"}, {"title": "Integrating Intrinsic and Extrinsic Explainability: The Relevance of Understanding Neural Networks for Human-Robot Interaction", "author": "T Weber"}, {"title": "Can Explanations Be Useful for Calibrating Black Box Models?", "author": "X Ye"}, {"title": "Towards explainable decision-making strategies of deep convolutional neural networks: An exploration into explainable ai and potential applications within cancer \u2026", "author": "T Hammarstr\u00f6m"}, {"title": "Sparse Subspace Clustering for Concept Discovery (SSCCD)", "author": "J Vielhaben"}, {"title": "Analyzing encoded concepts in transformer language models", "author": "H Sajjad", "abstract": "We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained language models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (eg", "keywords": {"keywords": ["representations", "language models", "latent concepts", "clustering", "lexical concepts"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Natural Language Processing"]}}, {"title": "Meaningfully debugging model mistakes using conceptual counterfactual explanations", "author": "A Abid"}, {"title": "Explanatory pragmatism: a context-sensitive framework for explainable medical AI", "author": "R Nyrup"}, {"title": "Algorithmic Behavioral Science: Machine Learning as a Tool for Scientific Discovery", "author": "J Ludwig"}, {"title": "Algorithmic Concept-Based Explainable Reasoning", "author": "D Georgiev"}, {"title": "Post hoc explanations may be ineffective for detecting unknown spurious correlation", "author": "J Adebayo"}, {"title": "Correcting classification: A bayesian framework using explanation feedback to improve classification abilities", "author": "Y Bekkemoen"}, {"title": "Automatic Concept Extraction for Concept Bottleneck-based Video Classification", "author": "JV Jeyakumar"}, {"title": "Visual probing: Cognitive framework for explaining self-supervised image representations", "author": "W Oleszkiewicz"}, {"title": "Cross-lingual Neural Vector Conceptualization", "author": "L Raithel"}, {"title": "Review on Interpretable Machine Learning in Smart Grid", "author": "C Xu", "abstract": "In recent years, machine learning, especially deep learning, has developed rapidly and has shown remarkable performance in many tasks of the smart grid field. The representation ability of machine learning algorithms is greatly improved, but with the increase of model complexity, the interpretability of machine learning algorithms is worse. The smart grid is a critical infrastructure area, so machine learning models involving it must be interpretable in order to increase user trust and improve system reliability. Unfortunately, the black-box", "keywords": {"keywords": ["Interpretable Machine Learning", "Smart Grid", "Model Complexity", "User Trust", "System Reliability"], "research type": ["Review"], "methodology": ["Mixed-Methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Smart Grid"]}}, {"title": "Cause and effect: Concept-based explanation of neural networks", "author": "M Nokhbeh Zaeem"}, {"title": "Learning invariances for interpretability using supervised VAE", "author": "A Nguyen"}, {"title": "Threading the Needle of On and Off-Manifold Value Functions for Shapley Explanations", "author": "CK Yeh"}, {"title": "SoFTNet: A concept-controlled deep learning architecture for interpretable image classification", "author": "T Zia"}, {"title": "From heatmaps to structured explanations of image classifiers", "author": "L Fuxin"}, {"title": "Hierarchical Visual Concept Interpretation for Medical Image Classification", "author": "M Khaleel"}, {"title": "Human-Centered Concept Explanations for Neural Networks", "author": "P Hitzler"}, {"title": "Path-based visual explanation", "author": "M Pourvali"}, {"title": "Towards a reliable evaluation of local interpretation methods", "author": "J Li"}, {"title": "Meaningfully explaining model mistakes using conceptual counterfactuals", "author": "A Abid", "abstract": "Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model's mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classifier makes a mistake on a", "keywords": {"keywords": ["model mistakes", "conceptual counterfactual explanations", "machine learning", "robustness", "concept drift"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Explainable artificial intelligence for education and training", "author": "K Fiok"}, {"title": "Interpretable image classification with differentiable prototypes assignment", "author": "D Rymarczyk"}, {"title": "\u041e\u0431\u0437\u043e\u0440 \u0441\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0439 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f \u0437\u043d\u0430\u043d\u0438\u0439 \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0439", "author": "\u0410\u0410 \u041c\u0443\u0441\u0430\u0435\u0432"}, {"title": "A Tour of Visualization Techniques for Computer Vision Datasets", "author": "B Alsallakh"}, {"title": "Interpretability for Multimodal Emotion Recognition using Concept Activation Vectors", "author": "AR Asokan"}, {"title": "Techniques for Combining Visualization with Machine Learning in Data Analysis", "author": "Z Zhao"}, {"title": "Simplifying the explanation of deep neural networks with sufficient and necessary feature-sets: case of text classification", "author": "JKF Flambeau"}, {"title": "Explaining Any ML Model?--On Goals and Capabilities of XAI", "author": "M Renftle"}, {"title": "Unsupervised Causal Binary Concepts Discovery with VAE for Black-box Model Explanation", "author": "TQ Tran"}, {"title": "From Correlation to Causation: Formalizing Interpretable Machine Learning as a Statistical Process", "author": "L Klein", "abstract": "Explainable AI (XAI) is a necessity in safety-critical systems such as in clinical diagnostics due to a high risk for fatal decisions. Currently, however, XAI resembles a loose collection of methods rather than a well-defined process. In this work, we elaborate on conceptual similarities between the largest subgroup of XAI, interpretable machine learning (IML), and classical statistics. Based on these similarities, we present a formalization of IML along the lines of a statistical process. Adopting this statistical view allows us to interpret machine", "keywords": {"keywords": ["Explainable AI", "Interpretable machine learning", "Statistical process", "Conceptual similarities", "Clinical diagnostics"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Explanatory", "discipline": "Computer science", "content": "Methodological", "application area": "Clinical diagnostics"}}, {"title": "ConceptDistil: Model-Agnostic Distillation of Concept Explanations", "author": "JB Sousa"}, {"title": "CACTUS: Detecting and Resolving Conflicts in Objective Functions", "author": "S Das"}, {"title": "What Do Navigation Agents Learn About Their Environment?", "author": "K Dwivedi"}, {"title": "Neural Activation Patterns (NAPs): Visual Explainability of Learned Concepts", "author": "A B\u00e4uerle"}, {"title": "Attention-based Interpretability with Concept Transformers", "author": "M Rigotti"}, {"title": "Learnable Visual Words for Interpretable Image Recognition", "author": "W Xiao"}, {"title": "The Role of Explanations in Human-Machine Learning", "author": "L Holmberg"}, {"title": "Geometric Deformation on Objects: Unsupervised Image Manipulation via Conjugation", "author": "C Fu"}, {"title": "Explanation-Driven Learning-Based Models for Visual Recognition Tasks", "author": "ZA Daniels"}, {"title": "Towards Generic Interface for Human-Neural Network Knowledge Exchange", "author": "Y Ge", "abstract": "Neural Networks (NN) outperform humans in multiple domains. Yet they suffer from a lack of transparency and interpretability, which hinders intuitive and effective human interactions with them. Especially when NN makes mistakes, humans can hardly locate the reason for the error, and correcting it is even harder. While recent advances in explainable AI have substantially improved the explainability of NNs, effective knowledge exchange between humans and NNs is still under-explored. To fill this gap, we propose Human-NN-Interface", "keywords": {"keywords": ["neural networks", "transparency", "interpretability", "knowledge exchange", "human-NN interface"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Measuring the interpretability of unsupervised representations via quantized reversed probing", "author": "I Laina"}, {"title": "Hierarchical Time Series Modeling with Business and Audio Applications", "author": "AK Yanchenko"}, {"title": "Right for the Right Reasons: Training Neural Networks to Be Interpretable, Robust, and Consistent with Expert Knowledge", "author": "AS Ross"}, {"title": "Deep Neural Network Models with Explainable Components for Urban Space Perception", "author": "AC Vidal"}, {"title": "HINT: Hierarchical Neuron Concept Explainer", "author": "A Wang"}, {"title": "Spatial-temporal Concept based Explanation of 3D ConvNets", "author": "Y Ji"}, {"title": "A Concept and Argumentation based Interpretable Model in High Risk Domains", "author": "H Chi"}, {"title": "Disentangling Embedding Spaces with Minimal Distributional Assumptions", "author": "T Leemann"}, {"title": "Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis", "author": "H Xuanyuan"}, {"title": "CPKD: Concepts-Prober-Guided Knowledge Distillation for Fine-Grained CNN Explanation", "author": "L Chen", "abstract": "Explainable artificial intelligence (XAI) for CNN has received increasing research attentions, especially in security-critical domains. Some researches exploit knowledge distillation based technique to transform the sophisticated deep black-box model to a simple yet explainable model. However, traditional distillation-based methods suffer from two typical dilemmas during the explanation:(1) they can hardly be self-explanatory;(2) they are unable to provide concept-based fine-grained explanation, which causes that the explanation result", "keywords": {"keywords": ["Concpts-Prober-Guided Knowledge Distillation", "Fine-Grained CNN Explanation", "XAI", "security-critical domains"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory", "Descriptive"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Mega Bot\u2013The Healthcare Chatbot", "author": "G Megarajan"}, {"title": "Explainable Deep Learning Methods in Medical Diagnosis: A Survey", "author": "C Patr\u00edcio"}, {"title": "Fair-doctor: Detecting and mitigating unfairness in neural networks", "author": "R Adhikari"}, {"title": "A Unified Study of Machine Learning Explanation Evaluation Metrics", "author": "Y Wang"}, {"title": "Beyond Importance Scores: Interpreting Tabular ML by Visualizing Feature Semantics", "author": "A Ghorbani"}, {"title": "Discerning Decision-Making Process of Deep Neural Networks with Hierarchical Voting Transformation", "author": "Y Sun"}, {"title": "Empowering users to communicate their preferences to machine learning models in Visual Analytics", "author": "S Das"}, {"title": "GLANCE: Global to Local Architecture-Neutral Concept-based Explanations", "author": "A Kori"}, {"title": "Encoding Concepts in Graph Neural Networks", "author": "LC Magister"}, {"title": "Two Frameworks for Reliable Machine Learning in Biology and Medicine", "author": "A Abid", "abstract": "Machine learning models are being deployed to biological and clinical settings, including here at Stanford, eg to analyze ultrasounds automatically or map ancestries from genomics data. However, machine learning models suffer from issues of reliability: even models with good test performance often fail in unpredictable ways when deployed to real-world settings.", "keywords": {"keywords": ["machine learning", "biology", "medicine", "reliability", "real-world settings"], "research type": "Review", "methodology": "Empirical", "purposes": "Explanatory", "discipline": "Biomedicine", "content": "Methodological", "application area": "Biological and Clinical Settings"}}, {"title": "Ontology-Driven Semantic Alignment of Artificial Neurons and Visual Concepts", "author": "R Massidda"}, {"title": "Inverse-Based Approach to Explaining and Visualizing Convolutional Neural Networks", "author": "\uad8c\ud601\uc9c4"}, {"title": "CEN: Concept Evolution Network for Image Classification Tasks", "author": "D Huang"}, {"title": "Hierarchical Symbolic Reasoning in Hyperbolic Space for Deep Discriminative Models", "author": "A Santhirasekaram"}, {"title": "ConceptEvo: Interpreting Concept Evolution in Deep Learning Training", "author": "H Park"}, {"title": "Extending explainability of generative classifiers with prototypical parts", "author": "MDJ Peters"}, {"title": "Towards Human-Centred Explainable AI", "author": "MSM ten Koppel"}, {"title": "On the Two-fold Role of Logic Constraints in Deep Learning", "author": "G Ciravegna"}, {"title": "Towards Reliable and Explainable AI Model for Solid Pulmonary Nodule Diagnosis", "author": "C Wang"}, {"title": "Towards Automating Model Explanations with Certified Robustness Guarantees", "author": "M Huai", "abstract": "Providing model explanations has gained significant popularity recently. In contrast with the traditional feature-level model explanations, concept-based explanations can provide explanations in the form of high-level human concepts. However, existing concept-based explanation methods implicitly follow a two-step procedure that involves human intervention. Specifically, they first need the human to be involved to define (or extract) the high-level concepts, and then manually compute the importance scores of these identified concepts in", "keywords": {"keywords": ["model explanations", "concept-based explanations", "robustness guarantees", "automating", "high-level concepts"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Explaining black-box neural ranking models using sentence impact analysis", "author": "L Holdijk"}, {"title": "Sonar data classification by using few-shot learning and concept extraction", "author": "M Ghavidel"}, {"title": "The range of harmful frequency for DNN corruption robustness", "author": "Z Zhang"}, {"title": "Interpretability of Deep Learning for Medical Image Classification: Improved Understandability and Generalization", "author": "M Graziani"}, {"title": "Cognitive Explainers of Graph Neural Networks Based on Medical Concepts", "author": "Y Wang"}, {"title": "Algorithms and Applications of Novel Capsule Networks", "author": "R LaLonde"}, {"title": "Concept Bottleneck Model With Additional Unsupervised Concepts", "author": "Y Sawada"}, {"title": "Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations", "author": "W Stammer"}, {"title": "Methods for the inspection of learning machines", "author": "R Schwarzenberg"}, {"title": "arXiv: Explainable Machine Learning for Breakdown Prediction in High Gradient RF Cavities", "author": "C Obermair", "abstract": "Radio Frequency (RF) breakdowns are one of the most prevalent limiting factors in RF cavities for particle accelerators. During a breakdown, field enhancement associated with small deformations on the cavity surface results in electrical arcs. Such arcs lead to beam aborts, reduce machine availability and can cause irreparable damage on the RF cavity surface. In this paper, we propose a machine learning strategy to discover breakdown precursors in CERN's Compact Linear Collider (CLIC) accelerating structures. By", "keywords": {"keywords": ["RF breakdowns", "high gradient RF cavities", "explainable machine learning", "breakdown prediction", "particle accelerators"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Explanatory", "discipline": "Physics", "content": "Technical", "application area": "Particle accelerators"}}, {"title": "Constraints-based explanation by visual feature learning", "author": "J Yan"}, {"title": "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis", "author": "S Omidshafiei"}, {"title": "Evaluation of Strategic Decision taken by Autonomous Agent using Explainable AI", "author": "RR Prasad"}, {"title": "Interpretable Machine Learning", "author": "T Martin"}, {"title": "Malware Analysis and Detection with Explainable Machine Learning", "author": "M Scalas"}, {"title": "Interpretable Machine Learning in EEG Abnormality Detection", "author": "F Knispel"}, {"title": "Label-Free Explainability for Unsupervised Models", "author": "J Crabb\u00e9"}, {"title": "Quantifying the Performance of Explainability Algorithms", "author": "ZQ Lin"}, {"title": "Exclusive Feature Constrained Class Activation Mapping for Better Visual Explanation", "author": "P Wang"}, {"title": "Do Vision-Language Pretrained Models Learn Primitive Concepts?", "author": "T Yun", "abstract": "Vision-language pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether the notion of primitive concepts, such as color and shape attributes, emerges automatically from these pretrained VL models. We propose to learn compositional derivations that map primitive concept activations into composite concepts, a task which we demonstrate to be", "keywords": {"keywords": ["Vision-language pretrained models", "Primitive concepts", "Compositional derivations", "Multimodal reasoning", "Zero-shot recognition"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "Exploiting auto-encoders and segmentation methods for middle-level explanations of image classification systems", "author": "A Apicella"}, {"title": "Intrinsically Interpretable Document Classification via Concept Lattices", "author": "EG Parakal"}, {"title": "Explainable Classification and Annotation through Relation Learning and Reasoning", "author": "R Pierrard"}, {"title": "Interpretability and fairness evaluation of deep learning models on MIMIC-IV dataset", "author": "C Meng"}, {"title": "Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF", "author": "J Parekh"}, {"title": "Robust and Interpretable Predictions for Multimodal Sensor Systems", "author": "JV Jeyakumar"}, {"title": "Post-hoc Concept Bottleneck Models", "author": "M Yuksekgonul"}, {"title": "KACE: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference", "author": "Q Chen"}, {"title": "Validating Automatic Concept-Based Explanations for AI-Based Digital Histopathology", "author": "D Sauter"}, {"title": "Explainable Supervised Domain Adaptation", "author": "V Kamakshi", "abstract": "domain, remains unclear. This paper proposes an explainable by design supervised domain  adaptation  domain adaptation settings on datasets popularly known to exhibit part-based", "keywords": {"keywords": ["explainable", "supervised domain adaptation", "part-based datasets", "domain adaptation settings", "by design"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["domain adaptation"]}}, {"title": "Towards Concept-based Interpretability of Pre-miRNA Detection using Convolutional Neural Networks", "author": "I van den Brandt"}, {"title": "Concept-based Explanations for Out-Of-Distribution Detectors", "author": "J Choi"}, {"title": "Generalization on the Enhancement of Layerwise Relevance Interpretability of Deep Neural Network", "author": "E Tjoa"}, {"title": "Explainable neural net architecture for multidimensional data", "author": "A Dalli"}, {"title": "TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency", "author": "L Cheng"}, {"title": "On the Robustness and Interpretability of Deep Learning Models", "author": "W Wu"}, {"title": "Learning Unsupervised Hierarchies of Audio Concepts", "author": "D Afchar"}, {"title": "Explanations in terms of Hierarchically organised Middle Level Features", "author": "A Apicella"}, {"title": "Interpretable Deep Learning: Beyond Feature-Importance with Concept-based Explanations", "author": "B Dimanov"}, {"title": "Perspectives on Incorporating Expert Feedback into Model Updates", "author": "V Chen", "abstract": "Machine learning (ML) practitioners are increasingly tasked with developing models that are aligned with non-technical experts' values and goals. However, there has been insufficient consideration on how practitioners should translate domain expertise into ML updates. In this paper, we consider how to capture interactions between practitioners and experts systematically. We devise a taxonomy to match expert feedback types with practitioner updates. A practitioner may receive feedback from an expert at the observation-or domain", "keywords": {"keywords": ["expert feedback", "model updates", "machine learning", "domain expertise", "taxonomy"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Self-supervised Semantic Segmentation Grounded in Visual Concepts", "author": "W He"}, {"title": "The Design and Evaluation of Neural Attention Mechanisms for Explaining Text Classifiers", "author": "S Carton"}, {"title": "Towards Human-Understandable Visual Explanations: Human Imperceptible Cues Can Better Be Removed", "author": "K Wang"}, {"title": "Representation Learning for Treatment Effect Estimation", "author": "L Yao"}, {"title": "G\\\" odel's Sentence Is An Adversarial Example But Unsolvable", "author": "X Qi"}, {"title": "Cycle-Consistent Counterfactuals by Latent Transformations", "author": "S Khorram"}, {"title": "Predicting Bulk Average Velocity with Rigid Vegetation in Open Channels Using Tree-Based Machine Learning: A Novel Approach Using Explainable Artificial \u2026", "author": "DPP Meddage"}, {"title": "Concept Embedding Analysis: A Review", "author": "G Schwalbe"}, {"title": "Defining Explanation and Explanatory Depth in XAI", "author": "S Buijsman"}, {"title": "Unifying Model Explainability and Robustness via Machine-Checkable Concepts", "author": "V Nanda", "abstract": "As deep neural networks (DNNs) get adopted in an ever-increasing number of applications, explainability has emerged as a crucial desideratum for these models. In many real-world tasks, one of the principal reasons for requiring explainability is to in turn assess prediction robustness, where predictions (ie, class labels) that do not conform to their respective explanations (eg, presence or absence of a concept in the input) are deemed to be unreliable. However, most, if not all, prior methods for checking explanation-conformity (eg", "keywords": {"keywords": ["deep neural networks", "explainability", "robustness", "machine-checkable concepts", "prediction reliability"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Explainable AI for Deep Learning Based Disease Detection", "author": "S Kinger"}, {"title": "Human-Interpretable Explanations for Black-Box Machine Learning Models: An Application to Fraud Detection", "author": "V Balayan"}, {"title": "Human-Centric Visual Diversity Auditing", "author": "JTA Andrews"}, {"title": "Model Interpretation and Data Valuation for Machine Learning", "author": "A Ghorbani"}, {"title": "Towards Human-Centered Optimality Criteria", "author": "A Ghandeharioun"}, {"title": "ConceptExplainer: Understanding the Mental Model of Deep Learning Algorithms via Interactive Concept-based Explanations", "author": "J Huang"}, {"title": "From\" Where\" to\" What\": Towards Human-Understandable Explanations through Concept Relevance Propagation", "author": "R Achtibat"}, {"title": "Scalable Interpretability via Polynomials", "author": "A Dubey"}, {"title": "Explainable AI foundations to support human-robot teaching and learning", "author": "SL Booth"}, {"title": "Deep neural network models with explainable components for urban space perception", "author": "A C\u00e1diz Vidal", "abstract": "La percepci\u00f3n urbana ha sido un tema de investigaci\u00f3n importante por al menos 60a\u00f1os, con trabajos hechos desde una variedad de disciplinas, usando distintas metodolog\u00edasprincipalmente basadas en encuestas sobre ambientes urbanos reales o simulados. Recientemente, el surgimiento de las tecnolog\u00edas web ha aumentado masivamente la disponibilidadde grandes vol\u00famenes de datos y de t\u00e9cnicas de recolecci\u00f3n de datos altamente escalablespermitiendo que t\u00e9cnicas pertenecientes a otros dominios", "keywords": {"keywords": ["Urban space perception", "Deep neural network models", "Explainable components", "Perception urbana", "Metodolog\u00edas"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Exploratory"], "discipline": ["Multidisciplinary"], "content": ["Application"], "application area": ["Urban planning and design"]}}, {"title": "Patch Shortcuts: Interpretable Proxy Models Efficiently Find Black-Box Vulnerabilities", "author": "J Rosenzweig"}, {"title": "Explaining Neural NLP Models for the Joint Analysis of Open-and-Closed-Ended Survey Answers", "author": "E Mosca"}, {"title": "Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces", "author": "K Alipour"}, {"title": "Towards Enabling Explanation in Safety-Critical Artificial Intelligence Systems", "author": "A Michel"}, {"title": "Explainability of Methods for Critical Information Extraction From Clinical Documents: A survey of representative works", "author": "TK Ho"}, {"title": "What Pushes Self-supervised Image Representations Away?", "author": "B Zieli\u0144ski"}, {"title": "Which Visual Features Impact the Performance of Target Task in Self-supervised Learning?", "author": "B Zieli\u0144ski"}, {"title": "On The Quality Assurance Of Concept-Based Representations", "author": "ME Zarlenga"}, {"title": "Concept-based Abstraction-Aggregation Network for Interpretable Document Classification", "author": "T Shi"}, {"title": "Attention-based and Causal Explanations in Computer Vision", "author": "K Alipour", "abstract": "Despite their potential unknown deficiencies and biases, the takeover of critical tasks by AI machines in different fields has created a demand for transparency alongside accuracy for these machines. Explainable AI (XAI) approaches have provided solutions by mitigating the lack of transparency and trust in AI and making these machines more interpretable to the lay users. This dissertation investigates the role of explanations for deep learning models in computer vision. This research explores new methods to produce more effective", "keywords": {"keywords": ["Attention-based", "Causal Explanations", "Computer Vision", "Explainable AI", "Deep Learning Models"], "research type": ["Empirical"], "methodology": ["Mixed-Methods"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["AI Transparency"]}}, {"title": "Knowledge Augmented Machine Learning with Applications in Autonomous Driving: A Survey", "author": "J W\u00f6rmann"}, {"title": "Novel Algorithms for Understanding Online Reviews", "author": "T Shi"}, {"title": "Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions", "author": "Z Li"}, {"title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset", "author": "L Sixt"}, {"title": "Proto2Proto: Can you recognize the car, the way I do?", "author": "M Keswani"}, {"title": "Towards Multi-Scale Visual Explainability for Convolutional Neural Networks", "author": "Y Song"}, {"title": "Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models", "author": "Z Liu"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Human-Understandable Decision Making for Visual Recognition", "author": "X Zhou"}, {"title": "Towards learning trustworthily, automatically, and with guarantees on graphs: An overview", "author": "L Oneto", "abstract": "The increasing digitization and datification of all aspects of people's daily life, and the consequent growth in the use of personal data, are increasingly challenging the current development and adoption of Machine Learning (ML). First, the sheer complexity and amount of data available in these applications strongly demands for ML algorithms that can be trained directly on complex structures, which can be naturally described by graphs. In fact, graphs inherently capture information about entities, their attributes, and relationships", "keywords": {"keywords": ["trustworthily", "automatically", "guarantees", "graphs", "Machine Learning"], "research type": ["overview"], "methodology": [], "purpose": [], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": []}}, {"title": "Which Visual Features Impact the Performance of Target Task in Self-supervised Learning?", "author": "W Oleszkiewicz"}, {"title": "Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees", "author": "J Brophy"}, {"title": "Chain Graph Explanation of Neural Network Based on Feature-Level Class Confusion", "author": "H Hwang"}, {"title": "Inducing Semantic Grouping of Latent Concepts for Explanations: An Ante-Hoc Approach", "author": "A Sarkar"}, {"title": "Subgroup Discovery in Unstructured Data", "author": "A Arab"}, {"title": "Explaining AI-based Decision Support Systems using Concept Localization Maps", "author": "S Ahmed"}, {"title": "Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis", "author": "E Schoop"}, {"title": "\uc124\uba85\uac00\ub2a5\ud55c \uc778\uacf5\uc9c0\ub2a5\uc744 \uc704\ud55c \ud2b9\uc131\uae30\uc5ec\ub3c4 \ubd84\uc11d \ubc29\ubc95\ub860 \uc870\uc0ac", "author": "\uace0\uae30\ud601\uff0c \uc784\uaddc\ubbfc\uff0c \uc870\ud638\ubb35"}, {"title": "Erkl\u00e4rbare KI in der medizinischen Diagnose\u2013Erfolge und Herausforderungen", "author": "A Lucieri"}], "coherence evaluation of visual concepts with objects and language": [{"title": "Disentangling Embedding Spaces with Minimal Distributional Assumptions", "author": "T Leemann", "abstract": "Interest in understanding and factorizing learned embedding spaces is growing. For instance, recent concept-based explanation techniques analyze a machine learning model in terms of interpretable latent components. Such components have to be discovered in the model's embedding space, eg, through independent component analysis (ICA) or modern disentanglement learning techniques. While these unsupervised approaches offer a sound formal framework, they either require access to a data generating function or impose rigid", "keywords": {"keywords": ["Embedding spaces", "Distributional assumptions", "Interpretable latent components", "Disentanglement learning techniques", "Unsupervised approaches"], "research type": ["Theoretical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Machine learning"]}}], "quantifying learnability and describability of visual concepts emerging in representation learning": [{"title": "Measuring the interpretability of unsupervised representations via quantized reversed probing", "author": "I Laina", "abstract": "Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, ie understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck", "keywords": {"keywords": ["self-supervised learning", "interpretability", "representation learning", "unsupervised learning", "mutual information"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Artificial intelligence"]}}, {"title": "RELAX: Representation Learning Explainability", "author": "KK Wickstr\u00f8m", "abstract": "Despite the significant improvements that representation learning via self-supervision has led to when learning from unlabeled data, no methods exist that explain what influences the learned representation. We address this need through our proposed approach, RELAX, which is the first approach for attribution-based explanations of representations. Our approach can also model the uncertainty in its explanations, which is essential to produce trustworthy explanations. RELAX explains representations by measuring similarities in the", "keywords": {"keywords": ["representation learning", "self-supervision", "explanations", "uncertainty", "similarity"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Machine Learning"]}}, {"title": "A HYPOTHESIS FOR THE COGNITIVE DIFFICULTY OF IMAGES", "author": "X Cheng", "abstract": "This paper proposes a hypothesis to analyze the underlying reason for the cognitive difficulty of an image from two perspectives, ie a cognitive image usually makes a DNN strongly activated by cognitive concepts; discarding massive non-cognitive concepts may also help the DNN focus on cognitive concepts. Based on this hypothesis, we use multi-variate interactions to represent cognitive concepts and non-cognitive concepts contained in an image, and further design a set of image revision operations to decrease the cognitive", "keywords": {"keywords": ["cognitive difficulty", "images", "hypothesis", "multi-variate interactions", "image revision operations"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["image analysis"]}}, {"title": "Investigating deep neural network internal clustering and generalization properties", "author": "G Peiffer", "abstract": "The topic of generalization in deep neural networks has long been one that garners a lot of interest from research groups all around the world [1, 2, 3, 4]. Despite all this interest, however, current knowledge about this topic remains limited. In this thesis, we present a novel way of looking at generalization, through the lens of how internal representations at the neuron level relate to the generalization performance of a deep neural network. We present three main contributions on this subject.(i) First, we extend some previous results in", "keywords": {"keywords": ["deep neural networks", "generalization", "internal representations", "neuron level", "clustering"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Technical"], "application area": ["Neuroscience"]}}, {"title": "Learnability and Expressiveness in Self-Supervised Learning", "author": "Y Lu", "abstract": "In this work, we argue that representations induced by self-supervised learning (SSL) methods should both be expressive and learnable. To measure expressiveness, we propose to use the Intrinsic Dimension (ID) of the dataset in representation space. Inspired by the human study of Laina et al.(2020), we introduce Cluster Learnability (CL), defined in terms of the learning speed of a KNN classifier trained to predict K-means cluster labels for held-out representations. By collecting 30 state-of-art checkpoints, both supervised and self", "keywords": {"keywords": ["self-supervised learning", "expressiveness", "learnability", "intrinsic dimension", "cluster learnability"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}], "do explanations make vqa models more predictable to a human?": [{"title": "From recognition to cognition: Visual commonsense reasoning", "author": "R Zellers", "abstract": "Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then", "keywords": {"keywords": ["visual commonsense reasoning", "visual understanding", "object recognition", "higher-order cognition", "commonsense reasoning"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["artificial intelligence"]}}, {"title": "Interpreting cnns via decision trees", "author": "Q Zhang"}, {"title": "What is AI literacy? Competencies and design considerations", "author": "D Long"}, {"title": "On human predictions with explanations and predictions of machine learning models: A case study on deception detection", "author": "V Lai"}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal"}, {"title": "Evaluating explainable AI: Which algorithmic explanations help users predict model behavior?", "author": "P Hase"}, {"title": "No explainability without accountability: An empirical study of explanations and feedback in interactive ml", "author": "A Smith-Renner"}, {"title": "Evaluating Explanations: How much do explanations from the teacher aid students?", "author": "D Pruthi"}, {"title": "Measuring association between labels and free-text rationales", "author": "S Wiegreffe"}, {"title": "Challenges and prospects in vision and language research", "author": "K Kafle"}, {"title": "A review on explainability in multimodal deep neural nets", "author": "G Joshi", "abstract": "Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature", "keywords": {"keywords": ["explainability", "multimodal deep neural nets", "Artificial Intelligence", "deep learning models", "modality"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI applications"]}}, {"title": "Gradient-based analysis of NLP models is manipulable", "author": "J Wang"}, {"title": "Generating user-friendly explanations for loan denials using gans", "author": "R Srinivasan"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations", "author": "S Arora"}, {"title": "Can you explain that? Lucid explanations help human-AI collaborative image retrieval", "author": "A Ray"}, {"title": "A study on multimodal and interactive explanations for visual question answering", "author": "K Alipour"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen"}, {"title": "Connecting attributions and QA model behavior on realistic counterfactuals", "author": "X Ye"}, {"title": "Sage advice? The impacts of explanations for machine learning models on human decision-making in spam detection", "author": "MC Stites"}, {"title": "No explainability without accountability: an empirical study of explanations and feedback in interactive ML", "author": "A Smith", "abstract": "Automatically generated explanations of how machine learning (ML) models reason can help users understand and accept them. However, explanations can have unintended consequences: promoting over-reliance or undermining trust. This paper investigates how explanations shape users' perceptions of ML models with or without the ability to provide feedback to them:(1) does revealing model flaws increase users' desire to\" fix\" them;(2) does providing explanations cause users to believe-wrongly-that models are introspective, and", "keywords": {"keywords": ["explanations", "feedback", "interactive ML", "accountability", "perceptions"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Machine Learning"]}}, {"title": "The impact of explanations on AI competency prediction in VQA", "author": "K Alipour"}, {"title": "What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods", "author": "T Fel"}, {"title": "DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations", "author": "Y Lyu"}, {"title": "Improving users' mental model with attention\u2010directed counterfactual edits", "author": "K Alipour"}, {"title": "VisQA: X-raying Vision and Language Reasoning in Transformers", "author": "T Jaunet"}, {"title": "Teaching humans when to defer to a classifier via exemplars", "author": "H Mozannar"}, {"title": "Should I Follow AI-based Advice? Measuring Appropriate Reliance in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes", "author": "K Xie"}, {"title": "Generating visual explanations with natural language", "author": "LA Hendricks"}, {"title": "Can Explanations Be Useful for Calibrating Black Box Models?", "author": "X Ye", "abstract": "One often wants to take an existing, trained NLP model and use it on data from a new domain. While fine-tuning or few-shot learning can be used to adapt the base model, there is no one simple recipe to getting these working; moreover, one may not have access to the original model weights if it is deployed as a black box. To this end, we study how to improve a black box model's performance on a new domain given examples from the new domain by leveraging explanations of the model's behavior. Our approach first extracts a set of features", "keywords": {"keywords": ["explanations", "black box models", "calibrating", "NLP", "domain adaptation"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["NLP domain adaptation"]}}, {"title": "Visual Understanding through Natural Language", "author": "LAM Hendricks"}, {"title": "Designing for the human in the loop: Transparency and control in interactive machine learning", "author": "A Renner"}, {"title": "Adaptive Fusion Techniques for Effective Multimodal Deep Learning", "author": "G Sahu"}, {"title": "Generating and evaluating explanations of attended and error\u2010inducing input regions for VQA models", "author": "A Ray"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Learning to Scaffold: Optimizing Model Explanations for Teaching", "author": "P Fernandes"}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "Use-Case-Grounded Simulations for Explanation Evaluation", "author": "V Chen"}, {"title": "Bias and reasoning in visual question answering", "author": "C Kervadec"}, {"title": "Advancing Multi-modal Deep Learning: Towards Language-grounded Visual Understanding", "author": "K Kafle", "abstract": "Using deep learning, computer vision now rivals people at object recognition and detection, opening doors to tackle new challenges in image understanding. Among these challenges, understanding and reasoning about language grounded visual content is of fundamental importance to advancing artificial intelligence. Recently, multiple datasets and algorithms have been created as proxy tasks towards this goal, with visual question answering (VQA) being the most widely studied. In VQA, an algorithm needs to produce an answer to a", "keywords": {"keywords": ["multi-modal deep learning", "language-grounded visual understanding", "computer vision", "visual question answering", "artificial intelligence"], "research type": ["Empirical"], "methodology": ["Mixed-methods"], "purposes": ["Explanatory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Image understanding"]}}, {"title": "Generating User-friendly Explanations for Loan Denials using GANs", "author": "P Pezeshkpour"}, {"title": "Leveraging Human Reasoning to Understand and Improve Visual Question Answering", "author": "HA Ayyubi"}, {"title": "SAGE Intrusion Detection System: Sensitivity Analysis Guided Explainability for Machine Learning.", "author": "M Smith"}, {"title": "MultiViz: An Analysis Benchmark for Visualizing and Understanding Multimodal Models", "author": "PP Liang"}, {"title": "A Meta-Analysis on the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making", "author": "M Schemmer"}, {"title": "Generating Rationales in Visual Question Answering", "author": "HA Ayyubi"}, {"title": "Attention-based and Causal Explanations in Computer Vision", "author": "K Alipour"}, {"title": "EvalAI: Towards Better Evaluation of AI Agents", "author": "D Yadav"}, {"title": "Code-Upload AI Challenges on EvalAI", "author": "R Jain"}, {"title": "Grounding Language by Seeing, Hearing, and Interacting", "author": "R Zellers", "abstract": "As humans, our understanding of language is grounded in a rich mental model about\" how the world works.\" As children, we learn this mental model gradually. We take in raw perceptual input about the world through all of our senses, and learn to make sense of people and objects around us--enough to take action in the world. Our understanding of language and vision is grounded in the world.", "keywords": {"keywords": ["grounding language", "seeing", "hearing", "interacting", "mental model"], "research type": ["theoretical"], "methodology": [], "purposes": [], "discipline": [], "content": [], "application area": []}}, {"title": "Machine Explanations and Human Understanding", "author": "C Chen"}], "'it's reducing a human being to a percentage' perceptions of justice in algorithmic decisions": [{"title": "Machine behaviour", "author": "I Rahwan", "abstract": "Machines powered by artificial intelligence increasingly mediate our social, cultural,  economic and political interactions. Understanding the behaviour of artificial intelligence", "keywords": {"keywords": ["machine behaviour", "artificial intelligence", "social interactions", "cultural interactions", "political interactions"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["societal interactions"]}}, {"title": "Improving fairness in machine learning systems: What do industry practitioners need?", "author": "K Holstein"}, {"title": "Manipulating and measuring model interpretability", "author": "F Poursabzi-Sangdeh"}, {"title": "Artificial intelligence and management: The automation\u2013augmentation paradox", "author": "S Raisch"}, {"title": "Actionable recourse in linear classification", "author": "B Ustun"}, {"title": "From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices", "author": "J Morley"}, {"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao"}, {"title": "Counterfactual explanations for machine learning: A review", "author": "S Verma"}, {"title": "Explaining models: an empirical study of how explanations impact fairness judgment", "author": "J Dodge"}, {"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "Transparency in algorithmic and human decision-making: is there a double standard?", "author": "J Zerilli", "abstract": "We are sceptical of concerns over the opacity of algorithmic decision tools. While transparency and explainability are certainly important desiderata in algorithmic governance, we worry that automated decision-making is being held to an unrealistically high standard, possibly owing to an unrealistically high estimate of the degree of transparency attainable from human decision-makers. In this paper, we review evidence demonstrating that much human decision-making is fraught with transparency problems", "keywords": {"keywords": ["transparency", "algorithmic decision-making", "human decision-making", "opacity", "double standard"], "research type": ["review"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["governance"]}}, {"title": "WeBuildAI: Participatory framework for algorithmic governance", "author": "MK Lee"}, {"title": "Automated rationale generation: a technique for explainable AI and its effects on human perceptions", "author": "U Ehsan"}, {"title": "The principles and limits of algorithm-in-the-loop decision making", "author": "B Green"}, {"title": "Efficient search for diverse coherent explanations", "author": "C Russell"}, {"title": "What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability", "author": "M Wieringa"}, {"title": "How do fairness definitions fare? Examining public attitudes towards algorithmic definitions of fairness", "author": "NA Saxena"}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal"}, {"title": "Toward algorithmic accountability in public services: A qualitative study of affected community perspectives on algorithmic decision-making in child welfare services", "author": "A Brown"}, {"title": "Mathematical notions vs. human perception of fairness: A descriptive approach to fairness for machine learning", "author": "M Srivastava"}, {"title": "Neural legal judgment prediction in English", "author": "I Chalkidis", "abstract": "Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case's facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (eg, using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass", "keywords": {"keywords": ["legal judgment prediction", "neural models", "English", "dataset", "European Court of Human Rights"], "research type": "Empirical", "methodology": "Quantitative", "purposes": "Exploratory", "discipline": "Computer science", "content": "Methodological", "application area": "Judicial system"}}, {"title": "Trust in data science: Collaboration, translation, and accountability in corporate data science projects", "author": "S Passi"}, {"title": "Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation", "author": "MK Lee"}, {"title": "A survey of evaluation methods and measures for interpretable machine learning", "author": "S Mohseni"}, {"title": "What do we want from Explainable Artificial Intelligence (XAI)?\u2013A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research", "author": "M Langer"}, {"title": "A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores", "author": "M De-Arteaga"}, {"title": "Artificial intelligence, transparency, and public decision-making", "author": "K de Fine Licht"}, {"title": "Highly automated job interviews: Acceptance under the influence of stakes", "author": "M Langer"}, {"title": "Algorithm-assisted decision-making in the public sector: framing the issues using administrative law rules governing discretionary power", "author": "M Oswald"}, {"title": "Progressive disclosure: empirically motivated approaches to designing effective transparency", "author": "A Springer"}, {"title": "Implications of AI (un-) fairness in higher education admissions: the effects of perceived AI (un-) fairness on exit, voice and organizational reputation", "author": "F Marcinkowski", "abstract": "Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision", "keywords": {"keywords": ["AI fairness", "higher education admissions", "algorithmic decision-making", "perceived fairness", "exit, voice, and reputation"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Explanatory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Higher Education"]}}, {"title": "This thing called fairness: Disciplinary confusion realizing a value in technology", "author": "DK Mulligan"}, {"title": "\" Why is' Chicago'deceptive?\" Towards Building Model-Driven Tutorials for Humans", "author": "V Lai"}, {"title": "An empirical study on the perceived fairness of realistic, imperfect machine learning models", "author": "G Harrison"}, {"title": "Artificial intelligence and patient-centered decision-making", "author": "JC Bjerring"}, {"title": "Human intervention in automated decision-making: Toward the construction of contestable systems", "author": "M Almada"}, {"title": "The theory and practice of social machines", "author": "N Shadbolt"}, {"title": "Crowdsourcing perceptions of fair predictors for machine learning: A recidivism case study", "author": "N Van Berkel"}, {"title": "Bias and Discrimination in AI: a cross-disciplinary perspective", "author": "X Ferrer"}, {"title": "Dissonance between human and machine understanding", "author": "Z Zhang"}, {"title": "Human Judgment in algorithmic loops: Individual justice and automated decision\u2010making", "author": "R Binns", "abstract": "Arguments in favor of tempering algorithmic decision making with human judgment often appeal to concepts and criteria derived from legal philosophy about the nature of law and legal reasoning, arguing that algorithmic systems cannot satisfy them (but humans can). Such arguments often make implicit appeal to the notion that each case needs to be assessed on its own merits, without comparison to or generalization from previous cases. This article argues that this notion of individual justice can only be meaningfully served", "keywords": {"keywords": ["human judgment", "algorithmic loops", "individual justice", "automated decision-making", "legal reasoning"], "research type": ["theoretical"], "methodology": [], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["application"], "application area": ["legal systems"]}}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "A human-centered agenda for intelligible machine learning", "author": "JW Vaughan"}, {"title": "Predictive multiplicity in classification", "author": "C Marx"}, {"title": "The future of artificial intelligence at work: A review on effects of decision automation and augmentation on workers targeted by algorithms and third-party observers", "author": "M Langer"}, {"title": "Measuring non-expert comprehension of machine learning fairness metrics", "author": "D Saha"}, {"title": "The impact of using algorithms for managerial decisions on public employees' procedural justice", "author": "R Nagtegaal"}, {"title": "Dece: Decision explorer with counterfactual explanations for machine learning models", "author": "F Cheng"}, {"title": "Shifting concepts of value: Designing algorithmic decision-support systems for public services", "author": "N Holten M\u00f8ller"}, {"title": "New foundations of ethical multiagent systems", "author": "PK Murukannaiah"}, {"title": "Ai-generated vs. human artworks. a perception bias towards artificial intelligence?", "author": "M Ragot", "abstract": "Via generative adversarial networks (GANs), artificial intelligence (AI) has influenced many areas, especially the artistic field, as symbol of a human task. In human-computer interaction (HCI) studies, perception biases against AI, machines, or computers are generally cited. However, experimental evidence is still lacking. This paper presents a wide-scale experiment in which 565 participants are asked to evaluate paintings (which were created by humans or AI) on four dimensions: liking, perceived beauty, novelty, and meaning. A", "keywords": {"keywords": ["artificial intelligence", "artworks", "perception bias", "generative adversarial networks", "human-computer interaction"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Artificial Intelligence"]}}, {"title": "Making data science systems work", "author": "S Passi"}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "Bridging machine learning and mechanism design towards algorithmic fairness", "author": "J Finocchiaro"}, {"title": "Evaluating the promise of human-algorithm collaborations in everyday work practices", "author": "C Wolf"}, {"title": "Who is the fairest of them all? Public attitudes and expectations regarding automated decision-making", "author": "N Helberger"}, {"title": "The who in explainable ai: How ai background shapes perceptions of ai explanations", "author": "U Ehsan"}, {"title": "Silva: Interactively assessing machine learning fairness using causality", "author": "JN Yan"}, {"title": "Human-centered approaches to fair and responsible AI", "author": "MK Lee"}, {"title": "How do fairness definitions fare? Testing public attitudes towards three algorithmic definitions of fairness in loan allocations", "author": "NA Saxena"}, {"title": "The Algorithmic Society", "author": "M Schuilenburg", "abstract": "an algorithmic society. Algorithms have become the main mediator through which power is  enacted in our society.  and social implications of algorithmic governance. Reporting from the", "keywords": {"keywords": ["algorithmic society", "algorithmic governance", "power mediation", "social implications", "reporting"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["sociology"], "content": ["social implications"], "application area": ["society"]}}, {"title": "Artificial intelligence and the value of transparency", "author": "J Walmsley"}, {"title": "I think i get your point, AI! the illusion of explanatory depth in explainable AI", "author": "M Chromik"}, {"title": "The material conditions of platforms: Monopolization through decentralization", "author": "T Blanke"}, {"title": "Algorithmic bias: review, synthesis, and future research directions", "author": "N Kordzadeh"}, {"title": "Exploring explainability: a definition, a model, and a knowledge catalogue", "author": "L Chazette"}, {"title": "How fair can we go in machine learning? Assessing the boundaries of accuracy and fairness", "author": "A Valdivia"}, {"title": "Explanations in autonomous driving: A survey", "author": "D Omeiza"}, {"title": "Explaining data-driven decisions made by AI systems: The counterfactual approach", "author": "C Fern\u00e1ndez-Lor\u00eda"}, {"title": "Outlining traceability: A principle for operationalizing accountability in computing systems", "author": "JA Kroll"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik", "abstract": "Training datasets fundamentally impact the performance of machine learning (ML) systems. Any biases introduced during training (implicit or explicit) are often reflected in the system's behaviors leading to questions about fairness and loss of trust in the system. Yet, information on training data is rarely communicated to stakeholders. In this work, we explore the concept of data-centric explanations for ML systems that describe the training data to end-users. Through a formative study, we investigate the potential utility of such an approach, including", "keywords": {"keywords": ["data-centric explanations", "machine learning systems", "training data", "transparency", "biases"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Machine Learning"]}}, {"title": "Effect of information presentation on fairness perceptions of machine learning predictors", "author": "N Van Berkel"}, {"title": "How to support users in understanding intelligent systems? Structuring the discussion", "author": "M Eiband"}, {"title": "Transparency for whom? assessing discriminatory artificial intelligence", "author": "T Van Nuenen"}, {"title": "Designing alternative representations of confusion matrices to support non-expert public understanding of algorithm performance", "author": "H Shen"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh"}, {"title": "Soliciting stakeholders' fairness notions in child maltreatment predictive systems", "author": "HF Cheng"}, {"title": "\" At the End of the Day Facebook Does What ItWants\" How Users Experience Contesting Algorithmic Content Moderation", "author": "K Vaccaro"}, {"title": "SCREEN-DR: Collaborative platform for diabetic retinopathy", "author": "M Pedrosa"}, {"title": "Fairness perceptions of algorithmic decision-making: A systematic review of the empirical literature", "author": "C Starke"}, {"title": "Player-AI interaction: What neural network games reveal about AI as play", "author": "J Zhu", "abstract": "The advent of artificial intelligence (AI) and machine learning (ML) bring human-AI interaction to the forefront of HCI research. This paper argues that games are an ideal domain for studying and experimenting with how humans interact with AI. Through a systematic survey of neural network games (n= 38), we identified the dominant interaction metaphors and AI interaction patterns in these games. In addition, we applied existing human-AI interaction guidelines to further shed light on player-AI interaction in the context of", "keywords": {"keywords": ["player-AI interaction", "neural network games", "human-AI interaction", "interaction metaphors", "AI interaction patterns"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Human-Computer Interaction (HCI) research"]}}, {"title": "I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions", "author": "M Kasinidou"}, {"title": "Human comprehension of fairness in machine learning", "author": "D Saha"}, {"title": "Data-driven game development: ethical considerations", "author": "M Seif El-Nasr"}, {"title": "Esr: Ethics and society review of artificial intelligence research", "author": "MS Bernstein"}, {"title": "Street-level realities of data practices in homeless services provision", "author": "N Karusala"}, {"title": "Psychological foundations of explainability and interpretability in artificial intelligence", "author": "DA Broniatowski"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Dimensions of diversity in human perceptions of algorithmic fairness", "author": "N Grgi\u0107-Hla\u010da"}, {"title": "Progressive disclosure: When, why, and how do users want algorithmic transparency information?", "author": "A Springer"}, {"title": "Conceptualising contestability: Perspectives on contesting algorithmic decisions", "author": "H Lyons", "abstract": "As the use of algorithmic systems in high-stakes decision-making increases, the ability to contest algorithmic decisions is being recognised as an important safeguard for individuals. Yet, there is little guidance on what'contestability'--the ability to contest decisions--in relation to algorithmic decision-making requires. Recent research presents different conceptualisations of contestability in algorithmic decision-making. We contribute to this growing body of work by describing and analysing the perspectives of people and", "keywords": {"keywords": ["contestability", "algorithmic decisions", "safeguard", "conceptualisations", "high-stakes"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["descriptive"], "discipline": ["sociology"], "content": ["methodological"], "application area": ["algorithmic decision-making"]}}, {"title": "Combating fake news with interpretable news feed algorithms", "author": "S Mohseni"}, {"title": "Procedural justice and risk-assessment algorithms", "author": "AJ Wang"}, {"title": "Some HCI priorities for GDPR-compliant machine learning", "author": "M Veale"}, {"title": "After-action review for AI (AAR/AI)", "author": "J Dodge"}, {"title": "Explain, edit, and understand: Rethinking user study design for evaluating model explanations", "author": "S Arora"}, {"title": "Artificial intelligence explainability: the technical and ethical dimensions", "author": "JA McDermid"}, {"title": "Designing visual markers for continuous artificial intelligence support: A colonoscopy case study", "author": "N van Berkel"}, {"title": "Would you do it?: enacting moral dilemmas in virtual reality for understanding ethical decision-making", "author": "E Niforatos"}, {"title": "Fairness-aware machine learning", "author": "J Dunkelau"}, {"title": "Making transparency clear", "author": "A Springer", "abstract": "Algorithmic transparency is currently invoked for two separate purposes: to improve trust in  systems and to provide insight into problems like algorithmic bias. Although transparency can", "keywords": {"keywords": ["algorithmic transparency", "trust", "insight", "algorithmic bias"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["descriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["data analysis"]}}, {"title": "The flaws of policies requiring human oversight of government algorithms", "author": "B Green"}, {"title": "An algorithmic framework for fairness elicitation", "author": "C Jung"}, {"title": "Exploring user opinions of fairness in recommender systems", "author": "J Smith"}, {"title": "Psychological Effects of the Allocation Process in Human\u2013Robot Interaction \u2013 A Model for Research on ad hoc Task Allocation", "author": "A Tausch"}, {"title": "Smart technologies and our sense of self: Going beyond epistemic counter-profiling", "author": "S Delacroix"}, {"title": "From what to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices", "author": "J Morley"}, {"title": "Challenging common interpretability assumptions in feature attribution explanations", "author": "J Dinu"}, {"title": "A framework and benchmarking study for counterfactual generating methods on tabular data", "author": "RMB de Oliveira"}, {"title": "A study on fairness and trust perceptions in automated decision making", "author": "J Schoeffer"}, {"title": "Geotechnical Analysis and 3D Fem Modeling of Ville San Pietro (Italy)", "author": "R Bovolenta", "abstract": "The paper describes the three-dimensional numerical model of Ville San Pietro, an Italian village subject to slope movements causing damage. The church (dating back to 1776), which is the most significant building of the area, is modelled too. The information from geotechnical and geophysical surveys on field are used to define the model geometry and the soil properties. A finite element code is adopted to simulate the slope behavior in occurrence of water table fluctuations, detected by piezometers, and to evaluate the slope", "keywords": {"keywords": ["geotechnical analysis", "3D Fem Modeling", "Ville San Pietro", "slope movements", "finite element code"], "research type": ["case study"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["engineering"], "content": ["application"], "application area": ["geotechnical engineering"]}}, {"title": "A top-level model of case-based argumentation for explanation: formalisation and experiments", "author": "H Prakken"}, {"title": "The edge-to-cloud continuum", "author": "D Milojicic"}, {"title": "Why did you do that?", "author": "G Pa\u00e7ac\u0131"}, {"title": "Trustworthy artificial intelligence: a review", "author": "D Kaur"}, {"title": "Denied by an (Unexplainable) Algorithm: Teleological Explanations for Algorithmic Decisions Enhance Customer Satisfaction", "author": "G Tomaino"}, {"title": "Position: We Can Measure XAI Explanations Better with Templates.", "author": "J Dodge"}, {"title": "Interfaces for Explanations in Human-AI Interaction: Proposing a Design Evaluation Approach", "author": "H Mucha"}, {"title": "Minimalistic explanations: capturing the essence of decisions", "author": "M Schuessler"}, {"title": "The need for sensemaking in networked privacy and algorithmic responsibility", "author": "M Van Kleek"}, {"title": "Privacy, trust and ethical issues", "author": "N Shadbolt", "abstract": "at the issues that social machines raise about privacy, trust and ethics. The chapter begins by  surveying the ethical  to generate trust, and how that might impact the wider ethical status of", "keywords": {"keywords": ["privacy", "trust", "ethical issues", "social machines", "ethics"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["application"], "application area": ["sociology"]}}, {"title": "Remarks on the possibility of ethical reasoning in an artificial intelligence system by means of abductive models", "author": "A Sans"}, {"title": "Adversarial xai methods in cybersecurity", "author": "A Kuppa"}, {"title": "What can crowd computing do for the next generation of AI systems?", "author": "U Gadiraju"}, {"title": "\" I had a solid theory before but it's falling apart\": Polarizing Effects of Algorithmic Transparency", "author": "A Springer"}, {"title": "Reasons, values, stakeholders: A philosophical framework for explainable artificial intelligence", "author": "A Kasirzadeh"}, {"title": "gbt-hips: Explaining the classifications of gradient boosted tree ensembles", "author": "J Hatwell"}, {"title": "Took a pic and got declined, vexed and perplexed: facial recognition in algorithmic management", "author": "EA Watkins"}, {"title": "Fairness through the lens of proportional equality", "author": "A Biswas"}, {"title": "Subplex: Towards a better understanding of black box model explanations at the subpopulation level", "author": "GYY Chan"}, {"title": "Protection of Natural Persons with Regard to Automated Individual Decision-Making in the GDPR", "author": "A Dro\u017cd\u017c", "abstract": "Increasingly, algorithms regulate our lives. Personal data is routinely processed on an unprecedented scale in both private and public sectors. This shift from more subjective and less structured human decision-making processes to automated ones has provoked numerous concerns with regard to the rights and freedoms of natural persons affected. In particular, those attached to profiling that can lead to discrimination influencing crucial opportunities of individuals, such as the ability to obtain credit, insurance, education, a job or", "keywords": {"keywords": ["automated individual decision-making", "GDPR", "natural persons", "profiling", "discrimination"], "research type": ["theoretical"], "methodology": ["mixed-methods"], "purposes": ["exploratory"], "discipline": ["law"], "content": ["application"], "application area": ["data protection"]}}, {"title": "Human-XAI interaction: a review and design principles for explanation user interfaces", "author": "M Chromik"}, {"title": "Towards accountability: providing intelligible explanations in autonomous driving", "author": "D Omeiza"}, {"title": "Perceptions of fairness", "author": "NA Saxena"}, {"title": "Explainable case-based reasoning: a survey", "author": "JM Schoenborn"}, {"title": "The ethical use of artificial intelligence in human resource management: a decision-making framework", "author": "S Bankins"}, {"title": "Project report: Perceptions of AI in hiring", "author": "R Gelles"}, {"title": "Wann ist K\u00fcnstliche Intelligenz (un-) fair?", "author": "F Marcinkowski"}, {"title": "Algorithmic reasoning: The production of subjectivity through data", "author": "G van Eijk"}, {"title": "Appropriate fairness perceptions? On the effectiveness of explanations in enabling people to assess the fairness of automated decision systems", "author": "J Schoeffer"}, {"title": "The Role of Accuracy in Algorithmic Process Fairness Across Multiple Domains", "author": "M Albach", "abstract": "Machine learning is often used to aid in human decision-making, sometimes for life-altering decisions like when determining whether or not to grant bail to a defendant or a loan to an applicant. Because of their importance, it is critical to ensure that the processes used to reach these decisions are considered fair. A common approach is to enforce some fairness constraint over the outcomes of a decision maker, but there is no single, generally-accepted definition of fairness. With notable exceptions, most of the literature on algorithmic fairness", "keywords": {"keywords": ["machine learning", "algorithmic process fairness", "accuracy", "decision-making", "fairness constraint"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["algorithmic fairness"]}}, {"title": "Bridging the gap between explainable AI and uncertainty quantification to enhance trustability", "author": "D Seu\u00df"}, {"title": "The fallacy of equating\" blindness\" with fairness: ensuring trust in machine learning applications to consumer credit", "author": "GM Abuhamad"}, {"title": "\" Look! It's a Computer Program! It's an Algorithm! It's AI!\": Does Terminology Affect Human Perceptions and Evaluations of Intelligent Systems?", "author": "M Langer"}, {"title": "Algorithmic and human decision making: for a double standard of transparency", "author": "M G\u00fcnther"}, {"title": "GDPR impacts and opportunities for computer-aided diagnosis guidelines and legal perspectives", "author": "M Pedrosa"}, {"title": "Learning to generate natural language rationales for game playing agents", "author": "U Ehsan"}, {"title": "AI decision making with dignity? Contrasting workers' justice perceptions of human and AI decision making in a human resource management context", "author": "S Bankins"}, {"title": "Designing civic technology with trust", "author": "E Corbett"}, {"title": "Datenqualit\u00e4t bei algorithmischen Entscheidungen-\u00dcberlegungen aus Anlass des Gutachtens der Datenethikkommission", "author": "J Stevens"}, {"title": "Legal perspective on possible fairness measures\u2013A legal discussion using the example of hiring decisions", "author": "MP Hauer", "abstract": "With the increasing use of AI in algorithmic decision making (eg based on neural networks), the question arises how bias can be excluded or mitigated. There are some promising approaches, but many of them are based on a\u201d fair\u201d ground truth, others are based on a subjective goal to be reached, which leads to the usual problem of how to define and compute\u201d fairness\u201d. The different functioning of algorithmic decision making in contrast to human decision making leads to a shift from a process-oriented to a result-oriented", "keywords": {"keywords": ["AI", "algorithmic decision making", "bias", "fairness measures", "legal perspective"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["explanatory"], "discipline": ["law"], "content": ["methodological"], "application area": ["hiring decisions"]}}, {"title": "Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation", "author": "J Hannan"}, {"title": "Cognitive and Emotional Response to Fairness in AI\u2013A Systematic Review", "author": "J Baleis"}, {"title": "Trust in Collaborative Automation in High Stakes Software Engineering Work: A Case Study at NASA", "author": "DG Widder"}, {"title": "A Typology of AI Ethics Tools, Methods and Research to Translate Principles into Practices", "author": "J Morley"}, {"title": "Towards Accountability: The Articulation and Formalization of Fairness in Machine Learning", "author": "L Naudts"}, {"title": "Exploring Understandable Algorithms to Suggest Fitness Tracker Goals that Foster Commitment", "author": "PW Wo\u017aniak"}, {"title": "Fair and Responsible AI: A focus on the ability to contest", "author": "H Lyons"}, {"title": "An overview of fairness in clustering", "author": "A Chhabra"}, {"title": "Understanding user perceptions of trustworthiness in e-recruitment systems", "author": "G Ogunniye"}, {"title": "Model-agnostic explanations using minimal forcing subsets", "author": "X Han", "abstract": "How can we find a subset of training samples that are most responsible for a specific prediction made by a complex black-box machine learning model? More generally, how can we explain the model's decisions to end-users in a transparent way? We propose a new model-agnostic algorithm to identify a minimal set of training samples that are indispensable for a given model's decision at a particular test point, ie, the model's decision would have changed upon the removal of this subset from the training dataset. Our algorithm identifies", "keywords": {"keywords": ["model-agnostic explanations", "minimal forcing subsets", "black-box machine learning model", "transparency", "algorithm"], "research type": ["theory"], "methodology": ["mixed-methods"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Perception of fairness in algorithmic decisions: Future developers' perspective", "author": "S Kleanthous"}, {"title": "Why reliabilism is not enough: Epistemic and moral justification in machine learning", "author": "A Smart"}, {"title": "\u201d Because AI is 100% right and safe\u201d: User Attitudes and Sources of AI Authority in India", "author": "S Kapania"}, {"title": "Counterfactual Explanations as Interventions in Latent Space", "author": "R Crupi"}, {"title": "Accurate, Fair, and Explainable: Building Human-Centered AI", "author": "A Springer"}, {"title": "Making Data Work: The Human and Organizational Lifeworlds of Data Science Practices", "author": "S Passi"}, {"title": "Interpretable data-based explanations for fairness debugging", "author": "R Pradhan"}, {"title": "\" How I Know For Sure\": People's Perspectives on Solely Automated {Decision-Making}({{{{{SADM}}}}})", "author": "S Kaushik"}, {"title": "Big Data und Governance im digitalen Zeitalter", "author": "L Ulbricht"}, {"title": "LEx: A Framework for Operationalising Layers of Machine Learning Explanations", "author": "R Singh", "abstract": "Several social factors impact how people respond to AI explanations used to justify AI decisions affecting them personally. In this position paper, we define a framework called the\\textit {layers of explanation}(LEx), a lens through which we can assess the appropriateness of different types of explanations. The framework uses the notions of\\textit {sensitivity}(emotional responsiveness) of features and the level of\\textit {stakes}(decision's consequence) in a domain to determine whether different types of explanations are\\textit", "keywords": {"keywords": ["machine learning explanations", "framework", "sensitivity", "stakes", "appropriateness"], "research type": ["theoretical"], "methodology": ["all"], "purposes": ["prescriptive"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["AI explanations"]}}, {"title": "Human-AI Collaboration for UX Evaluation: Effects of Explanation and Synchronization", "author": "M Fan"}, {"title": "Data protection for the prevention of algorithmic discrimination: Protecting from discrimination and other harms caused by algorithms through privacy in the EU \u2026", "author": "AS Arnanz"}, {"title": "Position: The Case Against Case-Based Explanation.", "author": "J Dodge"}, {"title": "Addressing cognitive biases in augmented business decision systems", "author": "T Baudel"}, {"title": "Factors Impacting K-12 Teachers in Understanding Explanations of Machine Learning Model on Students' Performance", "author": "H Guo"}, {"title": "Automatic information search for countering covid-19 misinformation through semantic similarity", "author": "\u00c1 Huertas Garc\u00eda"}, {"title": "Toward Engineering AI Software for Fairness", "author": "C Lazo"}, {"title": "Fairness, explainability and in-between: understanding the impact of different explanation methods on non-expert users' perceptions of fairness toward an algorithmic \u2026", "author": "A Shulner-Tal"}, {"title": "The Tension Between Information Justice and Security: Perceptions of Facial Recognition Targeting.", "author": "EA Watkins"}, {"title": "CounterNet: End-to-End Training of Counterfactual Aware Predictions", "author": "H Guo", "abstract": "This work presents CounterNet, a novel end-to-end learning framework which integrates the predictive model training and counterfactual (CF) explanation generation into a single end-to-end pipeline. Counterfactual explanations attempt to find the smallest modification to the feature values of an instance that changes the prediction of the ML model to a predefined output. Prior CF explanation techniques rely on solving separate time-intensive optimization problems for every single input instance to find CF examples, and also suffer from the", "keywords": {"keywords": ["CounterNet", "end-to-end learning", "predictive model training", "counterfactual explanations", "feature values modification"], "research type": ["theoretical"], "methodology": ["quantitative"], "purposes": ["explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Risky Reforms: A Sociotechnical Analysis of Algorithms as Tools for Social Change", "author": "B Green"}, {"title": "How Explainability Contributes to Trust in AI", "author": "A Ferrario"}, {"title": "Acceptability and 5G in the Medical Field: The Impact of the Level of Information", "author": "N Martin"}, {"title": "How Fake News Affect Trust in the Output of a Machine Learning System for News Curation", "author": "H Heuer"}, {"title": "On fairness and interpretability", "author": "JM Jose"}, {"title": "Multi-objective Explanations of GNN Predictions", "author": "Y Liu"}, {"title": "Towards a Standard for Identifying and Managing Bias in Artificial Intelligence", "author": "R Schwartz"}, {"title": "On the Relationship Between Explanations, Fairness Perceptions, and Decisions", "author": "J Schoeffer"}, {"title": "The Algorithmic Imprint", "author": "U Ehsan"}, {"title": "How Experienced Designers of Enterprise Applications Engage AI as a Design Material", "author": "N Yildirim", "abstract": "HCI research has explored AI as a design material, suggesting that designers can envision AI's design opportunities to improve UX. Recent research claimed that enterprise applications offer an opportunity for AI innovation at the user experience level. We conducted design workshops to explore the practices of experienced designers who work on cross-functional AI teams in the enterprise. We discussed how designers successfully work with and struggle with AI. Our findings revealed that designers can innovate at the", "keywords": {"keywords": ["AI", "design material", "enterprise applications", "HCI research", "user experience"], "research type": ["Empirical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["User Experience Design"]}}, {"title": "Correcting classification: A bayesian framework using explanation feedback to improve classification abilities", "author": "Y Bekkemoen"}, {"title": "When AI moderates online content: effects of human collaboration and interactive transparency on user trust", "author": "MD Molina"}, {"title": "A Framework for Fairness: A Systematic Review of Existing Fair AI Solutions", "author": "B Richardson"}, {"title": "Attitudes and Folk Theories of Data Subjects on Transparency and Accuracy in Emotion Recognition", "author": "G Grill"}, {"title": "Explaining an Argumentation. Differences and Structural Analysis as a Foundation to Improve Case-Based Explanation", "author": "JM Schoenborn"}, {"title": "Learning Relevant Explanations", "author": "C Russell"}, {"title": "Mental Models of Adversarial Machine Learning", "author": "L Bieringer"}, {"title": "How to explain AI systems to end users: a systematic literature review and research agenda", "author": "S Laato"}, {"title": "Artificial fairness? Trust in algorithmic police decision-making", "author": "Z Hobson"}, {"title": "Biases as Values: Evaluating Algorithms in Context", "author": "M D\u00edaz", "abstract": "This dissertation asks how researchers can create more equitable algorithmic systems. Ultimately, this thesis explores methods and implications of representing subjects of analysis in the design and evaluation of algorithmic systems. I also unpack how algorithmic tools measure and quantify human behavior, giving heed to the potential impacts of these systems on underrepresented communities. Building off of current work in HCI and algorithmic fairness, my research raises questions about how we can evaluate algorithms to", "keywords": {"keywords": ["algorithmic systems", "equitable", "underrepresented communities", "algorithmic fairness", "evaluation"], "research type": ["theoretical"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["algorithmic fairness"]}}, {"title": "Imagining data, between Laplace's demon and the rule of succession", "author": "R Binns"}, {"title": "Understanding User Sensemaking in Machine Learning Fairness Assessment Systems", "author": "Z Gu"}, {"title": "Counterfactual explanations for prediction and diagnosis in xai", "author": "X Dai"}, {"title": "Explainability auditing for intelligent systems: a rationale for multi-disciplinary perspectives", "author": "M Langer"}, {"title": "Explainable AI: Foundations, Applications, Opportunities for Data Management Research", "author": "R Pradhan"}, {"title": "The Many Facets of Trust in AI: Formalizing the Relation Between Trust and Fairness, Accountability, and Transparency", "author": "B Knowles"}, {"title": "Mitigating Bias in Algorithmic Systems: A Fish-Eye View of Problems and Solutions Across Domains", "author": "K Orphanou"}, {"title": "Deciding how to decide: six key questions for reducing AI's democratic deficit", "author": "J Cowls"}, {"title": "Code of ethics for data-based value creation", "author": "M Christen"}, {"title": "Principled Diverse Counterfactuals in Multilinear Models", "author": "I Papantonis", "abstract": "Machine learning (ML) applications have automated numerous real-life tasks, improving both private and public life. However, the black-box nature of many state-of-the-art models poses the challenge of model verification; how can one be sure that the algorithm bases its decisions on the proper criteria, or that it does not discriminate against certain minority groups? In this paper we propose a way to generate diverse counterfactual explanations from multilinear models, a broad class which includes Random Forests, as well as Bayesian", "keywords": {"keywords": ["counterfactual explanations", "multilinear models", "model verification", "discrimination", "diversity"], "research type": ["empirical"], "methodology": ["quantitative", "qualitative", "mixed-methods"], "purposes": ["exploratory", "descriptive", "explanatory"], "discipline": ["computer science"], "content": ["methodological"], "application area": ["machine learning"]}}, {"title": "Towards Improved Marketing Mix Decisions through Deep Learning and Human-AI Collaboration", "author": "M L\u00f6we"}, {"title": "Facing Many Objectives for Fairness in Machine Learning", "author": "D Villar"}, {"title": "Features of Explainability: How users understand counterfactual and causal explanations for categorical and continuous features in XAI", "author": "G Warren"}, {"title": "Fairness perceptions of artificial intelligence decision-making", "author": "L JIANG"}, {"title": "How to Support Users in Understanding Intelligent Systems? Structuring the", "author": "M EIBAND"}, {"title": "New Foundations of Ethical Multiagent Systems", "author": "PK Murukannaiah"}, {"title": "Ranking for Decision Making: Fairness and Usability", "author": "C Kuhlman"}, {"title": "What is the Bureaucratic Counterfactual? Categorical versus Algorithmic Prioritization in US Social Policy", "author": "RA Johnson"}, {"title": "Method for Appropriating the Brief Implicit Association Test to Elicit Biases in Users", "author": "T Dingler"}, {"title": "Towards improving automation with user input", "author": "J \u00c5str\u00f6m", "abstract": "As complex systems become more available, the possibility to leverage human intelligence to continuously train these systems is becoming increasingly valuable. Collecting and incorporating feedback from end-users into the system development processes could hold great potential for future development of autonomous systems, but it is not without difficulties A literature review was conducted with the aim to review and help categorize the different dynamics relevant to the act of collecting and implementing user feedback in system", "keywords": {"keywords": ["automation", "user input", "complex systems", "human intelligence", "autonomous systems"], "research type": "Review", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Computer Science", "content": "Methodological", "application area": "System development"}}, {"title": "Towards Model Understanding", "author": "D Pruthi"}, {"title": "Machine Behaviour", "author": "M Cebrian Ramos"}, {"title": "Towards using case-based explanations as a knowledge foundation.", "author": "JM Schoenborn"}, {"title": "Practical Algorithms for Resource Allocation and Decision Making", "author": "DC McElfresh"}, {"title": "Exploiting Domain-specific Knowledge for Classifier Learning-AU-based Facial Expression Analysis and Emotion Recognition", "author": "D Seuss"}, {"title": "AI Suffrage: A four-country survey on the acceptance of an automated voting system", "author": "V Suter"}, {"title": "Advancing Explainability and Fairness in AI with Human-Algorithm Collaborations", "author": "HF Cheng"}, {"title": "Machine Learning in Artificial Intelligence", "author": "A Chakraborty"}, {"title": "Antecedents of Perceived Fairness and User Trust in Scientific Recommender Systems", "author": "P Yadav"}, {"title": "The road to a human-centred digital society: opportunities, challenges and responsibilities for humans in the age of machines", "author": "D De Cremer", "abstract": "The growing adoption of intelligent technologies has brought us to a crossroad. The creators of intelligent technologies are acquiring the power to influence a wide variety of outcomes that are important to human end-users. In doing so, those same intelligent technologies are being used to undermine and even actively harm the interests of those same end-users. In the absence of a recalibration, we are almost certainly headed down a path wherein intelligent technologies will primarily serve the interests of developers and owners of", "keywords": {"keywords": ["human-centred", "digital society", "intelligent technologies", "opportunities", "challenges"], "research type": "Theoretical", "methodology": "Mixed-methods", "purposes": "Prescriptive", "discipline": "Sociology", "content": "Application", "application area": "Human-computer interaction"}}, {"title": "Enhancing Fairness Perception\u2013Towards Human-Centred AI and Personalized Explanations Understanding the Factors Influencing Laypeople's Fairness Perceptions \u2026", "author": "A Shulner-Tal"}, {"title": "Comparing AI and Human Feedback: The Role of Source in Feedback Perceptions", "author": "MN Nargesi"}, {"title": "Do Americans Think the Digital Economy is Fair? Using Supervised Learning to Explore Evaluations of Predictive Automation", "author": "E Lehoucq"}, {"title": "Entering a New World: The Minimal Amount of Knowledge to Act as a Trustworthy Adviser Using Case-Based Explanations in a New Domain", "author": "JM Schoenborn"}, {"title": "Fairness Preferences, Actual and Hypothetical: A Study of Crowdworker Incentives", "author": "A Peng"}, {"title": "Utrecht Data School", "author": "M Wieringa"}, {"title": "Columbia University Doctoral program in Communications New York, NY 10024 EAW2198@ Columbia. edu", "author": "EA Watkins"}, {"title": "Prediction of monetary penalties for data protection cases in multiple languages", "author": "A Ceross"}, {"title": "Fair Engineering of Machine Learning Systems\u2013Lessons Learned From", "author": "J Sengewald"}, {"title": "Trust in Collaborative Automation in High Stakes Software Engineering Work", "author": "D Widder", "abstract": "The amount of autonomy in software engineering tools is increasing as developers build increasingly complex systems. Research in other domains shows that too much or too little trust in autonomous tools can have negative consequences, but we are not aware of any study that has investigated trust in autonomous tools in the highly interactive context of a software engineering workplace. We present the results of a ten week ethnographic case study of engineers collaborating with autonomous tools to write flight software at a large", "keywords": {"keywords": ["Trust", "Collaborative Automation", "High Stakes", "Software Engineering Work", "Autonomous Tools"], "research type": ["Case study"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Application"], "application area": ["Flight software"]}}, {"title": "Explainable Information Security: Development of a Construct and Instrument", "author": "D Dang-Pham"}, {"title": "Everybody is talking about ethics and IT-What are they talking about?", "author": "O Levina"}, {"title": "Fairness and Explanation in AI-Informed Decision Making", "author": "A Angerschmid"}, {"title": "Revealing Unfair Models by Mining Interpretable Evidence", "author": "M Bajaj"}, {"title": "Designing for Contestation: Insights from Administrative Law", "author": "H Lyons"}, {"title": "Computational Perspectives on Democracy", "author": "A Kahng"}, {"title": "Governing Artificial Intelligence in Post-Pandemic Society", "author": "A Arunagiri"}, {"title": "Comparing the Perceived Legitimacy of Content Moderation Processes: Contractors, Algorithms, Expert Panels, and Digital Juries", "author": "CA Pan"}, {"title": "Towards responsible, lawful and ethical data processing: patient data in the UK", "author": "T Johnson"}, {"title": "ObjectivAIze: Measuring Performance and Biases in Augmented Business Decision Systems", "author": "T Baudel", "abstract": "Business process management organizes flows of information and decisions in large organizations. These systems now integrate algorithmic decision aids leveraging machine learning: each time a stakeholder needs to make a decision, such as a purchase, a quote, or hiring someone, the software leverages the inputs and outcomes of similar past decisions to provide guidance, as a recommendation. If the confidence is high, the process may be automated. Otherwise, it may still help provide consistency in the decisions. Yet, we may", "keywords": {"keywords": ["augmented business decision systems", "performance measurement", "biases", "algorithmic decision aids", "machine learning"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Business process management"]}}, {"title": "Understanding Decision Subjects' Fairness Perceptions and Retention in Repeated Interactions with AI-Based Decision Systems", "author": "MA Gemalmaz"}, {"title": "Explanations and Processes to Enable Humans to Assess AI with Respect to Manipulable Properties", "author": "JE Dodge"}, {"title": "Are we justified attributing a mistake in diagnosis to an AI diagnostic system?", "author": "D Babushkina"}, {"title": "Assessing the boundaries of accuracy and fairness", "author": "A Valdivia"}, {"title": "Towards Supporting and Documenting Algorithmic Fairness in the Data Science Workflow", "author": "G Harrison"}, {"title": "Investigating Explanations that Target Training Data.", "author": "AI Anik"}, {"title": "Responsible AI Systems: Who are the Stakeholders?", "author": "A Deshpande"}, {"title": "Contesting automated decisions", "author": "M ALMADA"}, {"title": "People are not coins. Morally distinct types of predictions necessitate different fairness constraints", "author": "E Vigano"}, {"title": "Morals, ethics, and trust: correctional officers' view of AI implementation: A study of the effects of ethical and moral values on trust and artificial implementation within \u2026", "author": "E Andersson", "abstract": "The use of AI within the judicial field has seen an increase in recent years, and the implementation of AI brings with it a new set of ethical and moral dilemmas that affect the field as well as the individuals working there. This study explores the moral and ethical values of correctional officers within the Swedish Prison and Probation service and how their values, together with trust in AI, affect their perception of a future AI implementation. This study conceptualizes AI as an emerging technology that has the potential to alter what it", "keywords": {"keywords": ["morals", "ethics", "trust", "correctional officers", "AI implementation"], "research type": "Empirical", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Sociology", "content": "Application", "application area": "Judicial system"}}, {"title": "Social impacts of algorithmic decision-making: A research agenda for the social sciences", "author": "F Gerdon"}, {"title": "TARGETED GENERATION OF COUNTERFACTUAL EXAMPLES FOR DATA AUGMENTATION AND TO MITIGATE THE", "author": "MGMM Hasan"}, {"title": "Contextualization and Exploration of Local Feature Importance Explanations to Improve Understanding and Satisfaction of Non-Expert Users", "author": "C Bove"}, {"title": "Explainable Information Security: Development of a Construct and Instrument", "author": "D Dang"}, {"title": "VU Research Portal", "author": "MA Sosa Hidalgo"}, {"title": "Towards Involving End-users in Interactive Human-in-the-loop AI Fairness", "author": "Y Nakao"}, {"title": "Explaining automated decision-making: a multinational study of the GDPR right to meaningful information", "author": "J Dexe"}, {"title": "New foundations of ethical multiagent systems blue sky ideas track", "author": "PK Murukannaiah"}, {"title": "Explainability's Gain is Optimality's Loss? How Explanations Bias Decision-making", "author": "C Wan"}, {"title": "An Assessment of the Barriers Impacting Responsible Artificial Intelligence", "author": "MI Merhi", "abstract": "Responsible Artificial Intelligence (AI) has recently gained a lot of attention, especially in the last few years. Scholars have conducted systematic literature reviews to gain more knowledge about responsible AI. However, no study has collected and evaluated the most significant barriers to responsible AI. We filled this gap in the literature by identifying eleven barriers and categorized them, using the Technology-Organization-Environment framework, into three categories. We collected data from seven experts and", "keywords": {"keywords": ["Responsible Artificial Intelligence", "barriers", "impact", "Technology-Organization-Environment framework", "experts"], "research type": ["Review"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Computer Science"], "content": ["Methodological"], "application area": ["Responsible AI"]}}, {"title": "Pass/Fail Prediction in Programming Courses", "author": "C Van Petegem"}, {"title": "An Empirical Evaluation of Estimated Outcomes as Explanations in Human-AI Decision-Making", "author": "J Jakubik"}, {"title": "Locating the work of artificial intelligence ethics", "author": "SC Slota"}, {"title": "Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness", "author": "Y Nakao"}, {"title": "Let the user have a say-voice in automated decision-making", "author": "P Hellwig"}, {"title": "HAEM: Obtaining Higher-Quality Classification Task Results with AI Workers", "author": "Y Yamashita"}, {"title": "Medical AI and human dignity: Contrasting perceptions of human and artificially intelligent (AI) decision making in diagnostic and medical resource allocation \u2026", "author": "P Formosa"}, {"title": "How Can Automated Vehicles Explain Their Driving Decisions? Generating Clarifying Summaries Automatically", "author": "F Henze"}, {"title": "\u201cIt's not wrong, but I'm quite disappointed\u201d: Toward an Inclusive Algorithmic Experience for Content Creators with Disabilities", "author": "D Choi"}, {"title": "ARTIFICIAL INTELLIGENCE IN CRIMINAL JUSTICE SETTINGS", "author": "OC Egido", "abstract": "The application of Artificial Intelligence (AI) systems for high-stakes decision making is currently out for debate. In the Criminal Justice System, it can provide great benefits as well as aggravate systematic biases and introduce unprecedented ones. Hence, should artificial devices be involved in the decision-making process? And if the answer is affirmative, where should be the limits of that involvement? To answer these questions, this dissertation examines two popular risk assessment tools currently in use in the United States, LS and", "keywords": {"keywords": ["Artificial Intelligence", "Criminal Justice System", "High-stakes decision making", "Systematic biases", "Risk assessment tools"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Exploratory"], "discipline": ["Sociology"], "content": ["Application"], "application area": ["Criminal Justice"]}}, {"title": "Mitigating Bias in Algorithmic Systems-A Fish-Eye View", "author": "K Orphanou"}, {"title": "The challenges of providing explanations of AI systems when they do not behave like users expect", "author": "M Riveiro"}, {"title": "A Responsive Engagement Approach to Promote the Development of 'Fairer'Algorithms", "author": "H Webb"}, {"title": "Being Profiled: Cogitas Ergo Sum", "author": "L Janssens"}, {"title": "Does distrust in humans predict greater trust in AI? Role of individual differences in user responses to content moderation", "author": "MD Molina"}, {"title": "Voices in the Code: A Story about People, Their Values, and the Algorithm They Made", "author": "DG Robinson"}, {"title": "An overview of Algorithmic Experience (AX)", "author": "D Dang-Pham"}, {"title": "Altruism and Selfishness in Believable Game Agents: Deep Reinforcement Learning in Modified Dictator Games", "author": "D Daylamani-Zad"}, {"title": "Social Robots and Children's Fundamental Rights: A Dynamic Four-Component Framework for Research, Development, and Deployment", "author": "V CHARISI"}, {"title": "How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India", "author": "D Ramesh", "abstract": "Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a'high-risk'AI system", "keywords": {"keywords": ["algorithmic accountability", "platform-user power relations", "instant loan platforms", "financially stressed users", "India"], "research type": ["case study"], "methodology": ["qualitative"], "purposes": ["exploratory"], "discipline": ["sociology"], "content": ["application"], "application area": "financial technology"}}, {"title": "Transparent but incomprehensible: Investigating the relation between transparency, explanations, and usability in automated decision-making", "author": "J Dexe"}, {"title": "What's the Appeal? Perceptions of Review Processes for Algorithmic Decisions", "author": "H Lyons"}, {"title": "Explainability of artificial intelligence in human resources", "author": "M Langer"}, {"title": "AI Systems and Respect for Human Autonomy", "author": "A Laitinen"}, {"title": "How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement and \u2026", "author": "D Buschek"}, {"title": "The Landscape of Artificial Intelligence Ethics: Analysis of Developments, Challenges, and Comparison of Different Markets", "author": "S Natrup"}, {"title": "Short Text Classification Using Contextual Analysis", "author": "S Al Sulaimani"}, {"title": "RESEARCH FINDINGS AND CONTRIBUTIONS", "author": "T Down"}, {"title": "Educating Computer Science Students about Algorithmic Fairness, Accountability, Transparency and Ethics", "author": "M Kasinidou"}, {"title": "Artificial Intelligence in Criminal Justice Settings:: Where should be the limits of Artificial Intelligence in legal decision-making? Should an AI device make a decision \u2026", "author": "O Cibrian Egido", "abstract": "The application of Artificial Intelligence (AI) systems for high-stakes decision making is currently out for debate. In the Criminal Justice System, it can provide great benefits as well as aggravate systematic biases and introduce unprecedented ones. Hence, should artificial devices be involved in the decision-making process? And if the answer is affirmative, where should be the limits of that involvement? To answer these questions, this dissertation examines two popular risk assessment tools currently in use in the United States, LS and", "keywords": {"keywords": ["Artificial Intelligence", "Criminal Justice", "Decision-making", "Bias", "Risk assessment"], "research type": ["Theoretical"], "methodology": ["Qualitative"], "purposes": ["Explanatory"], "discipline": ["Social Sciences"], "content": ["Methodological"], "application area": ["Criminal Justice System"]}}, {"title": "Datafied Society (research platform)", "author": "M Wieringa"}, {"title": "A Human-Centric Perspective on Fairness and Transparency in Algorithmic Decision-Making", "author": "J Schoeffer"}, {"title": "From Responsibility to Reason-Giving Explainable Artificial Intelligence", "author": "K Baum"}, {"title": "Industrial practitioners' mental models of adversarial machine learning", "author": "L Bieringer"}, {"title": "Public preferences for governing AI technology: Comparative evidence", "author": "S Ehret"}, {"title": "Explanation Strategies as an Empirical-Analytical Lens for Socio-Technical Contextualization of Machine Learning Interpretability", "author": "JJ Benjamin"}, {"title": "When the algorithm is your teacher: Perceptions of fairness, trust, and decision acceptance in education", "author": "FAC Koutsouris"}, {"title": "Heleen Rutjes h. rutjes@ tue. nl Eindhoven University of Technology Eindhoven, The Netherlands", "author": "WA IJsselsteijn"}, {"title": "\u201cLook! It'sa Computer Program! It's an Algorithm! It's AI!\u201d: Does Terminology Affect Human Perceptions and Evaluations of Algorithmic Decision-Making Systems?", "author": "M Langer"}, {"title": "Effects of AI Vs. Human Moderators and Interactive Transparency on Perceived Trust and Acceptance of Content Classification Systems", "author": "MM Davila", "abstract": "To identify and counter the upsurge of dangerous or harmful content posted on social media (eg, hate speech, dangerous challenges), companies like Facebook and Twitter are starting to leverage the power of artificial intelligence (AI) to moderate user-generated content. Through this process, when content is identified as belonging to a particular category (eg, hate speech), it is flagged and oftentimes taken down. These efforts have faced criticism, mainly due to lack of clarity regarding the source of classification and lack of understanding", "keywords": {"keywords": ["AI vs. Human Moderators", "Interactive Transparency", "Perceived Trust", "Acceptance", "Content Classification Systems"], "research type": "Empirical", "methodology": "Mixed-methods", "purposes": "Exploratory", "discipline": "Sociology", "content": "Methodological", "application area": "Social Media Moderation"}}, {"title": "Technology in the workplace: Opportunities and challenges", "author": "Y Griep"}, {"title": "The Role-Play of Privacy Perceptions in Contact Tracing App Models", "author": "H Farzand"}, {"title": "Distribution of Data Power in Human-Machine Collaboration", "author": "X Tang"}, {"title": "Transform Your Risk Processes Using Neural Networks", "author": "R Ferguson"}, {"title": "Human intervention in automated decision-making", "author": "M Almada"}, {"title": "Algorithmic explanations: to become a mockingbird", "author": "A Hare"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Multi-agent Social Choice for Dynamic Fairness-aware Recommendation", "author": "R Burke"}, {"title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons", "author": "X Wang"}, {"title": "\" There Is Not Enough Information\": On the Effects of Explanations on Perceptions of Informational Fairness and Trustworthiness in Automated Decision-Making", "author": "J Schoeffer", "abstract": "Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people's perceptions of informational fairness (ie, whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying", "keywords": {"keywords": ["automated decision systems", "explanations", "perceptions", "informational fairness", "trustworthiness"], "research type": ["Empirical"], "methodology": ["Quantitative"], "purposes": ["Exploratory"], "discipline": ["Computer science"], "content": ["Methodological"], "application area": ["Decision-making"]}}, {"title": "Workshop on Detection and Design for Cognitive Biases in People and Computing Systems", "author": "T Dingler"}, {"title": "Re-thinking smartness: designing more ethical connected devices for the home", "author": "W Seymour"}, {"title": "Applying Mathematics to the Natural and Social World", "author": "A Kasirzadeh"}, {"title": "Tensions in transparent urban AI: designing a smart electric vehicle charge point", "author": "K Alfrink"}, {"title": "False positives vs. false negatives: public opinion on the cost ratio in criminal justice risk assessment", "author": "B Kang"}, {"title": "T\u1ed5ng quan v\u1ec1 tr\u1ea3i nghi\u1ec7m thu\u1eadt to\u00e1n (AX)", "author": "D Dang-Pham"}, {"title": "\u65e5\u672c\u4eba\u306b\u3068\u3063\u3066\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u5dee\u5225\u306f\u3069\u3046\u611f\u3058\u3089\u308c\u308b\u304b?", "author": "\u524d\u7530\u6625\u9999"}, {"title": "La confianza en la experticia humana y artificial", "author": "KSP Mart\u00ednez"}, {"title": "Everyday engagement", "author": "J Niess"}, {"title": "O julgamento de processos por intelig\u00eancia artificial analisado \u00e0 luz dos Princ\u00edpios de Bangalore de Conduta Judicial", "author": "PE de Oliveira", "abstract": "A intelig\u00eancia artificial \u00e9 realidade no julgamento de processos e se verificou a necessidade de relacionar os poss\u00edveis reflexos desta ferramenta na conduta judicial. O estudo teve o prop\u00f3sito de verificar se os Princ\u00edpios de Bangalore de Conduta Judicial s\u00e3o compat\u00edveis com o uso de modelo preditivo de intelig\u00eancia artificial para julgar o processo judicial. Tratou-se de pesquisa multidisciplinar desenvolvida por meio de revis\u00e3o bibliogr\u00e1fica, realizando-se a an\u00e1lise das informa\u00e7\u00f5es pelo m\u00e9todo qualitativo. O resultado obtido \u00e9 de", "keywords": {"keywords": ["intelig\u00eancia artificial", "processos judiciais", "conduta judicial", "Princ\u00edpios de Bangalore", "modelo preditivo"], "research type": "Mixed-methods", "methodology": "Qualitative", "purposes": "Exploratory", "discipline": "Law", "content": "Application", "application area": "Judicial process"}}, {"title": "Justi\u00e7a atuarial: vis\u00e3o dos professores da Universidade Federal de Pernambuco", "author": "LLP SANTOS"}, {"title": "\u4eba\u5de5\u667a\u80fd\u51b3\u7b56\u7684\u516c\u5e73\u611f\u77e5", "author": "\u848b\u8def\u8fdc\uff0c \u66f9\u674e\u6885\uff0c \u79e6\u6615\uff0c \u8c2d\u73b2\uff0c \u9648\u6668\uff0c \u5f6d\u5c0f\u6590"}, {"title": "\uc778\uacf5\uc9c0\ub2a5 \uc54c\uace0\ub9ac\uc998 \uae30\ubc18 \uc758\uc0ac\uacb0\uc815\uc758 \uacf5\uc815\uc131 \uc9c0\uac01", "author": "\uc190\uc601\uc2e0"}, {"title": "\u672c\u4f53, \u8ba4\u8bc6\u4e0e\u4ef7\u503c: \u667a\u80fd\u6559\u80b2\u7684\u6280\u672f\u4f26\u7406\u98ce\u9669\u9690\u5fe7\u4e0e\u6cbb\u7406\u8fdb\u8def", "author": "\u97e6\u5999\uff0c \u4f55\u821f\u6d0b"}], "inherently explainable reinforcement learning in natural language": [{"title": "ScienceWorld: Is your Agent Smarter than a 5th Grader?", "author": "R Wang"}, {"title": "A survey of text games for reinforcement learning informed by natural language", "author": "P Osborne"}, {"title": "In-context policy iteration", "author": "E Brooks"}, {"title": "Explaining Autonomous Driving Actions with Visual Question Answering", "author": "S Atakishiyev"}, {"title": "In-context policy iteration", "author": "E Brooks"}, {"title": "A Minimal Approach for Natural Language Action Space in Text-based Games", "author": "DK Ryu"}, {"title": "Story Shaping: Teaching Agents Human-like Behavior with Stories", "author": "X Peng"}, {"title": "Dialogue Shaping: Empowering Agents through NPC Interaction", "author": "W Zhou"}, {"title": "Ambient Adventures: Teaching ChatGPT on Developing Complex Stories", "author": "Z Chen"}], "effect of confidence and explanation on accuracy and rrust calibration in ai-assisted decision making": [{"title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems", "author": "S Mohseni"}, {"title": "Trusting automation: Designing for responsivity and resilience", "author": "EK Chiou"}, {"title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance", "author": "G Bansal"}, {"title": "To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making", "author": "Z Bu\u00e7inca"}, {"title": "Expanding explainability: Towards social transparency in ai systems", "author": "U Ehsan"}, {"title": "Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models", "author": "P Vaithilingam"}, {"title": "Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty", "author": "U Bhatt"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "How do data science workers collaborate? roles, workflows, and tools", "author": "AX Zhang"}, {"title": "Human confidence in artificial intelligence and in themselves: The evolution and impact of confidence on adoption of AI advice", "author": "L Chong"}, {"title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "author": "Z Jiang"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "HIVE: Evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "Fairness and explanation in AI-informed decision making", "author": "A Angerschmid"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "author": "H Suresh"}, {"title": "Learning to complement humans", "author": "B Wilder"}, {"title": "Interactive model cards: A human-centered approach to model documentation", "author": "A Crisan"}, {"title": "Will humans-in-the-loop become borgs? Merits and pitfalls of working with AI", "author": "A F\u00fcgener"}, {"title": "Human-ai collaboration via conditional delegation: A case study of content moderation", "author": "V Lai"}, {"title": "How child welfare workers reduce racial disparities in algorithmic decisions", "author": "HF Cheng"}, {"title": "Designing for responsible trust in AI systems: A communication perspective", "author": "QV Liao"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "How to evaluate trust in AI-assisted decision making? A survey of empirical methodologies", "author": "O Vereschak"}, {"title": "Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems", "author": "C Panigutti"}, {"title": "Rapid trust calibration through interpretable and uncertainty-aware AI", "author": "R Tomsett"}, {"title": "When confidence meets accuracy: Exploring the effects of multiple performance indicators on trust in machine learning models", "author": "A Rechkemmer"}, {"title": "Human reliance on machine learning models when performance feedback is limited: Heuristics and risks", "author": "Z Lu"}, {"title": "Trust in AutoML: exploring information needs for establishing trust in automated machine learning systems", "author": "J Drozdal"}, {"title": "A systematic review of human\u2013computer interaction and explainable artificial intelligence in healthcare with artificial intelligence techniques", "author": "M Nazar"}, {"title": "Explanations can reduce overreliance on ai systems during decision-making", "author": "H Vasconcelos"}, {"title": "Do people engage cognitively with AI? Impact of AI assistance on incidental learning", "author": "KZ Gajos"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen"}, {"title": "The utility of explainable ai in ad hoc human-machine teaming", "author": "R Paleja"}, {"title": "Experimental evidence of effective human\u2013AI collaboration in medical decision-making", "author": "C Reverberi"}, {"title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspective", "author": "H Lakkaraju"}, {"title": "Understanding the effect of out-of-distribution examples and interactive explanations on human-ai decision making", "author": "H Liu"}, {"title": "Explainable artificial intelligence: Evaluating the objective and subjective impacts of xai on human-agent interaction", "author": "A Silva"}, {"title": "You complete me: Human-ai teams and complementary expertise", "author": "Q Zhang"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "MR Taesiri"}, {"title": "Influence of pedagogical beliefs and perceived trust on teachers' acceptance of educational artificial intelligence tools", "author": "S Choi"}, {"title": "Data-centric explanations: explaining training data of machine learning systems to promote transparency", "author": "AI Anik"}, {"title": "Assessing the impact of automated suggestions on decision making: Domain experts mediate model errors but take less initiative", "author": "A Levy"}, {"title": "Explanation-based human debugging of nlp models: A survey", "author": "P Lertvittayakumjorn"}, {"title": "Perfection not required? Human-AI partnerships in code translation", "author": "JD Weisz"}, {"title": "How explainability contributes to trust in AI", "author": "A Ferrario"}, {"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer"}, {"title": "The role of domain expertise in user trust and the impact of first impressions with intelligent systems", "author": "M Nourani"}, {"title": "A global taxonomy of interpretable AI: unifying the terminology for the technical and social sciences", "author": "M Graziani"}, {"title": "Does explainable artificial intelligence improve human decision-making?", "author": "Y Alufaisan"}, {"title": "Who goes first? Influences of human-AI workflow on decision making in clinical imaging", "author": "R Fogliato"}, {"title": "Operationalizing human-centered perspectives in explainable AI", "author": "U Ehsan"}, {"title": "\u201cThat's (not) the output I expected!\u201d On the role of end user expectations in creating explanations of AI systems", "author": "M Riveiro"}, {"title": "Fits and starts: Enterprise use of automl and the role of humans in the loop", "author": "A Crisan"}, {"title": "Towards unbiased and accurate deferral to multiple experts", "author": "V Keswani"}, {"title": "\u201cIt cannot do all of my work\u201d: community health worker perceptions of AI-enabled mobile health applications in rural India", "author": "CT Okolo"}, {"title": "The effects of domain knowledge on trust in explainable AI and task performance: A case of peer-to-peer lending", "author": "M Dikmen"}, {"title": "Human-Centered Explainable AI (HCXAI): beyond opening the black-box of AI", "author": "U Ehsan"}, {"title": "You'd better stop! Understanding human reliance on machine learning models under covariate shift", "author": "CW Chiang"}, {"title": "Interpreting deep learning models in natural language processing: A review", "author": "X Sun"}, {"title": "Uncalibrated models can improve human-ai collaboration", "author": "K Vodrahalli"}, {"title": "Overreliance on AI Literature Review", "author": "S Passi"}, {"title": "Glancee: An adaptable system for instructors to grasp student learning status in synchronous online classes", "author": "S Ma"}, {"title": "Connecting algorithmic research and usage contexts: a perspective of contextualized evaluation for explainable AI", "author": "QV Liao"}, {"title": "\" Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction", "author": "SSY Kim"}, {"title": "The impact of algorithmic risk assessments on human predictions and its analysis via crowdsourcing studies", "author": "R Fogliato"}, {"title": "Should I follow AI-based advice? Measuring appropriate reliance in human-AI decision-making", "author": "M Schemmer"}, {"title": "AutoAIViz: opening the blackbox of automated artificial intelligence with conditional parallel coordinates", "author": "DKI Weidele"}, {"title": "Deciding fast and slow: The role of cognitive biases in ai-assisted decision-making", "author": "C Rastogi"}, {"title": "Ignore, trust, or negotiate: understanding clinician acceptance of AI-based treatment recommendations in health care", "author": "V Sivaraman"}, {"title": "Exploring the effects of machine learning literacy interventions on laypeople's reliance on machine learning models", "author": "CW Chiang"}, {"title": "XAI Systems Evaluation: A Review of Human and Computer-Centred Methods", "author": "P Lopes"}, {"title": "How the different explanation classes impact trust calibration: The case of clinical decision support systems", "author": "M Naiseh"}, {"title": "The role of explainable AI in the context of the AI Act", "author": "C Panigutti"}, {"title": "Are visual explanations useful? a case study in model-in-the-loop prediction", "author": "E Chu"}, {"title": "Survey of explainable machine learning with visual and granular methods beyond quasi-explanations", "author": "B Kovalerchuk"}, {"title": "A meta-analysis of the utility of explainable artificial intelligence in human-AI decision-making", "author": "M Schemmer"}, {"title": "The impact of training on human\u2013autonomy team communications and trust calibration", "author": "CJ Johnson"}, {"title": "Effect of ai explanations on human perceptions of patient-facing ai-powered healthcare systems", "author": "Z Zhang"}, {"title": "The methodology of studying fairness perceptions in Artificial Intelligence: Contrasting CHI and FAccT", "author": "N van Berkel"}, {"title": "The need to move away from agential-AI: Empirical investigations, useful concepts and open issues", "author": "F Cabitza"}, {"title": "Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task", "author": "B Leichtmann"}, {"title": "Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty", "author": "J Jiang"}, {"title": "Bayesian modeling of human\u2013AI complementarity", "author": "M Steyvers"}, {"title": "Human-ai collaboration in a cooperative game setting: Measuring social perception and outcomes", "author": "Z Ashktorab"}, {"title": "\u201cWhy Do I Care What's Similar?\u201d Probing Challenges in AI-Assisted Child Welfare Decision-Making through Worker-AI Interface Design Concepts", "author": "A Kawakami"}, {"title": "Artificial Intelligence in Education (AIEd): A high-level academic and industry note 2021", "author": "MA Chaudhry"}, {"title": "Deep learning uncertainty in machine teaching", "author": "T Sanchez"}, {"title": "On the forces of driver distraction: Explainable predictions for the visual demand of in-vehicle touchscreen interactions", "author": "P Ebel"}, {"title": "Challenges in applying explainability methods to improve the fairness of NLP models", "author": "E Balkir"}, {"title": "On the influence of explainable AI on automation bias", "author": "M Schemmer"}, {"title": "The challenges of providing explanations of AI systems when they do not behave like users expect", "author": "M Riveiro"}, {"title": "Increasing the speed and accuracy of data labeling through an ai assisted interface", "author": "M Desmond"}, {"title": "Stepping out of the shadow of human-human teaming: Crafting a unique identity for human-autonomy teams", "author": "NJ McNeese"}, {"title": "Uncertainty quantification 360: A holistic toolkit for quantifying and communicating the uncertainty of ai", "author": "S Ghosh"}, {"title": "Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making", "author": "S Ma"}, {"title": "Semi-automated data labeling", "author": "M Desmond"}, {"title": "Assessing the communication gap between AI models and healthcare professionals: Explainability, utility and trust in AI-driven clinical decision-making", "author": "O Wysocki"}, {"title": "AI-based detection, classification and prediction/prognosis in medical imaging: towards radiophenomics", "author": "F Yousefirizi"}, {"title": "Onboarding Materials as Cross-functional Boundary Objects for Developing AI Assistants", "author": "CJ Cai"}, {"title": "The evolution and impact of human confidence in artificial intelligence and in themselves on ai-assisted decision-making in design", "author": "L Chong"}, {"title": "Investigations of performance and bias in human-AI teamwork in hiring", "author": "A Peng"}, {"title": "\" It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools", "author": "R Cheng"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "Explainable recommendation: when design meets trust calibration", "author": "M Naiseh"}, {"title": "The Effects of AI-based Credibility Indicators on the Detection and Spread of Misinformation under Social Influence", "author": "Z Lu"}, {"title": "On the effect of information asymmetry in human-AI teams", "author": "P Hemmer"}, {"title": "Effects of explanations in ai-assisted decision making: Principles and comparisons", "author": "X Wang"}, {"title": "HINT: Integration Testing for AI-based features with Humans in the Loop", "author": "QZ Chen"}, {"title": "Explainable ai is dead, long live explainable ai! hypothesis-driven decision support", "author": "T Miller"}, {"title": "Patients' perceptions of using artificial intelligence (AI)-based technology to comprehend radiology imaging data", "author": "Z Zhang"}, {"title": "AI-assisted decision-making: A cognitive modeling approach to infer latent reliance strategies", "author": "H Tejeda"}, {"title": "Humans and algorithms detecting fake news: Effects of individual and contextual confidence on trust in algorithmic advice", "author": "C Snijders"}, {"title": "Understanding the role of human intuition on reliance in human-AI decision-making with explanations", "author": "V Chen"}, {"title": "Modeling Adaptive Expression of Robot Learning Engagement and Exploring its Effects on Human Teachers", "author": "S Ma"}, {"title": "Fair machine learning in healthcare: A review", "author": "Q Feng"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "G Nguyen"}, {"title": "Human-ai collaboration for ux evaluation: Effects of explanation and synchronization", "author": "M Fan"}, {"title": "Toward a taxonomy of trust for probabilistic machine learning", "author": "T Broderick"}, {"title": "Trust and Reliance in XAI--Distinguishing Between Attitudinal and Behavioral Measures", "author": "N Scharowski"}, {"title": "Towards explainable evaluation metrics for natural language generation", "author": "C Leiter"}, {"title": "Machine explanations and human understanding", "author": "C Chen"}, {"title": "Calibrating trust of multi-hop question answering systems with decompositional probes", "author": "K Xie"}, {"title": "Do explanations increase the effectiveness of AI-crowd generated fake news warnings?", "author": "Z Epstein"}, {"title": "Selective explanations: Leveraging human input to align explainable ai", "author": "V Lai"}, {"title": "Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance", "author": "M Westphal"}, {"title": "Interfaces for explanations in human-AI interaction: proposing a design evaluation approach", "author": "H Mucha"}, {"title": "Understanding Decision Subjects' Fairness Perceptions and Retention in Repeated Interactions with AI-Based Decision Systems", "author": "MA Gemalmaz"}, {"title": "Understanding and Being Understood: User Strategies for Identifying and Recovering From Mistranslations in Machine Translation-Mediated Chat", "author": "S Robertson"}, {"title": "To what extent do human explanations of model behavior align with actual model behavior?", "author": "G Prasad"}, {"title": "Being Trustworthy is Not Enough: How Untrustworthy Artificial Intelligence (AI) Can Deceive the End-Users and Gain Their Trust", "author": "N Banovic"}, {"title": "Explainable Artificial Intelligence improves human decision-making: Results from a mushroom picking experiment at a public art festival", "author": "B Leichtmann"}, {"title": "The Many Facets of Trust in AI: Formalizing the Relation Between Trust and Fairness, Accountability, and Transparency", "author": "B Knowles"}, {"title": "Effects of reliability indicators on usage, acceptance and preference of predictive process management decision support systems", "author": "P Fr\u00f6hlich"}, {"title": "Will you accept the ai recommendation? predicting human behavior in ai-assisted decision making", "author": "X Wang"}, {"title": "Assessing Human-AI Interaction Early through Factorial Surveys: A Study on the Guidelines for Human-AI Interaction", "author": "T Li"}, {"title": "Appropriate reliance on AI advice: Conceptualization and the effect of explanations", "author": "M Schemmer"}, {"title": "A trustworthy human\u2013machine framework for collective decision making in food\u2013energy\u2013water management: The role of trust sensitivity", "author": "S Uslu"}, {"title": "Designing for conversational system trustworthiness: the impact of model transparency on trust and task performance", "author": "A Schmitt"}, {"title": "Co-design of human-centered, explainable AI for clinical decision support", "author": "C Panigutti"}, {"title": "Measuring and Understanding Trust Calibrations for Automated Systems: A Survey of the State-Of-The-Art and Future Directions", "author": "M Wischnewski"}, {"title": "Mitigating knowledge imbalance in AI-advised decision-making through collaborative user involvement", "author": "C Gomez"}, {"title": "Towards a trust reliance paradox? exploring the gap between perceived trust in and reliance on algorithmic advice", "author": "A Schmitt"}, {"title": "Improving human-AI collaboration with descriptions of AI behavior", "author": "\u00c1A Cabrera"}, {"title": "Extracting victim counts from text", "author": "M Zhong"}, {"title": "XAIR: A Framework of Explainable AI in Augmented Reality", "author": "X Xu"}, {"title": "People underestimate the errors made by algorithms for credit scoring and recidivism prediction but accept even fewer errors", "author": "FG Rebitschek"}, {"title": "Guidelines and evaluation of clinical explainable AI in medical image analysis", "author": "W Jin"}, {"title": "Understanding Uncertainty: How Lay Decision-makers Perceive and Interpret Uncertainty in Human-AI Decision Making", "author": "S Prabhudesai"}, {"title": "Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making", "author": "K Inkpen"}, {"title": "Learning Personalized Decision Support Policies", "author": "U Bhatt"}, {"title": "Reliance and Automation for Human-AI Collaborative Data Labeling Conflict Resolution", "author": "M Brachman"}, {"title": "Training novices: The role of human-ai collaboration and knowledge transfer", "author": "P Spitzer"}, {"title": "Advice Provision in Teleoperation of Autonomous Vehicles", "author": "Y Trabelsi"}, {"title": "Interaction Design With Multi-objective Bayesian Optimization", "author": "YC Liao"}, {"title": "Explainable recommendations and calibrated trust: two systematic user errors", "author": "M Naiseh"}, {"title": "Factors that influence the adoption of human-AI collaboration in clinical decision-making", "author": "P Hemmer"}, {"title": "Trustworthy AI explanations as an interface in medical diagnostic systems", "author": "D Kaur"}, {"title": "Explainable sentiment analysis application for social media crisis management in retail", "author": "D Cirqueira"}, {"title": "Taking Advice from (Dis) Similar Machines: The Impact of Human-Machine Similarity on Machine-Assisted Decision-Making", "author": "N Grgi\u0107-Hla\u010da"}, {"title": "Do Not Trust a Model Because It is Confident: Uncovering and Characterizing Unknown Unknowns to Student Success Predictors in Online-Based Learning", "author": "R Galici"}, {"title": "The value of measuring trust in AI-a socio-technical system perspective", "author": "M Benk"}, {"title": "On the Influence of Cognitive Styles on Users' Understanding of Explanations", "author": "L Riefle"}, {"title": "Improving model understanding and trust with counterfactual explanations of model confidence", "author": "T Le"}, {"title": "AI trust score: A user-centered approach to building, designing, and measuring the success of intelligent workplace features", "author": "J Wang"}, {"title": "Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Learning when to advise human decision makers", "author": "G Noti"}, {"title": "Resilience through appropriation: pilots' view on complex decision support", "author": "ZT Zhang"}, {"title": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies", "author": "V Lai"}, {"title": "Reasons, values, stakeholders: A philosophical framework for explainable artificial intelligence", "author": "A Kasirzadeh"}, {"title": "Pre-emptive learning-to-defer for sequential medical decision-making under uncertainty", "author": "S Joshi"}, {"title": "Supporting High-Uncertainty Decisions through AI and Logic-Style Explanations", "author": "FM Cau"}, {"title": "Generation probabilities are not enough: Exploring the effectiveness of uncertainty highlighting in AI-powered code completions", "author": "H Vasconcelos"}, {"title": "Investigating and Designing for Trust in AI-powered Code Generation Tools", "author": "R Wang"}, {"title": "Human-agent teaming and trust calibration: a theoretical framework, configurable testbed, empirical illustration, and implications for the development of adaptive \u2026", "author": "P Bobko"}, {"title": "Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven Decision Support using Evaluative AI", "author": "T Miller"}, {"title": "Rethinking AI Explainability and Plausibility", "author": "W Jin"}, {"title": "Build confidence and acceptance of AI-based decision support systems-Explainable and liable AI", "author": "C Nicodeme"}, {"title": "Human-Centered Evaluation of Explanations", "author": "J Boyd-Graber"}, {"title": "Caution or Trust in AI? How to design XAI in sensitive Use Cases?", "author": "A Kloker"}, {"title": "Explaining autonomous decisions in swarms of human-on-the-loop small unmanned aerial systems", "author": "A Agrawal"}, {"title": "An empirical survey on explainable ai technologies: Recent trends, use-cases, and categories from technical and application perspectives", "author": "M Nagahisarchoghaei"}, {"title": "Watch Out for Updates: Understanding the Effects of Model Explanation Updates in AI-Assisted Decision Making", "author": "X Wang"}, {"title": "Understanding relations between perception of fairness and trust in algorithmic decision making", "author": "J Zhou"}, {"title": "Are Two Heads Better Than One in AI-Assisted Decision Making? Comparing the Behavior and Performance of Groups and Individuals in Human-AI Collaborative \u2026", "author": "CW Chiang"}, {"title": "Humans, AI, and Context: Understanding End-Users' Trust in a Real-World Computer Vision Application", "author": "SSY Kim"}, {"title": "Trustworthy acceptance: a new metric for trustworthy artificial intelligence used in decision making in food\u2013energy\u2013water sectors", "author": "S Uslu"}, {"title": "Impact of explainable artificial intelligence assistance on clinical decision-making of novice dental clinicians", "author": "A Glick"}, {"title": "Exploring the Use of Personalized AI for Identifying Misinformation on Social Media", "author": "F Jahanbakhsh"}, {"title": "Exploring the role of local and global explanations in recommender systems", "author": "M Radensky"}, {"title": "When do XAI methods work? A cost-benefit approach to human-AI collaboration", "author": "H Vasconcelos"}, {"title": "Effects of AI and Logic-Style Explanations on Users' Decisions under Different Levels of Uncertainty", "author": "FM Cau"}, {"title": "Calibrated Trust as a Result of Accurate Trustworthiness Assessment\u2013Introducing the Trustworthiness Assessment Model", "author": "N Schlicker"}, {"title": "Measuring and predicting human trust in recommendations from an AI teammate", "author": "N Gurney"}, {"title": "Characterizing Manipulation from AI Systems", "author": "M Carroll"}, {"title": "Should I follow this model? The effect of uncertainty visualization on the acceptance of time series forecasts", "author": "D Leffrang"}, {"title": "Effects of uncertainty on the quality of feature importance explanations", "author": "T Shaikhina"}, {"title": "Ground (less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making", "author": "L Guerdan"}, {"title": "Review of Research on Human Trust in Artificial Intelligence", "author": "Y Li"}, {"title": "Adoption Barriers of AI: a Context-Specific Acceptance Model for Industrial Maintenance.", "author": "J Wanner"}, {"title": "User trust in assisted decision-making using miniaturized near-infrared spectroscopy", "author": "W Jiang"}, {"title": "Nudging through Friction: An Approach for Calibrating Trust in Explainable AI", "author": "M Naiseh"}, {"title": "Effects of Fairness and Explanation on Trust in Ethical AI", "author": "A Angerschmid"}, {"title": "Decision Making Strategies and Team Efficacy in Human-AI Teams", "author": "I Munyaka"}, {"title": "Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI", "author": "U Ehsan"}, {"title": "RetroLens: A Human-AI Collaborative System for Multi-step Retrosynthetic Route Planning", "author": "C Shi"}, {"title": "Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems", "author": "G He"}, {"title": "Risk and Exposure of XAI in Persuasion and Argumentation: The case of Manipulation", "author": "R Carli"}, {"title": "The Role of AI-Based Artifacts' Voice Capabilities for Agency Attribution", "author": "A Schmitt"}, {"title": "EUCA: The end-user-centered explainable AI framework", "author": "W Jin"}, {"title": "Explainable Artificial Intelligence (XAI): Concepts and Challenges in Healthcare", "author": "T Hulsen"}, {"title": "Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task", "author": "C Xu"}, {"title": "Developing a fidelity evaluation approach for interpretable machine learning", "author": "M Velmurugan"}, {"title": "Causal Explanations for Sequential Decision Making Under Uncertainty", "author": "SB Nashed"}, {"title": "Interrogating the T in FAccT", "author": "E Corbett"}, {"title": "When Do Humans Heed AI Agents' Advice? When Should They?", "author": "RE Dunning"}, {"title": "Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making", "author": "C Zheng"}, {"title": "Explaining interpretable machine learning: Theory, methods and applications", "author": "M Benk"}, {"title": "Algorithmic advice as a credence good", "author": "J Biermann"}, {"title": "Dignity and use of algorithm in performance evaluation", "author": "L Zhang"}, {"title": "Collaborative design decision-making with artificial intelligence: Exploring the evolution and impact of human confidence in AI and in themselves", "author": "L Chong"}, {"title": "Using fNIRS to identify transparency-and reliability-sensitive markers of trust across multiple timescales in collaborative human-human-agent triads", "author": "L Eloy"}, {"title": "On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making", "author": "J Schoeffer"}, {"title": "Improving Expert Predictions with Conformal Prediction", "author": "E Straitouri"}, {"title": "A Multi-Node Collaborative Decision-Making Scheme for Privacy Protection in IIoT", "author": "M Chen"}, {"title": "Do's and Don'ts for Human and Digital Worker Integration", "author": "V Muthusamy"}, {"title": "How to Manage Output Uncertainty: Targeting the Actual End User Problem in Interactions with AI.", "author": "ZT Zhang"}, {"title": "Trust in clinical AI: Expanding the unit of analysis", "author": "JT Browne"}, {"title": "\u201cI Think You Might Like This\u201d: Exploring Effects of Confidence Signal Patterns on Trust in and Reliance on Conversational Recommender Systems", "author": "M Radensky"}, {"title": "Knowing when to pass: The effect of AI reliability in risky decision contexts", "author": "H Elder"}, {"title": "User Trust on an Explainable AI-based Medical Diagnosis Support System", "author": "Y Rong"}, {"title": "Comparing Effects of Attribution-based, Example-based, and Feature-based Explanation Methods on AI-Assisted Decision-Making", "author": "C Humer"}, {"title": "Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction", "author": "P Hemmer"}, {"title": "Discrepancy between perceptions and acceptance of clinical decision support Systems: implementation of artificial intelligence for vancomycin dosing", "author": "X Liu"}, {"title": "Tell me something that will help me trust you: A survey of trust calibration in human-agent interaction", "author": "GJ Cancro"}, {"title": "Human-Centered Deferred Inference: Measuring User Interactions and Setting Deferral Criteria for Human-AI Teams", "author": "SJ Lemmer"}, {"title": "Uncertainty quantification 360: a hands-on tutorial", "author": "S Ghosh"}, {"title": "Multi-device trust transfer: Can trust be transferred among multiple devices?", "author": "K Okuoka"}, {"title": "Exploring the impact of classification probabilities on users' trust in ambiguous instances", "author": "DDS Ribeiro"}, {"title": "Painting the black box white: experimental findings from applying XAI to an ECG reading setting", "author": "F Cabitza"}, {"title": "Designing resilient AI-based robo-advisors: A prototype for real estate appraisal", "author": "M Schemmer"}, {"title": "Using ChatGPT in HCI Research\u2014A Trioethnography", "author": "S Desai"}, {"title": "Towards Explainable Evaluation Metrics for Machine Translation", "author": "C Leiter"}, {"title": "A Missing Piece in the Puzzle: Considering the Role of Task Complexity in Human-AI Decision Making", "author": "S Salimzadeh"}, {"title": "Specifying and interpreting reinforcement learning policies through simulatable machine learning", "author": "P Tambwekar"}, {"title": "Human Response to an AI-Based Decision Support System: A User Study on the Effects of Accuracy and Bias", "author": "D Solans"}, {"title": "Clinicians' Perceptions of Artificial Intelligence: Focus on Workload, Risk, Trust, Clinical Decision Making, and Clinical Integration", "author": "H Shamszare"}, {"title": "A Unifying Framework for Causal Explanation of Sequential Decision Making", "author": "SB Nashed"}, {"title": "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap", "author": "QV Liao"}, {"title": "The Influence of Disclosing the AI Potential Error to the User on the Efficiency of User\u2013AI Collaboration", "author": "O Lukashova-Sanz"}, {"title": "Model-centric verification of artificial intelligence", "author": "N Gisolfi"}, {"title": "What does well-designed adaptivity mean for drivers? A research approach to develop recommendations for adaptive in-vehicle user interfaces that are \u2026", "author": "J Graefe"}, {"title": "Homophily and Incentive Effects in Use of Algorithms", "author": "R Fogliato"}, {"title": "Doubting AI Predictions: Influence-Driven Second Opinion Recommendation", "author": "M De-Arteaga"}, {"title": "Explanation Interfaces for Sales Forecasting", "author": "TB Fahse"}, {"title": "Towards Reconciling Usability and Usefulness of Explainable AI Methodologies", "author": "P Tambwekar"}, {"title": "Seamful XAI: Operationalizing Seamful Design in Explainable AI", "author": "U Ehsan"}, {"title": "A Study of Explainability Features to Scrutinize Faceted Filtering Results", "author": "J Qu"}, {"title": "Appropriate Reliance, Explainable AI, Human-AI Collaboration, Human-AI Complementarity", "author": "M Schemmer"}, {"title": "Towards Explainable AI Writing Assistants for Non-native English Speakers", "author": "Y Kim"}, {"title": "Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation", "author": "J Sun"}, {"title": "Explaining Model Confidence Using Counterfactuals", "author": "T Le"}, {"title": "Provably improving expert predictions with prediction sets", "author": "E Straitouri"}, {"title": "Investigating Trust in Human-Machine Learning Collaboration: A Pilot Study on Estimating Public Anxiety from Speech", "author": "AA Tutul"}, {"title": "Fighting the disagreement in Explainable Machine Learning with consensus", "author": "AJ Banegas-Luna"}, {"title": "Integrity Based Explanations for Fostering Appropriate Trust in AI Agents", "author": "S Mehrotra"}, {"title": "Something Borrowed: Exploring the Influence of AI-Generated Explanation Text on the Composition of Human Explanations", "author": "SA Ferguson"}, {"title": "How do Users Experience Traceability of AI Systems? Examining Subjective Information Processing Awareness in Automated Insulin Delivery (AID) Systems", "author": "T Schrills"}, {"title": "Human-Aligned Calibration for AI-Assisted Decision Making", "author": "NLC Benz"}, {"title": "Explainable NLP for human-AI collaboration", "author": "P Lertvittayakumjorn"}, {"title": "Calibrating trust toward an autonomous image classifier", "author": "M Ingram"}, {"title": "Human-Centered Explainable AI (HCXAI): Coming of Age", "author": "U Ehsan"}, {"title": "Effectiveness of Example-Based Explanations to Improve Human Decision Quality in Machine Learning Forecasting Systems", "author": "TB Fahse"}, {"title": "Algorithm, machine learning and artificial intelligence", "author": "A Ferrario"}, {"title": "Towards a Praxis for Intercultural Ethics in Explainable AI", "author": "CT Okolo"}, {"title": "The Importance of Distrust in AI", "author": "TM Peters"}, {"title": "TAIRA-BSC-Trusting AI in Recruitment Applications through Blockchain Smart Contracts", "author": "M Aleisa"}, {"title": "Building trust by supporting situation awareness: exploring pilots' design requirements for decision support tools", "author": "C Storath"}, {"title": "Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning", "author": "W Zhang"}, {"title": "Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation", "author": "N Balepur"}, {"title": "Investigating the relationships between class probabilities and users' appropriate trust in computer vision classifications of ambiguous images", "author": "GDJ Barbosa"}, {"title": "Trustworthy Human Computation: A Survey", "author": "H Kashima"}, {"title": "Pilot attitudes toward AI in the cockpit: implications for design", "author": "ZT Zhang"}, {"title": "Artificial Intelligence Screening of Medical School Applications: Development and Validation of a Machine-Learning Algorithm", "author": "MM Triola"}, {"title": "How does machine advice influence design choice? The effect of error on design decision making", "author": "A Nandy"}, {"title": "Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring", "author": "N Mehdiyev"}, {"title": "Why not both? Complementing explanations with uncertainty, and the role of self-confidence in Human-AI collaboration", "author": "I Papantonis"}, {"title": "LEx: A Framework for Operationalising Layers of Machine Learning Explanations", "author": "R Singh"}, {"title": "What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on AI Systems", "author": "N Goyal"}, {"title": "OpenClinicalAI: An Open and Dynamic Model for Alzheimer's Disease Diagnosis", "author": "Y Huang"}, {"title": "Examining Trust in Conversational Systems: Conceptual and Empirical Findings on User Trust, Related Behavior, and System Trustworthiness", "author": "A Schmitt"}, {"title": "A cognitive work analysis approach to explainable artificial intelligence in non-expert financial decision-making", "author": "M Dikmen"}, {"title": "A Taxonomy of User-centered Explainable AI Studies", "author": "LV Herm"}, {"title": "Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models", "author": "J Durso-Finley"}, {"title": "Selectively Providing Reliance Calibration Cues With Reliance Prediction", "author": "Y Fukuchi"}, {"title": "Beyond end predictions: stop putting machine learning first and design human-centered AI for decision support", "author": "Z Bu\u00e7inca"}, {"title": "Empowering humans in human-ai decision making", "author": "V Lai"}, {"title": "Using Multiple Dermoscopic Photographs of One Lesion Improves Melanoma Classification via Deep Learning: A Prognostic Diagnostic Accuracy Study", "author": "A Hekler"}, {"title": "Predictive and Prescriptive Analytics in Business Decision Making: Needs and Concerns", "author": "S Gathani"}, {"title": "Bridging the Geospatial Education-Workforce Divide: A Case Study on How Higher Education Can Address the Emerging Geospatial drivers And Trends Of \u2026", "author": "WR Stout"}, {"title": "Smart Contracts, Real-Virtual World Convergence and Economic Implications", "author": "W Lehr"}, {"title": "People underestimate the errors by algorithms for credit scoring and recidivism but tolerate even fewer errors", "author": "F Rebitschek"}, {"title": "Trust in artificial intelligence: Clinicians are essential", "author": "U Bhatt"}, {"title": "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents", "author": "Z Huang"}, {"title": "Do Explanations Improve the Quality of AI-assisted Human Decisions? An Algorithm-in-the-Loop Analysis of Factual & Counterfactual Explanations", "author": "L Ibrahim"}, {"title": "Using data to\u201d Git Gud\u201d: a push for a player-centric approach to the use of data in esports", "author": "E Kleinman"}, {"title": "Workshop on Trust and Reliance in AI-Human Teams (TRAIT)", "author": "G Bansal"}, {"title": "Justifying our Credences in the Trustworthiness of AI Systems: A Reliabilistic Approach", "author": "A Ferrario"}, {"title": "Provenance-based explanations: are they useful?", "author": "S Oppold"}, {"title": "Investigating Unfavorable Factors That Impede MALDI-TOF-Based AI in Predicting Antibiotic Resistance", "author": "HY Wang"}, {"title": "Interacting with autonomous systems and intelligent algorithms\u2013new theoretical considerations on the relation of understanding and trust", "author": "N Kr\u00e4mer"}, {"title": "Three Challenges for AI-Assisted Decision-Making", "author": "M Steyvers"}, {"title": "Does a Compromise on Fairness Exist in Using AI Models?", "author": "J Zhou"}, {"title": "Exploring How Anomalous Model Input and Output Alerts Affect Decision-Making in Healthcare", "author": "M Radensky"}, {"title": "The Poetry of Prompts: The Collaborative Role of Generative Artificial Intelligence in the Creation of Poetry and the Anxiety of Machine Influence", "author": "J Hutson"}, {"title": "Measuring algorithmic interpretability: A human-learning-based framework and the corresponding cognitive complexity score", "author": "JP Lalor"}, {"title": "Surfacing AI Explainability in Enterprise Product Visual Design to Address User Tech Proficiency Differences", "author": "S Tandon"}, {"title": "Explainable Artificial Intelligence (XAI) in Healthcare", "author": "T Hulsen"}, {"title": "Medical AI for Radiology: The Lost Cognitive Perspective", "author": "L Tikhomirov"}, {"title": "Improving Expert Predictions with Prediction Sets", "author": "E Straitouri"}, {"title": "Reliability Evaluation of Individual Predictions: A Data-centric Approach", "author": "N Shahbazi"}, {"title": "Learning-to-defer for sequential medical decision-making under uncertainty", "author": "S Joshi"}, {"title": "Correcting classification: A bayesian framework using explanation feedback to improve classification abilities", "author": "Y Bekkemoen"}, {"title": "Knowledge Trap: Human Experts Distracted by Details When Teaming with AI", "author": "W Wang"}, {"title": "What confidence and the eyes can tell about interacting with a partner", "author": "R Sanchez"}, {"title": "A global taxonomy of interpretable AI: unifying the terminology for the technical and social sciences", "author": "G Mara"}, {"title": "Please take over: XAI, delegation of authority, and domain knowledge", "author": "K Bauer"}, {"title": "Towards Designing Robo-Advisory to Promote Consensus Efficient Group Decision-Making in New Types of Economic Scenarios", "author": "NM Namyslo"}, {"title": "Will Humans be Free-Riders? The Effects of Expectations for AI on Human-AI Team Performance", "author": "M Liu"}, {"title": "DESIGNING FAIR AI SYSTEMS: HOW EXPLANATION SPECIFICITY INFLUENCES USERS\u00e2\u20ac\u2122 PERCEIVED FAIRNESS AND TRUSTING INTENTIONS", "author": "Y Song"}, {"title": "Artificial Intelligence in the Construction Industry", "author": "AH Behzadan"}, {"title": "On the Way to Improving Experimental Protocols to Evaluate Users' Trust in AI-Assisted Clinical Decision Making", "author": "O Vereschak"}, {"title": "Human and AI Trust: Trust Attitude Measurement Instrument Development", "author": "R Larasati"}, {"title": "Calibrating trust in AI-assisted decision making", "author": "A Turner"}, {"title": "How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?", "author": "S Narayanan"}, {"title": "Is Explainable Medical AI a Medical Reversal Waiting to Happen? Exploring the Impact of AI Explanations on Clinical Decision Quality", "author": "J Clement"}, {"title": "Role of Human Self-Confidence and Their Confidence in Artificial Intelligence in AI-Assisted Decision-Making in Engineering Design", "author": "LM Chong"}, {"title": "People Perceive Algorithmic Assessments as Less Fair and Trustworthy Than Identical Human Assessments", "author": "L MOK"}, {"title": "Context and Participation in Machine Learning", "author": "H Suresh"}, {"title": "An Empirical Investigation of Reliance on AI-Assistance in a Noisy-Image Classification Task", "author": "H Tejeda Lemus"}, {"title": "Human-AI Collaboration in Content Moderation: The Effects of Information Cues and Time Constraints", "author": "H Li"}, {"title": "Advancing Explainability and Fairness in AI with Human-Algorithm Collaborations", "author": "HF Cheng"}, {"title": "The Impact of Advisers Salience on Peace of Mind through the Mediating Role of Fear of Negative Evaluation", "author": "N Ashraf"}, {"title": "Towards Scalable Structured Data from Clinical Text", "author": "M Agrawal"}, {"title": "Towards Fair and Explainable AI using a Human-Centered AI Approach", "author": "B Ghai"}, {"title": "Willing to Revise? Confidence and Recommendation Adoption in AI-Assisted Image Recognition", "author": "L R\u00f6seler"}, {"title": "Trust and Explanation in Artificial Intelligence Systems: A Healthcare Application in Disease Detection and Preliminary Diagnosis", "author": "R Larasati"}, {"title": "Structuring AI Teammate Communication: An Exploration of AI's Communication Strategies in Human-AI Teams", "author": "R Zhang"}, {"title": "Prediction, human decision and liability rules, CRED Working paper No 2022-06", "author": "M Obidzinski"}, {"title": "Exploring the Role of Trust During Human-AI Collaboration in Managerial Decision-Making Processes", "author": "S Tuncer"}, {"title": "Data and Humans in Algorithmic Risk Assessment", "author": "R Fogliato"}, {"title": "Explainable recommendation: When design meets trust calibration\u2013Research protocol", "author": "M Naiseh"}, {"title": "Artificial Intelligence, Fairness and Productivity", "author": "D Yuan"}, {"title": "Calibrating Users' Mental Models for Delegation to AI", "author": "A Taudien"}, {"title": "Designing for explanation-driven trust in Chatbots", "author": "Z Zhang"}, {"title": "Three Maxims for Developing Human-Centered AI for Decision Making", "author": "G Bansal"}, {"title": "Designing Process-Oriented Computational Assistance to Support Self-Regulated Learning in Complex Games", "author": "EM Kleinman"}, {"title": "Understanding and Addressing Uncertainty of the Crowd", "author": "QZ Chen"}, {"title": "Effects of Logic-Style Explanations and Uncertainty on Users' Decisions", "author": "F Cau"}, {"title": "AIS Electronic Library (AISeL)", "author": "A Schmitt"}, {"title": "Exploring the Effects of Human-Centered AI Explanations on Trust and Reliance", "author": "N Scharowski"}, {"title": "Algorithmic Decision-Making with Stakeholder Participation", "author": "V Keswani"}, {"title": "Perceive, Understand & Predict-Empirical Indication for Facets in Subjective Information Processing Awareness", "author": "TPP Schrills"}, {"title": "Data-Centric Distrust Quantification for Responsible AI: When Data-driven Outcomes Are Not Reliable.", "author": "N Shahbazi"}, {"title": "Understanding Human-AI Trust in the Context of Decision Making through the Lenses of Academia and Industry: Definitions, Factors, and Evaluation", "author": "O Vereschak"}, {"title": "Modeling Human Trust and Reliance in AI-Assisted Decision Making: A Markovian Approach", "author": "Z Li"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.0", "author": "JP Wanner"}, {"title": "Adaptive trust calibration in human-AI cooperation", "author": "K Okamura"}, {"title": "eXplainable AI for trustworthy healthcare applications", "author": "C Panigutti"}, {"title": "David Solans Noguero", "author": "C Castillo"}, {"title": "Measuring the Influence of Characteristics on Decision-Making Scenarios: A Prototype", "author": "S Reiners"}, {"title": "AI for Population Health: Melding Data and Algorithms on Networks", "author": "B Wilder"}, {"title": "Background Explanations Reduce Users' Over-reliance on AI: A Case Study on Multi-Hop Question Answering", "author": "A Vikander"}, {"title": "Designing with Machine Learning in Digital Pathology: Augmenting Medical Specialists Through Interaction Design", "author": "M Lindvall"}, {"title": "Applying Mathematics to the Natural and Social World", "author": "A Kasirzadeh"}, {"title": "Overtrust in Algorithms: An online behavioral study on trust and reliance in AI advice", "author": "P Schreck"}, {"title": "On the Calibration of Learning to Defer Systems", "author": "R Verma"}, {"title": "A Multidisciplinary Design and Evaluation Framework for Explainable AI Systems", "author": "S Mohseni"}, {"title": "Trust in AI Selection Procedures and Organizational Attractiveness", "author": "G Violatou"}, {"title": "Towards Responsible AI: Developing Explanations to Increase Human-AI Collaboration", "author": "R De Brito Duarte"}, {"title": "Implementing flipped classroom that used an intelligent tutoring system into learning process: Formative assessment in the design of instructional systems.", "author": "R Jordan"}, {"title": "How Displaying AI Confidence Affects Reliance and Hybrid Human-AI Performance", "author": "H Tejeda Lemus"}, {"title": "Studying the Overhead and Crowd-Sourced Risk Assessment Strategy of Dependency Management Bots", "author": "BJ Rombaut"}, {"title": "A Design Thinking Framework for Human-Centric Explainable Artificial Intelligence in Time-Critical Systems", "author": "PB Stone"}, {"title": "Investigating Explanations that Target Training Data.", "author": "AI Anik"}, {"title": "The Effects of AI Biases and Explanations on Human Decision Fairness: A Case Study of Bidding in Rental Housing Markets", "author": "X Wang"}, {"title": "Explaining the Uncertainty in AI-Assisted Decision Making", "author": "T Le"}, {"title": "Do you really want to know why? Effects of AI-based DSS Advice on Human Decisions.", "author": "J Wanner"}, {"title": "Strategic Adversarial Attacks in AI-assisted Decision Making to Reduce Human Trust and Reliance", "author": "Z Lu"}, {"title": "AI-Assisted Decision-Making: A Cognitive Modeling Approach to Infer Latent Reliance Strategies", "author": "HT Lemus"}, {"title": "Communicating uncertain information from deep learning models to users", "author": "HV Subramanian"}, {"title": "Hybrid intelligence decision making: successful human-AI integration in optical diagnosis", "author": "C Reverberi"}, {"title": "An Empirical Investigation of Reliance on AI-Assistance in a Noisy-Image Classification Task", "author": "HT Lemus"}, {"title": "Reassuring, Misleading, Debunking: Comparing Effects of XAI Methods on Human Decisions", "author": "C HUMER"}, {"title": "A Micro and Macro Perspective on Trustworthiness: Theoretical Underpinnings of the Trustworthiness Assessment Model (TrAM)", "author": "N Schlicker"}, {"title": "A dvisory algorithms and liability rules", "author": "M Obidzinski"}, {"title": "Trustworthy Acceptance: A New Metric for Trustworthy Artificial Intelligence used in Decision Making in Food\u2013Energy\u2013Water Sectors", "author": "L Barolli"}, {"title": "Understanding Systematic Miscalibration in Machine Learning Classifiers", "author": "M Kelly"}, {"title": "Trust and explainability as tools for improving automated writing evaluation", "author": "M Goldshtein"}, {"title": "Trust and AI in Clinical Decision Support", "author": "B Wilson"}, {"title": "Research Needs for Trusted Analytics in National Security Settings", "author": "A Speed"}, {"title": "Factors Influencing Trust in Automation", "author": "HM Smitherman"}, {"title": "5.2 Factors affecting confidence in artificial intelligence (AI) during clinical reasoning and decision making (CRDM)", "author": "M Close"}, {"title": "Confidence-based deferral time in joint human-AI decision-making", "author": "MJA Schmidt"}, {"title": "On the Way to Improving Experimental Protocols to Evaluate Users' Trust in AI-Assisted Decision Making", "author": "O Vereschak"}, {"title": "Is Automation the Future of Machine Learning?", "author": "Y Mahgoub"}, {"title": "Designing User Interactions around AI: The Effect of Accuracy Information of a Model on Trust and Sense of Control", "author": "S Park"}, {"title": "On the Calibration of Learning to Defer to Multiple Experts", "author": "R Verma"}, {"title": "The effect of explaining algorithmic decisions on the perception of fairness", "author": "BW van der Meulen"}, {"title": "Examining Trust in Conversational Systems", "author": "A Schmitt"}, {"title": "AI-designed versus human-designed recommendation systems in symbolic consumption contexts", "author": "S Kokaraki"}, {"title": "Utilisation dynamique de la transparence pour la coop\u00e9ration humain-machine", "author": "L Simon"}, {"title": "Anvendelse av kunstig intelligens i revisors vurdering av fortsatt drift: En eksperimentell studie", "author": "TE Aase"}, {"title": "AI: s p\u00e5verkan i tillverkningsindustrin: En analys av ett teaming-orienterat hj\u00e4lpmedel", "author": "J Andersson"}, {"title": "\u7b97\u6cd5\u6cbb\u7406 \u4e13\u680f\u6536\u5f55\u8be5\u5185\u5bb9", "author": "A Weller"}, {"title": "Adaptive Trust Calibration in Human-AI Cooperation (\u4eba\u9593-AI \u5354\u8abf\u306b\u304a\u3051\u308b\u9069\u5fdc\u7684\u4fe1\u983c\u8f03\u6b63)", "author": "\u5ca1\u6751\u548c\u7537\uff0c \u30aa\u30ab\u30e0\u30e9\u30ab\u30ba\u30aa"}, {"title": "Toward Trust-calibrated Customized Vehicle Automation", "author": "J Liu"}, {"title": "Machine Learning Explainability for External Stakeholders", "author": "A Weller"}], "progressive disclosure: empirically motivated approaches to designing effective transparency": [{"title": "Questioning the AI: informing design practices for explainable AI user experiences", "author": "QV Liao"}, {"title": "How cognitive biases affect XAI-assisted decision-making: A systematic review", "author": "A Bertrand"}, {"title": "Are explanations helpful? a comparative study of the effects of explanations in ai-assisted decision-making", "author": "X Wang"}, {"title": "Evaluating saliency map explanations for convolutional neural networks: a user study", "author": "A Alqaraawi"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Explainable active learning (xal) toward ai explanations as interfaces for machine teachers", "author": "B Ghai"}, {"title": "Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle", "author": "S Dhanorkar"}, {"title": "The Placebo Effect of Artificial Intelligence in Human\u2013Computer Interaction", "author": "T Kosch"}, {"title": "\u201cKnowing me, knowing you\u201d: personalized explanations for a music recommender system", "author": "M Martijn"}, {"title": "Data-driven personas for enhanced user understanding: Combining empathy with rationality for better insights to analytics", "author": "BJ Jansen"}, {"title": "Covi white paper", "author": "H Alsdurf"}, {"title": "Attitudes and folk theories of data subjects on transparency and accuracy in emotion recognition", "author": "G Grill"}, {"title": "What is\" intelligent\" in intelligent user interfaces? a meta-analysis of 25 years of IUI", "author": "ST V\u00f6lkel"}, {"title": "Explainable active learning (xal): An empirical study of how local explanations impact annotator experience", "author": "B Ghai"}, {"title": "A longitudinal evaluation of tablet-based child speech therapy with Apraxia World", "author": "A Hair"}, {"title": "Prediction for retrospection: Integrating algorithmic stress prediction into personal informatics systems for college students' mental health", "author": "T Kim"}, {"title": "Effects of explanations in ai-assisted decision making: Principles and comparisons", "author": "X Wang"}, {"title": "Developing conversational agents for use in criminal investigations", "author": "S Hepenstal"}, {"title": "Selective explanations: Leveraging human input to align explainable ai", "author": "V Lai"}, {"title": "Transparent-AI blueprint: developing a conceptual tool to support the design of transparent AI agents", "author": "Z Zhou"}, {"title": "Explainable recommendations in intelligent systems: delivery methods, modalities and risks", "author": "M Naiseh"}, {"title": "What are the users' needs? Design of a user-centered explainable artificial intelligence diagnostic system", "author": "X He"}, {"title": "Should we trust (x) AI? Design dimensions for structured experimental evaluations", "author": "F Sperrle"}, {"title": "Co-design of human-centered, explainable AI for clinical decision support", "author": "C Panigutti"}, {"title": "Transparency as manipulation? Uncovering the disciplinary power of algorithmic transparency", "author": "H Wang"}, {"title": "Facets of algorithmic literacy: Information, experience, and individual factors predict attitudes toward algorithmic systems", "author": "DE Silva"}, {"title": "Im receptivity and presentation-type preferences among users of a mobile app with automated receptivity-status adjustment", "author": "TW Wu"}, {"title": "Justification of recommender systems results: a service-based approach", "author": "N Mauro"}, {"title": "Because I'm Restricted, 2\u20134 PM Unable to See Messages: Exploring Users' Perceptions and Likely Practices around Exposing Attention Management Use on IM \u2026", "author": "YL Chou"}, {"title": "To explain or not to explain: the effects of personal characteristics when explaining feature-based recommendations in different domains", "author": "M Millecamp"}, {"title": "From flat file to interface: Synthesis of personas and analytics for enhanced user understanding", "author": "BJ Jansen"}, {"title": "Enhancing the accessibility and interactions of regional hydrologic projections for water managers", "author": "KM Whitney"}, {"title": "Exploring the need for transparency in educational recommender systems", "author": "J Barria-Pineda"}, {"title": "Cogito ergo quid? the effect of cognitive style in a transparent mobile music recommender system", "author": "M Millecamp"}, {"title": "Interrogating the T in FAccT", "author": "E Corbett"}, {"title": "On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations", "author": "A Bertrand"}, {"title": "Making transparency clear", "author": "A Springer"}, {"title": "Irf: A framework for enabling users to interact with recommenders through dialogue", "author": "\u00d6 Alkan"}, {"title": "A Missing Piece in the Puzzle: Considering the Role of Task Complexity in Human-AI Decision Making", "author": "S Salimzadeh"}, {"title": "Classifeye: Classification of personal characteristics based on eye tracking data in a recommender system interface", "author": "M Millecamp"}, {"title": "User-Centered Design of a Machine Learning Dashboard for Prediction of Postoperative Complications", "author": "BA Fritz"}, {"title": "Farming Your ML-based Query Optimizer's Food", "author": "R Van De Water"}, {"title": "Enabling effective transparency: towards user-centric intelligent systems", "author": "A Springer"}, {"title": "Explainable AI And Visual Reasoning: Insights From Radiology", "author": "R Kaufman"}, {"title": "A cognitive work analysis approach to explainable artificial intelligence in non-expert financial decision-making", "author": "M Dikmen"}, {"title": "Safe Environments to Understand Medical AI-Designing a Diabetes Simulation Interface for Users of Automated Insulin Delivery", "author": "T Schrills"}, {"title": "Ethical issues in explanations of personalized recommender systems", "author": "E Jafari"}, {"title": "The Impact of System Transparency on Analytical Reasoning", "author": "S Hepenstal"}, {"title": "Development of a Concept for Privacy Explanations and its Prototypical Evaluation", "author": "JRC Droste"}, {"title": "Personalized remedial recommendations for SQL programming practice system", "author": "J Barria-Pineda"}, {"title": "Tangible Interaction with In-Car Smart Intelligence", "author": "M Abd El Ghani"}, {"title": "Inform, Explain, or Control: Techniques to Adjust End-User Performance Expectations for a Conversational Agent Facilitating Group Chat Discussions", "author": "H DO"}, {"title": "Explainable Persuasion for Persuasive Interfaces: The Case of Online Gambling", "author": "DA Cemiloglu"}, {"title": "Design Specifications for Bidirectional Feedback on Indoor Environmental Quality", "author": "E Alston-Stepnitz"}, {"title": "Towards Fair and Explainable AI using a Human-Centered AI Approach", "author": "B Ghai"}, {"title": "User Interface Design Based on Human-Centered XAI Methods", "author": "J Zhang"}, {"title": "Artificial Intelligence for Human Decision-Makers: Systematization, Perception, and Adoption of Intelligent Decision Support Systems in Industry 4.0", "author": "JP Wanner"}, {"title": "Accurate, Fair, and Explainable: Building Human-Centered AI", "author": "A Springer"}, {"title": "Explaining Data Driven Personas to End Users.", "author": "S Jung"}, {"title": "Improving Support for Contingency Planning in Air Traffic Flow Management", "author": "A Fernandes"}, {"title": "Personalized interfaces for music recommender systems", "author": "M Millecamp"}, {"title": "An Enhanced Autonomy Approach to Automated Trajectory Negotiation", "author": "G Hunter"}, {"title": "Personal Informatics Systems and the Integration of Data from Novel Sensor Technologies", "author": "C Dudley"}, {"title": "A Longitudinal Evaluation of Tablet-Based Child Speech Therapy with Apraxia World", "author": "KJ BALLARD"}, {"title": "How Transparency and Cognitive Reflection Shape Trust in Algorithms", "author": "V Chernoskutova"}, {"title": "Exploring design practices for explaining music recommendations", "author": "H Ryu"}, {"title": "Human\u2013Computer Interaction", "author": "RJ Branaghan"}, {"title": "Delegating Agency? The Effects of XAI, Personality Traits, and the Moral Significance of the Application on the Reliance on Autonomous Systems: A User Study", "author": "E Kallina"}, {"title": "Apraxia World: Deploying a Mobile Game and Automatic Speech Recognition for Independent Child Speech Therapy", "author": "A Hair"}, {"title": "Modifying Visual Explanations to Improve the User Understand-ability of Explainable Artificial Intelligence Systems", "author": "R Mohan"}, {"title": "Understanding each-other: Engineering challenges and opportunities for users and systems in the deep learning era", "author": "LD Spano"}, {"title": "Explaining music recommendations: the effects of user feedback on explanations.", "author": "JM Geelhoed"}, {"title": "Data-Driven Personas as Interfaces for Persona Analytics System", "author": "BJ Jansen"}], "what i cannot predict, i do not understand: a human-centered evaluation framework for explainability methods": [{"title": "Harmonizing the object recognition strategies of deep neural networks with humans", "author": "T Fel"}, {"title": "The role of human knowledge in explainable AI", "author": "A Tocchetti"}, {"title": "Craft: Concept recursive activation factorization for explainability", "author": "T Fel"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "MR Taesiri"}, {"title": "Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis", "author": "T Fel"}, {"title": "\" Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction", "author": "SSY Kim"}, {"title": "OCTET: Object-aware Counterfactual Explanations", "author": "M Zemni"}, {"title": "Xplique: A deep learning explainability toolbox", "author": "T Fel"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "G Nguyen"}, {"title": "Machine explanations and human understanding", "author": "C Chen"}, {"title": "Explain Any Concept: Segment Anything Meets Concept-Based Explanation", "author": "A Sun"}, {"title": "Constructing Natural Language Explanations via Saliency Map Verbalization", "author": "N Feldhus"}, {"title": "Graphical Perception of Saliency-based Model Explanations", "author": "Y Zhao"}, {"title": "Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods", "author": "N Feldhus"}, {"title": "A Human-Centric Assessment Framework for AI", "author": "S Saralajew"}, {"title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods", "author": "R Hesse"}, {"title": "Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization", "author": "T Fel"}, {"title": "The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers", "author": "M Nauta"}, {"title": "COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks", "author": "F Jourdan"}, {"title": "Interpreting and Correcting Medical Image Classification with PIP-Net", "author": "M Nauta"}, {"title": "Evaluating Quality of Visual Explanations of Deep Learning Models for Vision Tasks", "author": "Y Yang"}, {"title": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation", "author": "T Fel"}, {"title": "Gradient strikes back: How filtering out high frequencies improves explanations", "author": "S Muzellec"}, {"title": "Assessing Biases through Visual Contexts", "author": "A Arias-Duart"}, {"title": "Two-Stage Holistic and Contrastive Explanation of Image Classification", "author": "W Xie"}, {"title": "SSE: A Metric for Evaluating Search System Explainability", "author": "C Chen"}, {"title": "FACEE: Framework for Automating CNN Explainability Evaluation", "author": "A Rezaei"}, {"title": "CRAFT: explaining using Concepts from Recursive Activation FacTorization", "author": "FEL Thomas"}, {"title": "Explaining the Outcomes of Deep Learning Models", "author": "X Situ"}, {"title": "Aligning deep neural network strategies for object recognition with humans", "author": "T Serre"}], "hive: evaluating the human interpretability of visual explanations": [{"title": "What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods", "author": "J Colin"}, {"title": "Craft: Concept recursive activation factorization for explainability", "author": "T Fel"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "MR Taesiri"}, {"title": "Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis", "author": "T Fel"}, {"title": "\" Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction", "author": "SSY Kim"}, {"title": "Adversarial Counterfactual Visual Explanations", "author": "G Jeanneret"}, {"title": "Understanding the role of human intuition on reliance in human-AI decision-making with explanations", "author": "V Chen"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "G Nguyen"}, {"title": "Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability", "author": "VV Ramaswamy"}, {"title": "Machine explanations and human understanding", "author": "C Chen"}, {"title": "Overlooked factors in concept-based explanations: Dataset choice, concept salience, and human capability", "author": "VV Ramaswamy"}, {"title": "Exploring Evaluation Methods for Interpretable Machine Learning: A Survey", "author": "N Alangari"}, {"title": "PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification", "author": "M Nauta"}, {"title": "Concept-based explanations for out-of-distribution detectors", "author": "J Choi"}, {"title": "Clevr-x: A visual reasoning dataset for natural language explanations", "author": "L Salewski"}, {"title": "Analyzing effects of mixed sample data augmentation on model interpretability", "author": "S Won"}, {"title": "Do humans and machines have the same eyes? Human-machine perceptual differences on image classification", "author": "M Liu"}, {"title": "Rethinking AI Explainability and Plausibility", "author": "W Jin"}, {"title": "Humans, AI, and Context: Understanding End-Users' Trust in a Real-World Computer Vision Application", "author": "SSY Kim"}, {"title": "Graphical Perception of Saliency-based Model Explanations", "author": "Y Zhao"}, {"title": "Scale Alone Does not Improve Mechanistic Interpretability in Vision Models", "author": "RS Zimmermann"}, {"title": "ICICLE: Interpretable Class Incremental Continual Learning", "author": "D Rymarczyk"}, {"title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods", "author": "R Hesse"}, {"title": "Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization", "author": "T Fel"}, {"title": "UFO: A unified method for controlling Understandability and Faithfulness Objectives in concept-based explanations for CNNs", "author": "VV Ramaswamy"}, {"title": "The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers", "author": "M Nauta"}, {"title": "Take 5: Interpretable Image Classification with a Handful of Features", "author": "T Norrenbrock"}, {"title": "Interactive Personalization of Classifiers for Explainability using Multi-Objective Bayesian Optimization", "author": "S Chandramouli"}, {"title": "Is ProtoPNet Really Explainable? Evaluating and Improving the Interpretability of Prototypes", "author": "Q Huang"}, {"title": "Evaluating Quality of Visual Explanations of Deep Learning Models for Vision Tasks", "author": "Y Yang"}, {"title": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation", "author": "T Fel"}, {"title": "Gradient strikes back: How filtering out high frequencies improves explanations", "author": "S Muzellec"}, {"title": "Towards a Praxis for Intercultural Ethics in Explainable AI", "author": "CT Okolo"}, {"title": "Towards Self-Explainability of Deep Neural Networks with Heatmap Captioning and Large-Language Models", "author": "O Tursun"}, {"title": "A non-global disturbance targeted adversarial example algorithm combined with C&W and Grad-Cam", "author": "Y Zhu"}, {"title": "Towards Explainable Navigation and Recounting", "author": "S Poppi"}, {"title": "Eye into AI: Evaluating the Interpretability of Explainable AI Techniques through a Game With a Purpose", "author": "K MORRISON"}, {"title": "CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations", "author": "Z Akata"}, {"title": "What I cannot predict, I do not understand: A human-centered evaluation framework for explainability methods", "author": "R Cadene"}, {"title": "Strategic Adversarial Attacks in AI-assisted Decision Making to Reduce Human Trust and Reliance", "author": "Z Lu"}, {"title": "Evaluation and Improvement of Interpretability for Self-Explainable Part-Prototype Networks", "author": "Q Huang"}, {"title": "L'IA explicable appliqu\u00e9e \u00e0 la d\u00e9tection de ceintures et de t\u00e9l\u00e9phones au volant", "author": "M Gornet"}], "how useful are the machine-generated interpretations to general users? a human evaluation on guessing the incorrectly predicted labels": [{"title": "Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.", "author": "P Hemmer"}, {"title": "Debugging tests for model explanations", "author": "J Adebayo"}, {"title": "XAI Systems Evaluation: A Review of Human and Computer-Centred Methods", "author": "P Lopes"}, {"title": "Post hoc explanations may be ineffective for detecting unknown spurious correlation", "author": "J Adebayo"}, {"title": "HIVE: Evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "author": "G Nguyen"}, {"title": "What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods", "author": "J Colin"}, {"title": "Craft: Concept recursive activation factorization for explainability", "author": "T Fel"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "MR Taesiri"}, {"title": "\" Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction", "author": "SSY Kim"}, {"title": "How transparency modulates trust in artificial intelligence", "author": "J Zerilli"}, {"title": "The role of human knowledge in explainable AI", "author": "A Tocchetti"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "G Nguyen"}, {"title": "Do users benefit from interpretable vision? a user study, baseline, and dataset", "author": "L Sixt"}, {"title": "Do humans and machines have the same eyes? Human-machine perceptual differences on image classification", "author": "M Liu"}, {"title": "Rethinking AI Explainability and Plausibility", "author": "W Jin"}, {"title": "Are Shortest Rationales the Best Explanations for Human Understanding?", "author": "H Shen"}, {"title": "Human-Centric Research for NLP: Towards a Definition and Guiding Questions", "author": "B Kotnis"}, {"title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods", "author": "R Hesse"}, {"title": "Design decision framework for ai explanations", "author": "O Anuyah"}, {"title": "COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks", "author": "F Jourdan"}, {"title": "From Explanation to Action: An End-to-End Human-in-the-loop Framework for Anomaly Reasoning and Management", "author": "X Ding"}, {"title": "FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis", "author": "Y He"}, {"title": "Interpreting and Correcting Medical Image Classification with PIP-Net", "author": "M Nauta"}, {"title": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation", "author": "T Fel"}, {"title": "ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing", "author": "H Shen"}, {"title": "Responsibility: An Example-based Explainable AI approach via Training Process Inspection", "author": "F Khadivpour"}, {"title": "Two4two: Evaluating interpretable machine learning-a synthetic dataset for controlled experiments", "author": "M Schuessler"}, {"title": "Invisible Users: Uncovering End-Users' Requirements for Explainable AI via Explanation Forms and Goals", "author": "W Jin"}, {"title": "Interpretability Through Invertibility: A Deep Convolutional Network With Ideal Counterfactuals And Isosurfaces", "author": "L Sixt"}, {"title": "Towards Effective Tools for Debugging Machine Learning Models", "author": "J Adebayo"}, {"title": "Crowdsourcing Human Oversight on Image Tagging Algorithms: An initial study of image diversity", "author": "K Kyriakou"}, {"title": "What I cannot predict, I do not understand: A human-centered evaluation framework for explainability methods", "author": "R Cadene"}, {"title": "Explaining the Road Not Taken", "author": "H Shen"}], "visual, textual or hybrid: the effect of user expertise on different explanations": [{"title": "How cognitive biases affect XAI-assisted decision-making: A systematic review", "author": "A Bertrand"}, {"title": "Towards a science of human-ai decision making: a survey of empirical studies", "author": "V Lai"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Designing for responsible trust in AI systems: A communication perspective", "author": "QV Liao"}, {"title": "Explainable Artificial Intelligence (XAI) from a user perspective: A synthesis of prior literature and problematizing avenues for future research", "author": "AKMB Haque"}, {"title": "Contextualization and exploration of local feature importance explanations to improve understanding and satisfaction of non-expert users", "author": "C Bove"}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Trust", "author": "J Ooge"}, {"title": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies", "author": "V Lai"}, {"title": "Is more always better? The effects of personal characteristics and level of detail on the perception of explanations in a recommender system", "author": "MA Chatti"}, {"title": "Better together? an evaluation of ai-supported code translation", "author": "JD Weisz"}, {"title": "Prediction for retrospection: Integrating algorithmic stress prediction into personal informatics systems for college students' mental health", "author": "T Kim"}, {"title": "Explaining user models with different levels of detail for transparent recommendation: A user study", "author": "M Guesmi"}, {"title": "On the Impact of Explanations on Understanding of Algorithmic Decision-Making", "author": "T Schmude"}, {"title": "Toward involving end-users in interactive human-in-the-loop AI fairness", "author": "Y Nakao"}, {"title": "Contextualizing User Perceptions about Biases for Human-Centered Explainable Artificial Intelligence", "author": "CW Yuan"}, {"title": "An objective metric for explainable AI: how and why to estimate the degree of explainability", "author": "F Sovrano"}, {"title": "On the Influence of Cognitive Styles on Users' Understanding of Explanations", "author": "L Riefle"}, {"title": "How should the results of artificial intelligence be explained to users?-Research on consumer preferences in user-centered explainable artificial intelligence", "author": "D Kim"}, {"title": "Understanding the Role of Explanation Modality in AI-assisted Decision-making", "author": "V Robbemond"}, {"title": "Exploring the role of local and global explanations in recommender systems", "author": "M Radensky"}, {"title": "Assessing explainability in reinforcement learning", "author": "AE Zelvelder"}, {"title": "The Effect of Explanation Design on User Perception of Smart Home Lighting Systems: A Mixed-method Investigation", "author": "J Dai"}, {"title": "NOTION OF EXPLAINABLE ARTIFICIAL INTELLIGENCE-AN EMPIRICAL INVESTIGATION FROM A USER'S PERSPECTIVE", "author": "BAKM Haque"}, {"title": "Constructing Natural Language Explanations via Saliency Map Verbalization", "author": "N Feldhus"}, {"title": "What if Interactive Explanation in a Scientific Literature Recommender System", "author": "M Guesmi"}, {"title": "Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK", "author": "L Nannini"}, {"title": "On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations", "author": "A Bertrand"}, {"title": "Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods", "author": "N Feldhus"}, {"title": "Questioning the ability of feature-based explanations to empower non-experts in robo-advised financial decision-making", "author": "A Bertrand"}, {"title": "Predicting the need for XAI from high-granularity interaction data", "author": "VF de Santana"}, {"title": "Follow the Successful Herd: Towards Explanations for Improved Use and Mental Models of Natural Language Systems", "author": "M Brachman"}, {"title": "Directive Explanations for Monitoring the Risk of Diabetes Onset: Introducing Directive Data-Centric Explanations and Combinations to Support What-If Explorations", "author": "A Bhattacharya"}, {"title": "A diary study of social explanations for recommendations in daily life", "author": "Z Zhang"}, {"title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications", "author": "M Nourani"}, {"title": "A Missing Piece in the Puzzle: Considering the Role of Task Complexity in Human-AI Decision Making", "author": "S Salimzadeh"}, {"title": "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap", "author": "QV Liao"}, {"title": "Seamful XAI: Operationalizing Seamful Design in Explainable AI", "author": "U Ehsan"}, {"title": "Service-based Presentation of Multimodal Information for the Justification of Recommender Systems Results", "author": "ZF Hu"}, {"title": "The Impact of Imperfect XAI on Human-AI Decision-Making", "author": "K Morrison"}, {"title": "How do Users Experience Traceability of AI Systems? Examining Subjective Information Processing Awareness in Automated Insulin Delivery (AID) Systems", "author": "T Schrills"}, {"title": "Justification vs. Transparency: Why and How Visual Explanations in a Scientific Literature Recommender System", "author": "M Guesmi"}, {"title": "Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System", "author": "M Guesmi"}, {"title": "On the Perception of Difficulty: Differences between Humans and AI", "author": "P Spitzer"}, {"title": "Safe Environments to Understand Medical AI-Designing a Diabetes Simulation Interface for Users of Automated Insulin Delivery", "author": "T Schrills"}, {"title": "Applying Interdisciplinary Frameworks to Understand Algorithmic Decision-Making", "author": "T Schmude"}, {"title": "Unjustified Sample Sizes and Generalizations in Explainable AI Research: Principles for More Inclusive User Studies", "author": "U Peters"}, {"title": "Please take over: XAI, delegation of authority, and domain knowledge", "author": "K Bauer"}, {"title": "Explaining Health Recommendations to Lay Users: The Dos and Dont's.", "author": "M Szymanski"}, {"title": "Workshop on Understanding and Mitigating Cognitive Biases in Human-AI Collaboration", "author": "N Boonprakong"}, {"title": "A Means to what End? Evaluating the Explainability of Software Systems using Goal-Oriented Heuristics", "author": "H Deters"}, {"title": "Designing a formal dialogue system with personalized explanations for XAI", "author": "V Buisman"}, {"title": "Exploring the Effects of Human-Centered AI Explanations on Trust and Reliance", "author": "N Scharowski"}, {"title": "Designing AI for Appropriation Will Calibrate Trust", "author": "ZT ZHANG"}, {"title": "How to explain: from theory to practice", "author": "F Sovrano"}, {"title": "Notion of Explainable Artificial Intelligence-An Empirical Investigation from A User's Perspective", "author": "P Mikalef"}, {"title": "Scenario-based design: Exploring the effect of explainability on user's adoption intention of smart home lighting systems", "author": "J Dai"}, {"title": "Criteria and Metrics for the Explainability of Software", "author": "H Deters"}, {"title": "DiaryMate: Exploring the Roles of Large Language Models in Facilitating AI-mediated Journaling", "author": "T Kim"}, {"title": "Explaining Recommendations in E-Learning: Effects on Adolescents' Initial Trust", "author": "J OOGE"}], "regularizing black-box models for improved interpretability": [], "visual correspondence-based explanations improve ai robustness and human-ai team accuracy": [], "beyond individualized recourse: interpretable and interactive summaries of actionable recourses": [{"title": "Counterfactual explanations and algorithmic recourses for machine learning: A review", "author": "S Verma"}, {"title": "Counterfactual explanations and how to find them: literature review and benchmarking", "author": "R Guidotti"}, {"title": "Deep neural networks and tabular data: A survey", "author": "V Borisov"}, {"title": "From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai", "author": "M Nauta"}, {"title": "A survey of algorithmic recourse: contrastive explanations and consequential recommendations", "author": "AH Karimi"}, {"title": "Counterfactuals and causability in explainable artificial intelligence: Theory, algorithms, and applications", "author": "YL Chou"}, {"title": "A survey of algorithmic recourse: definitions, formulations, solutions, and prospects", "author": "AH Karimi"}, {"title": "Four principles of explainable artificial intelligence", "author": "PJ Phillips"}, {"title": "Carla: a python library to benchmark algorithmic recourse and counterfactual explanation algorithms", "author": "M Pawelczyk"}, {"title": "Towards robust and reliable algorithmic recourse", "author": "S Upadhyay"}, {"title": "Exploring counterfactual explanations through the lens of adversarial examples: A theoretical and empirical analysis", "author": "M Pawelczyk"}, {"title": "On the privacy risks of algorithmic recourse", "author": "M Pawelczyk"}, {"title": "Interpretable machine learning: Moving from mythos to diagnostics", "author": "V Chen"}, {"title": "Counterfactual explanations in sequential decision making under uncertainty", "author": "S Tsirtsis"}, {"title": "Counterfactual shapley additive explanations", "author": "E Albini"}, {"title": "Counterfactual explanations for models of code", "author": "J Cito"}, {"title": "Counterfactual explanation trees: Transparent and consistent actionable recourse with decision trees", "author": "K Kanamori"}, {"title": "Probabilistically robust recourse: Navigating the trade-offs between costs and robustness in algorithmic recourse", "author": "M Pawelczyk"}, {"title": "On the Trade-Off between actionable explanations and the right to be forgotten", "author": "M Pawelczyk"}, {"title": "The roles and modes of human interactions with automated machine learning systems", "author": "TT Khuat"}, {"title": "Bayesian persuasion for algorithmic recourse", "author": "K Harris"}, {"title": "Fuzzy Rule-based Local Surrogate Models for Black-box Model Explanation", "author": "X Zhu"}, {"title": "GAM Coach: Towards Interactive and User-centered Algorithmic Recourse", "author": "ZJ Wang"}, {"title": "Counterfactual plans under distributional ambiguity", "author": "N Bui"}, {"title": "Global Counterfactual Explainer for Graph Neural Networks", "author": "Z Huang"}, {"title": "Density-based reliable and robust explainer for counterfactual explanation", "author": "S Zhang"}, {"title": "Global counterfactual explainer for graph neural networks", "author": "M Kosan"}, {"title": "Achieving Diversity in Counterfactual Explanations: a Review and Discussion", "author": "T Laugel"}, {"title": "It takes two flints to make a fire: Multitask learning of neural relation and explanation classifiers", "author": "Z Tang"}, {"title": "Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten", "author": "S Krishna"}, {"title": "Bayesian Hierarchical Models for Counterfactual Estimation", "author": "N Raman"}, {"title": "Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals", "author": "G Warren"}, {"title": "Explainable AI and Causal Understanding: Counterfactual Approaches Considered", "author": "S Baron"}, {"title": "On the Impact of Algorithmic Recourse on Social Segregation", "author": "R Gao"}, {"title": "Reason to explain: Interactive contrastive explanations (REASONX)", "author": "L State"}, {"title": "Rationalizing predictions by adversarial information calibration", "author": "L Sha"}, {"title": "Framing Algorithmic Recourse for Anomaly Detection", "author": "D Datta"}, {"title": "Global counterfactual explanations: investigations, implementations and improvements", "author": "D Ley"}, {"title": "Decomposing Counterfactual Explanations for Consequential Decision Making", "author": "M Pawelczyk"}, {"title": "Counterfactual Explanation with Missing Values", "author": "K Kanamori"}, {"title": "Fairness Aware Counterfactuals for Subgroups", "author": "L Kavouras"}, {"title": "\" How to make them stay?\"--Diverse Counterfactual Explanations of Employee Attrition", "author": "A Artelt"}, {"title": "Multi-agent algorithmic recourse", "author": "A O'Brien"}, {"title": "Distributionally robust recourse action", "author": "D Nguyen"}, {"title": "Rethinking Counterfactual Explanations as Local and Regional Counterfactual Policies", "author": "SI Amoukou"}, {"title": "GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations", "author": "D Ley"}, {"title": "Considerations when learning additive explanations for black-box models", "author": "S Tan"}, {"title": "Benchmarking Counterfactual Algorithms for XAI: From White Box to Black Box", "author": "C Moreira"}, {"title": "Everything is relative: Understanding fairness with optimal transport", "author": "K Kwegyir-Aggrey"}, {"title": "On the Impact of Adversarially Robust Models on Algorithmic Recourse", "author": "S Krishna"}, {"title": "Counterfactuals for Interpretable Machine Learning: Model Reasoning from \u201cWhat\u201d to \u201cHow\u201d", "author": "F Yang"}, {"title": "Global Counterfactual Explanations Are Reliable Or Efficient, But Not Both", "author": "D Ley"}, {"title": "Techniques for Interpretability and Transparency of Black-Box Models", "author": "Y Zhou"}, {"title": "Actionable Recourse Guided by User Preference", "author": "J Yetukuri"}, {"title": "Interpretable Natural Language Processing with Applications to Information Extraction", "author": "Z Tang"}, {"title": "A Framework for Automated Discovery and Analysis of Suspicious Trade Records", "author": "D Datta"}, {"title": "Toward Multi-Agent Algorithmic Recourse: Challenges From a Game-Theoretic Perspective", "author": "A O'Brien"}, {"title": "Studies on Explainable Machine Learning Based on Integer Linear Optimization", "author": "\u91d1\u68ee\u61b2\u592a\u6717"}, {"title": "Personalized Algorithmic Recourse with Preference Elicitation", "author": "G De Toni"}], "the effectiveness of feature attribution methods and its correlation with automatic evaluation scores": [{"title": "Post hoc explanations may be ineffective for detecting unknown spurious correlation", "author": "J Adebayo"}, {"title": "HIVE: Evaluating the human interpretability of visual explanations", "author": "SSY Kim"}, {"title": "What i cannot predict, i do not understand: A human-centered evaluation framework for explainability methods", "author": "J Colin"}, {"title": "Craft: Concept recursive activation factorization for explainability", "author": "T Fel"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "MR Taesiri"}, {"title": "Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis", "author": "T Fel"}, {"title": "\" Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction", "author": "SSY Kim"}, {"title": "OCTET: Object-aware Counterfactual Explanations", "author": "M Zemni"}, {"title": "A multi-institutional study using artificial intelligence to provide reliable and fair feedback to surgeons", "author": "D Kiyasseh"}, {"title": "Explaining latent representations with a corpus of examples", "author": "J Crabb\u00e9"}, {"title": "A meta-analysis of the utility of explainable artificial intelligence in human-AI decision-making", "author": "M Schemmer"}, {"title": "On the effect of information asymmetry in human-AI teams", "author": "P Hemmer"}, {"title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "author": "G Nguyen"}, {"title": "Machine explanations and human understanding", "author": "C Chen"}, {"title": "Explainable Artificial Intelligence improves human decision-making: Results from a mushroom picking experiment at a public art festival", "author": "B Leichtmann"}, {"title": "Uncovering expression signatures of synergistic drug responses via ensembles of explainable machine-learning models", "author": "JD Janizek"}, {"title": "Assessing out-of-domain language model performance from few examples", "author": "P Singhal"}, {"title": "Analyzing effects of mixed sample data augmentation on model interpretability", "author": "S Won"}, {"title": "Do humans and machines have the same eyes? Human-machine perceptual differences on image classification", "author": "M Liu"}, {"title": "Rethinking AI Explainability and Plausibility", "author": "W Jin"}, {"title": "Humans, AI, and Context: Understanding End-Users' Trust in a Real-World Computer Vision Application", "author": "SSY Kim"}, {"title": "Graphical Perception of Saliency-based Model Explanations", "author": "Y Zhao"}, {"title": "From heatmaps to structured explanations of image classifiers", "author": "L Fuxin"}, {"title": "SIGN: Statistical Inference Graphs based on probabilistic Network activity interpretation", "author": "Y Konforti"}, {"title": "Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction", "author": "P Hemmer"}, {"title": "Learning Human-Compatible Representations for Case-Based Decision Support", "author": "H Liu"}, {"title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods", "author": "R Hesse"}, {"title": "Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization", "author": "T Fel"}, {"title": "Neighboring Words Affect Human Interpretation of Saliency Explanations", "author": "A Jacovi"}, {"title": "The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers", "author": "M Nauta"}, {"title": "A Dual-Perspective Approach to Evaluating Feature Attribution Methods", "author": "Y Li"}, {"title": "When and how to fool explainable models (and humans) with adversarial examples", "author": "J Vadillo"}, {"title": "The Impact of Imperfect XAI on Human-AI Decision-Making", "author": "K Morrison"}, {"title": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation", "author": "T Fel"}, {"title": "Gradient strikes back: How filtering out high frequencies improves explanations", "author": "S Muzellec"}, {"title": "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning", "author": "Y Yang"}, {"title": "Overcoming Catastrophic Forgetting by XAI", "author": "G Nguyen"}, {"title": "Interpretable deep learning models for better clinician-AI communication in clinical mammography", "author": "AJ Barnett"}, {"title": "Invisible Users: Uncovering End-Users' Requirements for Explainable AI via Explanation Forms and Goals", "author": "W Jin"}, {"title": "Soundness and Completeness: An Algorithmic Perspective on Evaluation of Feature Attribution", "author": "Y Li"}, {"title": "A user interface to communicate interpretable AI decisions to radiologists", "author": "YJ Ou"}, {"title": "Human-Centered Explainability Attributes In Ai-Powered Eco-Driving: Understanding Truck Drivers' Perspective", "author": "E Gjona"}, {"title": "Techniques for Interpretability and Transparency of Black-Box Models", "author": "Y Zhou"}, {"title": "What I cannot predict, I do not understand: A human-centered evaluation framework for explainability methods", "author": "R Cadene"}, {"title": "Broadening the Horizon of Adversarial Attacks in Deep Learning", "author": "J Vadillo Jueguen"}, {"title": "\u0418\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u0430\u044f \u043f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043a\u043b\u0438\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u043f\u0440\u0438 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445 \u0447\u0438\u0441\u043b\u0430 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u043e\u0432", "author": "\u0410\u0421 \u0412\u0430\u0442\u044c\u044f\u043d"}], "deciding fast and slow: the role of cognitive biases in ai-assisted decision-making": [{"title": "How cognitive biases affect XAI-assisted decision-making: A systematic review", "author": "A Bertrand"}, {"title": "Human-centered explainable ai (xai): From algorithms to user experiences", "author": "QV Liao"}, {"title": "Who goes first? Influences of human-AI workflow on decision making in clinical imaging", "author": "R Fogliato"}, {"title": "Role of human-AI interaction in selective prediction", "author": "E Bondi"}, {"title": "Rams, hounds and white boxes: Investigating human\u2013AI collaboration protocols in medical diagnosis", "author": "F Cabitza"}, {"title": "Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making", "author": "S Ma"}, {"title": "Responsible Artificial Intelligence in Human Resources Technology: An innovative inclusive and fair by design matching algorithm for job recruitment \u2026", "author": "S Delecraz"}, {"title": "AI-assisted decision-making: A cognitive modeling approach to infer latent reliance strategies", "author": "H Tejeda"}, {"title": "Debiasing evaluations that are biased by evaluations", "author": "J Wang"}, {"title": "The effects of explanations on automation bias", "author": "M Vered"}, {"title": "A unifying framework for combining complementary strengths of humans and ML toward better predictive decision-making", "author": "C Rastogi"}, {"title": "Modeling, Replicating, and Predicting Human Behavior: A Survey", "author": "A Fuchs"}, {"title": "Mitigating knowledge imbalance in AI-advised decision-making through collaborative user involvement", "author": "C Gomez"}, {"title": "\u201cIf I Had All the Time in the World\u201d: Ophthalmologists' Perceptions of Anchoring Bias Mitigation in Clinical AI Support", "author": "AKP Bach"}, {"title": "Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making", "author": "K Inkpen"}, {"title": "Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task", "author": "C Xu"}, {"title": "Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction", "author": "P Hemmer"}, {"title": "Making recruitment more inclusive: unfairness monitoring with a job matching machine-learning algorithm", "author": "S Delecraz"}, {"title": "Unconscious Other's Impression Changer: A Method to Manipulate Cognitive Biases That Subtly Change Others' Impressions Positively/Negatively by Making AI Bias \u2026", "author": "K Futami"}, {"title": "Modeling Human Behavior Part II--Cognitive approaches and Uncertainty", "author": "A Fuchs"}, {"title": "Learning to Guide Human Experts via Personalized Large Language Models", "author": "D Banerjee"}, {"title": "Value-based Fast and Slow AI Nudging", "author": "MB Ganapini"}, {"title": "Artificial Intelligence-Assisted Decision-Making Method for Legal Judgment Based on Deep Neural Network", "author": "W Ma"}, {"title": "The Inherited Bias Effect: the propagation of artificial intelligence biases to human decisions", "author": "L Vicente"}, {"title": "Workshop on Understanding and Mitigating Cognitive Biases in Human-AI Collaboration", "author": "N Boonprakong"}, {"title": "Prediction, human decision and liability rules, CRED Working paper No 2022-06", "author": "M Obidzinski"}, {"title": "Data and Humans in Algorithmic Risk Assessment", "author": "R Fogliato"}, {"title": "Framing Fashion: Human-machine learning and the Amazon Echo Look", "author": "HA Horst"}, {"title": "Eyes are the Windows to AI Reliance: Toward Real-Time Human-AI Reliance Assessment", "author": "S CAO"}, {"title": "K-pop and fake facts: from texts to smart alerting for maritime security", "author": "M Prieur"}, {"title": "Translating AI to Impact: Uncertainty and Human-Agent Interactions in Multi-Agent Systems for Public Health and Conservation", "author": "EC Bondi-Kelly"}, {"title": "AI in the Cath Lab: Implications of Clinical AI-Enabled Assistance for Intravascular Ultrasound Procedures", "author": "M Borris"}, {"title": "AI-Assisted Decision-Making: A Cognitive Modeling Approach to Infer Latent Reliance Strategies", "author": "HT Lemus"}, {"title": "A dvisory algorithms and liability rules", "author": "M Obidzinski"}, {"title": "KI im Einsatz: Interdisziplin\u00e4re Herausforderungen im Projekt UAV-Rescue", "author": "M Philippi"}]}